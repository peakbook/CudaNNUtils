//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-19856038
// Cuda compilation tools, release 7.5, V7.5.17
// Based on LLVM 3.4svn
//

.version 4.3
.target sm_35
.address_size 64

	// .weak	cudaMalloc
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.weak .func  (.param .b32 func_retval0) cudaMalloc(
	.param .b64 cudaMalloc_param_0,
	.param .b64 cudaMalloc_param_1
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaFuncGetAttributes
.weak .func  (.param .b32 func_retval0) cudaFuncGetAttributes(
	.param .b64 cudaFuncGetAttributes_param_0,
	.param .b64 cudaFuncGetAttributes_param_1
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaDeviceGetAttribute
.weak .func  (.param .b32 func_retval0) cudaDeviceGetAttribute(
	.param .b64 cudaDeviceGetAttribute_param_0,
	.param .b32 cudaDeviceGetAttribute_param_1,
	.param .b32 cudaDeviceGetAttribute_param_2
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaGetDevice
.weak .func  (.param .b32 func_retval0) cudaGetDevice(
	.param .b64 cudaGetDevice_param_0
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaOccupancyMaxActiveBlocksPerMultiprocessor
.weak .func  (.param .b32 func_retval0) cudaOccupancyMaxActiveBlocksPerMultiprocessor(
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_0,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_1,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_2,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_3
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
.weak .func  (.param .b32 func_retval0) cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_0,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_1,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_2,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_3,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_4
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .globl	_Z13quaternionarg7double4
.visible .func  (.param .align 16 .b8 func_retval0[32]) _Z13quaternionarg7double4(
	.param .align 16 .b8 _Z13quaternionarg7double4_param_0[32]
)
{
	.local .align 4 .b8 	__local_depot6[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<189>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<346>;
	.reg .f64 	%fd<1950>;
	.reg .b64 	%rd<98>;


	mov.u64 	%rd97, __local_depot6;
	cvta.local.u64 	%SP, %rd97;
	ld.param.f64 	%fd4, [_Z13quaternionarg7double4_param_0+24];
	ld.param.f64 	%fd3, [_Z13quaternionarg7double4_param_0+16];
	ld.param.f64 	%fd1, [_Z13quaternionarg7double4_param_0];
	ld.param.f64 	%fd2, [_Z13quaternionarg7double4_param_0+8];
	add.u64 	%rd25, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd25;
	mul.f64 	%fd265, %fd2, %fd2;
	fma.rn.f64 	%fd266, %fd1, %fd1, %fd265;
	fma.rn.f64 	%fd267, %fd3, %fd3, %fd266;
	fma.rn.f64 	%fd268, %fd4, %fd4, %fd267;
	sqrt.rn.f64 	%fd5, %fd268;
	setp.eq.f64	%p1, %fd5, 0d0000000000000000;
	mov.f64 	%fd1949, 0d0000000000000000;
	mov.f64 	%fd1948, %fd1949;
	mov.f64 	%fd1947, %fd1949;
	mov.f64 	%fd1946, %fd1949;
	@%p1 bra 	BB6_173;

	div.rn.f64 	%fd6, %fd1, %fd5;
	div.rn.f64 	%fd7, %fd3, %fd5;
	div.rn.f64 	%fd8, %fd2, %fd5;
	mul.f64 	%fd270, %fd8, %fd7;
	div.rn.f64 	%fd9, %fd4, %fd5;
	mul.f64 	%fd271, %fd6, %fd9;
	sub.f64 	%fd272, %fd270, %fd271;
	add.f64 	%fd10, %fd272, %fd272;
	setp.gt.f64	%p2, %fd10, 0d3FF0000000000000;
	mov.f64 	%fd1842, 0d3FF0000000000000;
	@%p2 bra 	BB6_3;

	setp.lt.f64	%p3, %fd10, 0dBFF0000000000000;
	selp.f64	%fd1842, 0dBFF0000000000000, %fd10, %p3;

BB6_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd1842;
	}
	mov.b32 	 %f1, %r1;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p4, %f2, 0f3FE26666;
	@%p4 bra 	BB6_5;
	bra.uni 	BB6_4;

BB6_5:
	mul.f64 	%fd323, %fd1842, %fd1842;
	mov.f64 	%fd324, 0dBFB3823B180754AF;
	mov.f64 	%fd325, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd326, %fd325, %fd323, %fd324;
	mov.f64 	%fd327, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd328, %fd326, %fd323, %fd327;
	mov.f64 	%fd329, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd330, %fd328, %fd323, %fd329;
	mov.f64 	%fd331, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd332, %fd330, %fd323, %fd331;
	mov.f64 	%fd333, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd334, %fd332, %fd323, %fd333;
	mov.f64 	%fd335, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd336, %fd334, %fd323, %fd335;
	mov.f64 	%fd337, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd338, %fd336, %fd323, %fd337;
	mov.f64 	%fd339, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd340, %fd338, %fd323, %fd339;
	mov.f64 	%fd341, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd342, %fd340, %fd323, %fd341;
	mov.f64 	%fd343, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd344, %fd342, %fd323, %fd343;
	mov.f64 	%fd345, 0d3FB333333320F91B;
	fma.rn.f64 	%fd346, %fd344, %fd323, %fd345;
	mov.f64 	%fd347, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd348, %fd346, %fd323, %fd347;
	mul.f64 	%fd349, %fd323, %fd348;
	fma.rn.f64 	%fd1843, %fd349, %fd1842, %fd1842;
	bra.uni 	BB6_6;

BB6_4:
	abs.f64 	%fd275, %fd1842;
	mov.f64 	%fd276, 0d3FE0000000000000;
	mov.f64 	%fd277, 0dBFE0000000000000;
	fma.rn.f64 	%fd274, %fd277, %fd275, %fd276;
	// inline asm
	rsqrt.approx.ftz.f64 %fd273, %fd274;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r80, %temp}, %fd273;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r81}, %fd273;
	}
	add.s32 	%r82, %r81, -1048576;
	mov.b64 	%fd278, {%r80, %r82};
	mul.f64 	%fd279, %fd274, %fd273;
	neg.f64 	%fd280, %fd279;
	fma.rn.f64 	%fd281, %fd279, %fd280, %fd274;
	fma.rn.f64 	%fd282, %fd281, %fd278, %fd279;
	neg.f64 	%fd283, %fd282;
	mov.f64 	%fd284, 0d3FF0000000000000;
	fma.rn.f64 	%fd285, %fd273, %fd283, %fd284;
	fma.rn.f64 	%fd286, %fd285, %fd278, %fd278;
	fma.rn.f64 	%fd287, %fd282, %fd283, %fd274;
	fma.rn.f64 	%fd288, %fd287, %fd286, %fd282;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r83}, %fd274;
	}
	setp.lt.s32	%p5, %r83, 0;
	selp.f64	%fd289, 0dFFF8000000000000, %fd288, %p5;
	setp.equ.f64	%p6, %fd274, 0d0000000000000000;
	selp.f64	%fd290, %fd274, %fd289, %p6;
	mov.f64 	%fd291, 0dBFB3823B180754AF;
	mov.f64 	%fd292, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd293, %fd292, %fd274, %fd291;
	mov.f64 	%fd294, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd295, %fd293, %fd274, %fd294;
	mov.f64 	%fd296, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd297, %fd295, %fd274, %fd296;
	mov.f64 	%fd298, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd299, %fd297, %fd274, %fd298;
	mov.f64 	%fd300, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd301, %fd299, %fd274, %fd300;
	mov.f64 	%fd302, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd303, %fd301, %fd274, %fd302;
	mov.f64 	%fd304, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd305, %fd303, %fd274, %fd304;
	mov.f64 	%fd306, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd307, %fd305, %fd274, %fd306;
	mov.f64 	%fd308, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd309, %fd307, %fd274, %fd308;
	mov.f64 	%fd310, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd311, %fd309, %fd274, %fd310;
	mov.f64 	%fd312, 0d3FB333333320F91B;
	fma.rn.f64 	%fd313, %fd311, %fd274, %fd312;
	mov.f64 	%fd314, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd315, %fd313, %fd274, %fd314;
	mul.f64 	%fd316, %fd274, %fd315;
	mul.f64 	%fd317, %fd290, 0dC000000000000000;
	mov.f64 	%fd318, 0d3C91A62633145C07;
	fma.rn.f64 	%fd319, %fd317, %fd316, %fd318;
	add.f64 	%fd320, %fd317, 0d3FE921FB54442D18;
	add.f64 	%fd321, %fd320, %fd319;
	add.f64 	%fd322, %fd321, 0d3FE921FB54442D18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r84, %temp}, %fd322;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd322;
	}
	and.b32  	%r86, %r1, -2147483648;
	or.b32  	%r87, %r85, %r86;
	mov.b64 	%fd1843, {%r84, %r87};

BB6_6:
	mul.f64 	%fd16, %fd1843, 0dBFE0000000000000;
	setp.neu.f64	%p7, %fd16, 0d3FE921FB54442D18;
	setp.neu.f64	%p8, %fd16, 0dBFE921FB54442D18;
	and.pred  	%p9, %p7, %p8;
	mul.f64 	%fd350, %fd8, %fd8;
	mul.f64 	%fd17, %fd6, %fd6;
	sub.f64 	%fd18, %fd17, %fd350;
	@%p9 bra 	BB6_12;
	bra.uni 	BB6_7;

BB6_12:
	fma.rn.f64 	%fd415, %fd7, %fd7, %fd18;
	mul.f64 	%fd24, %fd9, %fd9;
	sub.f64 	%fd416, %fd415, %fd24;
	mul.f64 	%fd417, %fd6, %fd8;
	fma.rn.f64 	%fd418, %fd6, %fd8, %fd417;
	fma.rn.f64 	%fd419, %fd9, %fd7, %fd418;
	fma.rn.f64 	%fd420, %fd7, %fd9, %fd419;
	abs.f64 	%fd25, %fd416;
	abs.f64 	%fd26, %fd420;
	setp.eq.f64	%p21, %fd25, 0d0000000000000000;
	setp.eq.f64	%p22, %fd26, 0d0000000000000000;
	and.pred  	%p23, %p21, %p22;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd416;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r98}, %fd420;
	}
	and.b32  	%r5, %r98, -2147483648;
	@%p23 bra 	BB6_16;
	bra.uni 	BB6_13;

BB6_16:
	setp.lt.s32	%p31, %r4, 0;
	selp.f64	%fd473, 0d400921FB54442D18, 0d0000000000000000, %p31;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r105, %temp}, %fd473;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r106}, %fd473;
	}
	or.b32  	%r107, %r106, %r5;
	mov.b64 	%fd1844, {%r105, %r107};
	bra.uni 	BB6_17;

BB6_7:
	mul.f64 	%fd351, %fd7, %fd7;
	sub.f64 	%fd352, %fd18, %fd351;
	fma.rn.f64 	%fd353, %fd9, %fd9, %fd352;
	mul.f64 	%fd354, %fd6, %fd7;
	mul.f64 	%fd355, %fd8, %fd9;
	sub.f64 	%fd356, %fd354, %fd355;
	fma.rn.f64 	%fd357, %fd6, %fd7, %fd356;
	sub.f64 	%fd358, %fd357, %fd355;
	abs.f64 	%fd19, %fd353;
	abs.f64 	%fd20, %fd358;
	setp.eq.f64	%p10, %fd19, 0d0000000000000000;
	setp.eq.f64	%p11, %fd20, 0d0000000000000000;
	and.pred  	%p12, %p10, %p11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd353;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r88}, %fd358;
	}
	and.b32  	%r3, %r88, -2147483648;
	@%p12 bra 	BB6_11;
	bra.uni 	BB6_8;

BB6_11:
	setp.lt.s32	%p20, %r2, 0;
	selp.f64	%fd414, 0d400921FB54442D18, 0d0000000000000000, %p20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r95, %temp}, %fd414;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r96}, %fd414;
	}
	or.b32  	%r97, %r96, %r3;
	mov.b64 	%fd1845, {%r95, %r97};
	mov.f64 	%fd1945, 0d0000000000000000;
	bra.uni 	BB6_23;

BB6_13:
	setp.eq.f64	%p24, %fd25, 0d7FF0000000000000;
	setp.eq.f64	%p25, %fd26, 0d7FF0000000000000;
	and.pred  	%p26, %p24, %p25;
	@%p26 bra 	BB6_15;
	bra.uni 	BB6_14;

BB6_15:
	setp.lt.s32	%p30, %r4, 0;
	selp.f64	%fd472, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r102, %temp}, %fd472;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r103}, %fd472;
	}
	or.b32  	%r104, %r103, %r5;
	mov.b64 	%fd1844, {%r102, %r104};
	bra.uni 	BB6_17;

BB6_8:
	setp.eq.f64	%p13, %fd19, 0d7FF0000000000000;
	setp.eq.f64	%p14, %fd20, 0d7FF0000000000000;
	and.pred  	%p15, %p13, %p14;
	@%p15 bra 	BB6_10;
	bra.uni 	BB6_9;

BB6_10:
	setp.lt.s32	%p19, %r2, 0;
	selp.f64	%fd412, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p19;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r92, %temp}, %fd412;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd412;
	}
	or.b32  	%r94, %r93, %r3;
	mov.b64 	%fd1845, {%r92, %r94};
	mov.f64 	%fd1945, 0d0000000000000000;
	bra.uni 	BB6_23;

BB6_14:
	setp.lt.s32	%p27, %r4, 0;
	min.f64 	%fd421, %fd26, %fd25;
	max.f64 	%fd422, %fd26, %fd25;
	div.rn.f64 	%fd423, %fd421, %fd422;
	mul.f64 	%fd424, %fd423, %fd423;
	mov.f64 	%fd425, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd426, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd427, %fd426, %fd424, %fd425;
	mov.f64 	%fd428, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd429, %fd427, %fd424, %fd428;
	mov.f64 	%fd430, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd431, %fd429, %fd424, %fd430;
	mov.f64 	%fd432, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd433, %fd431, %fd424, %fd432;
	mov.f64 	%fd434, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd435, %fd433, %fd424, %fd434;
	mov.f64 	%fd436, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd437, %fd435, %fd424, %fd436;
	mov.f64 	%fd438, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd439, %fd437, %fd424, %fd438;
	mov.f64 	%fd440, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd441, %fd439, %fd424, %fd440;
	mov.f64 	%fd442, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd443, %fd441, %fd424, %fd442;
	mov.f64 	%fd444, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd445, %fd443, %fd424, %fd444;
	mov.f64 	%fd446, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd447, %fd445, %fd424, %fd446;
	mov.f64 	%fd448, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd449, %fd447, %fd424, %fd448;
	mov.f64 	%fd450, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd451, %fd449, %fd424, %fd450;
	mov.f64 	%fd452, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd453, %fd451, %fd424, %fd452;
	mov.f64 	%fd454, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd455, %fd453, %fd424, %fd454;
	mov.f64 	%fd456, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd457, %fd455, %fd424, %fd456;
	mov.f64 	%fd458, 0d3FC99999999840D2;
	fma.rn.f64 	%fd459, %fd457, %fd424, %fd458;
	mov.f64 	%fd460, 0dBFD555555555544C;
	fma.rn.f64 	%fd461, %fd459, %fd424, %fd460;
	mul.f64 	%fd462, %fd424, %fd461;
	fma.rn.f64 	%fd463, %fd462, %fd423, %fd423;
	mov.f64 	%fd464, 0d3FF921FB54442D18;
	sub.f64 	%fd465, %fd464, %fd463;
	setp.gt.f64	%p28, %fd26, %fd25;
	selp.f64	%fd466, %fd465, %fd463, %p28;
	mov.f64 	%fd467, 0d400921FB54442D18;
	sub.f64 	%fd468, %fd467, %fd466;
	selp.f64	%fd469, %fd468, %fd466, %p27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r99, %temp}, %fd469;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r100}, %fd469;
	}
	or.b32  	%r101, %r100, %r5;
	mov.b64 	%fd470, {%r99, %r101};
	add.f64 	%fd471, %fd25, %fd26;
	setp.gtu.f64	%p29, %fd471, 0d7FF0000000000000;
	selp.f64	%fd1844, %fd471, %fd470, %p29;

BB6_17:
	mul.f64 	%fd1627, %fd9, %fd9;
	mul.f64 	%fd1626, %fd6, %fd6;
	mul.f64 	%fd1945, %fd1844, 0d3FE0000000000000;
	fma.rn.f64 	%fd474, %fd8, %fd8, %fd1626;
	mul.f64 	%fd475, %fd7, %fd7;
	sub.f64 	%fd476, %fd474, %fd475;
	sub.f64 	%fd477, %fd476, %fd1627;
	mul.f64 	%fd478, %fd9, %fd8;
	fma.rn.f64 	%fd479, %fd6, %fd7, %fd478;
	fma.rn.f64 	%fd480, %fd6, %fd7, %fd479;
	add.f64 	%fd481, %fd480, %fd478;
	abs.f64 	%fd32, %fd477;
	abs.f64 	%fd33, %fd481;
	setp.eq.f64	%p32, %fd32, 0d0000000000000000;
	setp.eq.f64	%p33, %fd33, 0d0000000000000000;
	and.pred  	%p34, %p32, %p33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r6}, %fd477;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r108}, %fd481;
	}
	and.b32  	%r7, %r108, -2147483648;
	@%p34 bra 	BB6_21;
	bra.uni 	BB6_18;

BB6_21:
	setp.lt.s32	%p42, %r6, 0;
	selp.f64	%fd534, 0d400921FB54442D18, 0d0000000000000000, %p42;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r115, %temp}, %fd534;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r116}, %fd534;
	}
	or.b32  	%r117, %r116, %r7;
	mov.b64 	%fd1845, {%r115, %r117};
	bra.uni 	BB6_22;

BB6_18:
	setp.eq.f64	%p35, %fd32, 0d7FF0000000000000;
	setp.eq.f64	%p36, %fd33, 0d7FF0000000000000;
	and.pred  	%p37, %p35, %p36;
	@%p37 bra 	BB6_20;
	bra.uni 	BB6_19;

BB6_20:
	setp.lt.s32	%p41, %r6, 0;
	selp.f64	%fd533, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p41;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r112, %temp}, %fd533;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r113}, %fd533;
	}
	or.b32  	%r114, %r113, %r7;
	mov.b64 	%fd1845, {%r112, %r114};

BB6_22:
	bra.uni 	BB6_23;

BB6_9:
	setp.lt.s32	%p16, %r2, 0;
	min.f64 	%fd360, %fd20, %fd19;
	max.f64 	%fd361, %fd20, %fd19;
	div.rn.f64 	%fd362, %fd360, %fd361;
	mul.f64 	%fd363, %fd362, %fd362;
	mov.f64 	%fd364, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd365, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd366, %fd365, %fd363, %fd364;
	mov.f64 	%fd367, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd368, %fd366, %fd363, %fd367;
	mov.f64 	%fd369, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd370, %fd368, %fd363, %fd369;
	mov.f64 	%fd371, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd372, %fd370, %fd363, %fd371;
	mov.f64 	%fd373, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd374, %fd372, %fd363, %fd373;
	mov.f64 	%fd375, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd376, %fd374, %fd363, %fd375;
	mov.f64 	%fd377, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd378, %fd376, %fd363, %fd377;
	mov.f64 	%fd379, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd380, %fd378, %fd363, %fd379;
	mov.f64 	%fd381, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd382, %fd380, %fd363, %fd381;
	mov.f64 	%fd383, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd384, %fd382, %fd363, %fd383;
	mov.f64 	%fd385, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd386, %fd384, %fd363, %fd385;
	mov.f64 	%fd387, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd388, %fd386, %fd363, %fd387;
	mov.f64 	%fd389, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd390, %fd388, %fd363, %fd389;
	mov.f64 	%fd391, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd392, %fd390, %fd363, %fd391;
	mov.f64 	%fd393, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd394, %fd392, %fd363, %fd393;
	mov.f64 	%fd395, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd396, %fd394, %fd363, %fd395;
	mov.f64 	%fd397, 0d3FC99999999840D2;
	fma.rn.f64 	%fd398, %fd396, %fd363, %fd397;
	mov.f64 	%fd399, 0dBFD555555555544C;
	fma.rn.f64 	%fd400, %fd398, %fd363, %fd399;
	mul.f64 	%fd401, %fd363, %fd400;
	fma.rn.f64 	%fd402, %fd401, %fd362, %fd362;
	mov.f64 	%fd403, 0d3FF921FB54442D18;
	sub.f64 	%fd404, %fd403, %fd402;
	setp.gt.f64	%p17, %fd20, %fd19;
	selp.f64	%fd405, %fd404, %fd402, %p17;
	mov.f64 	%fd406, 0d400921FB54442D18;
	sub.f64 	%fd407, %fd406, %fd405;
	selp.f64	%fd408, %fd407, %fd405, %p16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r89, %temp}, %fd408;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r90}, %fd408;
	}
	or.b32  	%r91, %r90, %r3;
	mov.b64 	%fd409, {%r89, %r91};
	add.f64 	%fd410, %fd19, %fd20;
	setp.gtu.f64	%p18, %fd410, 0d7FF0000000000000;
	selp.f64	%fd1845, %fd410, %fd409, %p18;
	mov.f64 	%fd1945, 0d0000000000000000;
	bra.uni 	BB6_23;

BB6_19:
	setp.lt.s32	%p38, %r6, 0;
	min.f64 	%fd482, %fd33, %fd32;
	max.f64 	%fd483, %fd33, %fd32;
	div.rn.f64 	%fd484, %fd482, %fd483;
	mul.f64 	%fd485, %fd484, %fd484;
	mov.f64 	%fd486, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd487, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd488, %fd487, %fd485, %fd486;
	mov.f64 	%fd489, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd490, %fd488, %fd485, %fd489;
	mov.f64 	%fd491, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd492, %fd490, %fd485, %fd491;
	mov.f64 	%fd493, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd494, %fd492, %fd485, %fd493;
	mov.f64 	%fd495, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd496, %fd494, %fd485, %fd495;
	mov.f64 	%fd497, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd498, %fd496, %fd485, %fd497;
	mov.f64 	%fd499, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd500, %fd498, %fd485, %fd499;
	mov.f64 	%fd501, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd502, %fd500, %fd485, %fd501;
	mov.f64 	%fd503, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd504, %fd502, %fd485, %fd503;
	mov.f64 	%fd505, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd506, %fd504, %fd485, %fd505;
	mov.f64 	%fd507, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd508, %fd506, %fd485, %fd507;
	mov.f64 	%fd509, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd510, %fd508, %fd485, %fd509;
	mov.f64 	%fd511, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd512, %fd510, %fd485, %fd511;
	mov.f64 	%fd513, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd514, %fd512, %fd485, %fd513;
	mov.f64 	%fd515, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd516, %fd514, %fd485, %fd515;
	mov.f64 	%fd517, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd518, %fd516, %fd485, %fd517;
	mov.f64 	%fd519, 0d3FC99999999840D2;
	fma.rn.f64 	%fd520, %fd518, %fd485, %fd519;
	mov.f64 	%fd521, 0dBFD555555555544C;
	fma.rn.f64 	%fd522, %fd520, %fd485, %fd521;
	mul.f64 	%fd523, %fd485, %fd522;
	fma.rn.f64 	%fd524, %fd523, %fd484, %fd484;
	mov.f64 	%fd525, 0d3FF921FB54442D18;
	sub.f64 	%fd526, %fd525, %fd524;
	setp.gt.f64	%p39, %fd33, %fd32;
	selp.f64	%fd527, %fd526, %fd524, %p39;
	mov.f64 	%fd528, 0d400921FB54442D18;
	sub.f64 	%fd529, %fd528, %fd527;
	selp.f64	%fd530, %fd529, %fd527, %p38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r109, %temp}, %fd530;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r110}, %fd530;
	}
	or.b32  	%r111, %r110, %r7;
	mov.b64 	%fd531, {%r109, %r111};
	add.f64 	%fd532, %fd32, %fd33;
	setp.gtu.f64	%p40, %fd532, 0d7FF0000000000000;
	selp.f64	%fd1845, %fd532, %fd531, %p40;

BB6_23:
	mov.f64 	%fd37, %fd1945;
	mul.f64 	%fd39, %fd1845, 0d3FE0000000000000;
	abs.f64 	%fd40, %fd37;
	setp.neu.f64	%p43, %fd40, 0d7FF0000000000000;
	mov.f64 	%fd1944, %fd37;
	@%p43 bra 	BB6_25;

	mov.f64 	%fd535, 0d0000000000000000;
	mul.rn.f64 	%fd41, %fd37, %fd535;
	mov.f64 	%fd1944, %fd41;

BB6_25:
	mov.f64 	%fd42, %fd1944;
	mul.f64 	%fd536, %fd42, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r322, %fd536;
	st.local.u32 	[%rd1], %r322;
	cvt.rn.f64.s32	%fd537, %r322;
	neg.f64 	%fd538, %fd537;
	mov.f64 	%fd539, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd540, %fd538, %fd539, %fd42;
	mov.f64 	%fd541, 0d3C91A62633145C00;
	fma.rn.f64 	%fd542, %fd538, %fd541, %fd540;
	mov.f64 	%fd543, 0d397B839A252049C0;
	fma.rn.f64 	%fd1846, %fd538, %fd543, %fd542;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r118}, %fd42;
	}
	and.b32  	%r119, %r118, 2145386496;
	setp.lt.u32	%p44, %r119, 1105199104;
	@%p44 bra 	BB6_27;

	add.u64 	%rd96, %SP, 0;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd42;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd96;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1846, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r322, [%rd1];

BB6_27:
	mul.rn.f64 	%fd544, %fd1846, %fd1846;
	mov.f64 	%fd545, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd546, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd547, %fd546, %fd544, %fd545;
	mov.f64 	%fd548, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd549, %fd547, %fd544, %fd548;
	mov.f64 	%fd550, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd551, %fd549, %fd544, %fd550;
	mov.f64 	%fd552, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd553, %fd551, %fd544, %fd552;
	mov.f64 	%fd554, 0d3FA5555555555551;
	fma.rn.f64 	%fd555, %fd553, %fd544, %fd554;
	mov.f64 	%fd556, 0dBFE0000000000000;
	fma.rn.f64 	%fd557, %fd555, %fd544, %fd556;
	mov.f64 	%fd558, 0d3FF0000000000000;
	fma.rn.f64 	%fd559, %fd557, %fd544, %fd558;
	mov.f64 	%fd560, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd561, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd562, %fd561, %fd544, %fd560;
	mov.f64 	%fd563, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd564, %fd562, %fd544, %fd563;
	mov.f64 	%fd565, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd566, %fd564, %fd544, %fd565;
	mov.f64 	%fd567, 0d3F81111111110818;
	fma.rn.f64 	%fd568, %fd566, %fd544, %fd567;
	mov.f64 	%fd569, 0dBFC5555555555554;
	fma.rn.f64 	%fd570, %fd568, %fd544, %fd569;
	mov.f64 	%fd571, 0d0000000000000000;
	fma.rn.f64 	%fd572, %fd570, %fd544, %fd571;
	fma.rn.f64 	%fd573, %fd572, %fd1846, %fd1846;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r120}, %fd573;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r121, %temp}, %fd573;
	}
	xor.b32  	%r122, %r120, -2147483648;
	mov.b64 	%fd574, {%r121, %r122};
	and.b32  	%r123, %r322, 1;
	setp.eq.b32	%p45, %r123, 1;
	not.pred 	%p46, %p45;
	selp.f64	%fd1847, %fd559, %fd574, %p46;
	and.b32  	%r124, %r322, 2;
	setp.eq.s32	%p47, %r124, 0;
	@%p47 bra 	BB6_29;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r125}, %fd1847;
	}
	xor.b32  	%r126, %r125, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r127, %temp}, %fd1847;
	}
	mov.b64 	%fd1847, {%r127, %r126};

BB6_29:
	abs.f64 	%fd49, %fd39;
	setp.neu.f64	%p48, %fd49, 0d7FF0000000000000;
	mov.f64 	%fd1896, %fd39;
	@%p48 bra 	BB6_31;

	mul.rn.f64 	%fd50, %fd39, %fd571;
	mov.f64 	%fd1896, %fd50;

BB6_31:
	mov.f64 	%fd51, %fd1896;
	mov.f64 	%fd1682, 0d3FF921FB54442D18;
	mov.f64 	%fd1665, 0d397B839A252049C0;
	mov.f64 	%fd1664, 0d3C91A62633145C00;
	mul.f64 	%fd576, %fd51, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r323, %fd576;
	st.local.u32 	[%rd1], %r323;
	cvt.rn.f64.s32	%fd577, %r323;
	neg.f64 	%fd578, %fd577;
	fma.rn.f64 	%fd580, %fd578, %fd1682, %fd51;
	fma.rn.f64 	%fd582, %fd578, %fd1664, %fd580;
	fma.rn.f64 	%fd1848, %fd578, %fd1665, %fd582;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r128}, %fd51;
	}
	and.b32  	%r129, %r128, 2145386496;
	setp.lt.u32	%p49, %r129, 1105199104;
	@%p49 bra 	BB6_33;

	add.u64 	%rd95, %SP, 0;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd51;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd95;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1848, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r323, [%rd1];

BB6_33:
	mov.f64 	%fd1830, 0d3FF0000000000000;
	mov.f64 	%fd1810, 0dBFE0000000000000;
	mov.f64 	%fd1801, 0d3FA5555555555551;
	mov.f64 	%fd1768, 0dBF56C16C16C15D47;
	mov.f64 	%fd1758, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1757, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1756, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1739, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd584, %fd1848, %fd1848;
	fma.rn.f64 	%fd587, %fd1739, %fd584, %fd1756;
	fma.rn.f64 	%fd589, %fd587, %fd584, %fd1757;
	fma.rn.f64 	%fd591, %fd589, %fd584, %fd1758;
	fma.rn.f64 	%fd593, %fd591, %fd584, %fd1768;
	fma.rn.f64 	%fd595, %fd593, %fd584, %fd1801;
	fma.rn.f64 	%fd597, %fd595, %fd584, %fd1810;
	fma.rn.f64 	%fd599, %fd597, %fd584, %fd1830;
	fma.rn.f64 	%fd602, %fd561, %fd584, %fd560;
	fma.rn.f64 	%fd604, %fd602, %fd584, %fd563;
	fma.rn.f64 	%fd606, %fd604, %fd584, %fd565;
	fma.rn.f64 	%fd608, %fd606, %fd584, %fd567;
	fma.rn.f64 	%fd610, %fd608, %fd584, %fd569;
	fma.rn.f64 	%fd612, %fd610, %fd584, %fd571;
	fma.rn.f64 	%fd613, %fd612, %fd1848, %fd1848;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r130}, %fd613;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r131, %temp}, %fd613;
	}
	xor.b32  	%r132, %r130, -2147483648;
	mov.b64 	%fd614, {%r131, %r132};
	and.b32  	%r133, %r323, 1;
	setp.eq.b32	%p50, %r133, 1;
	not.pred 	%p51, %p50;
	selp.f64	%fd1849, %fd599, %fd614, %p51;
	and.b32  	%r134, %r323, 2;
	setp.eq.s32	%p52, %r134, 0;
	@%p52 bra 	BB6_35;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r135}, %fd1849;
	}
	xor.b32  	%r136, %r135, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r137, %temp}, %fd1849;
	}
	mov.b64 	%fd1849, {%r137, %r136};

BB6_35:
	mul.f64 	%fd58, %fd1847, %fd1849;
	abs.f64 	%fd59, %fd16;
	setp.neu.f64	%p53, %fd59, 0d7FF0000000000000;
	mov.f64 	%fd1911, %fd16;
	@%p53 bra 	BB6_37;

	mul.rn.f64 	%fd60, %fd16, %fd571;
	mov.f64 	%fd1911, %fd60;

BB6_37:
	mov.f64 	%fd61, %fd1911;
	mov.f64 	%fd1679, 0d3FF921FB54442D18;
	mov.f64 	%fd1663, 0d397B839A252049C0;
	mov.f64 	%fd1662, 0d3C91A62633145C00;
	mul.f64 	%fd616, %fd61, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r324, %fd616;
	st.local.u32 	[%rd1], %r324;
	cvt.rn.f64.s32	%fd617, %r324;
	neg.f64 	%fd618, %fd617;
	fma.rn.f64 	%fd620, %fd618, %fd1679, %fd61;
	fma.rn.f64 	%fd622, %fd618, %fd1662, %fd620;
	fma.rn.f64 	%fd1850, %fd618, %fd1663, %fd622;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r138}, %fd61;
	}
	and.b32  	%r139, %r138, 2145386496;
	setp.lt.u32	%p54, %r139, 1105199104;
	@%p54 bra 	BB6_39;

	add.u64 	%rd94, %SP, 0;
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd61;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd94;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1850, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r324, [%rd1];

BB6_39:
	mov.f64 	%fd1841, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1831, 0d3FF0000000000000;
	mov.f64 	%fd1811, 0dBFE0000000000000;
	mov.f64 	%fd1799, 0d3FA5555555555551;
	mov.f64 	%fd1767, 0dBF56C16C16C15D47;
	mov.f64 	%fd1754, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1753, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1752, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1737, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd624, %fd1850, %fd1850;
	fma.rn.f64 	%fd627, %fd1737, %fd624, %fd1752;
	fma.rn.f64 	%fd629, %fd627, %fd624, %fd1753;
	fma.rn.f64 	%fd631, %fd629, %fd624, %fd1754;
	fma.rn.f64 	%fd633, %fd631, %fd624, %fd1767;
	fma.rn.f64 	%fd635, %fd633, %fd624, %fd1799;
	fma.rn.f64 	%fd637, %fd635, %fd624, %fd1811;
	fma.rn.f64 	%fd639, %fd637, %fd624, %fd1831;
	fma.rn.f64 	%fd642, %fd1841, %fd624, %fd560;
	fma.rn.f64 	%fd644, %fd642, %fd624, %fd563;
	fma.rn.f64 	%fd646, %fd644, %fd624, %fd565;
	fma.rn.f64 	%fd648, %fd646, %fd624, %fd567;
	fma.rn.f64 	%fd650, %fd648, %fd624, %fd569;
	fma.rn.f64 	%fd652, %fd650, %fd624, %fd571;
	fma.rn.f64 	%fd653, %fd652, %fd1850, %fd1850;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r140}, %fd653;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r141, %temp}, %fd653;
	}
	xor.b32  	%r142, %r140, -2147483648;
	mov.b64 	%fd654, {%r141, %r142};
	and.b32  	%r143, %r324, 1;
	setp.eq.b32	%p55, %r143, 1;
	not.pred 	%p56, %p55;
	selp.f64	%fd1851, %fd639, %fd654, %p56;
	and.b32  	%r144, %r324, 2;
	setp.eq.s32	%p57, %r144, 0;
	@%p57 bra 	BB6_41;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r145}, %fd1851;
	}
	xor.b32  	%r146, %r145, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r147, %temp}, %fd1851;
	}
	mov.b64 	%fd1851, {%r147, %r146};

BB6_41:
	abs.f64 	%fd1614, %fd37;
	setp.neu.f64	%p184, %fd1614, 0d7FF0000000000000;
	mul.f64 	%fd68, %fd58, %fd1851;
	mov.f64 	%fd1943, %fd37;
	@%p184 bra 	BB6_43;

	mul.rn.f64 	%fd1943, %fd37, %fd571;

BB6_43:
	mov.f64 	%fd1680, 0d3FF921FB54442D18;
	mov.f64 	%fd1643, 0d397B839A252049C0;
	mov.f64 	%fd1642, 0d3C91A62633145C00;
	mul.f64 	%fd656, %fd1943, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r325, %fd656;
	st.local.u32 	[%rd1], %r325;
	cvt.rn.f64.s32	%fd657, %r325;
	neg.f64 	%fd658, %fd657;
	fma.rn.f64 	%fd660, %fd658, %fd1680, %fd1943;
	fma.rn.f64 	%fd662, %fd658, %fd1642, %fd660;
	fma.rn.f64 	%fd1852, %fd658, %fd1643, %fd662;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r148}, %fd1943;
	}
	and.b32  	%r149, %r148, 2145386496;
	setp.lt.u32	%p59, %r149, 1105199104;
	@%p59 bra 	BB6_45;

	add.u64 	%rd93, %SP, 0;
	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1943;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd93;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1852, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r325, [%rd1];

BB6_45:
	mov.f64 	%fd1839, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1832, 0d3FF0000000000000;
	mov.f64 	%fd1802, 0dBFE0000000000000;
	mov.f64 	%fd1800, 0d3FA5555555555551;
	mov.f64 	%fd1759, 0dBF56C16C16C15D47;
	mov.f64 	%fd1755, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1741, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1740, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1738, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd664, %fd1852, %fd1852;
	fma.rn.f64 	%fd667, %fd1738, %fd664, %fd1740;
	fma.rn.f64 	%fd669, %fd667, %fd664, %fd1741;
	fma.rn.f64 	%fd671, %fd669, %fd664, %fd1755;
	fma.rn.f64 	%fd673, %fd671, %fd664, %fd1759;
	fma.rn.f64 	%fd675, %fd673, %fd664, %fd1800;
	fma.rn.f64 	%fd677, %fd675, %fd664, %fd1802;
	fma.rn.f64 	%fd679, %fd677, %fd664, %fd1832;
	fma.rn.f64 	%fd682, %fd1839, %fd664, %fd560;
	fma.rn.f64 	%fd684, %fd682, %fd664, %fd563;
	fma.rn.f64 	%fd686, %fd684, %fd664, %fd565;
	fma.rn.f64 	%fd688, %fd686, %fd664, %fd567;
	fma.rn.f64 	%fd690, %fd688, %fd664, %fd569;
	fma.rn.f64 	%fd692, %fd690, %fd664, %fd571;
	fma.rn.f64 	%fd693, %fd692, %fd1852, %fd1852;
	and.b32  	%r150, %r325, 1;
	setp.eq.b32	%p60, %r150, 1;
	not.pred 	%p61, %p60;
	selp.f64	%fd1853, %fd693, %fd679, %p61;
	and.b32  	%r151, %r325, 2;
	setp.eq.s32	%p62, %r151, 0;
	@%p62 bra 	BB6_47;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r152}, %fd1853;
	}
	xor.b32  	%r153, %r152, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r154, %temp}, %fd1853;
	}
	mov.b64 	%fd1853, {%r154, %r153};

BB6_47:
	abs.f64 	%fd1615, %fd39;
	setp.neu.f64	%p185, %fd1615, 0d7FF0000000000000;
	mov.f64 	%fd1895, %fd39;
	@%p185 bra 	BB6_49;

	mul.rn.f64 	%fd1895, %fd39, %fd571;

BB6_49:
	mov.f64 	%fd1681, 0d3FF921FB54442D18;
	mov.f64 	%fd1645, 0d397B839A252049C0;
	mov.f64 	%fd1644, 0d3C91A62633145C00;
	mul.f64 	%fd695, %fd1895, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r326, %fd695;
	st.local.u32 	[%rd1], %r326;
	cvt.rn.f64.s32	%fd696, %r326;
	neg.f64 	%fd697, %fd696;
	fma.rn.f64 	%fd699, %fd697, %fd1681, %fd1895;
	fma.rn.f64 	%fd701, %fd697, %fd1644, %fd699;
	fma.rn.f64 	%fd1854, %fd697, %fd1645, %fd701;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r155}, %fd1895;
	}
	and.b32  	%r156, %r155, 2145386496;
	setp.lt.u32	%p64, %r156, 1105199104;
	@%p64 bra 	BB6_51;

	add.u64 	%rd92, %SP, 0;
	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1895;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd92;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1854, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r326, [%rd1];

BB6_51:
	mov.f64 	%fd1840, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1824, 0d3FF0000000000000;
	mov.f64 	%fd1803, 0dBFE0000000000000;
	mov.f64 	%fd1789, 0d3FA5555555555551;
	mov.f64 	%fd1760, 0dBF56C16C16C15D47;
	mov.f64 	%fd1744, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1743, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1742, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1722, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd703, %fd1854, %fd1854;
	fma.rn.f64 	%fd706, %fd1722, %fd703, %fd1742;
	fma.rn.f64 	%fd708, %fd706, %fd703, %fd1743;
	fma.rn.f64 	%fd710, %fd708, %fd703, %fd1744;
	fma.rn.f64 	%fd712, %fd710, %fd703, %fd1760;
	fma.rn.f64 	%fd714, %fd712, %fd703, %fd1789;
	fma.rn.f64 	%fd716, %fd714, %fd703, %fd1803;
	fma.rn.f64 	%fd718, %fd716, %fd703, %fd1824;
	fma.rn.f64 	%fd721, %fd1840, %fd703, %fd560;
	fma.rn.f64 	%fd723, %fd721, %fd703, %fd563;
	fma.rn.f64 	%fd725, %fd723, %fd703, %fd565;
	fma.rn.f64 	%fd727, %fd725, %fd703, %fd567;
	fma.rn.f64 	%fd729, %fd727, %fd703, %fd569;
	fma.rn.f64 	%fd731, %fd729, %fd703, %fd571;
	fma.rn.f64 	%fd732, %fd731, %fd1854, %fd1854;
	and.b32  	%r157, %r326, 1;
	setp.eq.b32	%p65, %r157, 1;
	not.pred 	%p66, %p65;
	selp.f64	%fd1855, %fd732, %fd718, %p66;
	and.b32  	%r158, %r326, 2;
	setp.eq.s32	%p67, %r158, 0;
	@%p67 bra 	BB6_53;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r159}, %fd1855;
	}
	xor.b32  	%r160, %r159, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r161, %temp}, %fd1855;
	}
	mov.b64 	%fd1855, {%r161, %r160};

BB6_53:
	abs.f64 	%fd1646, %fd16;
	setp.neu.f64	%p188, %fd1646, 0d7FF0000000000000;
	mul.f64 	%fd85, %fd1853, %fd1855;
	mov.f64 	%fd1910, %fd16;
	@%p188 bra 	BB6_55;

	mul.rn.f64 	%fd1910, %fd16, %fd571;

BB6_55:
	mov.f64 	%fd1666, 0d3FF921FB54442D18;
	mov.f64 	%fd1648, 0d397B839A252049C0;
	mov.f64 	%fd1647, 0d3C91A62633145C00;
	mul.f64 	%fd734, %fd1910, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r327, %fd734;
	st.local.u32 	[%rd1], %r327;
	cvt.rn.f64.s32	%fd735, %r327;
	neg.f64 	%fd736, %fd735;
	fma.rn.f64 	%fd738, %fd736, %fd1666, %fd1910;
	fma.rn.f64 	%fd740, %fd736, %fd1647, %fd738;
	fma.rn.f64 	%fd1856, %fd736, %fd1648, %fd740;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r162}, %fd1910;
	}
	and.b32  	%r163, %r162, 2145386496;
	setp.lt.u32	%p69, %r163, 1105199104;
	@%p69 bra 	BB6_57;

	add.u64 	%rd91, %SP, 0;
	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1910;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd91;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1856, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r327, [%rd1];

BB6_57:
	mov.f64 	%fd1838, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1825, 0d3FF0000000000000;
	mov.f64 	%fd1791, 0dBFE0000000000000;
	mov.f64 	%fd1790, 0d3FA5555555555551;
	mov.f64 	%fd1746, 0dBF56C16C16C15D47;
	mov.f64 	%fd1745, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1725, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1724, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1723, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd742, %fd1856, %fd1856;
	fma.rn.f64 	%fd745, %fd1723, %fd742, %fd1724;
	fma.rn.f64 	%fd747, %fd745, %fd742, %fd1725;
	fma.rn.f64 	%fd749, %fd747, %fd742, %fd1745;
	fma.rn.f64 	%fd751, %fd749, %fd742, %fd1746;
	fma.rn.f64 	%fd753, %fd751, %fd742, %fd1790;
	fma.rn.f64 	%fd755, %fd753, %fd742, %fd1791;
	fma.rn.f64 	%fd757, %fd755, %fd742, %fd1825;
	fma.rn.f64 	%fd760, %fd1838, %fd742, %fd560;
	fma.rn.f64 	%fd762, %fd760, %fd742, %fd563;
	fma.rn.f64 	%fd764, %fd762, %fd742, %fd565;
	fma.rn.f64 	%fd766, %fd764, %fd742, %fd567;
	fma.rn.f64 	%fd768, %fd766, %fd742, %fd569;
	fma.rn.f64 	%fd770, %fd768, %fd742, %fd571;
	fma.rn.f64 	%fd771, %fd770, %fd1856, %fd1856;
	and.b32  	%r164, %r327, 1;
	setp.eq.b32	%p70, %r164, 1;
	not.pred 	%p71, %p70;
	selp.f64	%fd1857, %fd771, %fd757, %p71;
	and.b32  	%r165, %r327, 2;
	setp.eq.s32	%p72, %r165, 0;
	@%p72 bra 	BB6_59;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r166}, %fd1857;
	}
	xor.b32  	%r167, %r166, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r168, %temp}, %fd1857;
	}
	mov.b64 	%fd1857, {%r168, %r167};

BB6_59:
	abs.f64 	%fd1594, %fd37;
	setp.neu.f64	%p181, %fd1594, 0d7FF0000000000000;
	fma.rn.f64 	%fd94, %fd85, %fd1857, %fd68;
	mov.f64 	%fd1942, %fd37;
	@%p181 bra 	BB6_61;

	mul.rn.f64 	%fd1942, %fd37, %fd571;

BB6_61:
	mov.f64 	%fd1667, 0d3FF921FB54442D18;
	mov.f64 	%fd1629, 0d397B839A252049C0;
	mov.f64 	%fd1628, 0d3C91A62633145C00;
	mul.f64 	%fd773, %fd1942, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r328, %fd773;
	st.local.u32 	[%rd1], %r328;
	cvt.rn.f64.s32	%fd774, %r328;
	neg.f64 	%fd775, %fd774;
	fma.rn.f64 	%fd777, %fd775, %fd1667, %fd1942;
	fma.rn.f64 	%fd779, %fd775, %fd1628, %fd777;
	fma.rn.f64 	%fd1858, %fd775, %fd1629, %fd779;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r169}, %fd1942;
	}
	and.b32  	%r170, %r169, 2145386496;
	setp.lt.u32	%p74, %r170, 1105199104;
	@%p74 bra 	BB6_63;

	add.u64 	%rd90, %SP, 0;
	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1942;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd90;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1858, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r328, [%rd1];

BB6_63:
	mov.f64 	%fd1836, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1818, 0d3FF0000000000000;
	mov.f64 	%fd1792, 0dBFE0000000000000;
	mov.f64 	%fd1780, 0d3FA5555555555551;
	mov.f64 	%fd1747, 0dBF56C16C16C15D47;
	mov.f64 	%fd1728, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1727, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1726, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1708, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd781, %fd1858, %fd1858;
	fma.rn.f64 	%fd784, %fd1708, %fd781, %fd1726;
	fma.rn.f64 	%fd786, %fd784, %fd781, %fd1727;
	fma.rn.f64 	%fd788, %fd786, %fd781, %fd1728;
	fma.rn.f64 	%fd790, %fd788, %fd781, %fd1747;
	fma.rn.f64 	%fd792, %fd790, %fd781, %fd1780;
	fma.rn.f64 	%fd794, %fd792, %fd781, %fd1792;
	fma.rn.f64 	%fd796, %fd794, %fd781, %fd1818;
	fma.rn.f64 	%fd799, %fd1836, %fd781, %fd560;
	fma.rn.f64 	%fd801, %fd799, %fd781, %fd563;
	fma.rn.f64 	%fd803, %fd801, %fd781, %fd565;
	fma.rn.f64 	%fd805, %fd803, %fd781, %fd567;
	fma.rn.f64 	%fd807, %fd805, %fd781, %fd569;
	fma.rn.f64 	%fd809, %fd807, %fd781, %fd571;
	fma.rn.f64 	%fd810, %fd809, %fd1858, %fd1858;
	and.b32  	%r171, %r328, 1;
	setp.eq.b32	%p75, %r171, 1;
	not.pred 	%p76, %p75;
	selp.f64	%fd1859, %fd810, %fd796, %p76;
	and.b32  	%r172, %r328, 2;
	setp.eq.s32	%p77, %r172, 0;
	@%p77 bra 	BB6_65;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r173}, %fd1859;
	}
	xor.b32  	%r174, %r173, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r175, %temp}, %fd1859;
	}
	mov.b64 	%fd1859, {%r175, %r174};

BB6_65:
	abs.f64 	%fd1595, %fd39;
	setp.neu.f64	%p182, %fd1595, 0d7FF0000000000000;
	mov.f64 	%fd1894, %fd39;
	@%p182 bra 	BB6_67;

	mul.rn.f64 	%fd1894, %fd39, %fd571;

BB6_67:
	mov.f64 	%fd1668, 0d3FF921FB54442D18;
	mov.f64 	%fd1631, 0d397B839A252049C0;
	mov.f64 	%fd1630, 0d3C91A62633145C00;
	mul.f64 	%fd812, %fd1894, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r329, %fd812;
	st.local.u32 	[%rd1], %r329;
	cvt.rn.f64.s32	%fd813, %r329;
	neg.f64 	%fd814, %fd813;
	fma.rn.f64 	%fd816, %fd814, %fd1668, %fd1894;
	fma.rn.f64 	%fd818, %fd814, %fd1630, %fd816;
	fma.rn.f64 	%fd1860, %fd814, %fd1631, %fd818;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd1894;
	}
	and.b32  	%r177, %r176, 2145386496;
	setp.lt.u32	%p79, %r177, 1105199104;
	@%p79 bra 	BB6_69;

	add.u64 	%rd89, %SP, 0;
	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1894;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd89;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1860, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r329, [%rd1];

BB6_69:
	mov.f64 	%fd1837, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1819, 0d3FF0000000000000;
	mov.f64 	%fd1782, 0dBFE0000000000000;
	mov.f64 	%fd1781, 0d3FA5555555555551;
	mov.f64 	%fd1730, 0dBF56C16C16C15D47;
	mov.f64 	%fd1729, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1711, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1710, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1709, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd820, %fd1860, %fd1860;
	fma.rn.f64 	%fd823, %fd1709, %fd820, %fd1710;
	fma.rn.f64 	%fd825, %fd823, %fd820, %fd1711;
	fma.rn.f64 	%fd827, %fd825, %fd820, %fd1729;
	fma.rn.f64 	%fd829, %fd827, %fd820, %fd1730;
	fma.rn.f64 	%fd831, %fd829, %fd820, %fd1781;
	fma.rn.f64 	%fd833, %fd831, %fd820, %fd1782;
	fma.rn.f64 	%fd835, %fd833, %fd820, %fd1819;
	fma.rn.f64 	%fd838, %fd1837, %fd820, %fd560;
	fma.rn.f64 	%fd840, %fd838, %fd820, %fd563;
	fma.rn.f64 	%fd842, %fd840, %fd820, %fd565;
	fma.rn.f64 	%fd844, %fd842, %fd820, %fd567;
	fma.rn.f64 	%fd846, %fd844, %fd820, %fd569;
	fma.rn.f64 	%fd848, %fd846, %fd820, %fd571;
	fma.rn.f64 	%fd849, %fd848, %fd1860, %fd1860;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r178}, %fd849;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r179, %temp}, %fd849;
	}
	xor.b32  	%r180, %r178, -2147483648;
	mov.b64 	%fd850, {%r179, %r180};
	and.b32  	%r181, %r329, 1;
	setp.eq.b32	%p80, %r181, 1;
	not.pred 	%p81, %p80;
	selp.f64	%fd1861, %fd835, %fd850, %p81;
	and.b32  	%r182, %r329, 2;
	setp.eq.s32	%p82, %r182, 0;
	@%p82 bra 	BB6_71;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r183}, %fd1861;
	}
	xor.b32  	%r184, %r183, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r185, %temp}, %fd1861;
	}
	mov.b64 	%fd1861, {%r185, %r184};

BB6_71:
	abs.f64 	%fd1632, %fd16;
	setp.neu.f64	%p187, %fd1632, 0d7FF0000000000000;
	mul.f64 	%fd111, %fd1859, %fd1861;
	mov.f64 	%fd1909, %fd16;
	@%p187 bra 	BB6_73;

	mul.rn.f64 	%fd1909, %fd16, %fd571;

BB6_73:
	mov.f64 	%fd1649, 0d3FF921FB54442D18;
	mov.f64 	%fd1634, 0d397B839A252049C0;
	mov.f64 	%fd1633, 0d3C91A62633145C00;
	mul.f64 	%fd852, %fd1909, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r330, %fd852;
	st.local.u32 	[%rd1], %r330;
	cvt.rn.f64.s32	%fd853, %r330;
	neg.f64 	%fd854, %fd853;
	fma.rn.f64 	%fd856, %fd854, %fd1649, %fd1909;
	fma.rn.f64 	%fd858, %fd854, %fd1633, %fd856;
	fma.rn.f64 	%fd1862, %fd854, %fd1634, %fd858;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r186}, %fd1909;
	}
	and.b32  	%r187, %r186, 2145386496;
	setp.lt.u32	%p84, %r187, 1105199104;
	@%p84 bra 	BB6_75;

	add.u64 	%rd88, %SP, 0;
	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1909;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd88;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1862, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r330, [%rd1];

BB6_75:
	mov.f64 	%fd1834, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1833, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1812, 0d3FF0000000000000;
	mov.f64 	%fd1783, 0dBFE0000000000000;
	mov.f64 	%fd1769, 0d3FA5555555555551;
	mov.f64 	%fd1731, 0dBF56C16C16C15D47;
	mov.f64 	%fd1714, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1713, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1712, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1695, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd860, %fd1862, %fd1862;
	fma.rn.f64 	%fd863, %fd1695, %fd860, %fd1712;
	fma.rn.f64 	%fd865, %fd863, %fd860, %fd1713;
	fma.rn.f64 	%fd867, %fd865, %fd860, %fd1714;
	fma.rn.f64 	%fd869, %fd867, %fd860, %fd1731;
	fma.rn.f64 	%fd871, %fd869, %fd860, %fd1769;
	fma.rn.f64 	%fd873, %fd871, %fd860, %fd1783;
	fma.rn.f64 	%fd875, %fd873, %fd860, %fd1812;
	fma.rn.f64 	%fd878, %fd1833, %fd860, %fd1834;
	fma.rn.f64 	%fd880, %fd878, %fd860, %fd563;
	fma.rn.f64 	%fd882, %fd880, %fd860, %fd565;
	fma.rn.f64 	%fd884, %fd882, %fd860, %fd567;
	fma.rn.f64 	%fd886, %fd884, %fd860, %fd569;
	fma.rn.f64 	%fd888, %fd886, %fd860, %fd571;
	fma.rn.f64 	%fd889, %fd888, %fd1862, %fd1862;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r188}, %fd889;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r189, %temp}, %fd889;
	}
	xor.b32  	%r190, %r188, -2147483648;
	mov.b64 	%fd890, {%r189, %r190};
	and.b32  	%r191, %r330, 1;
	setp.eq.b32	%p85, %r191, 1;
	not.pred 	%p86, %p85;
	selp.f64	%fd1863, %fd875, %fd890, %p86;
	and.b32  	%r192, %r330, 2;
	setp.eq.s32	%p87, %r192, 0;
	@%p87 bra 	BB6_77;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r193}, %fd1863;
	}
	xor.b32  	%r194, %r193, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r195, %temp}, %fd1863;
	}
	mov.b64 	%fd1863, {%r195, %r194};

BB6_77:
	abs.f64 	%fd1583, %fd37;
	setp.neu.f64	%p178, %fd1583, 0d7FF0000000000000;
	mul.f64 	%fd120, %fd111, %fd1863;
	mov.f64 	%fd1941, %fd37;
	@%p178 bra 	BB6_79;

	mul.rn.f64 	%fd1941, %fd37, %fd571;

BB6_79:
	mov.f64 	%fd1650, 0d3FF921FB54442D18;
	mov.f64 	%fd1617, 0d397B839A252049C0;
	mov.f64 	%fd1616, 0d3C91A62633145C00;
	mul.f64 	%fd892, %fd1941, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r331, %fd892;
	st.local.u32 	[%rd1], %r331;
	cvt.rn.f64.s32	%fd893, %r331;
	neg.f64 	%fd894, %fd893;
	fma.rn.f64 	%fd896, %fd894, %fd1650, %fd1941;
	fma.rn.f64 	%fd898, %fd894, %fd1616, %fd896;
	fma.rn.f64 	%fd1864, %fd894, %fd1617, %fd898;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r196}, %fd1941;
	}
	and.b32  	%r197, %r196, 2145386496;
	setp.lt.u32	%p89, %r197, 1105199104;
	@%p89 bra 	BB6_81;

	add.u64 	%rd87, %SP, 0;
	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1941;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd87;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1864, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r331, [%rd1];

BB6_81:
	mov.f64 	%fd1835, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1826, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1813, 0d3FF0000000000000;
	mov.f64 	%fd1771, 0dBFE0000000000000;
	mov.f64 	%fd1770, 0d3FA5555555555551;
	mov.f64 	%fd1716, 0dBF56C16C16C15D47;
	mov.f64 	%fd1715, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1698, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1697, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1696, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd900, %fd1864, %fd1864;
	fma.rn.f64 	%fd903, %fd1696, %fd900, %fd1697;
	fma.rn.f64 	%fd905, %fd903, %fd900, %fd1698;
	fma.rn.f64 	%fd907, %fd905, %fd900, %fd1715;
	fma.rn.f64 	%fd909, %fd907, %fd900, %fd1716;
	fma.rn.f64 	%fd911, %fd909, %fd900, %fd1770;
	fma.rn.f64 	%fd913, %fd911, %fd900, %fd1771;
	fma.rn.f64 	%fd915, %fd913, %fd900, %fd1813;
	fma.rn.f64 	%fd918, %fd1826, %fd900, %fd1835;
	fma.rn.f64 	%fd920, %fd918, %fd900, %fd563;
	fma.rn.f64 	%fd922, %fd920, %fd900, %fd565;
	fma.rn.f64 	%fd924, %fd922, %fd900, %fd567;
	fma.rn.f64 	%fd926, %fd924, %fd900, %fd569;
	fma.rn.f64 	%fd928, %fd926, %fd900, %fd571;
	fma.rn.f64 	%fd929, %fd928, %fd1864, %fd1864;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r198}, %fd929;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r199, %temp}, %fd929;
	}
	xor.b32  	%r200, %r198, -2147483648;
	mov.b64 	%fd930, {%r199, %r200};
	and.b32  	%r201, %r331, 1;
	setp.eq.b32	%p90, %r201, 1;
	not.pred 	%p91, %p90;
	selp.f64	%fd1865, %fd915, %fd930, %p91;
	and.b32  	%r202, %r331, 2;
	setp.eq.s32	%p92, %r202, 0;
	@%p92 bra 	BB6_83;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r203}, %fd1865;
	}
	xor.b32  	%r204, %r203, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r205, %temp}, %fd1865;
	}
	mov.b64 	%fd1865, {%r205, %r204};

BB6_83:
	abs.f64 	%fd1584, %fd39;
	setp.neu.f64	%p179, %fd1584, 0d7FF0000000000000;
	mov.f64 	%fd1893, %fd39;
	@%p179 bra 	BB6_85;

	mul.rn.f64 	%fd1893, %fd39, %fd571;

BB6_85:
	mov.f64 	%fd1651, 0d3FF921FB54442D18;
	mov.f64 	%fd1619, 0d397B839A252049C0;
	mov.f64 	%fd1618, 0d3C91A62633145C00;
	mul.f64 	%fd932, %fd1893, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r332, %fd932;
	st.local.u32 	[%rd1], %r332;
	cvt.rn.f64.s32	%fd933, %r332;
	neg.f64 	%fd934, %fd933;
	fma.rn.f64 	%fd936, %fd934, %fd1651, %fd1893;
	fma.rn.f64 	%fd938, %fd934, %fd1618, %fd936;
	fma.rn.f64 	%fd1866, %fd934, %fd1619, %fd938;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r206}, %fd1893;
	}
	and.b32  	%r207, %r206, 2145386496;
	setp.lt.u32	%p94, %r207, 1105199104;
	@%p94 bra 	BB6_87;

	add.u64 	%rd86, %SP, 0;
	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1893;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd86;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1866, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r332, [%rd1];

BB6_87:
	mov.f64 	%fd1828, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1827, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1804, 0d3FF0000000000000;
	mov.f64 	%fd1772, 0dBFE0000000000000;
	mov.f64 	%fd1761, 0d3FA5555555555551;
	mov.f64 	%fd1717, 0dBF56C16C16C15D47;
	mov.f64 	%fd1701, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1700, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1699, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1683, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd940, %fd1866, %fd1866;
	fma.rn.f64 	%fd943, %fd1683, %fd940, %fd1699;
	fma.rn.f64 	%fd945, %fd943, %fd940, %fd1700;
	fma.rn.f64 	%fd947, %fd945, %fd940, %fd1701;
	fma.rn.f64 	%fd949, %fd947, %fd940, %fd1717;
	fma.rn.f64 	%fd951, %fd949, %fd940, %fd1761;
	fma.rn.f64 	%fd953, %fd951, %fd940, %fd1772;
	fma.rn.f64 	%fd955, %fd953, %fd940, %fd1804;
	fma.rn.f64 	%fd958, %fd1827, %fd940, %fd1828;
	fma.rn.f64 	%fd960, %fd958, %fd940, %fd563;
	fma.rn.f64 	%fd962, %fd960, %fd940, %fd565;
	fma.rn.f64 	%fd964, %fd962, %fd940, %fd567;
	fma.rn.f64 	%fd966, %fd964, %fd940, %fd569;
	fma.rn.f64 	%fd968, %fd966, %fd940, %fd571;
	fma.rn.f64 	%fd969, %fd968, %fd1866, %fd1866;
	and.b32  	%r208, %r332, 1;
	setp.eq.b32	%p95, %r208, 1;
	not.pred 	%p96, %p95;
	selp.f64	%fd1867, %fd969, %fd955, %p96;
	and.b32  	%r209, %r332, 2;
	setp.eq.s32	%p97, %r209, 0;
	@%p97 bra 	BB6_89;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r210}, %fd1867;
	}
	xor.b32  	%r211, %r210, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r212, %temp}, %fd1867;
	}
	mov.b64 	%fd1867, {%r212, %r211};

BB6_89:
	abs.f64 	%fd1620, %fd16;
	setp.neu.f64	%p186, %fd1620, 0d7FF0000000000000;
	mul.f64 	%fd137, %fd1865, %fd1867;
	mov.f64 	%fd1908, %fd16;
	@%p186 bra 	BB6_91;

	mul.rn.f64 	%fd1908, %fd16, %fd571;

BB6_91:
	mov.f64 	%fd1623, 0d3FF921FB54442D18;
	mov.f64 	%fd1622, 0d397B839A252049C0;
	mov.f64 	%fd1621, 0d3C91A62633145C00;
	mul.f64 	%fd971, %fd1908, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r333, %fd971;
	st.local.u32 	[%rd1], %r333;
	cvt.rn.f64.s32	%fd972, %r333;
	neg.f64 	%fd973, %fd972;
	fma.rn.f64 	%fd975, %fd973, %fd1623, %fd1908;
	fma.rn.f64 	%fd977, %fd973, %fd1621, %fd975;
	fma.rn.f64 	%fd1868, %fd973, %fd1622, %fd977;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r213}, %fd1908;
	}
	and.b32  	%r214, %r213, 2145386496;
	setp.lt.u32	%p99, %r214, 1105199104;
	@%p99 bra 	BB6_93;

	add.u64 	%rd85, %SP, 0;
	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1908;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd85;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1868, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r333, [%rd1];

BB6_93:
	mov.f64 	%fd1829, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1820, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1805, 0d3FF0000000000000;
	mov.f64 	%fd1763, 0dBFE0000000000000;
	mov.f64 	%fd1762, 0d3FA5555555555551;
	mov.f64 	%fd1703, 0dBF56C16C16C15D47;
	mov.f64 	%fd1702, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1686, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1685, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1684, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd979, %fd1868, %fd1868;
	fma.rn.f64 	%fd982, %fd1684, %fd979, %fd1685;
	fma.rn.f64 	%fd984, %fd982, %fd979, %fd1686;
	fma.rn.f64 	%fd986, %fd984, %fd979, %fd1702;
	fma.rn.f64 	%fd988, %fd986, %fd979, %fd1703;
	fma.rn.f64 	%fd990, %fd988, %fd979, %fd1762;
	fma.rn.f64 	%fd992, %fd990, %fd979, %fd1763;
	fma.rn.f64 	%fd994, %fd992, %fd979, %fd1805;
	fma.rn.f64 	%fd997, %fd1820, %fd979, %fd1829;
	fma.rn.f64 	%fd999, %fd997, %fd979, %fd563;
	fma.rn.f64 	%fd1001, %fd999, %fd979, %fd565;
	fma.rn.f64 	%fd1003, %fd1001, %fd979, %fd567;
	fma.rn.f64 	%fd1005, %fd1003, %fd979, %fd569;
	fma.rn.f64 	%fd1007, %fd1005, %fd979, %fd571;
	fma.rn.f64 	%fd1008, %fd1007, %fd1868, %fd1868;
	and.b32  	%r215, %r333, 1;
	setp.eq.b32	%p100, %r215, 1;
	not.pred 	%p101, %p100;
	selp.f64	%fd1869, %fd1008, %fd994, %p101;
	and.b32  	%r216, %r333, 2;
	setp.eq.s32	%p102, %r216, 0;
	@%p102 bra 	BB6_95;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r217}, %fd1869;
	}
	xor.b32  	%r218, %r217, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r219, %temp}, %fd1869;
	}
	mov.b64 	%fd1869, {%r219, %r218};

BB6_95:
	abs.f64 	%fd1560, %fd37;
	setp.neu.f64	%p175, %fd1560, 0d7FF0000000000000;
	mul.f64 	%fd1009, %fd137, %fd1869;
	sub.f64 	%fd146, %fd120, %fd1009;
	mov.f64 	%fd1940, %fd37;
	@%p175 bra 	BB6_97;

	mul.rn.f64 	%fd1940, %fd37, %fd571;

BB6_97:
	mov.f64 	%fd1624, 0d3FF921FB54442D18;
	mov.f64 	%fd1597, 0d397B839A252049C0;
	mov.f64 	%fd1596, 0d3C91A62633145C00;
	mul.f64 	%fd1011, %fd1940, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r334, %fd1011;
	st.local.u32 	[%rd1], %r334;
	cvt.rn.f64.s32	%fd1012, %r334;
	neg.f64 	%fd1013, %fd1012;
	fma.rn.f64 	%fd1015, %fd1013, %fd1624, %fd1940;
	fma.rn.f64 	%fd1017, %fd1013, %fd1596, %fd1015;
	fma.rn.f64 	%fd1870, %fd1013, %fd1597, %fd1017;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r220}, %fd1940;
	}
	and.b32  	%r221, %r220, 2145386496;
	setp.lt.u32	%p104, %r221, 1105199104;
	@%p104 bra 	BB6_99;

	add.u64 	%rd84, %SP, 0;
	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1940;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd84;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1870, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r334, [%rd1];

BB6_99:
	mov.f64 	%fd1822, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1821, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1793, 0d3FF0000000000000;
	mov.f64 	%fd1764, 0dBFE0000000000000;
	mov.f64 	%fd1748, 0d3FA5555555555551;
	mov.f64 	%fd1704, 0dBF56C16C16C15D47;
	mov.f64 	%fd1689, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1688, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1687, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1669, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1019, %fd1870, %fd1870;
	fma.rn.f64 	%fd1022, %fd1669, %fd1019, %fd1687;
	fma.rn.f64 	%fd1024, %fd1022, %fd1019, %fd1688;
	fma.rn.f64 	%fd1026, %fd1024, %fd1019, %fd1689;
	fma.rn.f64 	%fd1028, %fd1026, %fd1019, %fd1704;
	fma.rn.f64 	%fd1030, %fd1028, %fd1019, %fd1748;
	fma.rn.f64 	%fd1032, %fd1030, %fd1019, %fd1764;
	fma.rn.f64 	%fd1034, %fd1032, %fd1019, %fd1793;
	fma.rn.f64 	%fd1037, %fd1821, %fd1019, %fd1822;
	fma.rn.f64 	%fd1039, %fd1037, %fd1019, %fd563;
	fma.rn.f64 	%fd1041, %fd1039, %fd1019, %fd565;
	fma.rn.f64 	%fd1043, %fd1041, %fd1019, %fd567;
	fma.rn.f64 	%fd1045, %fd1043, %fd1019, %fd569;
	fma.rn.f64 	%fd1047, %fd1045, %fd1019, %fd571;
	fma.rn.f64 	%fd1048, %fd1047, %fd1870, %fd1870;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r222}, %fd1048;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r223, %temp}, %fd1048;
	}
	xor.b32  	%r224, %r222, -2147483648;
	mov.b64 	%fd1049, {%r223, %r224};
	and.b32  	%r225, %r334, 1;
	setp.eq.b32	%p105, %r225, 1;
	not.pred 	%p106, %p105;
	selp.f64	%fd1871, %fd1034, %fd1049, %p106;
	and.b32  	%r226, %r334, 2;
	setp.eq.s32	%p107, %r226, 0;
	@%p107 bra 	BB6_101;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r227}, %fd1871;
	}
	xor.b32  	%r228, %r227, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r229, %temp}, %fd1871;
	}
	mov.b64 	%fd1871, {%r229, %r228};

BB6_101:
	abs.f64 	%fd1561, %fd39;
	setp.neu.f64	%p176, %fd1561, 0d7FF0000000000000;
	mov.f64 	%fd1892, %fd39;
	@%p176 bra 	BB6_103;

	mul.rn.f64 	%fd1892, %fd39, %fd571;

BB6_103:
	mov.f64 	%fd1625, 0d3FF921FB54442D18;
	mov.f64 	%fd1599, 0d397B839A252049C0;
	mov.f64 	%fd1598, 0d3C91A62633145C00;
	mul.f64 	%fd1051, %fd1892, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r335, %fd1051;
	st.local.u32 	[%rd1], %r335;
	cvt.rn.f64.s32	%fd1052, %r335;
	neg.f64 	%fd1053, %fd1052;
	fma.rn.f64 	%fd1055, %fd1053, %fd1625, %fd1892;
	fma.rn.f64 	%fd1057, %fd1053, %fd1598, %fd1055;
	fma.rn.f64 	%fd1872, %fd1053, %fd1599, %fd1057;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r230}, %fd1892;
	}
	and.b32  	%r231, %r230, 2145386496;
	setp.lt.u32	%p109, %r231, 1105199104;
	@%p109 bra 	BB6_105;

	add.u64 	%rd83, %SP, 0;
	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1892;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd83;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1872, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r335, [%rd1];

BB6_105:
	mov.f64 	%fd1823, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1814, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1794, 0d3FF0000000000000;
	mov.f64 	%fd1750, 0dBFE0000000000000;
	mov.f64 	%fd1749, 0d3FA5555555555551;
	mov.f64 	%fd1691, 0dBF56C16C16C15D47;
	mov.f64 	%fd1690, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1672, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1671, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1670, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1059, %fd1872, %fd1872;
	fma.rn.f64 	%fd1062, %fd1670, %fd1059, %fd1671;
	fma.rn.f64 	%fd1064, %fd1062, %fd1059, %fd1672;
	fma.rn.f64 	%fd1066, %fd1064, %fd1059, %fd1690;
	fma.rn.f64 	%fd1068, %fd1066, %fd1059, %fd1691;
	fma.rn.f64 	%fd1070, %fd1068, %fd1059, %fd1749;
	fma.rn.f64 	%fd1072, %fd1070, %fd1059, %fd1750;
	fma.rn.f64 	%fd1074, %fd1072, %fd1059, %fd1794;
	fma.rn.f64 	%fd1077, %fd1814, %fd1059, %fd1823;
	fma.rn.f64 	%fd1079, %fd1077, %fd1059, %fd563;
	fma.rn.f64 	%fd1081, %fd1079, %fd1059, %fd565;
	fma.rn.f64 	%fd1083, %fd1081, %fd1059, %fd567;
	fma.rn.f64 	%fd1085, %fd1083, %fd1059, %fd569;
	fma.rn.f64 	%fd1087, %fd1085, %fd1059, %fd571;
	fma.rn.f64 	%fd1088, %fd1087, %fd1872, %fd1872;
	and.b32  	%r232, %r335, 1;
	setp.eq.b32	%p110, %r232, 1;
	not.pred 	%p111, %p110;
	selp.f64	%fd1873, %fd1088, %fd1074, %p111;
	and.b32  	%r233, %r335, 2;
	setp.eq.s32	%p112, %r233, 0;
	@%p112 bra 	BB6_107;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r234}, %fd1873;
	}
	xor.b32  	%r235, %r234, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r236, %temp}, %fd1873;
	}
	mov.b64 	%fd1873, {%r236, %r235};

BB6_107:
	abs.f64 	%fd1600, %fd16;
	setp.neu.f64	%p183, %fd1600, 0d7FF0000000000000;
	mul.f64 	%fd163, %fd1871, %fd1873;
	mov.f64 	%fd1907, %fd16;
	@%p183 bra 	BB6_109;

	mul.rn.f64 	%fd1907, %fd16, %fd571;

BB6_109:
	mov.f64 	%fd1603, 0d3FF921FB54442D18;
	mov.f64 	%fd1602, 0d397B839A252049C0;
	mov.f64 	%fd1601, 0d3C91A62633145C00;
	mul.f64 	%fd1090, %fd1907, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r336, %fd1090;
	st.local.u32 	[%rd1], %r336;
	cvt.rn.f64.s32	%fd1091, %r336;
	neg.f64 	%fd1092, %fd1091;
	fma.rn.f64 	%fd1094, %fd1092, %fd1603, %fd1907;
	fma.rn.f64 	%fd1096, %fd1092, %fd1601, %fd1094;
	fma.rn.f64 	%fd1874, %fd1092, %fd1602, %fd1096;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r237}, %fd1907;
	}
	and.b32  	%r238, %r237, 2145386496;
	setp.lt.u32	%p114, %r238, 1105199104;
	@%p114 bra 	BB6_111;

	add.u64 	%rd82, %SP, 0;
	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1907;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd82;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1874, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r336, [%rd1];

BB6_111:
	mov.f64 	%fd1816, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1815, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1784, 0d3FF0000000000000;
	mov.f64 	%fd1751, 0dBFE0000000000000;
	mov.f64 	%fd1732, 0d3FA5555555555551;
	mov.f64 	%fd1692, 0dBF56C16C16C15D47;
	mov.f64 	%fd1675, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1674, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1673, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1652, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1098, %fd1874, %fd1874;
	fma.rn.f64 	%fd1101, %fd1652, %fd1098, %fd1673;
	fma.rn.f64 	%fd1103, %fd1101, %fd1098, %fd1674;
	fma.rn.f64 	%fd1105, %fd1103, %fd1098, %fd1675;
	fma.rn.f64 	%fd1107, %fd1105, %fd1098, %fd1692;
	fma.rn.f64 	%fd1109, %fd1107, %fd1098, %fd1732;
	fma.rn.f64 	%fd1111, %fd1109, %fd1098, %fd1751;
	fma.rn.f64 	%fd1113, %fd1111, %fd1098, %fd1784;
	fma.rn.f64 	%fd1116, %fd1815, %fd1098, %fd1816;
	fma.rn.f64 	%fd1118, %fd1116, %fd1098, %fd563;
	fma.rn.f64 	%fd1120, %fd1118, %fd1098, %fd565;
	fma.rn.f64 	%fd1122, %fd1120, %fd1098, %fd567;
	fma.rn.f64 	%fd1124, %fd1122, %fd1098, %fd569;
	fma.rn.f64 	%fd1126, %fd1124, %fd1098, %fd571;
	fma.rn.f64 	%fd1127, %fd1126, %fd1874, %fd1874;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r239}, %fd1127;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r240, %temp}, %fd1127;
	}
	xor.b32  	%r241, %r239, -2147483648;
	mov.b64 	%fd1128, {%r240, %r241};
	and.b32  	%r242, %r336, 1;
	setp.eq.b32	%p115, %r242, 1;
	not.pred 	%p116, %p115;
	selp.f64	%fd1875, %fd1113, %fd1128, %p116;
	and.b32  	%r243, %r336, 2;
	setp.eq.s32	%p117, %r243, 0;
	@%p117 bra 	BB6_113;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r244}, %fd1875;
	}
	xor.b32  	%r245, %r244, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r246, %temp}, %fd1875;
	}
	mov.b64 	%fd1875, {%r246, %r245};

BB6_113:
	abs.f64 	%fd1558, %fd37;
	setp.neu.f64	%p173, %fd1558, 0d7FF0000000000000;
	mul.f64 	%fd172, %fd163, %fd1875;
	mov.f64 	%fd1939, %fd37;
	@%p173 bra 	BB6_115;

	mul.rn.f64 	%fd1939, %fd37, %fd571;

BB6_115:
	mov.f64 	%fd1604, 0d3FF921FB54442D18;
	mov.f64 	%fd1586, 0d397B839A252049C0;
	mov.f64 	%fd1585, 0d3C91A62633145C00;
	mul.f64 	%fd1130, %fd1939, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r337, %fd1130;
	st.local.u32 	[%rd1], %r337;
	cvt.rn.f64.s32	%fd1131, %r337;
	neg.f64 	%fd1132, %fd1131;
	fma.rn.f64 	%fd1134, %fd1132, %fd1604, %fd1939;
	fma.rn.f64 	%fd1136, %fd1132, %fd1585, %fd1134;
	fma.rn.f64 	%fd1876, %fd1132, %fd1586, %fd1136;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r247}, %fd1939;
	}
	and.b32  	%r248, %r247, 2145386496;
	setp.lt.u32	%p119, %r248, 1105199104;
	@%p119 bra 	BB6_117;

	add.u64 	%rd81, %SP, 0;
	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1939;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd81;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1876, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r337, [%rd1];

BB6_117:
	mov.f64 	%fd1817, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1806, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1785, 0d3FF0000000000000;
	mov.f64 	%fd1734, 0dBFE0000000000000;
	mov.f64 	%fd1733, 0d3FA5555555555551;
	mov.f64 	%fd1677, 0dBF56C16C16C15D47;
	mov.f64 	%fd1676, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1655, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1654, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1653, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1138, %fd1876, %fd1876;
	fma.rn.f64 	%fd1141, %fd1653, %fd1138, %fd1654;
	fma.rn.f64 	%fd1143, %fd1141, %fd1138, %fd1655;
	fma.rn.f64 	%fd1145, %fd1143, %fd1138, %fd1676;
	fma.rn.f64 	%fd1147, %fd1145, %fd1138, %fd1677;
	fma.rn.f64 	%fd1149, %fd1147, %fd1138, %fd1733;
	fma.rn.f64 	%fd1151, %fd1149, %fd1138, %fd1734;
	fma.rn.f64 	%fd1153, %fd1151, %fd1138, %fd1785;
	fma.rn.f64 	%fd1156, %fd1806, %fd1138, %fd1817;
	fma.rn.f64 	%fd1158, %fd1156, %fd1138, %fd563;
	fma.rn.f64 	%fd1160, %fd1158, %fd1138, %fd565;
	fma.rn.f64 	%fd1162, %fd1160, %fd1138, %fd567;
	fma.rn.f64 	%fd1164, %fd1162, %fd1138, %fd569;
	fma.rn.f64 	%fd1166, %fd1164, %fd1138, %fd571;
	fma.rn.f64 	%fd1167, %fd1166, %fd1876, %fd1876;
	and.b32  	%r249, %r337, 1;
	setp.eq.b32	%p120, %r249, 1;
	not.pred 	%p121, %p120;
	selp.f64	%fd1877, %fd1167, %fd1153, %p121;
	and.b32  	%r250, %r337, 2;
	setp.eq.s32	%p122, %r250, 0;
	@%p122 bra 	BB6_119;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r251}, %fd1877;
	}
	xor.b32  	%r252, %r251, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r253, %temp}, %fd1877;
	}
	mov.b64 	%fd1877, {%r253, %r252};

BB6_119:
	abs.f64 	%fd1559, %fd39;
	setp.neu.f64	%p174, %fd1559, 0d7FF0000000000000;
	mov.f64 	%fd1891, %fd39;
	@%p174 bra 	BB6_121;

	mul.rn.f64 	%fd1891, %fd39, %fd571;

BB6_121:
	mov.f64 	%fd1605, 0d3FF921FB54442D18;
	mov.f64 	%fd1588, 0d397B839A252049C0;
	mov.f64 	%fd1587, 0d3C91A62633145C00;
	mul.f64 	%fd1169, %fd1891, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r338, %fd1169;
	st.local.u32 	[%rd1], %r338;
	cvt.rn.f64.s32	%fd1170, %r338;
	neg.f64 	%fd1171, %fd1170;
	fma.rn.f64 	%fd1173, %fd1171, %fd1605, %fd1891;
	fma.rn.f64 	%fd1175, %fd1171, %fd1587, %fd1173;
	fma.rn.f64 	%fd1878, %fd1171, %fd1588, %fd1175;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r254}, %fd1891;
	}
	and.b32  	%r255, %r254, 2145386496;
	setp.lt.u32	%p124, %r255, 1105199104;
	@%p124 bra 	BB6_123;

	add.u64 	%rd80, %SP, 0;
	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1891;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd80;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1878, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r338, [%rd1];

BB6_123:
	mov.f64 	%fd1808, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1807, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1773, 0d3FF0000000000000;
	mov.f64 	%fd1735, 0dBFE0000000000000;
	mov.f64 	%fd1718, 0d3FA5555555555551;
	mov.f64 	%fd1678, 0dBF56C16C16C15D47;
	mov.f64 	%fd1658, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1657, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1656, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1635, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1177, %fd1878, %fd1878;
	fma.rn.f64 	%fd1180, %fd1635, %fd1177, %fd1656;
	fma.rn.f64 	%fd1182, %fd1180, %fd1177, %fd1657;
	fma.rn.f64 	%fd1184, %fd1182, %fd1177, %fd1658;
	fma.rn.f64 	%fd1186, %fd1184, %fd1177, %fd1678;
	fma.rn.f64 	%fd1188, %fd1186, %fd1177, %fd1718;
	fma.rn.f64 	%fd1190, %fd1188, %fd1177, %fd1735;
	fma.rn.f64 	%fd1192, %fd1190, %fd1177, %fd1773;
	fma.rn.f64 	%fd1195, %fd1807, %fd1177, %fd1808;
	fma.rn.f64 	%fd1197, %fd1195, %fd1177, %fd563;
	fma.rn.f64 	%fd1199, %fd1197, %fd1177, %fd565;
	fma.rn.f64 	%fd1201, %fd1199, %fd1177, %fd567;
	fma.rn.f64 	%fd1203, %fd1201, %fd1177, %fd569;
	fma.rn.f64 	%fd1205, %fd1203, %fd1177, %fd571;
	fma.rn.f64 	%fd1206, %fd1205, %fd1878, %fd1878;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r256}, %fd1206;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r257, %temp}, %fd1206;
	}
	xor.b32  	%r258, %r256, -2147483648;
	mov.b64 	%fd1207, {%r257, %r258};
	and.b32  	%r259, %r338, 1;
	setp.eq.b32	%p125, %r259, 1;
	not.pred 	%p126, %p125;
	selp.f64	%fd1879, %fd1192, %fd1207, %p126;
	and.b32  	%r260, %r338, 2;
	setp.eq.s32	%p127, %r260, 0;
	@%p127 bra 	BB6_125;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r261}, %fd1879;
	}
	xor.b32  	%r262, %r261, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r263, %temp}, %fd1879;
	}
	mov.b64 	%fd1879, {%r263, %r262};

BB6_125:
	abs.f64 	%fd1589, %fd16;
	setp.neu.f64	%p180, %fd1589, 0d7FF0000000000000;
	mul.f64 	%fd189, %fd1877, %fd1879;
	mov.f64 	%fd1906, %fd16;
	@%p180 bra 	BB6_127;

	mul.rn.f64 	%fd1906, %fd16, %fd571;

BB6_127:
	mov.f64 	%fd1592, 0d3FF921FB54442D18;
	mov.f64 	%fd1591, 0d397B839A252049C0;
	mov.f64 	%fd1590, 0d3C91A62633145C00;
	mul.f64 	%fd1209, %fd1906, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r339, %fd1209;
	st.local.u32 	[%rd1], %r339;
	cvt.rn.f64.s32	%fd1210, %r339;
	neg.f64 	%fd1211, %fd1210;
	fma.rn.f64 	%fd1213, %fd1211, %fd1592, %fd1906;
	fma.rn.f64 	%fd1215, %fd1211, %fd1590, %fd1213;
	fma.rn.f64 	%fd1880, %fd1211, %fd1591, %fd1215;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r264}, %fd1906;
	}
	and.b32  	%r265, %r264, 2145386496;
	setp.lt.u32	%p129, %r265, 1105199104;
	@%p129 bra 	BB6_129;

	add.u64 	%rd79, %SP, 0;
	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1906;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd79;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1880, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r339, [%rd1];

BB6_129:
	mov.f64 	%fd1809, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1795, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1774, 0d3FF0000000000000;
	mov.f64 	%fd1720, 0dBFE0000000000000;
	mov.f64 	%fd1719, 0d3FA5555555555551;
	mov.f64 	%fd1660, 0dBF56C16C16C15D47;
	mov.f64 	%fd1659, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1638, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1637, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1636, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1217, %fd1880, %fd1880;
	fma.rn.f64 	%fd1220, %fd1636, %fd1217, %fd1637;
	fma.rn.f64 	%fd1222, %fd1220, %fd1217, %fd1638;
	fma.rn.f64 	%fd1224, %fd1222, %fd1217, %fd1659;
	fma.rn.f64 	%fd1226, %fd1224, %fd1217, %fd1660;
	fma.rn.f64 	%fd1228, %fd1226, %fd1217, %fd1719;
	fma.rn.f64 	%fd1230, %fd1228, %fd1217, %fd1720;
	fma.rn.f64 	%fd1232, %fd1230, %fd1217, %fd1774;
	fma.rn.f64 	%fd1235, %fd1795, %fd1217, %fd1809;
	fma.rn.f64 	%fd1237, %fd1235, %fd1217, %fd563;
	fma.rn.f64 	%fd1239, %fd1237, %fd1217, %fd565;
	fma.rn.f64 	%fd1241, %fd1239, %fd1217, %fd567;
	fma.rn.f64 	%fd1243, %fd1241, %fd1217, %fd569;
	fma.rn.f64 	%fd1245, %fd1243, %fd1217, %fd571;
	fma.rn.f64 	%fd1246, %fd1245, %fd1880, %fd1880;
	and.b32  	%r266, %r339, 1;
	setp.eq.b32	%p130, %r266, 1;
	not.pred 	%p131, %p130;
	selp.f64	%fd1881, %fd1246, %fd1232, %p131;
	and.b32  	%r267, %r339, 2;
	setp.eq.s32	%p132, %r267, 0;
	@%p132 bra 	BB6_131;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r268}, %fd1881;
	}
	xor.b32  	%r269, %r268, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r270, %temp}, %fd1881;
	}
	mov.b64 	%fd1881, {%r270, %r269};

BB6_131:
	abs.f64 	%fd1526, %fd37;
	setp.neu.f64	%p170, %fd1526, 0d7FF0000000000000;
	mul.f64 	%fd1247, %fd189, %fd1881;
	sub.f64 	%fd198, %fd172, %fd1247;
	mov.f64 	%fd1938, %fd37;
	@%p170 bra 	BB6_133;

	mul.rn.f64 	%fd1938, %fd37, %fd571;

BB6_133:
	mov.f64 	%fd1593, 0d3FF921FB54442D18;
	mov.f64 	%fd1563, 0d397B839A252049C0;
	mov.f64 	%fd1562, 0d3C91A62633145C00;
	mul.f64 	%fd1249, %fd1938, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r340, %fd1249;
	st.local.u32 	[%rd1], %r340;
	cvt.rn.f64.s32	%fd1250, %r340;
	neg.f64 	%fd1251, %fd1250;
	fma.rn.f64 	%fd1253, %fd1251, %fd1593, %fd1938;
	fma.rn.f64 	%fd1255, %fd1251, %fd1562, %fd1253;
	fma.rn.f64 	%fd1882, %fd1251, %fd1563, %fd1255;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r271}, %fd1938;
	}
	and.b32  	%r272, %r271, 2145386496;
	setp.lt.u32	%p134, %r272, 1105199104;
	@%p134 bra 	BB6_135;

	add.u64 	%rd78, %SP, 0;
	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1938;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd78;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1882, [retval0+0];
	
	//{
	}// Callseq End 18
	ld.local.u32 	%r340, [%rd1];

BB6_135:
	mov.f64 	%fd1797, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1796, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1765, 0d3FF0000000000000;
	mov.f64 	%fd1721, 0dBFE0000000000000;
	mov.f64 	%fd1705, 0d3FA5555555555551;
	mov.f64 	%fd1661, 0dBF56C16C16C15D47;
	mov.f64 	%fd1641, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1640, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1639, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1606, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1257, %fd1882, %fd1882;
	fma.rn.f64 	%fd1260, %fd1606, %fd1257, %fd1639;
	fma.rn.f64 	%fd1262, %fd1260, %fd1257, %fd1640;
	fma.rn.f64 	%fd1264, %fd1262, %fd1257, %fd1641;
	fma.rn.f64 	%fd1266, %fd1264, %fd1257, %fd1661;
	fma.rn.f64 	%fd1268, %fd1266, %fd1257, %fd1705;
	fma.rn.f64 	%fd1270, %fd1268, %fd1257, %fd1721;
	fma.rn.f64 	%fd1272, %fd1270, %fd1257, %fd1765;
	fma.rn.f64 	%fd1275, %fd1796, %fd1257, %fd1797;
	fma.rn.f64 	%fd1277, %fd1275, %fd1257, %fd563;
	fma.rn.f64 	%fd1279, %fd1277, %fd1257, %fd565;
	fma.rn.f64 	%fd1281, %fd1279, %fd1257, %fd567;
	fma.rn.f64 	%fd1283, %fd1281, %fd1257, %fd569;
	fma.rn.f64 	%fd1285, %fd1283, %fd1257, %fd571;
	fma.rn.f64 	%fd1286, %fd1285, %fd1882, %fd1882;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r273}, %fd1286;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r274, %temp}, %fd1286;
	}
	xor.b32  	%r275, %r273, -2147483648;
	mov.b64 	%fd1287, {%r274, %r275};
	and.b32  	%r276, %r340, 1;
	setp.eq.b32	%p135, %r276, 1;
	not.pred 	%p136, %p135;
	selp.f64	%fd1883, %fd1272, %fd1287, %p136;
	and.b32  	%r277, %r340, 2;
	setp.eq.s32	%p137, %r277, 0;
	@%p137 bra 	BB6_137;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r278}, %fd1883;
	}
	xor.b32  	%r279, %r278, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r280, %temp}, %fd1883;
	}
	mov.b64 	%fd1883, {%r280, %r279};

BB6_137:
	abs.f64 	%fd1527, %fd39;
	setp.neu.f64	%p171, %fd1527, 0d7FF0000000000000;
	mov.f64 	%fd1890, %fd39;
	@%p171 bra 	BB6_139;

	mul.rn.f64 	%fd1890, %fd39, %fd571;

BB6_139:
	mov.f64 	%fd1569, 0d3FF921FB54442D18;
	mov.f64 	%fd1565, 0d397B839A252049C0;
	mov.f64 	%fd1564, 0d3C91A62633145C00;
	mul.f64 	%fd1289, %fd1890, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r341, %fd1289;
	st.local.u32 	[%rd1], %r341;
	cvt.rn.f64.s32	%fd1290, %r341;
	neg.f64 	%fd1291, %fd1290;
	fma.rn.f64 	%fd1293, %fd1291, %fd1569, %fd1890;
	fma.rn.f64 	%fd1295, %fd1291, %fd1564, %fd1293;
	fma.rn.f64 	%fd1897, %fd1291, %fd1565, %fd1295;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r281}, %fd1890;
	}
	and.b32  	%r282, %r281, 2145386496;
	setp.lt.u32	%p139, %r282, 1105199104;
	@%p139 bra 	BB6_141;

	add.u64 	%rd77, %SP, 0;
	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1890;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd77;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1897, [retval0+0];
	
	//{
	}// Callseq End 19
	ld.local.u32 	%r341, [%rd1];

BB6_141:
	mov.f64 	%fd1798, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1786, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1766, 0d3FF0000000000000;
	mov.f64 	%fd1707, 0dBFE0000000000000;
	mov.f64 	%fd1706, 0d3FA5555555555551;
	mov.f64 	%fd1611, 0dBF56C16C16C15D47;
	mov.f64 	%fd1610, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1609, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1608, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1607, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1297, %fd1897, %fd1897;
	fma.rn.f64 	%fd1300, %fd1607, %fd1297, %fd1608;
	fma.rn.f64 	%fd1302, %fd1300, %fd1297, %fd1609;
	fma.rn.f64 	%fd1304, %fd1302, %fd1297, %fd1610;
	fma.rn.f64 	%fd1306, %fd1304, %fd1297, %fd1611;
	fma.rn.f64 	%fd1308, %fd1306, %fd1297, %fd1706;
	fma.rn.f64 	%fd1310, %fd1308, %fd1297, %fd1707;
	fma.rn.f64 	%fd1312, %fd1310, %fd1297, %fd1766;
	fma.rn.f64 	%fd1315, %fd1786, %fd1297, %fd1798;
	fma.rn.f64 	%fd1317, %fd1315, %fd1297, %fd563;
	fma.rn.f64 	%fd1319, %fd1317, %fd1297, %fd565;
	fma.rn.f64 	%fd1321, %fd1319, %fd1297, %fd567;
	fma.rn.f64 	%fd1323, %fd1321, %fd1297, %fd569;
	fma.rn.f64 	%fd1325, %fd1323, %fd1297, %fd571;
	fma.rn.f64 	%fd1326, %fd1325, %fd1897, %fd1897;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r283}, %fd1326;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r284, %temp}, %fd1326;
	}
	xor.b32  	%r285, %r283, -2147483648;
	mov.b64 	%fd1327, {%r284, %r285};
	and.b32  	%r286, %r341, 1;
	setp.eq.b32	%p140, %r286, 1;
	not.pred 	%p141, %p140;
	selp.f64	%fd1898, %fd1312, %fd1327, %p141;
	and.b32  	%r287, %r341, 2;
	setp.eq.s32	%p142, %r287, 0;
	@%p142 bra 	BB6_143;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r288}, %fd1898;
	}
	xor.b32  	%r289, %r288, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r290, %temp}, %fd1898;
	}
	mov.b64 	%fd1898, {%r290, %r289};

BB6_143:
	abs.f64 	%fd1566, %fd16;
	setp.neu.f64	%p177, %fd1566, 0d7FF0000000000000;
	mul.f64 	%fd215, %fd1883, %fd1898;
	mov.f64 	%fd1905, %fd16;
	@%p177 bra 	BB6_145;

	mul.rn.f64 	%fd1905, %fd16, %fd571;

BB6_145:
	mov.f64 	%fd1570, 0d3FF921FB54442D18;
	mov.f64 	%fd1568, 0d397B839A252049C0;
	mov.f64 	%fd1567, 0d3C91A62633145C00;
	mul.f64 	%fd1329, %fd1905, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r342, %fd1329;
	st.local.u32 	[%rd1], %r342;
	cvt.rn.f64.s32	%fd1330, %r342;
	neg.f64 	%fd1331, %fd1330;
	fma.rn.f64 	%fd1333, %fd1331, %fd1570, %fd1905;
	fma.rn.f64 	%fd1335, %fd1331, %fd1567, %fd1333;
	fma.rn.f64 	%fd1912, %fd1331, %fd1568, %fd1335;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r291}, %fd1905;
	}
	and.b32  	%r292, %r291, 2145386496;
	setp.lt.u32	%p144, %r292, 1105199104;
	@%p144 bra 	BB6_147;

	add.u64 	%rd76, %SP, 0;
	// Callseq Start 20
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1905;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd76;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1912, [retval0+0];
	
	//{
	}// Callseq End 20
	ld.local.u32 	%r342, [%rd1];

BB6_147:
	mov.f64 	%fd1788, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1787, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1736, 0d3FF0000000000000;
	mov.f64 	%fd1694, 0dBFE0000000000000;
	mov.f64 	%fd1693, 0d3FA5555555555551;
	mov.f64 	%fd1613, 0dBF56C16C16C15D47;
	mov.f64 	%fd1612, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1573, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1572, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1571, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1337, %fd1912, %fd1912;
	fma.rn.f64 	%fd1340, %fd1571, %fd1337, %fd1572;
	fma.rn.f64 	%fd1342, %fd1340, %fd1337, %fd1573;
	fma.rn.f64 	%fd1344, %fd1342, %fd1337, %fd1612;
	fma.rn.f64 	%fd1346, %fd1344, %fd1337, %fd1613;
	fma.rn.f64 	%fd1348, %fd1346, %fd1337, %fd1693;
	fma.rn.f64 	%fd1350, %fd1348, %fd1337, %fd1694;
	fma.rn.f64 	%fd1352, %fd1350, %fd1337, %fd1736;
	fma.rn.f64 	%fd1355, %fd1787, %fd1337, %fd1788;
	fma.rn.f64 	%fd1357, %fd1355, %fd1337, %fd563;
	fma.rn.f64 	%fd1359, %fd1357, %fd1337, %fd565;
	fma.rn.f64 	%fd1361, %fd1359, %fd1337, %fd567;
	fma.rn.f64 	%fd1363, %fd1361, %fd1337, %fd569;
	fma.rn.f64 	%fd1365, %fd1363, %fd1337, %fd571;
	fma.rn.f64 	%fd1366, %fd1365, %fd1912, %fd1912;
	and.b32  	%r293, %r342, 1;
	setp.eq.b32	%p145, %r293, 1;
	not.pred 	%p146, %p145;
	selp.f64	%fd1913, %fd1366, %fd1352, %p146;
	and.b32  	%r294, %r342, 2;
	setp.eq.s32	%p147, %r294, 0;
	@%p147 bra 	BB6_149;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r295}, %fd1913;
	}
	xor.b32  	%r296, %r295, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r297, %temp}, %fd1913;
	}
	mov.b64 	%fd1913, {%r297, %r296};

BB6_149:
	abs.f64 	%fd1523, %fd37;
	setp.neu.f64	%p168, %fd1523, 0d7FF0000000000000;
	mul.f64 	%fd224, %fd215, %fd1913;
	mov.f64 	%fd1937, %fd37;
	@%p168 bra 	BB6_151;

	mul.rn.f64 	%fd1937, %fd37, %fd571;

BB6_151:
	mov.f64 	%fd1530, 0d397B839A252049C0;
	mov.f64 	%fd1529, 0d3C91A62633145C00;
	mov.f64 	%fd1528, 0d3FF921FB54442D18;
	mul.f64 	%fd1368, %fd1937, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r343, %fd1368;
	st.local.u32 	[%rd1], %r343;
	cvt.rn.f64.s32	%fd1369, %r343;
	neg.f64 	%fd1370, %fd1369;
	fma.rn.f64 	%fd1372, %fd1370, %fd1528, %fd1937;
	fma.rn.f64 	%fd1374, %fd1370, %fd1529, %fd1372;
	fma.rn.f64 	%fd1914, %fd1370, %fd1530, %fd1374;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r298}, %fd1937;
	}
	and.b32  	%r299, %r298, 2145386496;
	setp.lt.u32	%p149, %r299, 1105199104;
	@%p149 bra 	BB6_153;

	add.u64 	%rd75, %SP, 0;
	// Callseq Start 21
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1937;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd75;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1914, [retval0+0];
	
	//{
	}// Callseq End 21
	ld.local.u32 	%r343, [%rd1];

BB6_153:
	mov.f64 	%fd1776, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1775, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1581, 0d3FF0000000000000;
	mov.f64 	%fd1580, 0dBFE0000000000000;
	mov.f64 	%fd1579, 0d3FA5555555555551;
	mov.f64 	%fd1578, 0dBF56C16C16C15D47;
	mov.f64 	%fd1577, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1576, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1575, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1574, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1376, %fd1914, %fd1914;
	fma.rn.f64 	%fd1379, %fd1574, %fd1376, %fd1575;
	fma.rn.f64 	%fd1381, %fd1379, %fd1376, %fd1576;
	fma.rn.f64 	%fd1383, %fd1381, %fd1376, %fd1577;
	fma.rn.f64 	%fd1385, %fd1383, %fd1376, %fd1578;
	fma.rn.f64 	%fd1387, %fd1385, %fd1376, %fd1579;
	fma.rn.f64 	%fd1389, %fd1387, %fd1376, %fd1580;
	fma.rn.f64 	%fd1391, %fd1389, %fd1376, %fd1581;
	fma.rn.f64 	%fd1394, %fd1775, %fd1376, %fd1776;
	fma.rn.f64 	%fd1396, %fd1394, %fd1376, %fd563;
	fma.rn.f64 	%fd1398, %fd1396, %fd1376, %fd565;
	fma.rn.f64 	%fd1400, %fd1398, %fd1376, %fd567;
	fma.rn.f64 	%fd1402, %fd1400, %fd1376, %fd569;
	fma.rn.f64 	%fd1404, %fd1402, %fd1376, %fd571;
	fma.rn.f64 	%fd1405, %fd1404, %fd1914, %fd1914;
	and.b32  	%r300, %r343, 1;
	setp.eq.b32	%p150, %r300, 1;
	not.pred 	%p151, %p150;
	selp.f64	%fd1915, %fd1405, %fd1391, %p151;
	and.b32  	%r301, %r343, 2;
	setp.eq.s32	%p152, %r301, 0;
	@%p152 bra 	BB6_155;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r302}, %fd1915;
	}
	xor.b32  	%r303, %r302, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r304, %temp}, %fd1915;
	}
	mov.b64 	%fd1915, {%r304, %r303};

BB6_155:
	mul.f64 	%fd1916, %fd1845, 0d3FE0000000000000;
	abs.f64 	%fd1524, %fd1916;
	setp.neu.f64	%p169, %fd1524, 0d7FF0000000000000;
	@%p169 bra 	BB6_157;

	mul.rn.f64 	%fd1916, %fd39, %fd571;

BB6_157:
	mov.f64 	%fd1533, 0d397B839A252049C0;
	mov.f64 	%fd1532, 0d3C91A62633145C00;
	mov.f64 	%fd1531, 0d3FF921FB54442D18;
	mul.f64 	%fd1407, %fd1916, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r344, %fd1407;
	st.local.u32 	[%rd1], %r344;
	cvt.rn.f64.s32	%fd1408, %r344;
	neg.f64 	%fd1409, %fd1408;
	fma.rn.f64 	%fd1411, %fd1409, %fd1531, %fd1916;
	fma.rn.f64 	%fd1413, %fd1409, %fd1532, %fd1411;
	fma.rn.f64 	%fd1917, %fd1409, %fd1533, %fd1413;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r305}, %fd1916;
	}
	and.b32  	%r306, %r305, 2145386496;
	setp.lt.u32	%p154, %r306, 1105199104;
	@%p154 bra 	BB6_159;

	add.u64 	%rd74, %SP, 0;
	// Callseq Start 22
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1916;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd74;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1917, [retval0+0];
	
	//{
	}// Callseq End 22
	ld.local.u32 	%r344, [%rd1];

BB6_159:
	mov.f64 	%fd1779, 0d3EC71DE369ACE392;
	mov.f64 	%fd1778, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1777, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1582, 0d3FF0000000000000;
	mov.f64 	%fd1545, 0dBFE0000000000000;
	mov.f64 	%fd1544, 0d3FA5555555555551;
	mov.f64 	%fd1543, 0dBF56C16C16C15D47;
	mov.f64 	%fd1542, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1541, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1540, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1539, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1415, %fd1917, %fd1917;
	fma.rn.f64 	%fd1418, %fd1539, %fd1415, %fd1540;
	fma.rn.f64 	%fd1420, %fd1418, %fd1415, %fd1541;
	fma.rn.f64 	%fd1422, %fd1420, %fd1415, %fd1542;
	fma.rn.f64 	%fd1424, %fd1422, %fd1415, %fd1543;
	fma.rn.f64 	%fd1426, %fd1424, %fd1415, %fd1544;
	fma.rn.f64 	%fd1428, %fd1426, %fd1415, %fd1545;
	fma.rn.f64 	%fd1430, %fd1428, %fd1415, %fd1582;
	fma.rn.f64 	%fd1433, %fd1777, %fd1415, %fd1778;
	fma.rn.f64 	%fd1435, %fd1433, %fd1415, %fd1779;
	fma.rn.f64 	%fd1437, %fd1435, %fd1415, %fd565;
	fma.rn.f64 	%fd1439, %fd1437, %fd1415, %fd567;
	fma.rn.f64 	%fd1441, %fd1439, %fd1415, %fd569;
	fma.rn.f64 	%fd1443, %fd1441, %fd1415, %fd571;
	fma.rn.f64 	%fd1444, %fd1443, %fd1917, %fd1917;
	and.b32  	%r307, %r344, 1;
	setp.eq.b32	%p155, %r307, 1;
	not.pred 	%p156, %p155;
	selp.f64	%fd1918, %fd1444, %fd1430, %p156;
	and.b32  	%r308, %r344, 2;
	setp.eq.s32	%p157, %r308, 0;
	@%p157 bra 	BB6_161;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r309}, %fd1918;
	}
	xor.b32  	%r310, %r309, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r311, %temp}, %fd1918;
	}
	mov.b64 	%fd1918, {%r311, %r310};

BB6_161:
	mul.f64 	%fd1919, %fd1843, 0dBFE0000000000000;
	abs.f64 	%fd1534, %fd1919;
	setp.neu.f64	%p172, %fd1534, 0d7FF0000000000000;
	mul.f64 	%fd241, %fd1915, %fd1918;
	@%p172 bra 	BB6_163;

	mul.rn.f64 	%fd1919, %fd16, %fd571;

BB6_163:
	mov.f64 	%fd1538, 0d397B839A252049C0;
	mov.f64 	%fd1537, 0d3C91A62633145C00;
	mov.f64 	%fd1536, 0d3FF921FB54442D18;
	mul.f64 	%fd1446, %fd1919, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r345, %fd1446;
	st.local.u32 	[%rd1], %r345;
	cvt.rn.f64.s32	%fd1447, %r345;
	neg.f64 	%fd1448, %fd1447;
	fma.rn.f64 	%fd1450, %fd1448, %fd1536, %fd1919;
	fma.rn.f64 	%fd1452, %fd1448, %fd1537, %fd1450;
	fma.rn.f64 	%fd1920, %fd1448, %fd1538, %fd1452;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r312}, %fd1919;
	}
	and.b32  	%r313, %r312, 2145386496;
	setp.lt.u32	%p159, %r313, 1105199104;
	@%p159 bra 	BB6_165;

	add.u64 	%rd73, %SP, 0;
	// Callseq Start 23
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1919;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd73;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1920, [retval0+0];
	
	//{
	}// Callseq End 23
	ld.local.u32 	%r345, [%rd1];

BB6_165:
	mov.f64 	%fd1557, 0dBF2A01A019DB62A1;
	mov.f64 	%fd1556, 0d3EC71DE369ACE392;
	mov.f64 	%fd1555, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1554, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd1553, 0d3FF0000000000000;
	mov.f64 	%fd1552, 0dBFE0000000000000;
	mov.f64 	%fd1551, 0d3FA5555555555551;
	mov.f64 	%fd1550, 0dBF56C16C16C15D47;
	mov.f64 	%fd1549, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd1548, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd1547, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1546, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1454, %fd1920, %fd1920;
	fma.rn.f64 	%fd1457, %fd1546, %fd1454, %fd1547;
	fma.rn.f64 	%fd1459, %fd1457, %fd1454, %fd1548;
	fma.rn.f64 	%fd1461, %fd1459, %fd1454, %fd1549;
	fma.rn.f64 	%fd1463, %fd1461, %fd1454, %fd1550;
	fma.rn.f64 	%fd1465, %fd1463, %fd1454, %fd1551;
	fma.rn.f64 	%fd1467, %fd1465, %fd1454, %fd1552;
	fma.rn.f64 	%fd1469, %fd1467, %fd1454, %fd1553;
	fma.rn.f64 	%fd1472, %fd1554, %fd1454, %fd1555;
	fma.rn.f64 	%fd1474, %fd1472, %fd1454, %fd1556;
	fma.rn.f64 	%fd1476, %fd1474, %fd1454, %fd1557;
	fma.rn.f64 	%fd1478, %fd1476, %fd1454, %fd567;
	fma.rn.f64 	%fd1480, %fd1478, %fd1454, %fd569;
	fma.rn.f64 	%fd1482, %fd1480, %fd1454, %fd571;
	fma.rn.f64 	%fd1483, %fd1482, %fd1920, %fd1920;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r314}, %fd1483;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r315, %temp}, %fd1483;
	}
	xor.b32  	%r316, %r314, -2147483648;
	mov.b64 	%fd1484, {%r315, %r316};
	and.b32  	%r317, %r345, 1;
	setp.eq.b32	%p160, %r317, 1;
	not.pred 	%p161, %p160;
	selp.f64	%fd1921, %fd1469, %fd1484, %p161;
	and.b32  	%r318, %r345, 2;
	setp.eq.s32	%p162, %r318, 0;
	@%p162 bra 	BB6_167;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r319}, %fd1921;
	}
	xor.b32  	%r320, %r319, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r321, %temp}, %fd1921;
	}
	mov.b64 	%fd1921, {%r321, %r320};

BB6_167:
	fma.rn.f64 	%fd250, %fd241, %fd1921, %fd224;
	neg.f64 	%fd1485, %fd6;
	add.f64 	%fd1486, %fd94, %fd6;
	abs.f64 	%fd1487, %fd1486;
	abs.f64 	%fd1488, %fd1485;
	abs.f64 	%fd1489, %fd94;
	max.f64 	%fd1490, %fd1489, %fd1488;
	mul.f64 	%fd1491, %fd1490, 0d3D83880000000000;
	setp.geu.f64	%p163, %fd1487, %fd1491;
	mov.f64 	%fd1936, %fd37;
	@%p163 bra 	BB6_172;

	neg.f64 	%fd1492, %fd8;
	add.f64 	%fd1493, %fd146, %fd8;
	abs.f64 	%fd1494, %fd1493;
	abs.f64 	%fd1495, %fd1492;
	abs.f64 	%fd1496, %fd146;
	max.f64 	%fd1497, %fd1496, %fd1495;
	mul.f64 	%fd1498, %fd1497, 0d3D83880000000000;
	setp.geu.f64	%p164, %fd1494, %fd1498;
	mov.f64 	%fd1936, %fd37;
	@%p164 bra 	BB6_172;

	neg.f64 	%fd1499, %fd7;
	add.f64 	%fd1500, %fd198, %fd7;
	abs.f64 	%fd1501, %fd1500;
	abs.f64 	%fd1502, %fd1499;
	abs.f64 	%fd1503, %fd198;
	max.f64 	%fd1504, %fd1503, %fd1502;
	mul.f64 	%fd1505, %fd1504, 0d3D83880000000000;
	setp.geu.f64	%p165, %fd1501, %fd1505;
	mov.f64 	%fd1936, %fd37;
	@%p165 bra 	BB6_172;

	neg.f64 	%fd1506, %fd9;
	add.f64 	%fd1507, %fd250, %fd9;
	abs.f64 	%fd1508, %fd1507;
	abs.f64 	%fd1509, %fd1506;
	abs.f64 	%fd1510, %fd250;
	max.f64 	%fd1511, %fd1510, %fd1509;
	mul.f64 	%fd1512, %fd1511, 0d3D83880000000000;
	setp.geu.f64	%p166, %fd1508, %fd1512;
	mov.f64 	%fd1936, %fd37;
	@%p166 bra 	BB6_172;

	setp.ltu.f64	%p167, %fd37, 0d0000000000000000;
	selp.f64	%fd1513, 0dC00921FB54442D18, 0d400921FB54442D18, %p167;
	sub.f64 	%fd1936, %fd37, %fd1513;

BB6_172:
	ld.param.f64 	%fd1522, [_Z13quaternionarg7double4_param_0+8];
	mul.f64 	%fd1521, %fd1522, %fd1522;
	ld.param.f64 	%fd1520, [_Z13quaternionarg7double4_param_0];
	fma.rn.f64 	%fd1519, %fd1520, %fd1520, %fd1521;
	ld.param.f64 	%fd1518, [_Z13quaternionarg7double4_param_0+16];
	fma.rn.f64 	%fd1517, %fd1518, %fd1518, %fd1519;
	ld.param.f64 	%fd1516, [_Z13quaternionarg7double4_param_0+24];
	fma.rn.f64 	%fd1515, %fd1516, %fd1516, %fd1517;
	sqrt.rn.f64 	%fd1514, %fd1515;
	mov.f64 	%fd1949, %fd16;
	mov.f64 	%fd1948, %fd39;
	mov.f64 	%fd1947, %fd1936;
	mov.f64 	%fd1946, %fd1514;

BB6_173:
	st.param.f64	[func_retval0+0], %fd1946;
	st.param.f64	[func_retval0+8], %fd1947;
	st.param.f64	[func_retval0+16], %fd1948;
	st.param.f64	[func_retval0+24], %fd1949;
	ret;
}

	// .globl	_Z5qnorm7double4
.visible .func  (.param .b64 func_retval0) _Z5qnorm7double4(
	.param .align 16 .b8 _Z5qnorm7double4_param_0[32]
)
{
	.reg .f64 	%fd<10>;


	ld.param.f64 	%fd1, [_Z5qnorm7double4_param_0+24];
	ld.param.f64 	%fd2, [_Z5qnorm7double4_param_0+16];
	ld.param.f64 	%fd3, [_Z5qnorm7double4_param_0];
	ld.param.f64 	%fd4, [_Z5qnorm7double4_param_0+8];
	mul.f64 	%fd5, %fd4, %fd4;
	fma.rn.f64 	%fd6, %fd3, %fd3, %fd5;
	fma.rn.f64 	%fd7, %fd2, %fd2, %fd6;
	fma.rn.f64 	%fd8, %fd1, %fd1, %fd7;
	sqrt.rn.f64 	%fd9, %fd8;
	st.param.f64	[func_retval0+0], %fd9;
	ret;
}

	// .globl	_Z4qdiv7double4d
.visible .func  (.param .align 16 .b8 func_retval0[32]) _Z4qdiv7double4d(
	.param .align 16 .b8 _Z4qdiv7double4d_param_0[32],
	.param .b64 _Z4qdiv7double4d_param_1
)
{
	.reg .f64 	%fd<10>;


	ld.param.f64 	%fd1, [_Z4qdiv7double4d_param_0+24];
	ld.param.f64 	%fd2, [_Z4qdiv7double4d_param_0+16];
	ld.param.f64 	%fd3, [_Z4qdiv7double4d_param_0+8];
	ld.param.f64 	%fd4, [_Z4qdiv7double4d_param_0];
	ld.param.f64 	%fd5, [_Z4qdiv7double4d_param_1];
	div.rn.f64 	%fd6, %fd4, %fd5;
	div.rn.f64 	%fd7, %fd3, %fd5;
	div.rn.f64 	%fd8, %fd2, %fd5;
	div.rn.f64 	%fd9, %fd1, %fd5;
	st.param.f64	[func_retval0+0], %fd6;
	st.param.f64	[func_retval0+8], %fd7;
	st.param.f64	[func_retval0+16], %fd8;
	st.param.f64	[func_retval0+24], %fd9;
	ret;
}

	// .globl	_Z4qmul7double4S_
.visible .func  (.param .align 16 .b8 func_retval0[32]) _Z4qmul7double4S_(
	.param .align 16 .b8 _Z4qmul7double4S__param_0[32],
	.param .align 16 .b8 _Z4qmul7double4S__param_1[32]
)
{
	.reg .f64 	%fd<31>;


	ld.param.f64 	%fd1, [_Z4qmul7double4S__param_0+24];
	ld.param.f64 	%fd2, [_Z4qmul7double4S__param_0+16];
	ld.param.f64 	%fd3, [_Z4qmul7double4S__param_0+8];
	ld.param.f64 	%fd4, [_Z4qmul7double4S__param_0];
	ld.param.f64 	%fd5, [_Z4qmul7double4S__param_1+24];
	ld.param.f64 	%fd6, [_Z4qmul7double4S__param_1+16];
	ld.param.f64 	%fd7, [_Z4qmul7double4S__param_1+8];
	ld.param.f64 	%fd8, [_Z4qmul7double4S__param_1];
	mul.f64 	%fd9, %fd4, %fd8;
	mul.f64 	%fd10, %fd3, %fd7;
	sub.f64 	%fd11, %fd9, %fd10;
	mul.f64 	%fd12, %fd2, %fd6;
	sub.f64 	%fd13, %fd11, %fd12;
	mul.f64 	%fd14, %fd1, %fd5;
	sub.f64 	%fd15, %fd13, %fd14;
	mul.f64 	%fd16, %fd4, %fd7;
	fma.rn.f64 	%fd17, %fd3, %fd8, %fd16;
	mul.f64 	%fd18, %fd1, %fd6;
	sub.f64 	%fd19, %fd17, %fd18;
	fma.rn.f64 	%fd20, %fd2, %fd5, %fd19;
	mul.f64 	%fd21, %fd1, %fd7;
	fma.rn.f64 	%fd22, %fd2, %fd8, %fd21;
	fma.rn.f64 	%fd23, %fd4, %fd6, %fd22;
	mul.f64 	%fd24, %fd3, %fd5;
	sub.f64 	%fd25, %fd23, %fd24;
	mul.f64 	%fd26, %fd1, %fd8;
	mul.f64 	%fd27, %fd2, %fd7;
	sub.f64 	%fd28, %fd26, %fd27;
	fma.rn.f64 	%fd29, %fd3, %fd6, %fd28;
	fma.rn.f64 	%fd30, %fd4, %fd5, %fd29;
	st.param.f64	[func_retval0+0], %fd15;
	st.param.f64	[func_retval0+8], %fd20;
	st.param.f64	[func_retval0+16], %fd25;
	st.param.f64	[func_retval0+24], %fd30;
	ret;
}

	// .globl	_Z4comp7double4S_
.visible .func  (.param .b32 func_retval0) _Z4comp7double4S_(
	.param .align 16 .b8 _Z4comp7double4S__param_0[32],
	.param .align 16 .b8 _Z4comp7double4S__param_1[32]
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<33>;


	ld.param.f64 	%fd4, [_Z4comp7double4S__param_0+24];
	ld.param.f64 	%fd3, [_Z4comp7double4S__param_0+16];
	ld.param.f64 	%fd2, [_Z4comp7double4S__param_0+8];
	ld.param.f64 	%fd1, [_Z4comp7double4S__param_0];
	ld.param.f64 	%fd8, [_Z4comp7double4S__param_1+24];
	ld.param.f64 	%fd7, [_Z4comp7double4S__param_1+16];
	ld.param.f64 	%fd6, [_Z4comp7double4S__param_1+8];
	ld.param.f64 	%fd5, [_Z4comp7double4S__param_1];
	sub.f64 	%fd9, %fd1, %fd5;
	abs.f64 	%fd10, %fd9;
	abs.f64 	%fd11, %fd1;
	abs.f64 	%fd12, %fd5;
	max.f64 	%fd13, %fd11, %fd12;
	mul.f64 	%fd14, %fd13, 0d3D83880000000000;
	mov.u16 	%rs6, 0;
	setp.geu.f64	%p1, %fd10, %fd14;
	@%p1 bra 	BB10_4;

	sub.f64 	%fd15, %fd2, %fd6;
	abs.f64 	%fd16, %fd15;
	abs.f64 	%fd17, %fd6;
	abs.f64 	%fd18, %fd2;
	max.f64 	%fd19, %fd18, %fd17;
	mul.f64 	%fd20, %fd19, 0d3D83880000000000;
	setp.geu.f64	%p2, %fd16, %fd20;
	@%p2 bra 	BB10_4;

	sub.f64 	%fd21, %fd3, %fd7;
	abs.f64 	%fd22, %fd21;
	abs.f64 	%fd23, %fd7;
	abs.f64 	%fd24, %fd3;
	max.f64 	%fd25, %fd24, %fd23;
	mul.f64 	%fd26, %fd25, 0d3D83880000000000;
	setp.geu.f64	%p3, %fd22, %fd26;
	@%p3 bra 	BB10_4;

	sub.f64 	%fd27, %fd4, %fd8;
	abs.f64 	%fd28, %fd27;
	abs.f64 	%fd29, %fd8;
	abs.f64 	%fd30, %fd4;
	max.f64 	%fd31, %fd30, %fd29;
	mul.f64 	%fd32, %fd31, 0d3D83880000000000;
	setp.lt.f64	%p4, %fd28, %fd32;
	selp.u16	%rs6, 1, 0, %p4;

BB10_4:
	cvt.u32.u16	%r1, %rs6;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .globl	_Z4qmul7double4d
.visible .func  (.param .align 16 .b8 func_retval0[32]) _Z4qmul7double4d(
	.param .align 16 .b8 _Z4qmul7double4d_param_0[32],
	.param .b64 _Z4qmul7double4d_param_1
)
{
	.reg .f64 	%fd<10>;


	ld.param.f64 	%fd1, [_Z4qmul7double4d_param_0+24];
	ld.param.f64 	%fd2, [_Z4qmul7double4d_param_0+16];
	ld.param.f64 	%fd3, [_Z4qmul7double4d_param_0+8];
	ld.param.f64 	%fd4, [_Z4qmul7double4d_param_0];
	ld.param.f64 	%fd5, [_Z4qmul7double4d_param_1];
	mul.f64 	%fd6, %fd4, %fd5;
	mul.f64 	%fd7, %fd3, %fd5;
	mul.f64 	%fd8, %fd2, %fd5;
	mul.f64 	%fd9, %fd1, %fd5;
	st.param.f64	[func_retval0+0], %fd6;
	st.param.f64	[func_retval0+8], %fd7;
	st.param.f64	[func_retval0+16], %fd8;
	st.param.f64	[func_retval0+24], %fd9;
	ret;
}

	// .globl	_Z13quaternionarg6float4
.visible .func  (.param .align 16 .b8 func_retval0[16]) _Z13quaternionarg6float4(
	.param .align 16 .b8 _Z13quaternionarg6float4_param_0[16]
)
{
	.local .align 4 .b8 	__local_depot12[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<353>;
	.reg .f32 	%f<1349>;
	.reg .b32 	%r<2237>;
	.reg .f64 	%fd<17>;
	.reg .b64 	%rd<244>;


	mov.u64 	%rd243, __local_depot12;
	cvta.local.u64 	%SP, %rd243;
	ld.param.f32 	%f4, [_Z13quaternionarg6float4_param_0+12];
	ld.param.f32 	%f3, [_Z13quaternionarg6float4_param_0+8];
	ld.param.f32 	%f1, [_Z13quaternionarg6float4_param_0];
	ld.param.f32 	%f2, [_Z13quaternionarg6float4_param_0+4];
	add.u64 	%rd123, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd123;
	mul.f32 	%f478, %f2, %f2;
	fma.rn.f32 	%f479, %f1, %f1, %f478;
	fma.rn.f32 	%f480, %f3, %f3, %f479;
	fma.rn.f32 	%f481, %f4, %f4, %f480;
	sqrt.rn.f32 	%f5, %f481;
	setp.eq.f32	%p1, %f5, 0f00000000;
	mov.f32 	%f1348, 0f00000000;
	mov.f32 	%f1347, %f1348;
	mov.f32 	%f1346, %f1348;
	mov.f32 	%f1345, %f1348;
	@%p1 bra 	BB12_554;

	div.rn.f32 	%f6, %f1, %f5;
	div.rn.f32 	%f7, %f3, %f5;
	div.rn.f32 	%f8, %f2, %f5;
	mul.f32 	%f483, %f8, %f7;
	div.rn.f32 	%f9, %f4, %f5;
	mul.f32 	%f484, %f6, %f9;
	sub.f32 	%f485, %f483, %f484;
	add.f32 	%f10, %f485, %f485;
	setp.gt.f32	%p2, %f10, 0f3F800000;
	mov.f32 	%f482, 0f3F800000;
	mov.f32 	%f1192, %f482;
	@%p2 bra 	BB12_3;

	setp.lt.f32	%p3, %f10, 0fBF800000;
	selp.f32	%f11, 0fBF800000, %f10, %p3;
	mov.f32 	%f1192, %f11;

BB12_3:
	mov.f32 	%f12, %f1192;
	abs.f32 	%f486, %f12;
	sub.f32 	%f488, %f482, %f486;
	mul.f32 	%f489, %f488, 0f3F000000;
	sqrt.rn.f32 	%f490, %f489;
	setp.gt.f32	%p4, %f486, 0f3F11EB85;
	selp.f32	%f491, %f490, %f486, %p4;
	mul.f32 	%f492, %f491, %f491;
	mov.f32 	%f493, 0f3C94D2E9;
	mov.f32 	%f494, 0f3D53F941;
	fma.rn.f32 	%f495, %f494, %f492, %f493;
	mov.f32 	%f496, 0f3D3F841F;
	fma.rn.f32 	%f497, %f495, %f492, %f496;
	mov.f32 	%f498, 0f3D994929;
	fma.rn.f32 	%f499, %f497, %f492, %f498;
	mov.f32 	%f500, 0f3E2AAB94;
	fma.rn.f32 	%f501, %f499, %f492, %f500;
	mul.f32 	%f502, %f492, %f501;
	fma.rn.f32 	%f503, %f502, %f491, %f491;
	mov.f32 	%f504, 0f3FC90FDB;
	mov.f32 	%f505, 0fC0000000;
	fma.rn.f32 	%f506, %f505, %f503, %f504;
	selp.f32	%f507, %f506, %f503, %p4;
	setp.gtu.f32	%p5, %f507, 0f7F800000;
	mov.b32 	 %r883, %f507;
	mov.b32 	 %r884, %f12;
	and.b32  	%r885, %r884, -2147483648;
	or.b32  	%r886, %r883, %r885;
	mov.b32 	 %f508, %r886;
	selp.f32	%f509, %f507, %f508, %p5;
	mul.f32 	%f13, %f509, 0fBF000000;
	cvt.f64.f32	%fd1, %f13;
	setp.neu.f64	%p6, %fd1, 0d3FE921FB54442D18;
	setp.neu.f64	%p7, %fd1, 0dBFE921FB54442D18;
	and.pred  	%p8, %p6, %p7;
	mul.f32 	%f510, %f8, %f8;
	mul.f32 	%f14, %f6, %f6;
	sub.f32 	%f15, %f14, %f510;
	@%p8 bra 	BB12_9;
	bra.uni 	BB12_4;

BB12_9:
	fma.rn.f32 	%f548, %f7, %f7, %f15;
	mul.f32 	%f21, %f9, %f9;
	sub.f32 	%f549, %f548, %f21;
	mul.f32 	%f550, %f6, %f8;
	fma.rn.f32 	%f551, %f6, %f8, %f550;
	fma.rn.f32 	%f552, %f9, %f7, %f551;
	fma.rn.f32 	%f553, %f7, %f9, %f552;
	abs.f32 	%f22, %f549;
	abs.f32 	%f23, %f553;
	setp.eq.f32	%p18, %f22, 0f00000000;
	setp.eq.f32	%p19, %f23, 0f00000000;
	and.pred  	%p20, %p18, %p19;
	mov.b32 	 %r3, %f549;
	mov.b32 	 %r897, %f553;
	and.b32  	%r4, %r897, -2147483648;
	@%p20 bra 	BB12_13;
	bra.uni 	BB12_10;

BB12_13:
	shr.s32 	%r904, %r3, 31;
	and.b32  	%r905, %r904, 1078530011;
	or.b32  	%r906, %r905, %r4;
	mov.b32 	 %f1193, %r906;
	bra.uni 	BB12_14;

BB12_4:
	mul.f32 	%f511, %f7, %f7;
	sub.f32 	%f512, %f15, %f511;
	fma.rn.f32 	%f513, %f9, %f9, %f512;
	mul.f32 	%f514, %f6, %f7;
	mul.f32 	%f515, %f8, %f9;
	sub.f32 	%f516, %f514, %f515;
	fma.rn.f32 	%f517, %f6, %f7, %f516;
	sub.f32 	%f518, %f517, %f515;
	abs.f32 	%f16, %f513;
	abs.f32 	%f17, %f518;
	setp.eq.f32	%p9, %f16, 0f00000000;
	setp.eq.f32	%p10, %f17, 0f00000000;
	and.pred  	%p11, %p9, %p10;
	mov.b32 	 %r1, %f513;
	mov.b32 	 %r887, %f518;
	and.b32  	%r2, %r887, -2147483648;
	@%p11 bra 	BB12_8;
	bra.uni 	BB12_5;

BB12_8:
	shr.s32 	%r894, %r1, 31;
	and.b32  	%r895, %r894, 1078530011;
	or.b32  	%r896, %r2, %r895;
	mov.b32 	 %f1194, %r896;
	mov.f32 	%f1344, 0f00000000;
	bra.uni 	BB12_20;

BB12_10:
	setp.eq.f32	%p21, %f22, 0f7F800000;
	setp.eq.f32	%p22, %f23, 0f7F800000;
	and.pred  	%p23, %p21, %p22;
	@%p23 bra 	BB12_12;
	bra.uni 	BB12_11;

BB12_12:
	shr.s32 	%r900, %r3, 31;
	and.b32  	%r901, %r900, 13483017;
	add.s32 	%r902, %r901, 1061752795;
	or.b32  	%r903, %r902, %r4;
	mov.b32 	 %f1193, %r903;
	bra.uni 	BB12_14;

BB12_5:
	setp.eq.f32	%p12, %f16, 0f7F800000;
	setp.eq.f32	%p13, %f17, 0f7F800000;
	and.pred  	%p14, %p12, %p13;
	@%p14 bra 	BB12_7;
	bra.uni 	BB12_6;

BB12_7:
	shr.s32 	%r890, %r1, 31;
	and.b32  	%r891, %r890, 13483017;
	add.s32 	%r892, %r891, 1061752795;
	or.b32  	%r893, %r892, %r2;
	mov.b32 	 %f1194, %r893;
	mov.f32 	%f1344, 0f00000000;
	bra.uni 	BB12_20;

BB12_11:
	max.f32 	%f554, %f23, %f22;
	min.f32 	%f555, %f23, %f22;
	div.rn.f32 	%f556, %f555, %f554;
	mul.rn.f32 	%f557, %f556, %f556;
	mov.f32 	%f558, 0fC0B59883;
	mov.f32 	%f559, 0fBF52C7EA;
	fma.rn.f32 	%f560, %f557, %f559, %f558;
	mov.f32 	%f561, 0fC0D21907;
	fma.rn.f32 	%f562, %f560, %f557, %f561;
	mul.f32 	%f563, %f557, %f562;
	mul.f32 	%f564, %f556, %f563;
	add.f32 	%f565, %f557, 0f41355DC0;
	mov.f32 	%f566, 0f41E6BD60;
	fma.rn.f32 	%f567, %f565, %f557, %f566;
	mov.f32 	%f568, 0f419D92C8;
	fma.rn.f32 	%f569, %f567, %f557, %f568;
	rcp.rn.f32 	%f570, %f569;
	fma.rn.f32 	%f571, %f564, %f570, %f556;
	sub.f32 	%f573, %f504, %f571;
	setp.gt.f32	%p24, %f23, %f22;
	selp.f32	%f574, %f573, %f571, %p24;
	mov.f32 	%f575, 0f40490FDB;
	sub.f32 	%f576, %f575, %f574;
	setp.lt.s32	%p25, %r3, 0;
	selp.f32	%f577, %f576, %f574, %p25;
	mov.b32 	 %r898, %f577;
	or.b32  	%r899, %r898, %r4;
	mov.b32 	 %f578, %r899;
	add.f32 	%f579, %f22, %f23;
	setp.gtu.f32	%p26, %f579, 0f7F800000;
	selp.f32	%f1193, %f579, %f578, %p26;

BB12_14:
	mul.f32 	%f1344, %f1193, 0f3F000000;
	fma.rn.f32 	%f580, %f8, %f8, %f14;
	mul.f32 	%f581, %f7, %f7;
	sub.f32 	%f582, %f580, %f581;
	sub.f32 	%f583, %f582, %f21;
	mul.f32 	%f584, %f9, %f8;
	fma.rn.f32 	%f585, %f6, %f7, %f584;
	fma.rn.f32 	%f586, %f6, %f7, %f585;
	add.f32 	%f587, %f586, %f584;
	abs.f32 	%f29, %f583;
	abs.f32 	%f30, %f587;
	setp.eq.f32	%p27, %f29, 0f00000000;
	setp.eq.f32	%p28, %f30, 0f00000000;
	and.pred  	%p29, %p27, %p28;
	mov.b32 	 %r5, %f583;
	mov.b32 	 %r907, %f587;
	and.b32  	%r6, %r907, -2147483648;
	@%p29 bra 	BB12_18;
	bra.uni 	BB12_15;

BB12_18:
	shr.s32 	%r914, %r5, 31;
	and.b32  	%r915, %r914, 1078530011;
	or.b32  	%r916, %r915, %r6;
	mov.b32 	 %f1194, %r916;
	bra.uni 	BB12_19;

BB12_15:
	setp.eq.f32	%p30, %f29, 0f7F800000;
	setp.eq.f32	%p31, %f30, 0f7F800000;
	and.pred  	%p32, %p30, %p31;
	@%p32 bra 	BB12_17;
	bra.uni 	BB12_16;

BB12_17:
	shr.s32 	%r910, %r5, 31;
	and.b32  	%r911, %r910, 13483017;
	add.s32 	%r912, %r911, 1061752795;
	or.b32  	%r913, %r912, %r6;
	mov.b32 	 %f1194, %r913;

BB12_19:
	bra.uni 	BB12_20;

BB12_6:
	max.f32 	%f520, %f17, %f16;
	min.f32 	%f521, %f17, %f16;
	div.rn.f32 	%f522, %f521, %f520;
	mul.rn.f32 	%f523, %f522, %f522;
	mov.f32 	%f524, 0fC0B59883;
	mov.f32 	%f525, 0fBF52C7EA;
	fma.rn.f32 	%f526, %f523, %f525, %f524;
	mov.f32 	%f527, 0fC0D21907;
	fma.rn.f32 	%f528, %f526, %f523, %f527;
	mul.f32 	%f529, %f523, %f528;
	mul.f32 	%f530, %f522, %f529;
	add.f32 	%f531, %f523, 0f41355DC0;
	mov.f32 	%f532, 0f41E6BD60;
	fma.rn.f32 	%f533, %f531, %f523, %f532;
	mov.f32 	%f534, 0f419D92C8;
	fma.rn.f32 	%f535, %f533, %f523, %f534;
	rcp.rn.f32 	%f536, %f535;
	fma.rn.f32 	%f537, %f530, %f536, %f522;
	sub.f32 	%f539, %f504, %f537;
	setp.gt.f32	%p15, %f17, %f16;
	selp.f32	%f540, %f539, %f537, %p15;
	mov.f32 	%f541, 0f40490FDB;
	sub.f32 	%f542, %f541, %f540;
	setp.lt.s32	%p16, %r1, 0;
	selp.f32	%f543, %f542, %f540, %p16;
	mov.b32 	 %r888, %f543;
	or.b32  	%r889, %r888, %r2;
	mov.b32 	 %f544, %r889;
	add.f32 	%f545, %f16, %f17;
	setp.gtu.f32	%p17, %f545, 0f7F800000;
	selp.f32	%f1194, %f545, %f544, %p17;
	mov.f32 	%f1344, 0f00000000;
	bra.uni 	BB12_20;

BB12_16:
	max.f32 	%f588, %f30, %f29;
	min.f32 	%f589, %f30, %f29;
	div.rn.f32 	%f590, %f589, %f588;
	mul.rn.f32 	%f591, %f590, %f590;
	mov.f32 	%f592, 0fC0B59883;
	mov.f32 	%f593, 0fBF52C7EA;
	fma.rn.f32 	%f594, %f591, %f593, %f592;
	mov.f32 	%f595, 0fC0D21907;
	fma.rn.f32 	%f596, %f594, %f591, %f595;
	mul.f32 	%f597, %f591, %f596;
	mul.f32 	%f598, %f590, %f597;
	add.f32 	%f599, %f591, 0f41355DC0;
	mov.f32 	%f600, 0f41E6BD60;
	fma.rn.f32 	%f601, %f599, %f591, %f600;
	mov.f32 	%f602, 0f419D92C8;
	fma.rn.f32 	%f603, %f601, %f591, %f602;
	rcp.rn.f32 	%f604, %f603;
	fma.rn.f32 	%f605, %f598, %f604, %f590;
	sub.f32 	%f607, %f504, %f605;
	setp.gt.f32	%p33, %f30, %f29;
	selp.f32	%f608, %f607, %f605, %p33;
	mov.f32 	%f609, 0f40490FDB;
	sub.f32 	%f610, %f609, %f608;
	setp.lt.s32	%p34, %r5, 0;
	selp.f32	%f611, %f610, %f608, %p34;
	mov.b32 	 %r908, %f611;
	or.b32  	%r909, %r908, %r6;
	mov.b32 	 %f612, %r909;
	add.f32 	%f613, %f29, %f30;
	setp.gtu.f32	%p35, %f613, 0f7F800000;
	selp.f32	%f1194, %f613, %f612, %p35;

BB12_20:
	mov.f32 	%f34, %f1344;
	mul.f32 	%f36, %f1194, 0f3F000000;
	abs.f32 	%f37, %f34;
	setp.neu.f32	%p36, %f37, 0f7F800000;
	mov.f32 	%f1343, %f34;
	@%p36 bra 	BB12_22;

	mov.f32 	%f614, 0f00000000;
	mul.rn.f32 	%f38, %f34, %f614;
	mov.f32 	%f1343, %f38;

BB12_22:
	mov.f32 	%f39, %f1343;
	mul.f32 	%f615, %f39, 0f3F22F983;
	cvt.rni.s32.f32	%r2006, %f615;
	cvt.rn.f32.s32	%f616, %r2006;
	neg.f32 	%f617, %f616;
	mov.f32 	%f618, 0f3FC90FDA;
	fma.rn.f32 	%f619, %f617, %f618, %f39;
	mov.f32 	%f620, 0f33A22168;
	fma.rn.f32 	%f621, %f617, %f620, %f619;
	mov.f32 	%f622, 0f27C234C5;
	fma.rn.f32 	%f1195, %f617, %f622, %f621;
	abs.f32 	%f623, %f39;
	add.s64 	%rd2, %rd1, 24;
	setp.leu.f32	%p37, %f623, 0f47CE4780;
	@%p37 bra 	BB12_32;

	mov.b32 	 %r8, %f39;
	shr.u32 	%r9, %r8, 23;
	bfe.u32 	%r919, %r8, 23, 8;
	add.s32 	%r920, %r919, -128;
	shl.b32 	%r921, %r8, 8;
	or.b32  	%r10, %r921, -2147483648;
	shr.u32 	%r11, %r920, 5;
	mov.u32 	%r1998, 0;
	mov.u64 	%rd172, __cudart_i2opi_f;
	mov.u32 	%r1997, -6;
	mov.u64 	%rd242, %rd1;

BB12_24:
	.pragma "nounroll";
	mov.u64 	%rd4, %rd242;
	ld.const.u32 	%r924, [%rd172];
	// inline asm
	{
	mad.lo.cc.u32   %r922, %r924, %r10, %r1998;
	madc.hi.u32     %r1998, %r924, %r10,  0;
	}
	// inline asm
	st.local.u32 	[%rd4], %r922;
	add.s64 	%rd5, %rd4, 4;
	add.s64 	%rd172, %rd172, 4;
	add.s32 	%r1997, %r1997, 1;
	setp.ne.s32	%p38, %r1997, 0;
	mov.u64 	%rd242, %rd5;
	@%p38 bra 	BB12_24;

	and.b32  	%r16, %r8, -2147483648;
	st.local.u32 	[%rd2], %r1998;
	mov.u32 	%r927, 6;
	sub.s32 	%r928, %r927, %r11;
	mul.wide.s32 	%rd125, %r928, 4;
	add.s64 	%rd7, %rd1, %rd125;
	ld.local.u32 	%r1999, [%rd7];
	ld.local.u32 	%r2000, [%rd7+-4];
	and.b32  	%r19, %r9, 31;
	setp.eq.s32	%p39, %r19, 0;
	@%p39 bra 	BB12_27;

	mov.u32 	%r929, 32;
	sub.s32 	%r930, %r929, %r19;
	shr.u32 	%r931, %r2000, %r930;
	shl.b32 	%r932, %r1999, %r19;
	add.s32 	%r1999, %r931, %r932;
	ld.local.u32 	%r933, [%rd7+-8];
	shr.u32 	%r934, %r933, %r930;
	shl.b32 	%r935, %r2000, %r19;
	add.s32 	%r2000, %r934, %r935;

BB12_27:
	shr.u32 	%r936, %r2000, 30;
	shl.b32 	%r937, %r1999, 2;
	add.s32 	%r2001, %r936, %r937;
	shl.b32 	%r25, %r2000, 2;
	shr.u32 	%r938, %r2001, 31;
	shr.u32 	%r939, %r1999, 30;
	add.s32 	%r26, %r938, %r939;
	setp.eq.s32	%p40, %r938, 0;
	mov.u32 	%r2002, %r16;
	mov.u32 	%r2003, %r25;
	@%p40 bra 	BB12_29;

	not.b32 	%r940, %r2001;
	neg.s32 	%r27, %r25;
	setp.eq.s32	%p41, %r25, 0;
	selp.u32	%r941, 1, 0, %p41;
	add.s32 	%r2001, %r941, %r940;
	xor.b32  	%r29, %r16, -2147483648;
	mov.u32 	%r2002, %r29;
	mov.u32 	%r2003, %r27;

BB12_29:
	mov.u32 	%r31, %r2002;
	neg.s32 	%r942, %r26;
	setp.eq.s32	%p42, %r16, 0;
	selp.b32	%r2006, %r26, %r942, %p42;
	clz.b32 	%r2005, %r2001;
	setp.eq.s32	%p43, %r2005, 0;
	shl.b32 	%r943, %r2001, %r2005;
	mov.u32 	%r944, 32;
	sub.s32 	%r945, %r944, %r2005;
	shr.u32 	%r946, %r2003, %r945;
	add.s32 	%r947, %r946, %r943;
	selp.b32	%r35, %r2001, %r947, %p43;
	mov.u32 	%r948, -921707870;
	mul.hi.u32 	%r2004, %r35, %r948;
	setp.lt.s32	%p44, %r2004, 1;
	@%p44 bra 	BB12_31;

	mul.lo.s32 	%r949, %r35, -921707870;
	shr.u32 	%r950, %r949, 31;
	shl.b32 	%r951, %r2004, 1;
	add.s32 	%r2004, %r950, %r951;
	add.s32 	%r2005, %r2005, 1;

BB12_31:
	mov.u32 	%r952, 126;
	sub.s32 	%r953, %r952, %r2005;
	shl.b32 	%r954, %r953, 23;
	add.s32 	%r955, %r2004, 1;
	shr.u32 	%r956, %r955, 7;
	add.s32 	%r957, %r956, 1;
	shr.u32 	%r958, %r957, 1;
	add.s32 	%r959, %r958, %r954;
	or.b32  	%r960, %r959, %r31;
	mov.b32 	 %f1195, %r960;

BB12_32:
	mul.rn.f32 	%f43, %f1195, %f1195;
	add.s32 	%r42, %r2006, 1;
	and.b32  	%r43, %r42, 1;
	setp.eq.s32	%p45, %r43, 0;
	@%p45 bra 	BB12_34;

	mov.f32 	%f624, 0fBAB6061A;
	mov.f32 	%f625, 0f37CCF5CE;
	fma.rn.f32 	%f1196, %f625, %f43, %f624;
	bra.uni 	BB12_35;

BB12_34:
	mov.f32 	%f626, 0f3C08839E;
	mov.f32 	%f627, 0fB94CA1F9;
	fma.rn.f32 	%f1196, %f627, %f43, %f626;

BB12_35:
	@%p45 bra 	BB12_37;

	mov.f32 	%f628, 0f3D2AAAA5;
	fma.rn.f32 	%f629, %f1196, %f43, %f628;
	mov.f32 	%f630, 0fBF000000;
	fma.rn.f32 	%f1197, %f629, %f43, %f630;
	bra.uni 	BB12_38;

BB12_37:
	mov.f32 	%f631, 0fBE2AAAA3;
	fma.rn.f32 	%f632, %f1196, %f43, %f631;
	mov.f32 	%f633, 0f00000000;
	fma.rn.f32 	%f1197, %f632, %f43, %f633;

BB12_38:
	fma.rn.f32 	%f1198, %f1197, %f1195, %f1195;
	@%p45 bra 	BB12_40;

	mov.f32 	%f634, 0f3F800000;
	fma.rn.f32 	%f1198, %f1197, %f43, %f634;

BB12_40:
	and.b32  	%r961, %r42, 2;
	setp.eq.s32	%p48, %r961, 0;
	@%p48 bra 	BB12_42;

	mov.f32 	%f635, 0f00000000;
	mov.f32 	%f636, 0fBF800000;
	fma.rn.f32 	%f1198, %f1198, %f636, %f635;

BB12_42:
	abs.f32 	%f55, %f36;
	setp.neu.f32	%p49, %f55, 0f7F800000;
	mov.f32 	%f1297, %f36;
	@%p49 bra 	BB12_44;

	mov.f32 	%f637, 0f00000000;
	mul.rn.f32 	%f56, %f36, %f637;
	mov.f32 	%f1297, %f56;

BB12_44:
	mov.f32 	%f57, %f1297;
	mul.f32 	%f638, %f57, 0f3F22F983;
	cvt.rni.s32.f32	%r2016, %f638;
	cvt.rn.f32.s32	%f639, %r2016;
	neg.f32 	%f640, %f639;
	fma.rn.f32 	%f642, %f640, %f618, %f57;
	fma.rn.f32 	%f644, %f640, %f620, %f642;
	fma.rn.f32 	%f1199, %f640, %f622, %f644;
	abs.f32 	%f646, %f57;
	setp.leu.f32	%p50, %f646, 0f47CE4780;
	@%p50 bra 	BB12_54;

	mov.b32 	 %r45, %f57;
	shr.u32 	%r46, %r45, 23;
	bfe.u32 	%r964, %r45, 23, 8;
	add.s32 	%r965, %r964, -128;
	shl.b32 	%r966, %r45, 8;
	or.b32  	%r47, %r966, -2147483648;
	shr.u32 	%r48, %r965, 5;
	mov.u32 	%r2008, 0;
	mov.u64 	%rd173, __cudart_i2opi_f;
	mov.u32 	%r2007, -6;
	mov.u64 	%rd241, %rd1;

BB12_46:
	.pragma "nounroll";
	ld.const.u32 	%r969, [%rd173];
	// inline asm
	{
	mad.lo.cc.u32   %r967, %r969, %r47, %r2008;
	madc.hi.u32     %r2008, %r969, %r47,  0;
	}
	// inline asm
	st.local.u32 	[%rd241], %r967;
	add.s64 	%rd241, %rd241, 4;
	add.s64 	%rd173, %rd173, 4;
	add.s32 	%r2007, %r2007, 1;
	setp.ne.s32	%p51, %r2007, 0;
	@%p51 bra 	BB12_46;

	and.b32  	%r53, %r45, -2147483648;
	st.local.u32 	[%rd2], %r2008;
	mov.u32 	%r972, 6;
	sub.s32 	%r973, %r972, %r48;
	mul.wide.s32 	%rd127, %r973, 4;
	add.s64 	%rd12, %rd1, %rd127;
	ld.local.u32 	%r2009, [%rd12];
	ld.local.u32 	%r2010, [%rd12+-4];
	and.b32  	%r56, %r46, 31;
	setp.eq.s32	%p52, %r56, 0;
	@%p52 bra 	BB12_49;

	mov.u32 	%r974, 32;
	sub.s32 	%r975, %r974, %r56;
	shr.u32 	%r976, %r2010, %r975;
	shl.b32 	%r977, %r2009, %r56;
	add.s32 	%r2009, %r976, %r977;
	ld.local.u32 	%r978, [%rd12+-8];
	shr.u32 	%r979, %r978, %r975;
	shl.b32 	%r980, %r2010, %r56;
	add.s32 	%r2010, %r979, %r980;

BB12_49:
	shr.u32 	%r981, %r2010, 30;
	shl.b32 	%r982, %r2009, 2;
	add.s32 	%r2011, %r981, %r982;
	shl.b32 	%r62, %r2010, 2;
	shr.u32 	%r983, %r2011, 31;
	shr.u32 	%r984, %r2009, 30;
	add.s32 	%r63, %r983, %r984;
	setp.eq.s32	%p53, %r983, 0;
	mov.u32 	%r2012, %r53;
	mov.u32 	%r2013, %r62;
	@%p53 bra 	BB12_51;

	not.b32 	%r985, %r2011;
	neg.s32 	%r64, %r62;
	setp.eq.s32	%p54, %r62, 0;
	selp.u32	%r986, 1, 0, %p54;
	add.s32 	%r2011, %r986, %r985;
	xor.b32  	%r66, %r53, -2147483648;
	mov.u32 	%r2012, %r66;
	mov.u32 	%r2013, %r64;

BB12_51:
	mov.u32 	%r68, %r2012;
	neg.s32 	%r987, %r63;
	setp.eq.s32	%p55, %r53, 0;
	selp.b32	%r2016, %r63, %r987, %p55;
	clz.b32 	%r2015, %r2011;
	setp.eq.s32	%p56, %r2015, 0;
	shl.b32 	%r988, %r2011, %r2015;
	mov.u32 	%r989, 32;
	sub.s32 	%r990, %r989, %r2015;
	shr.u32 	%r991, %r2013, %r990;
	add.s32 	%r992, %r991, %r988;
	selp.b32	%r72, %r2011, %r992, %p56;
	mov.u32 	%r993, -921707870;
	mul.hi.u32 	%r2014, %r72, %r993;
	setp.lt.s32	%p57, %r2014, 1;
	@%p57 bra 	BB12_53;

	mul.lo.s32 	%r994, %r72, -921707870;
	shr.u32 	%r995, %r994, 31;
	shl.b32 	%r996, %r2014, 1;
	add.s32 	%r2014, %r995, %r996;
	add.s32 	%r2015, %r2015, 1;

BB12_53:
	mov.u32 	%r997, 126;
	sub.s32 	%r998, %r997, %r2015;
	shl.b32 	%r999, %r998, 23;
	add.s32 	%r1000, %r2014, 1;
	shr.u32 	%r1001, %r1000, 7;
	add.s32 	%r1002, %r1001, 1;
	shr.u32 	%r1003, %r1002, 1;
	add.s32 	%r1004, %r1003, %r999;
	or.b32  	%r1005, %r1004, %r68;
	mov.b32 	 %f1199, %r1005;

BB12_54:
	mul.rn.f32 	%f61, %f1199, %f1199;
	add.s32 	%r79, %r2016, 1;
	and.b32  	%r80, %r79, 1;
	setp.eq.s32	%p58, %r80, 0;
	@%p58 bra 	BB12_56;

	mov.f32 	%f647, 0fBAB6061A;
	mov.f32 	%f648, 0f37CCF5CE;
	fma.rn.f32 	%f1200, %f648, %f61, %f647;
	bra.uni 	BB12_57;

BB12_56:
	mov.f32 	%f649, 0f3C08839E;
	mov.f32 	%f650, 0fB94CA1F9;
	fma.rn.f32 	%f1200, %f650, %f61, %f649;

BB12_57:
	@%p58 bra 	BB12_59;

	mov.f32 	%f651, 0f3D2AAAA5;
	fma.rn.f32 	%f652, %f1200, %f61, %f651;
	mov.f32 	%f653, 0fBF000000;
	fma.rn.f32 	%f1201, %f652, %f61, %f653;
	bra.uni 	BB12_60;

BB12_59:
	mov.f32 	%f654, 0fBE2AAAA3;
	fma.rn.f32 	%f655, %f1200, %f61, %f654;
	mov.f32 	%f656, 0f00000000;
	fma.rn.f32 	%f1201, %f655, %f61, %f656;

BB12_60:
	fma.rn.f32 	%f1202, %f1201, %f1199, %f1199;
	@%p58 bra 	BB12_62;

	mov.f32 	%f657, 0f3F800000;
	fma.rn.f32 	%f1202, %f1201, %f61, %f657;

BB12_62:
	and.b32  	%r1006, %r79, 2;
	setp.eq.s32	%p61, %r1006, 0;
	@%p61 bra 	BB12_64;

	mov.f32 	%f658, 0f00000000;
	mov.f32 	%f659, 0fBF800000;
	fma.rn.f32 	%f1202, %f1202, %f659, %f658;

BB12_64:
	mul.f32 	%f73, %f1198, %f1202;
	abs.f32 	%f74, %f13;
	setp.neu.f32	%p62, %f74, 0f7F800000;
	mov.f32 	%f1316, %f13;
	@%p62 bra 	BB12_66;

	mov.f32 	%f660, 0f00000000;
	mul.rn.f32 	%f75, %f13, %f660;
	mov.f32 	%f1316, %f75;

BB12_66:
	mov.f32 	%f76, %f1316;
	mul.f32 	%f661, %f76, 0f3F22F983;
	cvt.rni.s32.f32	%r2026, %f661;
	cvt.rn.f32.s32	%f662, %r2026;
	neg.f32 	%f663, %f662;
	fma.rn.f32 	%f665, %f663, %f618, %f76;
	fma.rn.f32 	%f667, %f663, %f620, %f665;
	fma.rn.f32 	%f1203, %f663, %f622, %f667;
	abs.f32 	%f669, %f76;
	setp.leu.f32	%p63, %f669, 0f47CE4780;
	@%p63 bra 	BB12_76;

	mov.b32 	 %r82, %f76;
	shr.u32 	%r83, %r82, 23;
	bfe.u32 	%r1009, %r82, 23, 8;
	add.s32 	%r1010, %r1009, -128;
	shl.b32 	%r1011, %r82, 8;
	or.b32  	%r84, %r1011, -2147483648;
	shr.u32 	%r85, %r1010, 5;
	mov.u32 	%r2018, 0;
	mov.u64 	%rd174, __cudart_i2opi_f;
	mov.u32 	%r2017, -6;
	mov.u64 	%rd240, %rd1;

BB12_68:
	.pragma "nounroll";
	ld.const.u32 	%r1014, [%rd174];
	// inline asm
	{
	mad.lo.cc.u32   %r1012, %r1014, %r84, %r2018;
	madc.hi.u32     %r2018, %r1014, %r84,  0;
	}
	// inline asm
	st.local.u32 	[%rd240], %r1012;
	add.s64 	%rd240, %rd240, 4;
	add.s64 	%rd174, %rd174, 4;
	add.s32 	%r2017, %r2017, 1;
	setp.ne.s32	%p64, %r2017, 0;
	@%p64 bra 	BB12_68;

	and.b32  	%r90, %r82, -2147483648;
	st.local.u32 	[%rd2], %r2018;
	mov.u32 	%r1017, 6;
	sub.s32 	%r1018, %r1017, %r85;
	mul.wide.s32 	%rd129, %r1018, 4;
	add.s64 	%rd17, %rd1, %rd129;
	ld.local.u32 	%r2019, [%rd17];
	ld.local.u32 	%r2020, [%rd17+-4];
	and.b32  	%r93, %r83, 31;
	setp.eq.s32	%p65, %r93, 0;
	@%p65 bra 	BB12_71;

	mov.u32 	%r1019, 32;
	sub.s32 	%r1020, %r1019, %r93;
	shr.u32 	%r1021, %r2020, %r1020;
	shl.b32 	%r1022, %r2019, %r93;
	add.s32 	%r2019, %r1021, %r1022;
	ld.local.u32 	%r1023, [%rd17+-8];
	shr.u32 	%r1024, %r1023, %r1020;
	shl.b32 	%r1025, %r2020, %r93;
	add.s32 	%r2020, %r1024, %r1025;

BB12_71:
	shr.u32 	%r1026, %r2020, 30;
	shl.b32 	%r1027, %r2019, 2;
	add.s32 	%r2021, %r1026, %r1027;
	shl.b32 	%r99, %r2020, 2;
	shr.u32 	%r1028, %r2021, 31;
	shr.u32 	%r1029, %r2019, 30;
	add.s32 	%r100, %r1028, %r1029;
	setp.eq.s32	%p66, %r1028, 0;
	mov.u32 	%r2022, %r90;
	mov.u32 	%r2023, %r99;
	@%p66 bra 	BB12_73;

	not.b32 	%r1030, %r2021;
	neg.s32 	%r101, %r99;
	setp.eq.s32	%p67, %r99, 0;
	selp.u32	%r1031, 1, 0, %p67;
	add.s32 	%r2021, %r1031, %r1030;
	xor.b32  	%r103, %r90, -2147483648;
	mov.u32 	%r2022, %r103;
	mov.u32 	%r2023, %r101;

BB12_73:
	mov.u32 	%r105, %r2022;
	neg.s32 	%r1032, %r100;
	setp.eq.s32	%p68, %r90, 0;
	selp.b32	%r2026, %r100, %r1032, %p68;
	clz.b32 	%r2025, %r2021;
	setp.eq.s32	%p69, %r2025, 0;
	shl.b32 	%r1033, %r2021, %r2025;
	mov.u32 	%r1034, 32;
	sub.s32 	%r1035, %r1034, %r2025;
	shr.u32 	%r1036, %r2023, %r1035;
	add.s32 	%r1037, %r1036, %r1033;
	selp.b32	%r109, %r2021, %r1037, %p69;
	mov.u32 	%r1038, -921707870;
	mul.hi.u32 	%r2024, %r109, %r1038;
	setp.lt.s32	%p70, %r2024, 1;
	@%p70 bra 	BB12_75;

	mul.lo.s32 	%r1039, %r109, -921707870;
	shr.u32 	%r1040, %r1039, 31;
	shl.b32 	%r1041, %r2024, 1;
	add.s32 	%r2024, %r1040, %r1041;
	add.s32 	%r2025, %r2025, 1;

BB12_75:
	mov.u32 	%r1042, 126;
	sub.s32 	%r1043, %r1042, %r2025;
	shl.b32 	%r1044, %r1043, 23;
	add.s32 	%r1045, %r2024, 1;
	shr.u32 	%r1046, %r1045, 7;
	add.s32 	%r1047, %r1046, 1;
	shr.u32 	%r1048, %r1047, 1;
	add.s32 	%r1049, %r1048, %r1044;
	or.b32  	%r1050, %r1049, %r105;
	mov.b32 	 %f1203, %r1050;

BB12_76:
	mul.rn.f32 	%f80, %f1203, %f1203;
	add.s32 	%r116, %r2026, 1;
	and.b32  	%r117, %r116, 1;
	setp.eq.s32	%p71, %r117, 0;
	@%p71 bra 	BB12_78;

	mov.f32 	%f670, 0fBAB6061A;
	mov.f32 	%f671, 0f37CCF5CE;
	fma.rn.f32 	%f1204, %f671, %f80, %f670;
	bra.uni 	BB12_79;

BB12_78:
	mov.f32 	%f672, 0f3C08839E;
	mov.f32 	%f673, 0fB94CA1F9;
	fma.rn.f32 	%f1204, %f673, %f80, %f672;

BB12_79:
	@%p71 bra 	BB12_81;

	mov.f32 	%f674, 0f3D2AAAA5;
	fma.rn.f32 	%f675, %f1204, %f80, %f674;
	mov.f32 	%f676, 0fBF000000;
	fma.rn.f32 	%f1205, %f675, %f80, %f676;
	bra.uni 	BB12_82;

BB12_81:
	mov.f32 	%f677, 0fBE2AAAA3;
	fma.rn.f32 	%f678, %f1204, %f80, %f677;
	mov.f32 	%f679, 0f00000000;
	fma.rn.f32 	%f1205, %f678, %f80, %f679;

BB12_82:
	fma.rn.f32 	%f1206, %f1205, %f1203, %f1203;
	@%p71 bra 	BB12_84;

	mov.f32 	%f680, 0f3F800000;
	fma.rn.f32 	%f1206, %f1205, %f80, %f680;

BB12_84:
	and.b32  	%r1051, %r116, 2;
	setp.eq.s32	%p74, %r1051, 0;
	@%p74 bra 	BB12_86;

	mov.f32 	%f681, 0f00000000;
	mov.f32 	%f682, 0fBF800000;
	fma.rn.f32 	%f1206, %f1206, %f682, %f681;

BB12_86:
	mul.f32 	%f92, %f73, %f1206;
	mov.f32 	%f1342, %f34;
	@%p36 bra 	BB12_88;

	mov.f32 	%f683, 0f00000000;
	mul.rn.f32 	%f1342, %f34, %f683;

BB12_88:
	mul.f32 	%f684, %f1342, 0f3F22F983;
	cvt.rni.s32.f32	%r2036, %f684;
	cvt.rn.f32.s32	%f685, %r2036;
	neg.f32 	%f686, %f685;
	fma.rn.f32 	%f688, %f686, %f618, %f1342;
	fma.rn.f32 	%f690, %f686, %f620, %f688;
	fma.rn.f32 	%f1207, %f686, %f622, %f690;
	abs.f32 	%f692, %f1342;
	setp.leu.f32	%p76, %f692, 0f47CE4780;
	@%p76 bra 	BB12_98;

	mov.b32 	 %r119, %f1342;
	shr.u32 	%r120, %r119, 23;
	bfe.u32 	%r1054, %r119, 23, 8;
	add.s32 	%r1055, %r1054, -128;
	shl.b32 	%r1056, %r119, 8;
	or.b32  	%r121, %r1056, -2147483648;
	shr.u32 	%r122, %r1055, 5;
	mov.u32 	%r2028, 0;
	mov.u64 	%rd175, __cudart_i2opi_f;
	mov.u32 	%r2027, -6;
	mov.u64 	%rd239, %rd1;

BB12_90:
	.pragma "nounroll";
	ld.const.u32 	%r1059, [%rd175];
	// inline asm
	{
	mad.lo.cc.u32   %r1057, %r1059, %r121, %r2028;
	madc.hi.u32     %r2028, %r1059, %r121,  0;
	}
	// inline asm
	st.local.u32 	[%rd239], %r1057;
	add.s64 	%rd239, %rd239, 4;
	add.s64 	%rd175, %rd175, 4;
	add.s32 	%r2027, %r2027, 1;
	setp.ne.s32	%p77, %r2027, 0;
	@%p77 bra 	BB12_90;

	and.b32  	%r127, %r119, -2147483648;
	st.local.u32 	[%rd2], %r2028;
	mov.u32 	%r1062, 6;
	sub.s32 	%r1063, %r1062, %r122;
	mul.wide.s32 	%rd131, %r1063, 4;
	add.s64 	%rd22, %rd1, %rd131;
	ld.local.u32 	%r2029, [%rd22];
	ld.local.u32 	%r2030, [%rd22+-4];
	and.b32  	%r130, %r120, 31;
	setp.eq.s32	%p78, %r130, 0;
	@%p78 bra 	BB12_93;

	mov.u32 	%r1064, 32;
	sub.s32 	%r1065, %r1064, %r130;
	shr.u32 	%r1066, %r2030, %r1065;
	shl.b32 	%r1067, %r2029, %r130;
	add.s32 	%r2029, %r1066, %r1067;
	ld.local.u32 	%r1068, [%rd22+-8];
	shr.u32 	%r1069, %r1068, %r1065;
	shl.b32 	%r1070, %r2030, %r130;
	add.s32 	%r2030, %r1069, %r1070;

BB12_93:
	shr.u32 	%r1071, %r2030, 30;
	shl.b32 	%r1072, %r2029, 2;
	add.s32 	%r2031, %r1071, %r1072;
	shl.b32 	%r136, %r2030, 2;
	shr.u32 	%r1073, %r2031, 31;
	shr.u32 	%r1074, %r2029, 30;
	add.s32 	%r137, %r1073, %r1074;
	setp.eq.s32	%p79, %r1073, 0;
	mov.u32 	%r2032, %r127;
	mov.u32 	%r2033, %r136;
	@%p79 bra 	BB12_95;

	not.b32 	%r1075, %r2031;
	neg.s32 	%r138, %r136;
	setp.eq.s32	%p80, %r136, 0;
	selp.u32	%r1076, 1, 0, %p80;
	add.s32 	%r2031, %r1076, %r1075;
	xor.b32  	%r140, %r127, -2147483648;
	mov.u32 	%r2032, %r140;
	mov.u32 	%r2033, %r138;

BB12_95:
	mov.u32 	%r142, %r2032;
	neg.s32 	%r1077, %r137;
	setp.eq.s32	%p81, %r127, 0;
	selp.b32	%r2036, %r137, %r1077, %p81;
	clz.b32 	%r2035, %r2031;
	setp.eq.s32	%p82, %r2035, 0;
	shl.b32 	%r1078, %r2031, %r2035;
	mov.u32 	%r1079, 32;
	sub.s32 	%r1080, %r1079, %r2035;
	shr.u32 	%r1081, %r2033, %r1080;
	add.s32 	%r1082, %r1081, %r1078;
	selp.b32	%r146, %r2031, %r1082, %p82;
	mov.u32 	%r1083, -921707870;
	mul.hi.u32 	%r2034, %r146, %r1083;
	setp.lt.s32	%p83, %r2034, 1;
	@%p83 bra 	BB12_97;

	mul.lo.s32 	%r1084, %r146, -921707870;
	shr.u32 	%r1085, %r1084, 31;
	shl.b32 	%r1086, %r2034, 1;
	add.s32 	%r2034, %r1085, %r1086;
	add.s32 	%r2035, %r2035, 1;

BB12_97:
	mov.u32 	%r1087, 126;
	sub.s32 	%r1088, %r1087, %r2035;
	shl.b32 	%r1089, %r1088, 23;
	add.s32 	%r1090, %r2034, 1;
	shr.u32 	%r1091, %r1090, 7;
	add.s32 	%r1092, %r1091, 1;
	shr.u32 	%r1093, %r1092, 1;
	add.s32 	%r1094, %r1093, %r1089;
	or.b32  	%r1095, %r1094, %r142;
	mov.b32 	 %f1207, %r1095;

BB12_98:
	mul.rn.f32 	%f98, %f1207, %f1207;
	and.b32  	%r153, %r2036, 1;
	setp.eq.s32	%p84, %r153, 0;
	@%p84 bra 	BB12_100;

	mov.f32 	%f693, 0fBAB6061A;
	mov.f32 	%f694, 0f37CCF5CE;
	fma.rn.f32 	%f1208, %f694, %f98, %f693;
	bra.uni 	BB12_101;

BB12_100:
	mov.f32 	%f695, 0f3C08839E;
	mov.f32 	%f696, 0fB94CA1F9;
	fma.rn.f32 	%f1208, %f696, %f98, %f695;

BB12_101:
	@%p84 bra 	BB12_103;

	mov.f32 	%f697, 0f3D2AAAA5;
	fma.rn.f32 	%f698, %f1208, %f98, %f697;
	mov.f32 	%f699, 0fBF000000;
	fma.rn.f32 	%f1209, %f698, %f98, %f699;
	bra.uni 	BB12_104;

BB12_103:
	mov.f32 	%f700, 0fBE2AAAA3;
	fma.rn.f32 	%f701, %f1208, %f98, %f700;
	mov.f32 	%f702, 0f00000000;
	fma.rn.f32 	%f1209, %f701, %f98, %f702;

BB12_104:
	fma.rn.f32 	%f1210, %f1209, %f1207, %f1207;
	@%p84 bra 	BB12_106;

	mov.f32 	%f703, 0f3F800000;
	fma.rn.f32 	%f1210, %f1209, %f98, %f703;

BB12_106:
	and.b32  	%r1096, %r2036, 2;
	setp.eq.s32	%p87, %r1096, 0;
	@%p87 bra 	BB12_108;

	mov.f32 	%f704, 0f00000000;
	mov.f32 	%f705, 0fBF800000;
	fma.rn.f32 	%f1210, %f1210, %f705, %f704;

BB12_108:
	mov.f32 	%f1296, %f36;
	@%p49 bra 	BB12_110;

	mov.f32 	%f706, 0f00000000;
	mul.rn.f32 	%f1296, %f36, %f706;

BB12_110:
	mul.f32 	%f707, %f1296, 0f3F22F983;
	cvt.rni.s32.f32	%r2046, %f707;
	cvt.rn.f32.s32	%f708, %r2046;
	neg.f32 	%f709, %f708;
	fma.rn.f32 	%f711, %f709, %f618, %f1296;
	fma.rn.f32 	%f713, %f709, %f620, %f711;
	fma.rn.f32 	%f1211, %f709, %f622, %f713;
	abs.f32 	%f715, %f1296;
	setp.leu.f32	%p89, %f715, 0f47CE4780;
	@%p89 bra 	BB12_120;

	mov.b32 	 %r155, %f1296;
	shr.u32 	%r156, %r155, 23;
	bfe.u32 	%r1099, %r155, 23, 8;
	add.s32 	%r1100, %r1099, -128;
	shl.b32 	%r1101, %r155, 8;
	or.b32  	%r157, %r1101, -2147483648;
	shr.u32 	%r158, %r1100, 5;
	mov.u32 	%r2038, 0;
	mov.u64 	%rd176, __cudart_i2opi_f;
	mov.u32 	%r2037, -6;
	mov.u64 	%rd238, %rd1;

BB12_112:
	.pragma "nounroll";
	ld.const.u32 	%r1104, [%rd176];
	// inline asm
	{
	mad.lo.cc.u32   %r1102, %r1104, %r157, %r2038;
	madc.hi.u32     %r2038, %r1104, %r157,  0;
	}
	// inline asm
	st.local.u32 	[%rd238], %r1102;
	add.s64 	%rd238, %rd238, 4;
	add.s64 	%rd176, %rd176, 4;
	add.s32 	%r2037, %r2037, 1;
	setp.ne.s32	%p90, %r2037, 0;
	@%p90 bra 	BB12_112;

	and.b32  	%r163, %r155, -2147483648;
	st.local.u32 	[%rd2], %r2038;
	mov.u32 	%r1107, 6;
	sub.s32 	%r1108, %r1107, %r158;
	mul.wide.s32 	%rd133, %r1108, 4;
	add.s64 	%rd27, %rd1, %rd133;
	ld.local.u32 	%r2039, [%rd27];
	ld.local.u32 	%r2040, [%rd27+-4];
	and.b32  	%r166, %r156, 31;
	setp.eq.s32	%p91, %r166, 0;
	@%p91 bra 	BB12_115;

	mov.u32 	%r1109, 32;
	sub.s32 	%r1110, %r1109, %r166;
	shr.u32 	%r1111, %r2040, %r1110;
	shl.b32 	%r1112, %r2039, %r166;
	add.s32 	%r2039, %r1111, %r1112;
	ld.local.u32 	%r1113, [%rd27+-8];
	shr.u32 	%r1114, %r1113, %r1110;
	shl.b32 	%r1115, %r2040, %r166;
	add.s32 	%r2040, %r1114, %r1115;

BB12_115:
	shr.u32 	%r1116, %r2040, 30;
	shl.b32 	%r1117, %r2039, 2;
	add.s32 	%r2041, %r1116, %r1117;
	shl.b32 	%r172, %r2040, 2;
	shr.u32 	%r1118, %r2041, 31;
	shr.u32 	%r1119, %r2039, 30;
	add.s32 	%r173, %r1118, %r1119;
	setp.eq.s32	%p92, %r1118, 0;
	mov.u32 	%r2042, %r163;
	mov.u32 	%r2043, %r172;
	@%p92 bra 	BB12_117;

	not.b32 	%r1120, %r2041;
	neg.s32 	%r174, %r172;
	setp.eq.s32	%p93, %r172, 0;
	selp.u32	%r1121, 1, 0, %p93;
	add.s32 	%r2041, %r1121, %r1120;
	xor.b32  	%r176, %r163, -2147483648;
	mov.u32 	%r2042, %r176;
	mov.u32 	%r2043, %r174;

BB12_117:
	mov.u32 	%r178, %r2042;
	neg.s32 	%r1122, %r173;
	setp.eq.s32	%p94, %r163, 0;
	selp.b32	%r2046, %r173, %r1122, %p94;
	clz.b32 	%r2045, %r2041;
	setp.eq.s32	%p95, %r2045, 0;
	shl.b32 	%r1123, %r2041, %r2045;
	mov.u32 	%r1124, 32;
	sub.s32 	%r1125, %r1124, %r2045;
	shr.u32 	%r1126, %r2043, %r1125;
	add.s32 	%r1127, %r1126, %r1123;
	selp.b32	%r182, %r2041, %r1127, %p95;
	mov.u32 	%r1128, -921707870;
	mul.hi.u32 	%r2044, %r182, %r1128;
	setp.lt.s32	%p96, %r2044, 1;
	@%p96 bra 	BB12_119;

	mul.lo.s32 	%r1129, %r182, -921707870;
	shr.u32 	%r1130, %r1129, 31;
	shl.b32 	%r1131, %r2044, 1;
	add.s32 	%r2044, %r1130, %r1131;
	add.s32 	%r2045, %r2045, 1;

BB12_119:
	mov.u32 	%r1132, 126;
	sub.s32 	%r1133, %r1132, %r2045;
	shl.b32 	%r1134, %r1133, 23;
	add.s32 	%r1135, %r2044, 1;
	shr.u32 	%r1136, %r1135, 7;
	add.s32 	%r1137, %r1136, 1;
	shr.u32 	%r1138, %r1137, 1;
	add.s32 	%r1139, %r1138, %r1134;
	or.b32  	%r1140, %r1139, %r178;
	mov.b32 	 %f1211, %r1140;

BB12_120:
	mul.rn.f32 	%f115, %f1211, %f1211;
	and.b32  	%r189, %r2046, 1;
	setp.eq.s32	%p97, %r189, 0;
	@%p97 bra 	BB12_122;

	mov.f32 	%f716, 0fBAB6061A;
	mov.f32 	%f717, 0f37CCF5CE;
	fma.rn.f32 	%f1212, %f717, %f115, %f716;
	bra.uni 	BB12_123;

BB12_122:
	mov.f32 	%f718, 0f3C08839E;
	mov.f32 	%f719, 0fB94CA1F9;
	fma.rn.f32 	%f1212, %f719, %f115, %f718;

BB12_123:
	@%p97 bra 	BB12_125;

	mov.f32 	%f720, 0f3D2AAAA5;
	fma.rn.f32 	%f721, %f1212, %f115, %f720;
	mov.f32 	%f722, 0fBF000000;
	fma.rn.f32 	%f1213, %f721, %f115, %f722;
	bra.uni 	BB12_126;

BB12_125:
	mov.f32 	%f723, 0fBE2AAAA3;
	fma.rn.f32 	%f724, %f1212, %f115, %f723;
	mov.f32 	%f725, 0f00000000;
	fma.rn.f32 	%f1213, %f724, %f115, %f725;

BB12_126:
	fma.rn.f32 	%f1214, %f1213, %f1211, %f1211;
	@%p97 bra 	BB12_128;

	mov.f32 	%f726, 0f3F800000;
	fma.rn.f32 	%f1214, %f1213, %f115, %f726;

BB12_128:
	and.b32  	%r1141, %r2046, 2;
	setp.eq.s32	%p100, %r1141, 0;
	@%p100 bra 	BB12_130;

	mov.f32 	%f727, 0f00000000;
	mov.f32 	%f728, 0fBF800000;
	fma.rn.f32 	%f1214, %f1214, %f728, %f727;

BB12_130:
	mul.f32 	%f127, %f1210, %f1214;
	mov.f32 	%f1315, %f13;
	@%p62 bra 	BB12_132;

	mov.f32 	%f729, 0f00000000;
	mul.rn.f32 	%f1315, %f13, %f729;

BB12_132:
	mul.f32 	%f730, %f1315, 0f3F22F983;
	cvt.rni.s32.f32	%r2056, %f730;
	cvt.rn.f32.s32	%f731, %r2056;
	neg.f32 	%f732, %f731;
	fma.rn.f32 	%f734, %f732, %f618, %f1315;
	fma.rn.f32 	%f736, %f732, %f620, %f734;
	fma.rn.f32 	%f1215, %f732, %f622, %f736;
	abs.f32 	%f738, %f1315;
	setp.leu.f32	%p102, %f738, 0f47CE4780;
	@%p102 bra 	BB12_142;

	mov.b32 	 %r191, %f1315;
	shr.u32 	%r192, %r191, 23;
	bfe.u32 	%r1144, %r191, 23, 8;
	add.s32 	%r1145, %r1144, -128;
	shl.b32 	%r1146, %r191, 8;
	or.b32  	%r193, %r1146, -2147483648;
	shr.u32 	%r194, %r1145, 5;
	mov.u32 	%r2048, 0;
	mov.u64 	%rd177, __cudart_i2opi_f;
	mov.u32 	%r2047, -6;
	mov.u64 	%rd237, %rd1;

BB12_134:
	.pragma "nounroll";
	ld.const.u32 	%r1149, [%rd177];
	// inline asm
	{
	mad.lo.cc.u32   %r1147, %r1149, %r193, %r2048;
	madc.hi.u32     %r2048, %r1149, %r193,  0;
	}
	// inline asm
	st.local.u32 	[%rd237], %r1147;
	add.s64 	%rd237, %rd237, 4;
	add.s64 	%rd177, %rd177, 4;
	add.s32 	%r2047, %r2047, 1;
	setp.ne.s32	%p103, %r2047, 0;
	@%p103 bra 	BB12_134;

	and.b32  	%r199, %r191, -2147483648;
	st.local.u32 	[%rd2], %r2048;
	mov.u32 	%r1152, 6;
	sub.s32 	%r1153, %r1152, %r194;
	mul.wide.s32 	%rd135, %r1153, 4;
	add.s64 	%rd32, %rd1, %rd135;
	ld.local.u32 	%r2049, [%rd32];
	ld.local.u32 	%r2050, [%rd32+-4];
	and.b32  	%r202, %r192, 31;
	setp.eq.s32	%p104, %r202, 0;
	@%p104 bra 	BB12_137;

	mov.u32 	%r1154, 32;
	sub.s32 	%r1155, %r1154, %r202;
	shr.u32 	%r1156, %r2050, %r1155;
	shl.b32 	%r1157, %r2049, %r202;
	add.s32 	%r2049, %r1156, %r1157;
	ld.local.u32 	%r1158, [%rd32+-8];
	shr.u32 	%r1159, %r1158, %r1155;
	shl.b32 	%r1160, %r2050, %r202;
	add.s32 	%r2050, %r1159, %r1160;

BB12_137:
	shr.u32 	%r1161, %r2050, 30;
	shl.b32 	%r1162, %r2049, 2;
	add.s32 	%r2051, %r1161, %r1162;
	shl.b32 	%r208, %r2050, 2;
	shr.u32 	%r1163, %r2051, 31;
	shr.u32 	%r1164, %r2049, 30;
	add.s32 	%r209, %r1163, %r1164;
	setp.eq.s32	%p105, %r1163, 0;
	mov.u32 	%r2052, %r199;
	mov.u32 	%r2053, %r208;
	@%p105 bra 	BB12_139;

	not.b32 	%r1165, %r2051;
	neg.s32 	%r210, %r208;
	setp.eq.s32	%p106, %r208, 0;
	selp.u32	%r1166, 1, 0, %p106;
	add.s32 	%r2051, %r1166, %r1165;
	xor.b32  	%r212, %r199, -2147483648;
	mov.u32 	%r2052, %r212;
	mov.u32 	%r2053, %r210;

BB12_139:
	mov.u32 	%r214, %r2052;
	neg.s32 	%r1167, %r209;
	setp.eq.s32	%p107, %r199, 0;
	selp.b32	%r2056, %r209, %r1167, %p107;
	clz.b32 	%r2055, %r2051;
	setp.eq.s32	%p108, %r2055, 0;
	shl.b32 	%r1168, %r2051, %r2055;
	mov.u32 	%r1169, 32;
	sub.s32 	%r1170, %r1169, %r2055;
	shr.u32 	%r1171, %r2053, %r1170;
	add.s32 	%r1172, %r1171, %r1168;
	selp.b32	%r218, %r2051, %r1172, %p108;
	mov.u32 	%r1173, -921707870;
	mul.hi.u32 	%r2054, %r218, %r1173;
	setp.lt.s32	%p109, %r2054, 1;
	@%p109 bra 	BB12_141;

	mul.lo.s32 	%r1174, %r218, -921707870;
	shr.u32 	%r1175, %r1174, 31;
	shl.b32 	%r1176, %r2054, 1;
	add.s32 	%r2054, %r1175, %r1176;
	add.s32 	%r2055, %r2055, 1;

BB12_141:
	mov.u32 	%r1177, 126;
	sub.s32 	%r1178, %r1177, %r2055;
	shl.b32 	%r1179, %r1178, 23;
	add.s32 	%r1180, %r2054, 1;
	shr.u32 	%r1181, %r1180, 7;
	add.s32 	%r1182, %r1181, 1;
	shr.u32 	%r1183, %r1182, 1;
	add.s32 	%r1184, %r1183, %r1179;
	or.b32  	%r1185, %r1184, %r214;
	mov.b32 	 %f1215, %r1185;

BB12_142:
	mul.rn.f32 	%f133, %f1215, %f1215;
	and.b32  	%r225, %r2056, 1;
	setp.eq.s32	%p110, %r225, 0;
	@%p110 bra 	BB12_144;

	mov.f32 	%f739, 0fBAB6061A;
	mov.f32 	%f740, 0f37CCF5CE;
	fma.rn.f32 	%f1216, %f740, %f133, %f739;
	bra.uni 	BB12_145;

BB12_144:
	mov.f32 	%f741, 0f3C08839E;
	mov.f32 	%f742, 0fB94CA1F9;
	fma.rn.f32 	%f1216, %f742, %f133, %f741;

BB12_145:
	@%p110 bra 	BB12_147;

	mov.f32 	%f743, 0f3D2AAAA5;
	fma.rn.f32 	%f744, %f1216, %f133, %f743;
	mov.f32 	%f745, 0fBF000000;
	fma.rn.f32 	%f1217, %f744, %f133, %f745;
	bra.uni 	BB12_148;

BB12_147:
	mov.f32 	%f746, 0fBE2AAAA3;
	fma.rn.f32 	%f747, %f1216, %f133, %f746;
	mov.f32 	%f748, 0f00000000;
	fma.rn.f32 	%f1217, %f747, %f133, %f748;

BB12_148:
	fma.rn.f32 	%f1218, %f1217, %f1215, %f1215;
	@%p110 bra 	BB12_150;

	mov.f32 	%f749, 0f3F800000;
	fma.rn.f32 	%f1218, %f1217, %f133, %f749;

BB12_150:
	and.b32  	%r1186, %r2056, 2;
	setp.eq.s32	%p113, %r1186, 0;
	@%p113 bra 	BB12_152;

	mov.f32 	%f750, 0f00000000;
	mov.f32 	%f751, 0fBF800000;
	fma.rn.f32 	%f1218, %f1218, %f751, %f750;

BB12_152:
	fma.rn.f32 	%f145, %f127, %f1218, %f92;
	mov.f32 	%f1341, %f34;
	@%p36 bra 	BB12_154;

	mov.f32 	%f752, 0f00000000;
	mul.rn.f32 	%f1341, %f34, %f752;

BB12_154:
	mul.f32 	%f753, %f1341, 0f3F22F983;
	cvt.rni.s32.f32	%r2066, %f753;
	cvt.rn.f32.s32	%f754, %r2066;
	neg.f32 	%f755, %f754;
	fma.rn.f32 	%f757, %f755, %f618, %f1341;
	fma.rn.f32 	%f759, %f755, %f620, %f757;
	fma.rn.f32 	%f1219, %f755, %f622, %f759;
	abs.f32 	%f761, %f1341;
	setp.leu.f32	%p115, %f761, 0f47CE4780;
	@%p115 bra 	BB12_164;

	mov.b32 	 %r227, %f1341;
	shr.u32 	%r228, %r227, 23;
	bfe.u32 	%r1189, %r227, 23, 8;
	add.s32 	%r1190, %r1189, -128;
	shl.b32 	%r1191, %r227, 8;
	or.b32  	%r229, %r1191, -2147483648;
	shr.u32 	%r230, %r1190, 5;
	mov.u32 	%r2058, 0;
	mov.u64 	%rd178, __cudart_i2opi_f;
	mov.u32 	%r2057, -6;
	mov.u64 	%rd236, %rd1;

BB12_156:
	.pragma "nounroll";
	ld.const.u32 	%r1194, [%rd178];
	// inline asm
	{
	mad.lo.cc.u32   %r1192, %r1194, %r229, %r2058;
	madc.hi.u32     %r2058, %r1194, %r229,  0;
	}
	// inline asm
	st.local.u32 	[%rd236], %r1192;
	add.s64 	%rd236, %rd236, 4;
	add.s64 	%rd178, %rd178, 4;
	add.s32 	%r2057, %r2057, 1;
	setp.ne.s32	%p116, %r2057, 0;
	@%p116 bra 	BB12_156;

	and.b32  	%r235, %r227, -2147483648;
	st.local.u32 	[%rd2], %r2058;
	mov.u32 	%r1197, 6;
	sub.s32 	%r1198, %r1197, %r230;
	mul.wide.s32 	%rd137, %r1198, 4;
	add.s64 	%rd37, %rd1, %rd137;
	ld.local.u32 	%r2059, [%rd37];
	ld.local.u32 	%r2060, [%rd37+-4];
	and.b32  	%r238, %r228, 31;
	setp.eq.s32	%p117, %r238, 0;
	@%p117 bra 	BB12_159;

	mov.u32 	%r1199, 32;
	sub.s32 	%r1200, %r1199, %r238;
	shr.u32 	%r1201, %r2060, %r1200;
	shl.b32 	%r1202, %r2059, %r238;
	add.s32 	%r2059, %r1201, %r1202;
	ld.local.u32 	%r1203, [%rd37+-8];
	shr.u32 	%r1204, %r1203, %r1200;
	shl.b32 	%r1205, %r2060, %r238;
	add.s32 	%r2060, %r1204, %r1205;

BB12_159:
	shr.u32 	%r1206, %r2060, 30;
	shl.b32 	%r1207, %r2059, 2;
	add.s32 	%r2061, %r1206, %r1207;
	shl.b32 	%r244, %r2060, 2;
	shr.u32 	%r1208, %r2061, 31;
	shr.u32 	%r1209, %r2059, 30;
	add.s32 	%r245, %r1208, %r1209;
	setp.eq.s32	%p118, %r1208, 0;
	mov.u32 	%r2062, %r235;
	mov.u32 	%r2063, %r244;
	@%p118 bra 	BB12_161;

	not.b32 	%r1210, %r2061;
	neg.s32 	%r246, %r244;
	setp.eq.s32	%p119, %r244, 0;
	selp.u32	%r1211, 1, 0, %p119;
	add.s32 	%r2061, %r1211, %r1210;
	xor.b32  	%r248, %r235, -2147483648;
	mov.u32 	%r2062, %r248;
	mov.u32 	%r2063, %r246;

BB12_161:
	mov.u32 	%r250, %r2062;
	neg.s32 	%r1212, %r245;
	setp.eq.s32	%p120, %r235, 0;
	selp.b32	%r2066, %r245, %r1212, %p120;
	clz.b32 	%r2065, %r2061;
	setp.eq.s32	%p121, %r2065, 0;
	shl.b32 	%r1213, %r2061, %r2065;
	mov.u32 	%r1214, 32;
	sub.s32 	%r1215, %r1214, %r2065;
	shr.u32 	%r1216, %r2063, %r1215;
	add.s32 	%r1217, %r1216, %r1213;
	selp.b32	%r254, %r2061, %r1217, %p121;
	mov.u32 	%r1218, -921707870;
	mul.hi.u32 	%r2064, %r254, %r1218;
	setp.lt.s32	%p122, %r2064, 1;
	@%p122 bra 	BB12_163;

	mul.lo.s32 	%r1219, %r254, -921707870;
	shr.u32 	%r1220, %r1219, 31;
	shl.b32 	%r1221, %r2064, 1;
	add.s32 	%r2064, %r1220, %r1221;
	add.s32 	%r2065, %r2065, 1;

BB12_163:
	mov.u32 	%r1222, 126;
	sub.s32 	%r1223, %r1222, %r2065;
	shl.b32 	%r1224, %r1223, 23;
	add.s32 	%r1225, %r2064, 1;
	shr.u32 	%r1226, %r1225, 7;
	add.s32 	%r1227, %r1226, 1;
	shr.u32 	%r1228, %r1227, 1;
	add.s32 	%r1229, %r1228, %r1224;
	or.b32  	%r1230, %r1229, %r250;
	mov.b32 	 %f1219, %r1230;

BB12_164:
	mul.rn.f32 	%f151, %f1219, %f1219;
	and.b32  	%r261, %r2066, 1;
	setp.eq.s32	%p123, %r261, 0;
	@%p123 bra 	BB12_166;

	mov.f32 	%f762, 0fBAB6061A;
	mov.f32 	%f763, 0f37CCF5CE;
	fma.rn.f32 	%f1220, %f763, %f151, %f762;
	bra.uni 	BB12_167;

BB12_166:
	mov.f32 	%f764, 0f3C08839E;
	mov.f32 	%f765, 0fB94CA1F9;
	fma.rn.f32 	%f1220, %f765, %f151, %f764;

BB12_167:
	@%p123 bra 	BB12_169;

	mov.f32 	%f766, 0f3D2AAAA5;
	fma.rn.f32 	%f767, %f1220, %f151, %f766;
	mov.f32 	%f768, 0fBF000000;
	fma.rn.f32 	%f1221, %f767, %f151, %f768;
	bra.uni 	BB12_170;

BB12_169:
	mov.f32 	%f769, 0fBE2AAAA3;
	fma.rn.f32 	%f770, %f1220, %f151, %f769;
	mov.f32 	%f771, 0f00000000;
	fma.rn.f32 	%f1221, %f770, %f151, %f771;

BB12_170:
	fma.rn.f32 	%f1222, %f1221, %f1219, %f1219;
	@%p123 bra 	BB12_172;

	mov.f32 	%f772, 0f3F800000;
	fma.rn.f32 	%f1222, %f1221, %f151, %f772;

BB12_172:
	and.b32  	%r1231, %r2066, 2;
	setp.eq.s32	%p126, %r1231, 0;
	@%p126 bra 	BB12_174;

	mov.f32 	%f773, 0f00000000;
	mov.f32 	%f774, 0fBF800000;
	fma.rn.f32 	%f1222, %f1222, %f774, %f773;

BB12_174:
	mov.f32 	%f1295, %f36;
	@%p49 bra 	BB12_176;

	mov.f32 	%f775, 0f00000000;
	mul.rn.f32 	%f1295, %f36, %f775;

BB12_176:
	mul.f32 	%f776, %f1295, 0f3F22F983;
	cvt.rni.s32.f32	%r2076, %f776;
	cvt.rn.f32.s32	%f777, %r2076;
	neg.f32 	%f778, %f777;
	fma.rn.f32 	%f780, %f778, %f618, %f1295;
	fma.rn.f32 	%f782, %f778, %f620, %f780;
	fma.rn.f32 	%f1223, %f778, %f622, %f782;
	abs.f32 	%f784, %f1295;
	setp.leu.f32	%p128, %f784, 0f47CE4780;
	@%p128 bra 	BB12_186;

	mov.b32 	 %r263, %f1295;
	shr.u32 	%r264, %r263, 23;
	bfe.u32 	%r1234, %r263, 23, 8;
	add.s32 	%r1235, %r1234, -128;
	shl.b32 	%r1236, %r263, 8;
	or.b32  	%r265, %r1236, -2147483648;
	shr.u32 	%r266, %r1235, 5;
	mov.u32 	%r2068, 0;
	mov.u64 	%rd179, __cudart_i2opi_f;
	mov.u32 	%r2067, -6;
	mov.u64 	%rd235, %rd1;

BB12_178:
	.pragma "nounroll";
	ld.const.u32 	%r1239, [%rd179];
	// inline asm
	{
	mad.lo.cc.u32   %r1237, %r1239, %r265, %r2068;
	madc.hi.u32     %r2068, %r1239, %r265,  0;
	}
	// inline asm
	st.local.u32 	[%rd235], %r1237;
	add.s64 	%rd235, %rd235, 4;
	add.s64 	%rd179, %rd179, 4;
	add.s32 	%r2067, %r2067, 1;
	setp.ne.s32	%p129, %r2067, 0;
	@%p129 bra 	BB12_178;

	and.b32  	%r271, %r263, -2147483648;
	st.local.u32 	[%rd2], %r2068;
	mov.u32 	%r1242, 6;
	sub.s32 	%r1243, %r1242, %r266;
	mul.wide.s32 	%rd139, %r1243, 4;
	add.s64 	%rd42, %rd1, %rd139;
	ld.local.u32 	%r2069, [%rd42];
	ld.local.u32 	%r2070, [%rd42+-4];
	and.b32  	%r274, %r264, 31;
	setp.eq.s32	%p130, %r274, 0;
	@%p130 bra 	BB12_181;

	mov.u32 	%r1244, 32;
	sub.s32 	%r1245, %r1244, %r274;
	shr.u32 	%r1246, %r2070, %r1245;
	shl.b32 	%r1247, %r2069, %r274;
	add.s32 	%r2069, %r1246, %r1247;
	ld.local.u32 	%r1248, [%rd42+-8];
	shr.u32 	%r1249, %r1248, %r1245;
	shl.b32 	%r1250, %r2070, %r274;
	add.s32 	%r2070, %r1249, %r1250;

BB12_181:
	shr.u32 	%r1251, %r2070, 30;
	shl.b32 	%r1252, %r2069, 2;
	add.s32 	%r2071, %r1251, %r1252;
	shl.b32 	%r280, %r2070, 2;
	shr.u32 	%r1253, %r2071, 31;
	shr.u32 	%r1254, %r2069, 30;
	add.s32 	%r281, %r1253, %r1254;
	setp.eq.s32	%p131, %r1253, 0;
	mov.u32 	%r2072, %r271;
	mov.u32 	%r2073, %r280;
	@%p131 bra 	BB12_183;

	not.b32 	%r1255, %r2071;
	neg.s32 	%r282, %r280;
	setp.eq.s32	%p132, %r280, 0;
	selp.u32	%r1256, 1, 0, %p132;
	add.s32 	%r2071, %r1256, %r1255;
	xor.b32  	%r284, %r271, -2147483648;
	mov.u32 	%r2072, %r284;
	mov.u32 	%r2073, %r282;

BB12_183:
	mov.u32 	%r286, %r2072;
	neg.s32 	%r1257, %r281;
	setp.eq.s32	%p133, %r271, 0;
	selp.b32	%r2076, %r281, %r1257, %p133;
	clz.b32 	%r2075, %r2071;
	setp.eq.s32	%p134, %r2075, 0;
	shl.b32 	%r1258, %r2071, %r2075;
	mov.u32 	%r1259, 32;
	sub.s32 	%r1260, %r1259, %r2075;
	shr.u32 	%r1261, %r2073, %r1260;
	add.s32 	%r1262, %r1261, %r1258;
	selp.b32	%r290, %r2071, %r1262, %p134;
	mov.u32 	%r1263, -921707870;
	mul.hi.u32 	%r2074, %r290, %r1263;
	setp.lt.s32	%p135, %r2074, 1;
	@%p135 bra 	BB12_185;

	mul.lo.s32 	%r1264, %r290, -921707870;
	shr.u32 	%r1265, %r1264, 31;
	shl.b32 	%r1266, %r2074, 1;
	add.s32 	%r2074, %r1265, %r1266;
	add.s32 	%r2075, %r2075, 1;

BB12_185:
	mov.u32 	%r1267, 126;
	sub.s32 	%r1268, %r1267, %r2075;
	shl.b32 	%r1269, %r1268, 23;
	add.s32 	%r1270, %r2074, 1;
	shr.u32 	%r1271, %r1270, 7;
	add.s32 	%r1272, %r1271, 1;
	shr.u32 	%r1273, %r1272, 1;
	add.s32 	%r1274, %r1273, %r1269;
	or.b32  	%r1275, %r1274, %r286;
	mov.b32 	 %f1223, %r1275;

BB12_186:
	mul.rn.f32 	%f168, %f1223, %f1223;
	add.s32 	%r297, %r2076, 1;
	and.b32  	%r298, %r297, 1;
	setp.eq.s32	%p136, %r298, 0;
	@%p136 bra 	BB12_188;

	mov.f32 	%f785, 0fBAB6061A;
	mov.f32 	%f786, 0f37CCF5CE;
	fma.rn.f32 	%f1224, %f786, %f168, %f785;
	bra.uni 	BB12_189;

BB12_188:
	mov.f32 	%f787, 0f3C08839E;
	mov.f32 	%f788, 0fB94CA1F9;
	fma.rn.f32 	%f1224, %f788, %f168, %f787;

BB12_189:
	@%p136 bra 	BB12_191;

	mov.f32 	%f789, 0f3D2AAAA5;
	fma.rn.f32 	%f790, %f1224, %f168, %f789;
	mov.f32 	%f791, 0fBF000000;
	fma.rn.f32 	%f1225, %f790, %f168, %f791;
	bra.uni 	BB12_192;

BB12_191:
	mov.f32 	%f792, 0fBE2AAAA3;
	fma.rn.f32 	%f793, %f1224, %f168, %f792;
	mov.f32 	%f794, 0f00000000;
	fma.rn.f32 	%f1225, %f793, %f168, %f794;

BB12_192:
	fma.rn.f32 	%f1226, %f1225, %f1223, %f1223;
	@%p136 bra 	BB12_194;

	mov.f32 	%f795, 0f3F800000;
	fma.rn.f32 	%f1226, %f1225, %f168, %f795;

BB12_194:
	and.b32  	%r1276, %r297, 2;
	setp.eq.s32	%p139, %r1276, 0;
	@%p139 bra 	BB12_196;

	mov.f32 	%f796, 0f00000000;
	mov.f32 	%f797, 0fBF800000;
	fma.rn.f32 	%f1226, %f1226, %f797, %f796;

BB12_196:
	mul.f32 	%f180, %f1222, %f1226;
	mov.f32 	%f1314, %f13;
	@%p62 bra 	BB12_198;

	mov.f32 	%f798, 0f00000000;
	mul.rn.f32 	%f1314, %f13, %f798;

BB12_198:
	mul.f32 	%f799, %f1314, 0f3F22F983;
	cvt.rni.s32.f32	%r2086, %f799;
	cvt.rn.f32.s32	%f800, %r2086;
	neg.f32 	%f801, %f800;
	fma.rn.f32 	%f803, %f801, %f618, %f1314;
	fma.rn.f32 	%f805, %f801, %f620, %f803;
	fma.rn.f32 	%f1227, %f801, %f622, %f805;
	abs.f32 	%f807, %f1314;
	setp.leu.f32	%p141, %f807, 0f47CE4780;
	@%p141 bra 	BB12_208;

	mov.b32 	 %r300, %f1314;
	shr.u32 	%r301, %r300, 23;
	bfe.u32 	%r1279, %r300, 23, 8;
	add.s32 	%r1280, %r1279, -128;
	shl.b32 	%r1281, %r300, 8;
	or.b32  	%r302, %r1281, -2147483648;
	shr.u32 	%r303, %r1280, 5;
	mov.u32 	%r2078, 0;
	mov.u64 	%rd180, __cudart_i2opi_f;
	mov.u32 	%r2077, -6;
	mov.u64 	%rd234, %rd1;

BB12_200:
	.pragma "nounroll";
	ld.const.u32 	%r1284, [%rd180];
	// inline asm
	{
	mad.lo.cc.u32   %r1282, %r1284, %r302, %r2078;
	madc.hi.u32     %r2078, %r1284, %r302,  0;
	}
	// inline asm
	st.local.u32 	[%rd234], %r1282;
	add.s64 	%rd234, %rd234, 4;
	add.s64 	%rd180, %rd180, 4;
	add.s32 	%r2077, %r2077, 1;
	setp.ne.s32	%p142, %r2077, 0;
	@%p142 bra 	BB12_200;

	and.b32  	%r308, %r300, -2147483648;
	st.local.u32 	[%rd2], %r2078;
	mov.u32 	%r1287, 6;
	sub.s32 	%r1288, %r1287, %r303;
	mul.wide.s32 	%rd141, %r1288, 4;
	add.s64 	%rd47, %rd1, %rd141;
	ld.local.u32 	%r2079, [%rd47];
	ld.local.u32 	%r2080, [%rd47+-4];
	and.b32  	%r311, %r301, 31;
	setp.eq.s32	%p143, %r311, 0;
	@%p143 bra 	BB12_203;

	mov.u32 	%r1289, 32;
	sub.s32 	%r1290, %r1289, %r311;
	shr.u32 	%r1291, %r2080, %r1290;
	shl.b32 	%r1292, %r2079, %r311;
	add.s32 	%r2079, %r1291, %r1292;
	ld.local.u32 	%r1293, [%rd47+-8];
	shr.u32 	%r1294, %r1293, %r1290;
	shl.b32 	%r1295, %r2080, %r311;
	add.s32 	%r2080, %r1294, %r1295;

BB12_203:
	shr.u32 	%r1296, %r2080, 30;
	shl.b32 	%r1297, %r2079, 2;
	add.s32 	%r2081, %r1296, %r1297;
	shl.b32 	%r317, %r2080, 2;
	shr.u32 	%r1298, %r2081, 31;
	shr.u32 	%r1299, %r2079, 30;
	add.s32 	%r318, %r1298, %r1299;
	setp.eq.s32	%p144, %r1298, 0;
	mov.u32 	%r2082, %r308;
	mov.u32 	%r2083, %r317;
	@%p144 bra 	BB12_205;

	not.b32 	%r1300, %r2081;
	neg.s32 	%r319, %r317;
	setp.eq.s32	%p145, %r317, 0;
	selp.u32	%r1301, 1, 0, %p145;
	add.s32 	%r2081, %r1301, %r1300;
	xor.b32  	%r321, %r308, -2147483648;
	mov.u32 	%r2082, %r321;
	mov.u32 	%r2083, %r319;

BB12_205:
	mov.u32 	%r323, %r2082;
	neg.s32 	%r1302, %r318;
	setp.eq.s32	%p146, %r308, 0;
	selp.b32	%r2086, %r318, %r1302, %p146;
	clz.b32 	%r2085, %r2081;
	setp.eq.s32	%p147, %r2085, 0;
	shl.b32 	%r1303, %r2081, %r2085;
	mov.u32 	%r1304, 32;
	sub.s32 	%r1305, %r1304, %r2085;
	shr.u32 	%r1306, %r2083, %r1305;
	add.s32 	%r1307, %r1306, %r1303;
	selp.b32	%r327, %r2081, %r1307, %p147;
	mov.u32 	%r1308, -921707870;
	mul.hi.u32 	%r2084, %r327, %r1308;
	setp.lt.s32	%p148, %r2084, 1;
	@%p148 bra 	BB12_207;

	mul.lo.s32 	%r1309, %r327, -921707870;
	shr.u32 	%r1310, %r1309, 31;
	shl.b32 	%r1311, %r2084, 1;
	add.s32 	%r2084, %r1310, %r1311;
	add.s32 	%r2085, %r2085, 1;

BB12_207:
	mov.u32 	%r1312, 126;
	sub.s32 	%r1313, %r1312, %r2085;
	shl.b32 	%r1314, %r1313, 23;
	add.s32 	%r1315, %r2084, 1;
	shr.u32 	%r1316, %r1315, 7;
	add.s32 	%r1317, %r1316, 1;
	shr.u32 	%r1318, %r1317, 1;
	add.s32 	%r1319, %r1318, %r1314;
	or.b32  	%r1320, %r1319, %r323;
	mov.b32 	 %f1227, %r1320;

BB12_208:
	mul.rn.f32 	%f186, %f1227, %f1227;
	add.s32 	%r334, %r2086, 1;
	and.b32  	%r335, %r334, 1;
	setp.eq.s32	%p149, %r335, 0;
	@%p149 bra 	BB12_210;

	mov.f32 	%f808, 0fBAB6061A;
	mov.f32 	%f809, 0f37CCF5CE;
	fma.rn.f32 	%f1228, %f809, %f186, %f808;
	bra.uni 	BB12_211;

BB12_210:
	mov.f32 	%f810, 0f3C08839E;
	mov.f32 	%f811, 0fB94CA1F9;
	fma.rn.f32 	%f1228, %f811, %f186, %f810;

BB12_211:
	@%p149 bra 	BB12_213;

	mov.f32 	%f812, 0f3D2AAAA5;
	fma.rn.f32 	%f813, %f1228, %f186, %f812;
	mov.f32 	%f814, 0fBF000000;
	fma.rn.f32 	%f1229, %f813, %f186, %f814;
	bra.uni 	BB12_214;

BB12_213:
	mov.f32 	%f815, 0fBE2AAAA3;
	fma.rn.f32 	%f816, %f1228, %f186, %f815;
	mov.f32 	%f817, 0f00000000;
	fma.rn.f32 	%f1229, %f816, %f186, %f817;

BB12_214:
	fma.rn.f32 	%f1230, %f1229, %f1227, %f1227;
	@%p149 bra 	BB12_216;

	mov.f32 	%f818, 0f3F800000;
	fma.rn.f32 	%f1230, %f1229, %f186, %f818;

BB12_216:
	and.b32  	%r1321, %r334, 2;
	setp.eq.s32	%p152, %r1321, 0;
	@%p152 bra 	BB12_218;

	mov.f32 	%f819, 0f00000000;
	mov.f32 	%f820, 0fBF800000;
	fma.rn.f32 	%f1230, %f1230, %f820, %f819;

BB12_218:
	mul.f32 	%f198, %f180, %f1230;
	mov.f32 	%f1340, %f34;
	@%p36 bra 	BB12_220;

	mov.f32 	%f821, 0f00000000;
	mul.rn.f32 	%f1340, %f34, %f821;

BB12_220:
	mul.f32 	%f822, %f1340, 0f3F22F983;
	cvt.rni.s32.f32	%r2096, %f822;
	cvt.rn.f32.s32	%f823, %r2096;
	neg.f32 	%f824, %f823;
	fma.rn.f32 	%f826, %f824, %f618, %f1340;
	fma.rn.f32 	%f828, %f824, %f620, %f826;
	fma.rn.f32 	%f1231, %f824, %f622, %f828;
	abs.f32 	%f830, %f1340;
	setp.leu.f32	%p154, %f830, 0f47CE4780;
	@%p154 bra 	BB12_230;

	mov.b32 	 %r337, %f1340;
	shr.u32 	%r338, %r337, 23;
	bfe.u32 	%r1324, %r337, 23, 8;
	add.s32 	%r1325, %r1324, -128;
	shl.b32 	%r1326, %r337, 8;
	or.b32  	%r339, %r1326, -2147483648;
	shr.u32 	%r340, %r1325, 5;
	mov.u32 	%r2088, 0;
	mov.u64 	%rd181, __cudart_i2opi_f;
	mov.u32 	%r2087, -6;
	mov.u64 	%rd233, %rd1;

BB12_222:
	.pragma "nounroll";
	ld.const.u32 	%r1329, [%rd181];
	// inline asm
	{
	mad.lo.cc.u32   %r1327, %r1329, %r339, %r2088;
	madc.hi.u32     %r2088, %r1329, %r339,  0;
	}
	// inline asm
	st.local.u32 	[%rd233], %r1327;
	add.s64 	%rd233, %rd233, 4;
	add.s64 	%rd181, %rd181, 4;
	add.s32 	%r2087, %r2087, 1;
	setp.ne.s32	%p155, %r2087, 0;
	@%p155 bra 	BB12_222;

	and.b32  	%r345, %r337, -2147483648;
	st.local.u32 	[%rd2], %r2088;
	mov.u32 	%r1332, 6;
	sub.s32 	%r1333, %r1332, %r340;
	mul.wide.s32 	%rd143, %r1333, 4;
	add.s64 	%rd52, %rd1, %rd143;
	ld.local.u32 	%r2089, [%rd52];
	ld.local.u32 	%r2090, [%rd52+-4];
	and.b32  	%r348, %r338, 31;
	setp.eq.s32	%p156, %r348, 0;
	@%p156 bra 	BB12_225;

	mov.u32 	%r1334, 32;
	sub.s32 	%r1335, %r1334, %r348;
	shr.u32 	%r1336, %r2090, %r1335;
	shl.b32 	%r1337, %r2089, %r348;
	add.s32 	%r2089, %r1336, %r1337;
	ld.local.u32 	%r1338, [%rd52+-8];
	shr.u32 	%r1339, %r1338, %r1335;
	shl.b32 	%r1340, %r2090, %r348;
	add.s32 	%r2090, %r1339, %r1340;

BB12_225:
	shr.u32 	%r1341, %r2090, 30;
	shl.b32 	%r1342, %r2089, 2;
	add.s32 	%r2091, %r1341, %r1342;
	shl.b32 	%r354, %r2090, 2;
	shr.u32 	%r1343, %r2091, 31;
	shr.u32 	%r1344, %r2089, 30;
	add.s32 	%r355, %r1343, %r1344;
	setp.eq.s32	%p157, %r1343, 0;
	mov.u32 	%r2092, %r345;
	mov.u32 	%r2093, %r354;
	@%p157 bra 	BB12_227;

	not.b32 	%r1345, %r2091;
	neg.s32 	%r356, %r354;
	setp.eq.s32	%p158, %r354, 0;
	selp.u32	%r1346, 1, 0, %p158;
	add.s32 	%r2091, %r1346, %r1345;
	xor.b32  	%r358, %r345, -2147483648;
	mov.u32 	%r2092, %r358;
	mov.u32 	%r2093, %r356;

BB12_227:
	mov.u32 	%r360, %r2092;
	neg.s32 	%r1347, %r355;
	setp.eq.s32	%p159, %r345, 0;
	selp.b32	%r2096, %r355, %r1347, %p159;
	clz.b32 	%r2095, %r2091;
	setp.eq.s32	%p160, %r2095, 0;
	shl.b32 	%r1348, %r2091, %r2095;
	mov.u32 	%r1349, 32;
	sub.s32 	%r1350, %r1349, %r2095;
	shr.u32 	%r1351, %r2093, %r1350;
	add.s32 	%r1352, %r1351, %r1348;
	selp.b32	%r364, %r2091, %r1352, %p160;
	mov.u32 	%r1353, -921707870;
	mul.hi.u32 	%r2094, %r364, %r1353;
	setp.lt.s32	%p161, %r2094, 1;
	@%p161 bra 	BB12_229;

	mul.lo.s32 	%r1354, %r364, -921707870;
	shr.u32 	%r1355, %r1354, 31;
	shl.b32 	%r1356, %r2094, 1;
	add.s32 	%r2094, %r1355, %r1356;
	add.s32 	%r2095, %r2095, 1;

BB12_229:
	mov.u32 	%r1357, 126;
	sub.s32 	%r1358, %r1357, %r2095;
	shl.b32 	%r1359, %r1358, 23;
	add.s32 	%r1360, %r2094, 1;
	shr.u32 	%r1361, %r1360, 7;
	add.s32 	%r1362, %r1361, 1;
	shr.u32 	%r1363, %r1362, 1;
	add.s32 	%r1364, %r1363, %r1359;
	or.b32  	%r1365, %r1364, %r360;
	mov.b32 	 %f1231, %r1365;

BB12_230:
	mul.rn.f32 	%f204, %f1231, %f1231;
	add.s32 	%r371, %r2096, 1;
	and.b32  	%r372, %r371, 1;
	setp.eq.s32	%p162, %r372, 0;
	@%p162 bra 	BB12_232;

	mov.f32 	%f831, 0fBAB6061A;
	mov.f32 	%f832, 0f37CCF5CE;
	fma.rn.f32 	%f1232, %f832, %f204, %f831;
	bra.uni 	BB12_233;

BB12_232:
	mov.f32 	%f833, 0f3C08839E;
	mov.f32 	%f834, 0fB94CA1F9;
	fma.rn.f32 	%f1232, %f834, %f204, %f833;

BB12_233:
	@%p162 bra 	BB12_235;

	mov.f32 	%f835, 0f3D2AAAA5;
	fma.rn.f32 	%f836, %f1232, %f204, %f835;
	mov.f32 	%f837, 0fBF000000;
	fma.rn.f32 	%f1233, %f836, %f204, %f837;
	bra.uni 	BB12_236;

BB12_235:
	mov.f32 	%f838, 0fBE2AAAA3;
	fma.rn.f32 	%f839, %f1232, %f204, %f838;
	mov.f32 	%f840, 0f00000000;
	fma.rn.f32 	%f1233, %f839, %f204, %f840;

BB12_236:
	fma.rn.f32 	%f1234, %f1233, %f1231, %f1231;
	@%p162 bra 	BB12_238;

	mov.f32 	%f841, 0f3F800000;
	fma.rn.f32 	%f1234, %f1233, %f204, %f841;

BB12_238:
	and.b32  	%r1366, %r371, 2;
	setp.eq.s32	%p165, %r1366, 0;
	@%p165 bra 	BB12_240;

	mov.f32 	%f842, 0f00000000;
	mov.f32 	%f843, 0fBF800000;
	fma.rn.f32 	%f1234, %f1234, %f843, %f842;

BB12_240:
	mov.f32 	%f1294, %f36;
	@%p49 bra 	BB12_242;

	mov.f32 	%f844, 0f00000000;
	mul.rn.f32 	%f1294, %f36, %f844;

BB12_242:
	mul.f32 	%f845, %f1294, 0f3F22F983;
	cvt.rni.s32.f32	%r2106, %f845;
	cvt.rn.f32.s32	%f846, %r2106;
	neg.f32 	%f847, %f846;
	fma.rn.f32 	%f849, %f847, %f618, %f1294;
	fma.rn.f32 	%f851, %f847, %f620, %f849;
	fma.rn.f32 	%f1235, %f847, %f622, %f851;
	abs.f32 	%f853, %f1294;
	setp.leu.f32	%p167, %f853, 0f47CE4780;
	@%p167 bra 	BB12_252;

	mov.b32 	 %r374, %f1294;
	shr.u32 	%r375, %r374, 23;
	bfe.u32 	%r1369, %r374, 23, 8;
	add.s32 	%r1370, %r1369, -128;
	shl.b32 	%r1371, %r374, 8;
	or.b32  	%r376, %r1371, -2147483648;
	shr.u32 	%r377, %r1370, 5;
	mov.u32 	%r2098, 0;
	mov.u64 	%rd182, __cudart_i2opi_f;
	mov.u32 	%r2097, -6;
	mov.u64 	%rd232, %rd1;

BB12_244:
	.pragma "nounroll";
	ld.const.u32 	%r1374, [%rd182];
	// inline asm
	{
	mad.lo.cc.u32   %r1372, %r1374, %r376, %r2098;
	madc.hi.u32     %r2098, %r1374, %r376,  0;
	}
	// inline asm
	st.local.u32 	[%rd232], %r1372;
	add.s64 	%rd232, %rd232, 4;
	add.s64 	%rd182, %rd182, 4;
	add.s32 	%r2097, %r2097, 1;
	setp.ne.s32	%p168, %r2097, 0;
	@%p168 bra 	BB12_244;

	and.b32  	%r382, %r374, -2147483648;
	st.local.u32 	[%rd2], %r2098;
	mov.u32 	%r1377, 6;
	sub.s32 	%r1378, %r1377, %r377;
	mul.wide.s32 	%rd145, %r1378, 4;
	add.s64 	%rd57, %rd1, %rd145;
	ld.local.u32 	%r2099, [%rd57];
	ld.local.u32 	%r2100, [%rd57+-4];
	and.b32  	%r385, %r375, 31;
	setp.eq.s32	%p169, %r385, 0;
	@%p169 bra 	BB12_247;

	mov.u32 	%r1379, 32;
	sub.s32 	%r1380, %r1379, %r385;
	shr.u32 	%r1381, %r2100, %r1380;
	shl.b32 	%r1382, %r2099, %r385;
	add.s32 	%r2099, %r1381, %r1382;
	ld.local.u32 	%r1383, [%rd57+-8];
	shr.u32 	%r1384, %r1383, %r1380;
	shl.b32 	%r1385, %r2100, %r385;
	add.s32 	%r2100, %r1384, %r1385;

BB12_247:
	shr.u32 	%r1386, %r2100, 30;
	shl.b32 	%r1387, %r2099, 2;
	add.s32 	%r2101, %r1386, %r1387;
	shl.b32 	%r391, %r2100, 2;
	shr.u32 	%r1388, %r2101, 31;
	shr.u32 	%r1389, %r2099, 30;
	add.s32 	%r392, %r1388, %r1389;
	setp.eq.s32	%p170, %r1388, 0;
	mov.u32 	%r2102, %r382;
	mov.u32 	%r2103, %r391;
	@%p170 bra 	BB12_249;

	not.b32 	%r1390, %r2101;
	neg.s32 	%r393, %r391;
	setp.eq.s32	%p171, %r391, 0;
	selp.u32	%r1391, 1, 0, %p171;
	add.s32 	%r2101, %r1391, %r1390;
	xor.b32  	%r395, %r382, -2147483648;
	mov.u32 	%r2102, %r395;
	mov.u32 	%r2103, %r393;

BB12_249:
	mov.u32 	%r397, %r2102;
	neg.s32 	%r1392, %r392;
	setp.eq.s32	%p172, %r382, 0;
	selp.b32	%r2106, %r392, %r1392, %p172;
	clz.b32 	%r2105, %r2101;
	setp.eq.s32	%p173, %r2105, 0;
	shl.b32 	%r1393, %r2101, %r2105;
	mov.u32 	%r1394, 32;
	sub.s32 	%r1395, %r1394, %r2105;
	shr.u32 	%r1396, %r2103, %r1395;
	add.s32 	%r1397, %r1396, %r1393;
	selp.b32	%r401, %r2101, %r1397, %p173;
	mov.u32 	%r1398, -921707870;
	mul.hi.u32 	%r2104, %r401, %r1398;
	setp.lt.s32	%p174, %r2104, 1;
	@%p174 bra 	BB12_251;

	mul.lo.s32 	%r1399, %r401, -921707870;
	shr.u32 	%r1400, %r1399, 31;
	shl.b32 	%r1401, %r2104, 1;
	add.s32 	%r2104, %r1400, %r1401;
	add.s32 	%r2105, %r2105, 1;

BB12_251:
	mov.u32 	%r1402, 126;
	sub.s32 	%r1403, %r1402, %r2105;
	shl.b32 	%r1404, %r1403, 23;
	add.s32 	%r1405, %r2104, 1;
	shr.u32 	%r1406, %r1405, 7;
	add.s32 	%r1407, %r1406, 1;
	shr.u32 	%r1408, %r1407, 1;
	add.s32 	%r1409, %r1408, %r1404;
	or.b32  	%r1410, %r1409, %r397;
	mov.b32 	 %f1235, %r1410;

BB12_252:
	mul.rn.f32 	%f221, %f1235, %f1235;
	and.b32  	%r408, %r2106, 1;
	setp.eq.s32	%p175, %r408, 0;
	@%p175 bra 	BB12_254;

	mov.f32 	%f854, 0fBAB6061A;
	mov.f32 	%f855, 0f37CCF5CE;
	fma.rn.f32 	%f1236, %f855, %f221, %f854;
	bra.uni 	BB12_255;

BB12_254:
	mov.f32 	%f856, 0f3C08839E;
	mov.f32 	%f857, 0fB94CA1F9;
	fma.rn.f32 	%f1236, %f857, %f221, %f856;

BB12_255:
	@%p175 bra 	BB12_257;

	mov.f32 	%f858, 0f3D2AAAA5;
	fma.rn.f32 	%f859, %f1236, %f221, %f858;
	mov.f32 	%f860, 0fBF000000;
	fma.rn.f32 	%f1237, %f859, %f221, %f860;
	bra.uni 	BB12_258;

BB12_257:
	mov.f32 	%f861, 0fBE2AAAA3;
	fma.rn.f32 	%f862, %f1236, %f221, %f861;
	mov.f32 	%f863, 0f00000000;
	fma.rn.f32 	%f1237, %f862, %f221, %f863;

BB12_258:
	fma.rn.f32 	%f1238, %f1237, %f1235, %f1235;
	@%p175 bra 	BB12_260;

	mov.f32 	%f864, 0f3F800000;
	fma.rn.f32 	%f1238, %f1237, %f221, %f864;

BB12_260:
	and.b32  	%r1411, %r2106, 2;
	setp.eq.s32	%p178, %r1411, 0;
	@%p178 bra 	BB12_262;

	mov.f32 	%f865, 0f00000000;
	mov.f32 	%f866, 0fBF800000;
	fma.rn.f32 	%f1238, %f1238, %f866, %f865;

BB12_262:
	mul.f32 	%f233, %f1234, %f1238;
	mov.f32 	%f1313, %f13;
	@%p62 bra 	BB12_264;

	mov.f32 	%f867, 0f00000000;
	mul.rn.f32 	%f1313, %f13, %f867;

BB12_264:
	mul.f32 	%f868, %f1313, 0f3F22F983;
	cvt.rni.s32.f32	%r2116, %f868;
	cvt.rn.f32.s32	%f869, %r2116;
	neg.f32 	%f870, %f869;
	fma.rn.f32 	%f872, %f870, %f618, %f1313;
	fma.rn.f32 	%f874, %f870, %f620, %f872;
	fma.rn.f32 	%f1239, %f870, %f622, %f874;
	abs.f32 	%f876, %f1313;
	setp.leu.f32	%p180, %f876, 0f47CE4780;
	@%p180 bra 	BB12_274;

	mov.b32 	 %r410, %f1313;
	shr.u32 	%r411, %r410, 23;
	bfe.u32 	%r1414, %r410, 23, 8;
	add.s32 	%r1415, %r1414, -128;
	shl.b32 	%r1416, %r410, 8;
	or.b32  	%r412, %r1416, -2147483648;
	shr.u32 	%r413, %r1415, 5;
	mov.u32 	%r2108, 0;
	mov.u64 	%rd183, __cudart_i2opi_f;
	mov.u32 	%r2107, -6;
	mov.u64 	%rd231, %rd1;

BB12_266:
	.pragma "nounroll";
	ld.const.u32 	%r1419, [%rd183];
	// inline asm
	{
	mad.lo.cc.u32   %r1417, %r1419, %r412, %r2108;
	madc.hi.u32     %r2108, %r1419, %r412,  0;
	}
	// inline asm
	st.local.u32 	[%rd231], %r1417;
	add.s64 	%rd231, %rd231, 4;
	add.s64 	%rd183, %rd183, 4;
	add.s32 	%r2107, %r2107, 1;
	setp.ne.s32	%p181, %r2107, 0;
	@%p181 bra 	BB12_266;

	and.b32  	%r418, %r410, -2147483648;
	st.local.u32 	[%rd2], %r2108;
	mov.u32 	%r1422, 6;
	sub.s32 	%r1423, %r1422, %r413;
	mul.wide.s32 	%rd147, %r1423, 4;
	add.s64 	%rd62, %rd1, %rd147;
	ld.local.u32 	%r2109, [%rd62];
	ld.local.u32 	%r2110, [%rd62+-4];
	and.b32  	%r421, %r411, 31;
	setp.eq.s32	%p182, %r421, 0;
	@%p182 bra 	BB12_269;

	mov.u32 	%r1424, 32;
	sub.s32 	%r1425, %r1424, %r421;
	shr.u32 	%r1426, %r2110, %r1425;
	shl.b32 	%r1427, %r2109, %r421;
	add.s32 	%r2109, %r1426, %r1427;
	ld.local.u32 	%r1428, [%rd62+-8];
	shr.u32 	%r1429, %r1428, %r1425;
	shl.b32 	%r1430, %r2110, %r421;
	add.s32 	%r2110, %r1429, %r1430;

BB12_269:
	shr.u32 	%r1431, %r2110, 30;
	shl.b32 	%r1432, %r2109, 2;
	add.s32 	%r2111, %r1431, %r1432;
	shl.b32 	%r427, %r2110, 2;
	shr.u32 	%r1433, %r2111, 31;
	shr.u32 	%r1434, %r2109, 30;
	add.s32 	%r428, %r1433, %r1434;
	setp.eq.s32	%p183, %r1433, 0;
	mov.u32 	%r2112, %r418;
	mov.u32 	%r2113, %r427;
	@%p183 bra 	BB12_271;

	not.b32 	%r1435, %r2111;
	neg.s32 	%r429, %r427;
	setp.eq.s32	%p184, %r427, 0;
	selp.u32	%r1436, 1, 0, %p184;
	add.s32 	%r2111, %r1436, %r1435;
	xor.b32  	%r431, %r418, -2147483648;
	mov.u32 	%r2112, %r431;
	mov.u32 	%r2113, %r429;

BB12_271:
	mov.u32 	%r433, %r2112;
	neg.s32 	%r1437, %r428;
	setp.eq.s32	%p185, %r418, 0;
	selp.b32	%r2116, %r428, %r1437, %p185;
	clz.b32 	%r2115, %r2111;
	setp.eq.s32	%p186, %r2115, 0;
	shl.b32 	%r1438, %r2111, %r2115;
	mov.u32 	%r1439, 32;
	sub.s32 	%r1440, %r1439, %r2115;
	shr.u32 	%r1441, %r2113, %r1440;
	add.s32 	%r1442, %r1441, %r1438;
	selp.b32	%r437, %r2111, %r1442, %p186;
	mov.u32 	%r1443, -921707870;
	mul.hi.u32 	%r2114, %r437, %r1443;
	setp.lt.s32	%p187, %r2114, 1;
	@%p187 bra 	BB12_273;

	mul.lo.s32 	%r1444, %r437, -921707870;
	shr.u32 	%r1445, %r1444, 31;
	shl.b32 	%r1446, %r2114, 1;
	add.s32 	%r2114, %r1445, %r1446;
	add.s32 	%r2115, %r2115, 1;

BB12_273:
	mov.u32 	%r1447, 126;
	sub.s32 	%r1448, %r1447, %r2115;
	shl.b32 	%r1449, %r1448, 23;
	add.s32 	%r1450, %r2114, 1;
	shr.u32 	%r1451, %r1450, 7;
	add.s32 	%r1452, %r1451, 1;
	shr.u32 	%r1453, %r1452, 1;
	add.s32 	%r1454, %r1453, %r1449;
	or.b32  	%r1455, %r1454, %r433;
	mov.b32 	 %f1239, %r1455;

BB12_274:
	mul.rn.f32 	%f239, %f1239, %f1239;
	and.b32  	%r444, %r2116, 1;
	setp.eq.s32	%p188, %r444, 0;
	@%p188 bra 	BB12_276;

	mov.f32 	%f877, 0fBAB6061A;
	mov.f32 	%f878, 0f37CCF5CE;
	fma.rn.f32 	%f1240, %f878, %f239, %f877;
	bra.uni 	BB12_277;

BB12_276:
	mov.f32 	%f879, 0f3C08839E;
	mov.f32 	%f880, 0fB94CA1F9;
	fma.rn.f32 	%f1240, %f880, %f239, %f879;

BB12_277:
	@%p188 bra 	BB12_279;

	mov.f32 	%f881, 0f3D2AAAA5;
	fma.rn.f32 	%f882, %f1240, %f239, %f881;
	mov.f32 	%f883, 0fBF000000;
	fma.rn.f32 	%f1241, %f882, %f239, %f883;
	bra.uni 	BB12_280;

BB12_279:
	mov.f32 	%f884, 0fBE2AAAA3;
	fma.rn.f32 	%f885, %f1240, %f239, %f884;
	mov.f32 	%f886, 0f00000000;
	fma.rn.f32 	%f1241, %f885, %f239, %f886;

BB12_280:
	fma.rn.f32 	%f1242, %f1241, %f1239, %f1239;
	@%p188 bra 	BB12_282;

	mov.f32 	%f887, 0f3F800000;
	fma.rn.f32 	%f1242, %f1241, %f239, %f887;

BB12_282:
	and.b32  	%r1456, %r2116, 2;
	setp.eq.s32	%p191, %r1456, 0;
	@%p191 bra 	BB12_284;

	mov.f32 	%f888, 0f00000000;
	mov.f32 	%f889, 0fBF800000;
	fma.rn.f32 	%f1242, %f1242, %f889, %f888;

BB12_284:
	mul.f32 	%f890, %f233, %f1242;
	sub.f32 	%f251, %f198, %f890;
	mov.f32 	%f1339, %f34;
	@%p36 bra 	BB12_286;

	mov.f32 	%f891, 0f00000000;
	mul.rn.f32 	%f1339, %f34, %f891;

BB12_286:
	mul.f32 	%f892, %f1339, 0f3F22F983;
	cvt.rni.s32.f32	%r2126, %f892;
	cvt.rn.f32.s32	%f893, %r2126;
	neg.f32 	%f894, %f893;
	fma.rn.f32 	%f896, %f894, %f618, %f1339;
	fma.rn.f32 	%f898, %f894, %f620, %f896;
	fma.rn.f32 	%f1243, %f894, %f622, %f898;
	abs.f32 	%f900, %f1339;
	setp.leu.f32	%p193, %f900, 0f47CE4780;
	@%p193 bra 	BB12_296;

	mov.b32 	 %r446, %f1339;
	shr.u32 	%r447, %r446, 23;
	bfe.u32 	%r1459, %r446, 23, 8;
	add.s32 	%r1460, %r1459, -128;
	shl.b32 	%r1461, %r446, 8;
	or.b32  	%r448, %r1461, -2147483648;
	shr.u32 	%r449, %r1460, 5;
	mov.u32 	%r2118, 0;
	mov.u64 	%rd184, __cudart_i2opi_f;
	mov.u32 	%r2117, -6;
	mov.u64 	%rd230, %rd1;

BB12_288:
	.pragma "nounroll";
	ld.const.u32 	%r1464, [%rd184];
	// inline asm
	{
	mad.lo.cc.u32   %r1462, %r1464, %r448, %r2118;
	madc.hi.u32     %r2118, %r1464, %r448,  0;
	}
	// inline asm
	st.local.u32 	[%rd230], %r1462;
	add.s64 	%rd230, %rd230, 4;
	add.s64 	%rd184, %rd184, 4;
	add.s32 	%r2117, %r2117, 1;
	setp.ne.s32	%p194, %r2117, 0;
	@%p194 bra 	BB12_288;

	and.b32  	%r454, %r446, -2147483648;
	st.local.u32 	[%rd2], %r2118;
	mov.u32 	%r1467, 6;
	sub.s32 	%r1468, %r1467, %r449;
	mul.wide.s32 	%rd149, %r1468, 4;
	add.s64 	%rd67, %rd1, %rd149;
	ld.local.u32 	%r2119, [%rd67];
	ld.local.u32 	%r2120, [%rd67+-4];
	and.b32  	%r457, %r447, 31;
	setp.eq.s32	%p195, %r457, 0;
	@%p195 bra 	BB12_291;

	mov.u32 	%r1469, 32;
	sub.s32 	%r1470, %r1469, %r457;
	shr.u32 	%r1471, %r2120, %r1470;
	shl.b32 	%r1472, %r2119, %r457;
	add.s32 	%r2119, %r1471, %r1472;
	ld.local.u32 	%r1473, [%rd67+-8];
	shr.u32 	%r1474, %r1473, %r1470;
	shl.b32 	%r1475, %r2120, %r457;
	add.s32 	%r2120, %r1474, %r1475;

BB12_291:
	shr.u32 	%r1476, %r2120, 30;
	shl.b32 	%r1477, %r2119, 2;
	add.s32 	%r2121, %r1476, %r1477;
	shl.b32 	%r463, %r2120, 2;
	shr.u32 	%r1478, %r2121, 31;
	shr.u32 	%r1479, %r2119, 30;
	add.s32 	%r464, %r1478, %r1479;
	setp.eq.s32	%p196, %r1478, 0;
	mov.u32 	%r2122, %r454;
	mov.u32 	%r2123, %r463;
	@%p196 bra 	BB12_293;

	not.b32 	%r1480, %r2121;
	neg.s32 	%r465, %r463;
	setp.eq.s32	%p197, %r463, 0;
	selp.u32	%r1481, 1, 0, %p197;
	add.s32 	%r2121, %r1481, %r1480;
	xor.b32  	%r467, %r454, -2147483648;
	mov.u32 	%r2122, %r467;
	mov.u32 	%r2123, %r465;

BB12_293:
	mov.u32 	%r469, %r2122;
	neg.s32 	%r1482, %r464;
	setp.eq.s32	%p198, %r454, 0;
	selp.b32	%r2126, %r464, %r1482, %p198;
	clz.b32 	%r2125, %r2121;
	setp.eq.s32	%p199, %r2125, 0;
	shl.b32 	%r1483, %r2121, %r2125;
	mov.u32 	%r1484, 32;
	sub.s32 	%r1485, %r1484, %r2125;
	shr.u32 	%r1486, %r2123, %r1485;
	add.s32 	%r1487, %r1486, %r1483;
	selp.b32	%r473, %r2121, %r1487, %p199;
	mov.u32 	%r1488, -921707870;
	mul.hi.u32 	%r2124, %r473, %r1488;
	setp.lt.s32	%p200, %r2124, 1;
	@%p200 bra 	BB12_295;

	mul.lo.s32 	%r1489, %r473, -921707870;
	shr.u32 	%r1490, %r1489, 31;
	shl.b32 	%r1491, %r2124, 1;
	add.s32 	%r2124, %r1490, %r1491;
	add.s32 	%r2125, %r2125, 1;

BB12_295:
	mov.u32 	%r1492, 126;
	sub.s32 	%r1493, %r1492, %r2125;
	shl.b32 	%r1494, %r1493, 23;
	add.s32 	%r1495, %r2124, 1;
	shr.u32 	%r1496, %r1495, 7;
	add.s32 	%r1497, %r1496, 1;
	shr.u32 	%r1498, %r1497, 1;
	add.s32 	%r1499, %r1498, %r1494;
	or.b32  	%r1500, %r1499, %r469;
	mov.b32 	 %f1243, %r1500;

BB12_296:
	mul.rn.f32 	%f257, %f1243, %f1243;
	add.s32 	%r480, %r2126, 1;
	and.b32  	%r481, %r480, 1;
	setp.eq.s32	%p201, %r481, 0;
	@%p201 bra 	BB12_298;

	mov.f32 	%f901, 0fBAB6061A;
	mov.f32 	%f902, 0f37CCF5CE;
	fma.rn.f32 	%f1244, %f902, %f257, %f901;
	bra.uni 	BB12_299;

BB12_298:
	mov.f32 	%f903, 0f3C08839E;
	mov.f32 	%f904, 0fB94CA1F9;
	fma.rn.f32 	%f1244, %f904, %f257, %f903;

BB12_299:
	@%p201 bra 	BB12_301;

	mov.f32 	%f905, 0f3D2AAAA5;
	fma.rn.f32 	%f906, %f1244, %f257, %f905;
	mov.f32 	%f907, 0fBF000000;
	fma.rn.f32 	%f1245, %f906, %f257, %f907;
	bra.uni 	BB12_302;

BB12_301:
	mov.f32 	%f908, 0fBE2AAAA3;
	fma.rn.f32 	%f909, %f1244, %f257, %f908;
	mov.f32 	%f910, 0f00000000;
	fma.rn.f32 	%f1245, %f909, %f257, %f910;

BB12_302:
	fma.rn.f32 	%f1246, %f1245, %f1243, %f1243;
	@%p201 bra 	BB12_304;

	mov.f32 	%f911, 0f3F800000;
	fma.rn.f32 	%f1246, %f1245, %f257, %f911;

BB12_304:
	and.b32  	%r1501, %r480, 2;
	setp.eq.s32	%p204, %r1501, 0;
	@%p204 bra 	BB12_306;

	mov.f32 	%f912, 0f00000000;
	mov.f32 	%f913, 0fBF800000;
	fma.rn.f32 	%f1246, %f1246, %f913, %f912;

BB12_306:
	mov.f32 	%f1293, %f36;
	@%p49 bra 	BB12_308;

	mov.f32 	%f914, 0f00000000;
	mul.rn.f32 	%f1293, %f36, %f914;

BB12_308:
	mul.f32 	%f915, %f1293, 0f3F22F983;
	cvt.rni.s32.f32	%r2136, %f915;
	cvt.rn.f32.s32	%f916, %r2136;
	neg.f32 	%f917, %f916;
	fma.rn.f32 	%f919, %f917, %f618, %f1293;
	fma.rn.f32 	%f921, %f917, %f620, %f919;
	fma.rn.f32 	%f1247, %f917, %f622, %f921;
	abs.f32 	%f923, %f1293;
	setp.leu.f32	%p206, %f923, 0f47CE4780;
	@%p206 bra 	BB12_318;

	mov.b32 	 %r483, %f1293;
	shr.u32 	%r484, %r483, 23;
	bfe.u32 	%r1504, %r483, 23, 8;
	add.s32 	%r1505, %r1504, -128;
	shl.b32 	%r1506, %r483, 8;
	or.b32  	%r485, %r1506, -2147483648;
	shr.u32 	%r486, %r1505, 5;
	mov.u32 	%r2128, 0;
	mov.u64 	%rd185, __cudart_i2opi_f;
	mov.u32 	%r2127, -6;
	mov.u64 	%rd229, %rd1;

BB12_310:
	.pragma "nounroll";
	ld.const.u32 	%r1509, [%rd185];
	// inline asm
	{
	mad.lo.cc.u32   %r1507, %r1509, %r485, %r2128;
	madc.hi.u32     %r2128, %r1509, %r485,  0;
	}
	// inline asm
	st.local.u32 	[%rd229], %r1507;
	add.s64 	%rd229, %rd229, 4;
	add.s64 	%rd185, %rd185, 4;
	add.s32 	%r2127, %r2127, 1;
	setp.ne.s32	%p207, %r2127, 0;
	@%p207 bra 	BB12_310;

	and.b32  	%r491, %r483, -2147483648;
	st.local.u32 	[%rd2], %r2128;
	mov.u32 	%r1512, 6;
	sub.s32 	%r1513, %r1512, %r486;
	mul.wide.s32 	%rd151, %r1513, 4;
	add.s64 	%rd72, %rd1, %rd151;
	ld.local.u32 	%r2129, [%rd72];
	ld.local.u32 	%r2130, [%rd72+-4];
	and.b32  	%r494, %r484, 31;
	setp.eq.s32	%p208, %r494, 0;
	@%p208 bra 	BB12_313;

	mov.u32 	%r1514, 32;
	sub.s32 	%r1515, %r1514, %r494;
	shr.u32 	%r1516, %r2130, %r1515;
	shl.b32 	%r1517, %r2129, %r494;
	add.s32 	%r2129, %r1516, %r1517;
	ld.local.u32 	%r1518, [%rd72+-8];
	shr.u32 	%r1519, %r1518, %r1515;
	shl.b32 	%r1520, %r2130, %r494;
	add.s32 	%r2130, %r1519, %r1520;

BB12_313:
	shr.u32 	%r1521, %r2130, 30;
	shl.b32 	%r1522, %r2129, 2;
	add.s32 	%r2131, %r1521, %r1522;
	shl.b32 	%r500, %r2130, 2;
	shr.u32 	%r1523, %r2131, 31;
	shr.u32 	%r1524, %r2129, 30;
	add.s32 	%r501, %r1523, %r1524;
	setp.eq.s32	%p209, %r1523, 0;
	mov.u32 	%r2132, %r491;
	mov.u32 	%r2133, %r500;
	@%p209 bra 	BB12_315;

	not.b32 	%r1525, %r2131;
	neg.s32 	%r502, %r500;
	setp.eq.s32	%p210, %r500, 0;
	selp.u32	%r1526, 1, 0, %p210;
	add.s32 	%r2131, %r1526, %r1525;
	xor.b32  	%r504, %r491, -2147483648;
	mov.u32 	%r2132, %r504;
	mov.u32 	%r2133, %r502;

BB12_315:
	mov.u32 	%r506, %r2132;
	neg.s32 	%r1527, %r501;
	setp.eq.s32	%p211, %r491, 0;
	selp.b32	%r2136, %r501, %r1527, %p211;
	clz.b32 	%r2135, %r2131;
	setp.eq.s32	%p212, %r2135, 0;
	shl.b32 	%r1528, %r2131, %r2135;
	mov.u32 	%r1529, 32;
	sub.s32 	%r1530, %r1529, %r2135;
	shr.u32 	%r1531, %r2133, %r1530;
	add.s32 	%r1532, %r1531, %r1528;
	selp.b32	%r510, %r2131, %r1532, %p212;
	mov.u32 	%r1533, -921707870;
	mul.hi.u32 	%r2134, %r510, %r1533;
	setp.lt.s32	%p213, %r2134, 1;
	@%p213 bra 	BB12_317;

	mul.lo.s32 	%r1534, %r510, -921707870;
	shr.u32 	%r1535, %r1534, 31;
	shl.b32 	%r1536, %r2134, 1;
	add.s32 	%r2134, %r1535, %r1536;
	add.s32 	%r2135, %r2135, 1;

BB12_317:
	mov.u32 	%r1537, 126;
	sub.s32 	%r1538, %r1537, %r2135;
	shl.b32 	%r1539, %r1538, 23;
	add.s32 	%r1540, %r2134, 1;
	shr.u32 	%r1541, %r1540, 7;
	add.s32 	%r1542, %r1541, 1;
	shr.u32 	%r1543, %r1542, 1;
	add.s32 	%r1544, %r1543, %r1539;
	or.b32  	%r1545, %r1544, %r506;
	mov.b32 	 %f1247, %r1545;

BB12_318:
	mul.rn.f32 	%f274, %f1247, %f1247;
	and.b32  	%r517, %r2136, 1;
	setp.eq.s32	%p214, %r517, 0;
	@%p214 bra 	BB12_320;

	mov.f32 	%f924, 0fBAB6061A;
	mov.f32 	%f925, 0f37CCF5CE;
	fma.rn.f32 	%f1248, %f925, %f274, %f924;
	bra.uni 	BB12_321;

BB12_320:
	mov.f32 	%f926, 0f3C08839E;
	mov.f32 	%f927, 0fB94CA1F9;
	fma.rn.f32 	%f1248, %f927, %f274, %f926;

BB12_321:
	@%p214 bra 	BB12_323;

	mov.f32 	%f928, 0f3D2AAAA5;
	fma.rn.f32 	%f929, %f1248, %f274, %f928;
	mov.f32 	%f930, 0fBF000000;
	fma.rn.f32 	%f1249, %f929, %f274, %f930;
	bra.uni 	BB12_324;

BB12_323:
	mov.f32 	%f931, 0fBE2AAAA3;
	fma.rn.f32 	%f932, %f1248, %f274, %f931;
	mov.f32 	%f933, 0f00000000;
	fma.rn.f32 	%f1249, %f932, %f274, %f933;

BB12_324:
	fma.rn.f32 	%f1250, %f1249, %f1247, %f1247;
	@%p214 bra 	BB12_326;

	mov.f32 	%f934, 0f3F800000;
	fma.rn.f32 	%f1250, %f1249, %f274, %f934;

BB12_326:
	and.b32  	%r1546, %r2136, 2;
	setp.eq.s32	%p217, %r1546, 0;
	@%p217 bra 	BB12_328;

	mov.f32 	%f935, 0f00000000;
	mov.f32 	%f936, 0fBF800000;
	fma.rn.f32 	%f1250, %f1250, %f936, %f935;

BB12_328:
	mul.f32 	%f286, %f1246, %f1250;
	mov.f32 	%f1312, %f13;
	@%p62 bra 	BB12_330;

	mov.f32 	%f937, 0f00000000;
	mul.rn.f32 	%f1312, %f13, %f937;

BB12_330:
	mul.f32 	%f938, %f1312, 0f3F22F983;
	cvt.rni.s32.f32	%r2146, %f938;
	cvt.rn.f32.s32	%f939, %r2146;
	neg.f32 	%f940, %f939;
	fma.rn.f32 	%f942, %f940, %f618, %f1312;
	fma.rn.f32 	%f944, %f940, %f620, %f942;
	fma.rn.f32 	%f1251, %f940, %f622, %f944;
	abs.f32 	%f946, %f1312;
	setp.leu.f32	%p219, %f946, 0f47CE4780;
	@%p219 bra 	BB12_340;

	mov.b32 	 %r519, %f1312;
	shr.u32 	%r520, %r519, 23;
	bfe.u32 	%r1549, %r519, 23, 8;
	add.s32 	%r1550, %r1549, -128;
	shl.b32 	%r1551, %r519, 8;
	or.b32  	%r521, %r1551, -2147483648;
	shr.u32 	%r522, %r1550, 5;
	mov.u32 	%r2138, 0;
	mov.u64 	%rd186, __cudart_i2opi_f;
	mov.u32 	%r2137, -6;
	mov.u64 	%rd228, %rd1;

BB12_332:
	.pragma "nounroll";
	ld.const.u32 	%r1554, [%rd186];
	// inline asm
	{
	mad.lo.cc.u32   %r1552, %r1554, %r521, %r2138;
	madc.hi.u32     %r2138, %r1554, %r521,  0;
	}
	// inline asm
	st.local.u32 	[%rd228], %r1552;
	add.s64 	%rd228, %rd228, 4;
	add.s64 	%rd186, %rd186, 4;
	add.s32 	%r2137, %r2137, 1;
	setp.ne.s32	%p220, %r2137, 0;
	@%p220 bra 	BB12_332;

	and.b32  	%r527, %r519, -2147483648;
	st.local.u32 	[%rd2], %r2138;
	mov.u32 	%r1557, 6;
	sub.s32 	%r1558, %r1557, %r522;
	mul.wide.s32 	%rd153, %r1558, 4;
	add.s64 	%rd77, %rd1, %rd153;
	ld.local.u32 	%r2139, [%rd77];
	ld.local.u32 	%r2140, [%rd77+-4];
	and.b32  	%r530, %r520, 31;
	setp.eq.s32	%p221, %r530, 0;
	@%p221 bra 	BB12_335;

	mov.u32 	%r1559, 32;
	sub.s32 	%r1560, %r1559, %r530;
	shr.u32 	%r1561, %r2140, %r1560;
	shl.b32 	%r1562, %r2139, %r530;
	add.s32 	%r2139, %r1561, %r1562;
	ld.local.u32 	%r1563, [%rd77+-8];
	shr.u32 	%r1564, %r1563, %r1560;
	shl.b32 	%r1565, %r2140, %r530;
	add.s32 	%r2140, %r1564, %r1565;

BB12_335:
	shr.u32 	%r1566, %r2140, 30;
	shl.b32 	%r1567, %r2139, 2;
	add.s32 	%r2141, %r1566, %r1567;
	shl.b32 	%r536, %r2140, 2;
	shr.u32 	%r1568, %r2141, 31;
	shr.u32 	%r1569, %r2139, 30;
	add.s32 	%r537, %r1568, %r1569;
	setp.eq.s32	%p222, %r1568, 0;
	mov.u32 	%r2142, %r527;
	mov.u32 	%r2143, %r536;
	@%p222 bra 	BB12_337;

	not.b32 	%r1570, %r2141;
	neg.s32 	%r538, %r536;
	setp.eq.s32	%p223, %r536, 0;
	selp.u32	%r1571, 1, 0, %p223;
	add.s32 	%r2141, %r1571, %r1570;
	xor.b32  	%r540, %r527, -2147483648;
	mov.u32 	%r2142, %r540;
	mov.u32 	%r2143, %r538;

BB12_337:
	mov.u32 	%r542, %r2142;
	neg.s32 	%r1572, %r537;
	setp.eq.s32	%p224, %r527, 0;
	selp.b32	%r2146, %r537, %r1572, %p224;
	clz.b32 	%r2145, %r2141;
	setp.eq.s32	%p225, %r2145, 0;
	shl.b32 	%r1573, %r2141, %r2145;
	mov.u32 	%r1574, 32;
	sub.s32 	%r1575, %r1574, %r2145;
	shr.u32 	%r1576, %r2143, %r1575;
	add.s32 	%r1577, %r1576, %r1573;
	selp.b32	%r546, %r2141, %r1577, %p225;
	mov.u32 	%r1578, -921707870;
	mul.hi.u32 	%r2144, %r546, %r1578;
	setp.lt.s32	%p226, %r2144, 1;
	@%p226 bra 	BB12_339;

	mul.lo.s32 	%r1579, %r546, -921707870;
	shr.u32 	%r1580, %r1579, 31;
	shl.b32 	%r1581, %r2144, 1;
	add.s32 	%r2144, %r1580, %r1581;
	add.s32 	%r2145, %r2145, 1;

BB12_339:
	mov.u32 	%r1582, 126;
	sub.s32 	%r1583, %r1582, %r2145;
	shl.b32 	%r1584, %r1583, 23;
	add.s32 	%r1585, %r2144, 1;
	shr.u32 	%r1586, %r1585, 7;
	add.s32 	%r1587, %r1586, 1;
	shr.u32 	%r1588, %r1587, 1;
	add.s32 	%r1589, %r1588, %r1584;
	or.b32  	%r1590, %r1589, %r542;
	mov.b32 	 %f1251, %r1590;

BB12_340:
	mul.rn.f32 	%f292, %f1251, %f1251;
	add.s32 	%r553, %r2146, 1;
	and.b32  	%r554, %r553, 1;
	setp.eq.s32	%p227, %r554, 0;
	@%p227 bra 	BB12_342;

	mov.f32 	%f947, 0fBAB6061A;
	mov.f32 	%f948, 0f37CCF5CE;
	fma.rn.f32 	%f1252, %f948, %f292, %f947;
	bra.uni 	BB12_343;

BB12_342:
	mov.f32 	%f949, 0f3C08839E;
	mov.f32 	%f950, 0fB94CA1F9;
	fma.rn.f32 	%f1252, %f950, %f292, %f949;

BB12_343:
	@%p227 bra 	BB12_345;

	mov.f32 	%f951, 0f3D2AAAA5;
	fma.rn.f32 	%f952, %f1252, %f292, %f951;
	mov.f32 	%f953, 0fBF000000;
	fma.rn.f32 	%f1253, %f952, %f292, %f953;
	bra.uni 	BB12_346;

BB12_345:
	mov.f32 	%f954, 0fBE2AAAA3;
	fma.rn.f32 	%f955, %f1252, %f292, %f954;
	mov.f32 	%f956, 0f00000000;
	fma.rn.f32 	%f1253, %f955, %f292, %f956;

BB12_346:
	fma.rn.f32 	%f1254, %f1253, %f1251, %f1251;
	@%p227 bra 	BB12_348;

	mov.f32 	%f957, 0f3F800000;
	fma.rn.f32 	%f1254, %f1253, %f292, %f957;

BB12_348:
	and.b32  	%r1591, %r553, 2;
	setp.eq.s32	%p230, %r1591, 0;
	@%p230 bra 	BB12_350;

	mov.f32 	%f958, 0f00000000;
	mov.f32 	%f959, 0fBF800000;
	fma.rn.f32 	%f1254, %f1254, %f959, %f958;

BB12_350:
	mul.f32 	%f304, %f286, %f1254;
	mov.f32 	%f1338, %f34;
	@%p36 bra 	BB12_352;

	mov.f32 	%f960, 0f00000000;
	mul.rn.f32 	%f1338, %f34, %f960;

BB12_352:
	mul.f32 	%f961, %f1338, 0f3F22F983;
	cvt.rni.s32.f32	%r2156, %f961;
	cvt.rn.f32.s32	%f962, %r2156;
	neg.f32 	%f963, %f962;
	fma.rn.f32 	%f965, %f963, %f618, %f1338;
	fma.rn.f32 	%f967, %f963, %f620, %f965;
	fma.rn.f32 	%f1255, %f963, %f622, %f967;
	abs.f32 	%f969, %f1338;
	setp.leu.f32	%p232, %f969, 0f47CE4780;
	@%p232 bra 	BB12_362;

	mov.b32 	 %r556, %f1338;
	shr.u32 	%r557, %r556, 23;
	bfe.u32 	%r1594, %r556, 23, 8;
	add.s32 	%r1595, %r1594, -128;
	shl.b32 	%r1596, %r556, 8;
	or.b32  	%r558, %r1596, -2147483648;
	shr.u32 	%r559, %r1595, 5;
	mov.u32 	%r2148, 0;
	mov.u64 	%rd187, __cudart_i2opi_f;
	mov.u32 	%r2147, -6;
	mov.u64 	%rd227, %rd1;

BB12_354:
	.pragma "nounroll";
	ld.const.u32 	%r1599, [%rd187];
	// inline asm
	{
	mad.lo.cc.u32   %r1597, %r1599, %r558, %r2148;
	madc.hi.u32     %r2148, %r1599, %r558,  0;
	}
	// inline asm
	st.local.u32 	[%rd227], %r1597;
	add.s64 	%rd227, %rd227, 4;
	add.s64 	%rd187, %rd187, 4;
	add.s32 	%r2147, %r2147, 1;
	setp.ne.s32	%p233, %r2147, 0;
	@%p233 bra 	BB12_354;

	and.b32  	%r564, %r556, -2147483648;
	st.local.u32 	[%rd2], %r2148;
	mov.u32 	%r1602, 6;
	sub.s32 	%r1603, %r1602, %r559;
	mul.wide.s32 	%rd155, %r1603, 4;
	add.s64 	%rd82, %rd1, %rd155;
	ld.local.u32 	%r2149, [%rd82];
	ld.local.u32 	%r2150, [%rd82+-4];
	and.b32  	%r567, %r557, 31;
	setp.eq.s32	%p234, %r567, 0;
	@%p234 bra 	BB12_357;

	mov.u32 	%r1604, 32;
	sub.s32 	%r1605, %r1604, %r567;
	shr.u32 	%r1606, %r2150, %r1605;
	shl.b32 	%r1607, %r2149, %r567;
	add.s32 	%r2149, %r1606, %r1607;
	ld.local.u32 	%r1608, [%rd82+-8];
	shr.u32 	%r1609, %r1608, %r1605;
	shl.b32 	%r1610, %r2150, %r567;
	add.s32 	%r2150, %r1609, %r1610;

BB12_357:
	shr.u32 	%r1611, %r2150, 30;
	shl.b32 	%r1612, %r2149, 2;
	add.s32 	%r2151, %r1611, %r1612;
	shl.b32 	%r573, %r2150, 2;
	shr.u32 	%r1613, %r2151, 31;
	shr.u32 	%r1614, %r2149, 30;
	add.s32 	%r574, %r1613, %r1614;
	setp.eq.s32	%p235, %r1613, 0;
	mov.u32 	%r2152, %r564;
	mov.u32 	%r2153, %r573;
	@%p235 bra 	BB12_359;

	not.b32 	%r1615, %r2151;
	neg.s32 	%r575, %r573;
	setp.eq.s32	%p236, %r573, 0;
	selp.u32	%r1616, 1, 0, %p236;
	add.s32 	%r2151, %r1616, %r1615;
	xor.b32  	%r577, %r564, -2147483648;
	mov.u32 	%r2152, %r577;
	mov.u32 	%r2153, %r575;

BB12_359:
	mov.u32 	%r579, %r2152;
	neg.s32 	%r1617, %r574;
	setp.eq.s32	%p237, %r564, 0;
	selp.b32	%r2156, %r574, %r1617, %p237;
	clz.b32 	%r2155, %r2151;
	setp.eq.s32	%p238, %r2155, 0;
	shl.b32 	%r1618, %r2151, %r2155;
	mov.u32 	%r1619, 32;
	sub.s32 	%r1620, %r1619, %r2155;
	shr.u32 	%r1621, %r2153, %r1620;
	add.s32 	%r1622, %r1621, %r1618;
	selp.b32	%r583, %r2151, %r1622, %p238;
	mov.u32 	%r1623, -921707870;
	mul.hi.u32 	%r2154, %r583, %r1623;
	setp.lt.s32	%p239, %r2154, 1;
	@%p239 bra 	BB12_361;

	mul.lo.s32 	%r1624, %r583, -921707870;
	shr.u32 	%r1625, %r1624, 31;
	shl.b32 	%r1626, %r2154, 1;
	add.s32 	%r2154, %r1625, %r1626;
	add.s32 	%r2155, %r2155, 1;

BB12_361:
	mov.u32 	%r1627, 126;
	sub.s32 	%r1628, %r1627, %r2155;
	shl.b32 	%r1629, %r1628, 23;
	add.s32 	%r1630, %r2154, 1;
	shr.u32 	%r1631, %r1630, 7;
	add.s32 	%r1632, %r1631, 1;
	shr.u32 	%r1633, %r1632, 1;
	add.s32 	%r1634, %r1633, %r1629;
	or.b32  	%r1635, %r1634, %r579;
	mov.b32 	 %f1255, %r1635;

BB12_362:
	mul.rn.f32 	%f310, %f1255, %f1255;
	and.b32  	%r590, %r2156, 1;
	setp.eq.s32	%p240, %r590, 0;
	@%p240 bra 	BB12_364;

	mov.f32 	%f970, 0fBAB6061A;
	mov.f32 	%f971, 0f37CCF5CE;
	fma.rn.f32 	%f1256, %f971, %f310, %f970;
	bra.uni 	BB12_365;

BB12_364:
	mov.f32 	%f972, 0f3C08839E;
	mov.f32 	%f973, 0fB94CA1F9;
	fma.rn.f32 	%f1256, %f973, %f310, %f972;

BB12_365:
	@%p240 bra 	BB12_367;

	mov.f32 	%f974, 0f3D2AAAA5;
	fma.rn.f32 	%f975, %f1256, %f310, %f974;
	mov.f32 	%f976, 0fBF000000;
	fma.rn.f32 	%f1257, %f975, %f310, %f976;
	bra.uni 	BB12_368;

BB12_367:
	mov.f32 	%f977, 0fBE2AAAA3;
	fma.rn.f32 	%f978, %f1256, %f310, %f977;
	mov.f32 	%f979, 0f00000000;
	fma.rn.f32 	%f1257, %f978, %f310, %f979;

BB12_368:
	fma.rn.f32 	%f1258, %f1257, %f1255, %f1255;
	@%p240 bra 	BB12_370;

	mov.f32 	%f980, 0f3F800000;
	fma.rn.f32 	%f1258, %f1257, %f310, %f980;

BB12_370:
	and.b32  	%r1636, %r2156, 2;
	setp.eq.s32	%p243, %r1636, 0;
	@%p243 bra 	BB12_372;

	mov.f32 	%f981, 0f00000000;
	mov.f32 	%f982, 0fBF800000;
	fma.rn.f32 	%f1258, %f1258, %f982, %f981;

BB12_372:
	mov.f32 	%f1292, %f36;
	@%p49 bra 	BB12_374;

	mov.f32 	%f983, 0f00000000;
	mul.rn.f32 	%f1292, %f36, %f983;

BB12_374:
	mul.f32 	%f984, %f1292, 0f3F22F983;
	cvt.rni.s32.f32	%r2166, %f984;
	cvt.rn.f32.s32	%f985, %r2166;
	neg.f32 	%f986, %f985;
	fma.rn.f32 	%f988, %f986, %f618, %f1292;
	fma.rn.f32 	%f990, %f986, %f620, %f988;
	fma.rn.f32 	%f1259, %f986, %f622, %f990;
	abs.f32 	%f992, %f1292;
	setp.leu.f32	%p245, %f992, 0f47CE4780;
	@%p245 bra 	BB12_384;

	mov.b32 	 %r592, %f1292;
	shr.u32 	%r593, %r592, 23;
	bfe.u32 	%r1639, %r592, 23, 8;
	add.s32 	%r1640, %r1639, -128;
	shl.b32 	%r1641, %r592, 8;
	or.b32  	%r594, %r1641, -2147483648;
	shr.u32 	%r595, %r1640, 5;
	mov.u32 	%r2158, 0;
	mov.u64 	%rd188, __cudart_i2opi_f;
	mov.u32 	%r2157, -6;
	mov.u64 	%rd226, %rd1;

BB12_376:
	.pragma "nounroll";
	ld.const.u32 	%r1644, [%rd188];
	// inline asm
	{
	mad.lo.cc.u32   %r1642, %r1644, %r594, %r2158;
	madc.hi.u32     %r2158, %r1644, %r594,  0;
	}
	// inline asm
	st.local.u32 	[%rd226], %r1642;
	add.s64 	%rd226, %rd226, 4;
	add.s64 	%rd188, %rd188, 4;
	add.s32 	%r2157, %r2157, 1;
	setp.ne.s32	%p246, %r2157, 0;
	@%p246 bra 	BB12_376;

	and.b32  	%r600, %r592, -2147483648;
	st.local.u32 	[%rd2], %r2158;
	mov.u32 	%r1647, 6;
	sub.s32 	%r1648, %r1647, %r595;
	mul.wide.s32 	%rd157, %r1648, 4;
	add.s64 	%rd87, %rd1, %rd157;
	ld.local.u32 	%r2159, [%rd87];
	ld.local.u32 	%r2160, [%rd87+-4];
	and.b32  	%r603, %r593, 31;
	setp.eq.s32	%p247, %r603, 0;
	@%p247 bra 	BB12_379;

	mov.u32 	%r1649, 32;
	sub.s32 	%r1650, %r1649, %r603;
	shr.u32 	%r1651, %r2160, %r1650;
	shl.b32 	%r1652, %r2159, %r603;
	add.s32 	%r2159, %r1651, %r1652;
	ld.local.u32 	%r1653, [%rd87+-8];
	shr.u32 	%r1654, %r1653, %r1650;
	shl.b32 	%r1655, %r2160, %r603;
	add.s32 	%r2160, %r1654, %r1655;

BB12_379:
	shr.u32 	%r1656, %r2160, 30;
	shl.b32 	%r1657, %r2159, 2;
	add.s32 	%r2161, %r1656, %r1657;
	shl.b32 	%r609, %r2160, 2;
	shr.u32 	%r1658, %r2161, 31;
	shr.u32 	%r1659, %r2159, 30;
	add.s32 	%r610, %r1658, %r1659;
	setp.eq.s32	%p248, %r1658, 0;
	mov.u32 	%r2162, %r600;
	mov.u32 	%r2163, %r609;
	@%p248 bra 	BB12_381;

	not.b32 	%r1660, %r2161;
	neg.s32 	%r611, %r609;
	setp.eq.s32	%p249, %r609, 0;
	selp.u32	%r1661, 1, 0, %p249;
	add.s32 	%r2161, %r1661, %r1660;
	xor.b32  	%r613, %r600, -2147483648;
	mov.u32 	%r2162, %r613;
	mov.u32 	%r2163, %r611;

BB12_381:
	mov.u32 	%r615, %r2162;
	neg.s32 	%r1662, %r610;
	setp.eq.s32	%p250, %r600, 0;
	selp.b32	%r2166, %r610, %r1662, %p250;
	clz.b32 	%r2165, %r2161;
	setp.eq.s32	%p251, %r2165, 0;
	shl.b32 	%r1663, %r2161, %r2165;
	mov.u32 	%r1664, 32;
	sub.s32 	%r1665, %r1664, %r2165;
	shr.u32 	%r1666, %r2163, %r1665;
	add.s32 	%r1667, %r1666, %r1663;
	selp.b32	%r619, %r2161, %r1667, %p251;
	mov.u32 	%r1668, -921707870;
	mul.hi.u32 	%r2164, %r619, %r1668;
	setp.lt.s32	%p252, %r2164, 1;
	@%p252 bra 	BB12_383;

	mul.lo.s32 	%r1669, %r619, -921707870;
	shr.u32 	%r1670, %r1669, 31;
	shl.b32 	%r1671, %r2164, 1;
	add.s32 	%r2164, %r1670, %r1671;
	add.s32 	%r2165, %r2165, 1;

BB12_383:
	mov.u32 	%r1672, 126;
	sub.s32 	%r1673, %r1672, %r2165;
	shl.b32 	%r1674, %r1673, 23;
	add.s32 	%r1675, %r2164, 1;
	shr.u32 	%r1676, %r1675, 7;
	add.s32 	%r1677, %r1676, 1;
	shr.u32 	%r1678, %r1677, 1;
	add.s32 	%r1679, %r1678, %r1674;
	or.b32  	%r1680, %r1679, %r615;
	mov.b32 	 %f1259, %r1680;

BB12_384:
	mul.rn.f32 	%f327, %f1259, %f1259;
	add.s32 	%r626, %r2166, 1;
	and.b32  	%r627, %r626, 1;
	setp.eq.s32	%p253, %r627, 0;
	@%p253 bra 	BB12_386;

	mov.f32 	%f993, 0fBAB6061A;
	mov.f32 	%f994, 0f37CCF5CE;
	fma.rn.f32 	%f1260, %f994, %f327, %f993;
	bra.uni 	BB12_387;

BB12_386:
	mov.f32 	%f995, 0f3C08839E;
	mov.f32 	%f996, 0fB94CA1F9;
	fma.rn.f32 	%f1260, %f996, %f327, %f995;

BB12_387:
	@%p253 bra 	BB12_389;

	mov.f32 	%f997, 0f3D2AAAA5;
	fma.rn.f32 	%f998, %f1260, %f327, %f997;
	mov.f32 	%f999, 0fBF000000;
	fma.rn.f32 	%f1261, %f998, %f327, %f999;
	bra.uni 	BB12_390;

BB12_389:
	mov.f32 	%f1000, 0fBE2AAAA3;
	fma.rn.f32 	%f1001, %f1260, %f327, %f1000;
	mov.f32 	%f1002, 0f00000000;
	fma.rn.f32 	%f1261, %f1001, %f327, %f1002;

BB12_390:
	fma.rn.f32 	%f1262, %f1261, %f1259, %f1259;
	@%p253 bra 	BB12_392;

	mov.f32 	%f1003, 0f3F800000;
	fma.rn.f32 	%f1262, %f1261, %f327, %f1003;

BB12_392:
	and.b32  	%r1681, %r626, 2;
	setp.eq.s32	%p256, %r1681, 0;
	@%p256 bra 	BB12_394;

	mov.f32 	%f1004, 0f00000000;
	mov.f32 	%f1005, 0fBF800000;
	fma.rn.f32 	%f1262, %f1262, %f1005, %f1004;

BB12_394:
	mul.f32 	%f339, %f1258, %f1262;
	mov.f32 	%f1311, %f13;
	@%p62 bra 	BB12_396;

	mov.f32 	%f1006, 0f00000000;
	mul.rn.f32 	%f1311, %f13, %f1006;

BB12_396:
	mul.f32 	%f1007, %f1311, 0f3F22F983;
	cvt.rni.s32.f32	%r2176, %f1007;
	cvt.rn.f32.s32	%f1008, %r2176;
	neg.f32 	%f1009, %f1008;
	fma.rn.f32 	%f1011, %f1009, %f618, %f1311;
	fma.rn.f32 	%f1013, %f1009, %f620, %f1011;
	fma.rn.f32 	%f1263, %f1009, %f622, %f1013;
	abs.f32 	%f1015, %f1311;
	setp.leu.f32	%p258, %f1015, 0f47CE4780;
	@%p258 bra 	BB12_406;

	mov.b32 	 %r629, %f1311;
	shr.u32 	%r630, %r629, 23;
	bfe.u32 	%r1684, %r629, 23, 8;
	add.s32 	%r1685, %r1684, -128;
	shl.b32 	%r1686, %r629, 8;
	or.b32  	%r631, %r1686, -2147483648;
	shr.u32 	%r632, %r1685, 5;
	mov.u32 	%r2168, 0;
	mov.u64 	%rd189, __cudart_i2opi_f;
	mov.u32 	%r2167, -6;
	mov.u64 	%rd225, %rd1;

BB12_398:
	.pragma "nounroll";
	ld.const.u32 	%r1689, [%rd189];
	// inline asm
	{
	mad.lo.cc.u32   %r1687, %r1689, %r631, %r2168;
	madc.hi.u32     %r2168, %r1689, %r631,  0;
	}
	// inline asm
	st.local.u32 	[%rd225], %r1687;
	add.s64 	%rd225, %rd225, 4;
	add.s64 	%rd189, %rd189, 4;
	add.s32 	%r2167, %r2167, 1;
	setp.ne.s32	%p259, %r2167, 0;
	@%p259 bra 	BB12_398;

	and.b32  	%r637, %r629, -2147483648;
	st.local.u32 	[%rd2], %r2168;
	mov.u32 	%r1692, 6;
	sub.s32 	%r1693, %r1692, %r632;
	mul.wide.s32 	%rd159, %r1693, 4;
	add.s64 	%rd92, %rd1, %rd159;
	ld.local.u32 	%r2169, [%rd92];
	ld.local.u32 	%r2170, [%rd92+-4];
	and.b32  	%r640, %r630, 31;
	setp.eq.s32	%p260, %r640, 0;
	@%p260 bra 	BB12_401;

	mov.u32 	%r1694, 32;
	sub.s32 	%r1695, %r1694, %r640;
	shr.u32 	%r1696, %r2170, %r1695;
	shl.b32 	%r1697, %r2169, %r640;
	add.s32 	%r2169, %r1696, %r1697;
	ld.local.u32 	%r1698, [%rd92+-8];
	shr.u32 	%r1699, %r1698, %r1695;
	shl.b32 	%r1700, %r2170, %r640;
	add.s32 	%r2170, %r1699, %r1700;

BB12_401:
	shr.u32 	%r1701, %r2170, 30;
	shl.b32 	%r1702, %r2169, 2;
	add.s32 	%r2171, %r1701, %r1702;
	shl.b32 	%r646, %r2170, 2;
	shr.u32 	%r1703, %r2171, 31;
	shr.u32 	%r1704, %r2169, 30;
	add.s32 	%r647, %r1703, %r1704;
	setp.eq.s32	%p261, %r1703, 0;
	mov.u32 	%r2172, %r637;
	mov.u32 	%r2173, %r646;
	@%p261 bra 	BB12_403;

	not.b32 	%r1705, %r2171;
	neg.s32 	%r648, %r646;
	setp.eq.s32	%p262, %r646, 0;
	selp.u32	%r1706, 1, 0, %p262;
	add.s32 	%r2171, %r1706, %r1705;
	xor.b32  	%r650, %r637, -2147483648;
	mov.u32 	%r2172, %r650;
	mov.u32 	%r2173, %r648;

BB12_403:
	mov.u32 	%r652, %r2172;
	neg.s32 	%r1707, %r647;
	setp.eq.s32	%p263, %r637, 0;
	selp.b32	%r2176, %r647, %r1707, %p263;
	clz.b32 	%r2175, %r2171;
	setp.eq.s32	%p264, %r2175, 0;
	shl.b32 	%r1708, %r2171, %r2175;
	mov.u32 	%r1709, 32;
	sub.s32 	%r1710, %r1709, %r2175;
	shr.u32 	%r1711, %r2173, %r1710;
	add.s32 	%r1712, %r1711, %r1708;
	selp.b32	%r656, %r2171, %r1712, %p264;
	mov.u32 	%r1713, -921707870;
	mul.hi.u32 	%r2174, %r656, %r1713;
	setp.lt.s32	%p265, %r2174, 1;
	@%p265 bra 	BB12_405;

	mul.lo.s32 	%r1714, %r656, -921707870;
	shr.u32 	%r1715, %r1714, 31;
	shl.b32 	%r1716, %r2174, 1;
	add.s32 	%r2174, %r1715, %r1716;
	add.s32 	%r2175, %r2175, 1;

BB12_405:
	mov.u32 	%r1717, 126;
	sub.s32 	%r1718, %r1717, %r2175;
	shl.b32 	%r1719, %r1718, 23;
	add.s32 	%r1720, %r2174, 1;
	shr.u32 	%r1721, %r1720, 7;
	add.s32 	%r1722, %r1721, 1;
	shr.u32 	%r1723, %r1722, 1;
	add.s32 	%r1724, %r1723, %r1719;
	or.b32  	%r1725, %r1724, %r652;
	mov.b32 	 %f1263, %r1725;

BB12_406:
	mul.rn.f32 	%f345, %f1263, %f1263;
	and.b32  	%r663, %r2176, 1;
	setp.eq.s32	%p266, %r663, 0;
	@%p266 bra 	BB12_408;

	mov.f32 	%f1016, 0fBAB6061A;
	mov.f32 	%f1017, 0f37CCF5CE;
	fma.rn.f32 	%f1264, %f1017, %f345, %f1016;
	bra.uni 	BB12_409;

BB12_408:
	mov.f32 	%f1018, 0f3C08839E;
	mov.f32 	%f1019, 0fB94CA1F9;
	fma.rn.f32 	%f1264, %f1019, %f345, %f1018;

BB12_409:
	@%p266 bra 	BB12_411;

	mov.f32 	%f1020, 0f3D2AAAA5;
	fma.rn.f32 	%f1021, %f1264, %f345, %f1020;
	mov.f32 	%f1022, 0fBF000000;
	fma.rn.f32 	%f1265, %f1021, %f345, %f1022;
	bra.uni 	BB12_412;

BB12_411:
	mov.f32 	%f1023, 0fBE2AAAA3;
	fma.rn.f32 	%f1024, %f1264, %f345, %f1023;
	mov.f32 	%f1025, 0f00000000;
	fma.rn.f32 	%f1265, %f1024, %f345, %f1025;

BB12_412:
	fma.rn.f32 	%f1266, %f1265, %f1263, %f1263;
	@%p266 bra 	BB12_414;

	mov.f32 	%f1026, 0f3F800000;
	fma.rn.f32 	%f1266, %f1265, %f345, %f1026;

BB12_414:
	and.b32  	%r1726, %r2176, 2;
	setp.eq.s32	%p269, %r1726, 0;
	@%p269 bra 	BB12_416;

	mov.f32 	%f1027, 0f00000000;
	mov.f32 	%f1028, 0fBF800000;
	fma.rn.f32 	%f1266, %f1266, %f1028, %f1027;

BB12_416:
	mul.f32 	%f1029, %f339, %f1266;
	sub.f32 	%f357, %f304, %f1029;
	mov.f32 	%f1337, %f34;
	@%p36 bra 	BB12_418;

	mov.f32 	%f1030, 0f00000000;
	mul.rn.f32 	%f1337, %f34, %f1030;

BB12_418:
	mul.f32 	%f1031, %f1337, 0f3F22F983;
	cvt.rni.s32.f32	%r2186, %f1031;
	cvt.rn.f32.s32	%f1032, %r2186;
	neg.f32 	%f1033, %f1032;
	fma.rn.f32 	%f1035, %f1033, %f618, %f1337;
	fma.rn.f32 	%f1037, %f1033, %f620, %f1035;
	fma.rn.f32 	%f1267, %f1033, %f622, %f1037;
	abs.f32 	%f1039, %f1337;
	setp.leu.f32	%p271, %f1039, 0f47CE4780;
	@%p271 bra 	BB12_428;

	mov.b32 	 %r665, %f1337;
	shr.u32 	%r666, %r665, 23;
	bfe.u32 	%r1729, %r665, 23, 8;
	add.s32 	%r1730, %r1729, -128;
	shl.b32 	%r1731, %r665, 8;
	or.b32  	%r667, %r1731, -2147483648;
	shr.u32 	%r668, %r1730, 5;
	mov.u32 	%r2178, 0;
	mov.u64 	%rd190, __cudart_i2opi_f;
	mov.u32 	%r2177, -6;
	mov.u64 	%rd224, %rd1;

BB12_420:
	.pragma "nounroll";
	ld.const.u32 	%r1734, [%rd190];
	// inline asm
	{
	mad.lo.cc.u32   %r1732, %r1734, %r667, %r2178;
	madc.hi.u32     %r2178, %r1734, %r667,  0;
	}
	// inline asm
	st.local.u32 	[%rd224], %r1732;
	add.s64 	%rd224, %rd224, 4;
	add.s64 	%rd190, %rd190, 4;
	add.s32 	%r2177, %r2177, 1;
	setp.ne.s32	%p272, %r2177, 0;
	@%p272 bra 	BB12_420;

	and.b32  	%r673, %r665, -2147483648;
	st.local.u32 	[%rd2], %r2178;
	mov.u32 	%r1737, 6;
	sub.s32 	%r1738, %r1737, %r668;
	mul.wide.s32 	%rd161, %r1738, 4;
	add.s64 	%rd97, %rd1, %rd161;
	ld.local.u32 	%r2179, [%rd97];
	ld.local.u32 	%r2180, [%rd97+-4];
	and.b32  	%r676, %r666, 31;
	setp.eq.s32	%p273, %r676, 0;
	@%p273 bra 	BB12_423;

	mov.u32 	%r1739, 32;
	sub.s32 	%r1740, %r1739, %r676;
	shr.u32 	%r1741, %r2180, %r1740;
	shl.b32 	%r1742, %r2179, %r676;
	add.s32 	%r2179, %r1741, %r1742;
	ld.local.u32 	%r1743, [%rd97+-8];
	shr.u32 	%r1744, %r1743, %r1740;
	shl.b32 	%r1745, %r2180, %r676;
	add.s32 	%r2180, %r1744, %r1745;

BB12_423:
	shr.u32 	%r1746, %r2180, 30;
	shl.b32 	%r1747, %r2179, 2;
	add.s32 	%r2181, %r1746, %r1747;
	shl.b32 	%r682, %r2180, 2;
	shr.u32 	%r1748, %r2181, 31;
	shr.u32 	%r1749, %r2179, 30;
	add.s32 	%r683, %r1748, %r1749;
	setp.eq.s32	%p274, %r1748, 0;
	mov.u32 	%r2182, %r673;
	mov.u32 	%r2183, %r682;
	@%p274 bra 	BB12_425;

	not.b32 	%r1750, %r2181;
	neg.s32 	%r684, %r682;
	setp.eq.s32	%p275, %r682, 0;
	selp.u32	%r1751, 1, 0, %p275;
	add.s32 	%r2181, %r1751, %r1750;
	xor.b32  	%r686, %r673, -2147483648;
	mov.u32 	%r2182, %r686;
	mov.u32 	%r2183, %r684;

BB12_425:
	mov.u32 	%r688, %r2182;
	neg.s32 	%r1752, %r683;
	setp.eq.s32	%p276, %r673, 0;
	selp.b32	%r2186, %r683, %r1752, %p276;
	clz.b32 	%r2185, %r2181;
	setp.eq.s32	%p277, %r2185, 0;
	shl.b32 	%r1753, %r2181, %r2185;
	mov.u32 	%r1754, 32;
	sub.s32 	%r1755, %r1754, %r2185;
	shr.u32 	%r1756, %r2183, %r1755;
	add.s32 	%r1757, %r1756, %r1753;
	selp.b32	%r692, %r2181, %r1757, %p277;
	mov.u32 	%r1758, -921707870;
	mul.hi.u32 	%r2184, %r692, %r1758;
	setp.lt.s32	%p278, %r2184, 1;
	@%p278 bra 	BB12_427;

	mul.lo.s32 	%r1759, %r692, -921707870;
	shr.u32 	%r1760, %r1759, 31;
	shl.b32 	%r1761, %r2184, 1;
	add.s32 	%r2184, %r1760, %r1761;
	add.s32 	%r2185, %r2185, 1;

BB12_427:
	mov.u32 	%r1762, 126;
	sub.s32 	%r1763, %r1762, %r2185;
	shl.b32 	%r1764, %r1763, 23;
	add.s32 	%r1765, %r2184, 1;
	shr.u32 	%r1766, %r1765, 7;
	add.s32 	%r1767, %r1766, 1;
	shr.u32 	%r1768, %r1767, 1;
	add.s32 	%r1769, %r1768, %r1764;
	or.b32  	%r1770, %r1769, %r688;
	mov.b32 	 %f1267, %r1770;

BB12_428:
	mul.rn.f32 	%f363, %f1267, %f1267;
	add.s32 	%r699, %r2186, 1;
	and.b32  	%r700, %r699, 1;
	setp.eq.s32	%p279, %r700, 0;
	@%p279 bra 	BB12_430;

	mov.f32 	%f1040, 0fBAB6061A;
	mov.f32 	%f1041, 0f37CCF5CE;
	fma.rn.f32 	%f1268, %f1041, %f363, %f1040;
	bra.uni 	BB12_431;

BB12_430:
	mov.f32 	%f1042, 0f3C08839E;
	mov.f32 	%f1043, 0fB94CA1F9;
	fma.rn.f32 	%f1268, %f1043, %f363, %f1042;

BB12_431:
	@%p279 bra 	BB12_433;

	mov.f32 	%f1044, 0f3D2AAAA5;
	fma.rn.f32 	%f1045, %f1268, %f363, %f1044;
	mov.f32 	%f1046, 0fBF000000;
	fma.rn.f32 	%f1269, %f1045, %f363, %f1046;
	bra.uni 	BB12_434;

BB12_433:
	mov.f32 	%f1047, 0fBE2AAAA3;
	fma.rn.f32 	%f1048, %f1268, %f363, %f1047;
	mov.f32 	%f1049, 0f00000000;
	fma.rn.f32 	%f1269, %f1048, %f363, %f1049;

BB12_434:
	fma.rn.f32 	%f1270, %f1269, %f1267, %f1267;
	@%p279 bra 	BB12_436;

	mov.f32 	%f1050, 0f3F800000;
	fma.rn.f32 	%f1270, %f1269, %f363, %f1050;

BB12_436:
	and.b32  	%r1771, %r699, 2;
	setp.eq.s32	%p282, %r1771, 0;
	@%p282 bra 	BB12_438;

	mov.f32 	%f1051, 0f00000000;
	mov.f32 	%f1052, 0fBF800000;
	fma.rn.f32 	%f1270, %f1270, %f1052, %f1051;

BB12_438:
	mov.f32 	%f1291, %f36;
	@%p49 bra 	BB12_440;

	mov.f32 	%f1053, 0f00000000;
	mul.rn.f32 	%f1291, %f36, %f1053;

BB12_440:
	mul.f32 	%f1054, %f1291, 0f3F22F983;
	cvt.rni.s32.f32	%r2196, %f1054;
	cvt.rn.f32.s32	%f1055, %r2196;
	neg.f32 	%f1056, %f1055;
	fma.rn.f32 	%f1058, %f1056, %f618, %f1291;
	fma.rn.f32 	%f1060, %f1056, %f620, %f1058;
	fma.rn.f32 	%f1271, %f1056, %f622, %f1060;
	abs.f32 	%f1062, %f1291;
	setp.leu.f32	%p284, %f1062, 0f47CE4780;
	@%p284 bra 	BB12_450;

	mov.b32 	 %r702, %f1291;
	shr.u32 	%r703, %r702, 23;
	bfe.u32 	%r1774, %r702, 23, 8;
	add.s32 	%r1775, %r1774, -128;
	shl.b32 	%r1776, %r702, 8;
	or.b32  	%r704, %r1776, -2147483648;
	shr.u32 	%r705, %r1775, 5;
	mov.u32 	%r2188, 0;
	mov.u64 	%rd191, __cudart_i2opi_f;
	mov.u32 	%r2187, -6;
	mov.u64 	%rd223, %rd1;

BB12_442:
	.pragma "nounroll";
	ld.const.u32 	%r1779, [%rd191];
	// inline asm
	{
	mad.lo.cc.u32   %r1777, %r1779, %r704, %r2188;
	madc.hi.u32     %r2188, %r1779, %r704,  0;
	}
	// inline asm
	st.local.u32 	[%rd223], %r1777;
	add.s64 	%rd223, %rd223, 4;
	add.s64 	%rd191, %rd191, 4;
	add.s32 	%r2187, %r2187, 1;
	setp.ne.s32	%p285, %r2187, 0;
	@%p285 bra 	BB12_442;

	and.b32  	%r710, %r702, -2147483648;
	st.local.u32 	[%rd2], %r2188;
	mov.u32 	%r1782, 6;
	sub.s32 	%r1783, %r1782, %r705;
	mul.wide.s32 	%rd163, %r1783, 4;
	add.s64 	%rd102, %rd1, %rd163;
	ld.local.u32 	%r2189, [%rd102];
	ld.local.u32 	%r2190, [%rd102+-4];
	and.b32  	%r713, %r703, 31;
	setp.eq.s32	%p286, %r713, 0;
	@%p286 bra 	BB12_445;

	mov.u32 	%r1784, 32;
	sub.s32 	%r1785, %r1784, %r713;
	shr.u32 	%r1786, %r2190, %r1785;
	shl.b32 	%r1787, %r2189, %r713;
	add.s32 	%r2189, %r1786, %r1787;
	ld.local.u32 	%r1788, [%rd102+-8];
	shr.u32 	%r1789, %r1788, %r1785;
	shl.b32 	%r1790, %r2190, %r713;
	add.s32 	%r2190, %r1789, %r1790;

BB12_445:
	shr.u32 	%r1791, %r2190, 30;
	shl.b32 	%r1792, %r2189, 2;
	add.s32 	%r2191, %r1791, %r1792;
	shl.b32 	%r719, %r2190, 2;
	shr.u32 	%r1793, %r2191, 31;
	shr.u32 	%r1794, %r2189, 30;
	add.s32 	%r720, %r1793, %r1794;
	setp.eq.s32	%p287, %r1793, 0;
	mov.u32 	%r2192, %r710;
	mov.u32 	%r2193, %r719;
	@%p287 bra 	BB12_447;

	not.b32 	%r1795, %r2191;
	neg.s32 	%r721, %r719;
	setp.eq.s32	%p288, %r719, 0;
	selp.u32	%r1796, 1, 0, %p288;
	add.s32 	%r2191, %r1796, %r1795;
	xor.b32  	%r723, %r710, -2147483648;
	mov.u32 	%r2192, %r723;
	mov.u32 	%r2193, %r721;

BB12_447:
	mov.u32 	%r725, %r2192;
	neg.s32 	%r1797, %r720;
	setp.eq.s32	%p289, %r710, 0;
	selp.b32	%r2196, %r720, %r1797, %p289;
	clz.b32 	%r2195, %r2191;
	setp.eq.s32	%p290, %r2195, 0;
	shl.b32 	%r1798, %r2191, %r2195;
	mov.u32 	%r1799, 32;
	sub.s32 	%r1800, %r1799, %r2195;
	shr.u32 	%r1801, %r2193, %r1800;
	add.s32 	%r1802, %r1801, %r1798;
	selp.b32	%r729, %r2191, %r1802, %p290;
	mov.u32 	%r1803, -921707870;
	mul.hi.u32 	%r2194, %r729, %r1803;
	setp.lt.s32	%p291, %r2194, 1;
	@%p291 bra 	BB12_449;

	mul.lo.s32 	%r1804, %r729, -921707870;
	shr.u32 	%r1805, %r1804, 31;
	shl.b32 	%r1806, %r2194, 1;
	add.s32 	%r2194, %r1805, %r1806;
	add.s32 	%r2195, %r2195, 1;

BB12_449:
	mov.u32 	%r1807, 126;
	sub.s32 	%r1808, %r1807, %r2195;
	shl.b32 	%r1809, %r1808, 23;
	add.s32 	%r1810, %r2194, 1;
	shr.u32 	%r1811, %r1810, 7;
	add.s32 	%r1812, %r1811, 1;
	shr.u32 	%r1813, %r1812, 1;
	add.s32 	%r1814, %r1813, %r1809;
	or.b32  	%r1815, %r1814, %r725;
	mov.b32 	 %f1271, %r1815;

BB12_450:
	mul.rn.f32 	%f380, %f1271, %f1271;
	add.s32 	%r736, %r2196, 1;
	and.b32  	%r737, %r736, 1;
	setp.eq.s32	%p292, %r737, 0;
	@%p292 bra 	BB12_452;

	mov.f32 	%f1063, 0fBAB6061A;
	mov.f32 	%f1064, 0f37CCF5CE;
	fma.rn.f32 	%f1272, %f1064, %f380, %f1063;
	bra.uni 	BB12_453;

BB12_452:
	mov.f32 	%f1065, 0f3C08839E;
	mov.f32 	%f1066, 0fB94CA1F9;
	fma.rn.f32 	%f1272, %f1066, %f380, %f1065;

BB12_453:
	@%p292 bra 	BB12_455;

	mov.f32 	%f1067, 0f3D2AAAA5;
	fma.rn.f32 	%f1068, %f1272, %f380, %f1067;
	mov.f32 	%f1069, 0fBF000000;
	fma.rn.f32 	%f1273, %f1068, %f380, %f1069;
	bra.uni 	BB12_456;

BB12_455:
	mov.f32 	%f1070, 0fBE2AAAA3;
	fma.rn.f32 	%f1071, %f1272, %f380, %f1070;
	mov.f32 	%f1072, 0f00000000;
	fma.rn.f32 	%f1273, %f1071, %f380, %f1072;

BB12_456:
	fma.rn.f32 	%f1274, %f1273, %f1271, %f1271;
	@%p292 bra 	BB12_458;

	mov.f32 	%f1073, 0f3F800000;
	fma.rn.f32 	%f1274, %f1273, %f380, %f1073;

BB12_458:
	and.b32  	%r1816, %r736, 2;
	setp.eq.s32	%p295, %r1816, 0;
	@%p295 bra 	BB12_460;

	mov.f32 	%f1074, 0f00000000;
	mov.f32 	%f1075, 0fBF800000;
	fma.rn.f32 	%f1274, %f1274, %f1075, %f1074;

BB12_460:
	mul.f32 	%f392, %f1270, %f1274;
	mov.f32 	%f1310, %f13;
	@%p62 bra 	BB12_462;

	mov.f32 	%f1076, 0f00000000;
	mul.rn.f32 	%f1310, %f13, %f1076;

BB12_462:
	mul.f32 	%f1077, %f1310, 0f3F22F983;
	cvt.rni.s32.f32	%r2206, %f1077;
	cvt.rn.f32.s32	%f1078, %r2206;
	neg.f32 	%f1079, %f1078;
	fma.rn.f32 	%f1081, %f1079, %f618, %f1310;
	fma.rn.f32 	%f1083, %f1079, %f620, %f1081;
	fma.rn.f32 	%f1275, %f1079, %f622, %f1083;
	abs.f32 	%f1085, %f1310;
	setp.leu.f32	%p297, %f1085, 0f47CE4780;
	@%p297 bra 	BB12_472;

	mov.b32 	 %r739, %f1310;
	shr.u32 	%r740, %r739, 23;
	bfe.u32 	%r1819, %r739, 23, 8;
	add.s32 	%r1820, %r1819, -128;
	shl.b32 	%r1821, %r739, 8;
	or.b32  	%r741, %r1821, -2147483648;
	shr.u32 	%r742, %r1820, 5;
	mov.u32 	%r2198, 0;
	mov.u64 	%rd192, __cudart_i2opi_f;
	mov.u32 	%r2197, -6;
	mov.u64 	%rd222, %rd1;

BB12_464:
	.pragma "nounroll";
	ld.const.u32 	%r1824, [%rd192];
	// inline asm
	{
	mad.lo.cc.u32   %r1822, %r1824, %r741, %r2198;
	madc.hi.u32     %r2198, %r1824, %r741,  0;
	}
	// inline asm
	st.local.u32 	[%rd222], %r1822;
	add.s64 	%rd222, %rd222, 4;
	add.s64 	%rd192, %rd192, 4;
	add.s32 	%r2197, %r2197, 1;
	setp.ne.s32	%p298, %r2197, 0;
	@%p298 bra 	BB12_464;

	and.b32  	%r747, %r739, -2147483648;
	st.local.u32 	[%rd2], %r2198;
	mov.u32 	%r1827, 6;
	sub.s32 	%r1828, %r1827, %r742;
	mul.wide.s32 	%rd165, %r1828, 4;
	add.s64 	%rd107, %rd1, %rd165;
	ld.local.u32 	%r2199, [%rd107];
	ld.local.u32 	%r2200, [%rd107+-4];
	and.b32  	%r750, %r740, 31;
	setp.eq.s32	%p299, %r750, 0;
	@%p299 bra 	BB12_467;

	mov.u32 	%r1829, 32;
	sub.s32 	%r1830, %r1829, %r750;
	shr.u32 	%r1831, %r2200, %r1830;
	shl.b32 	%r1832, %r2199, %r750;
	add.s32 	%r2199, %r1831, %r1832;
	ld.local.u32 	%r1833, [%rd107+-8];
	shr.u32 	%r1834, %r1833, %r1830;
	shl.b32 	%r1835, %r2200, %r750;
	add.s32 	%r2200, %r1834, %r1835;

BB12_467:
	shr.u32 	%r1836, %r2200, 30;
	shl.b32 	%r1837, %r2199, 2;
	add.s32 	%r2201, %r1836, %r1837;
	shl.b32 	%r756, %r2200, 2;
	shr.u32 	%r1838, %r2201, 31;
	shr.u32 	%r1839, %r2199, 30;
	add.s32 	%r757, %r1838, %r1839;
	setp.eq.s32	%p300, %r1838, 0;
	mov.u32 	%r2202, %r747;
	mov.u32 	%r2203, %r756;
	@%p300 bra 	BB12_469;

	not.b32 	%r1840, %r2201;
	neg.s32 	%r758, %r756;
	setp.eq.s32	%p301, %r756, 0;
	selp.u32	%r1841, 1, 0, %p301;
	add.s32 	%r2201, %r1841, %r1840;
	xor.b32  	%r760, %r747, -2147483648;
	mov.u32 	%r2202, %r760;
	mov.u32 	%r2203, %r758;

BB12_469:
	mov.u32 	%r762, %r2202;
	neg.s32 	%r1842, %r757;
	setp.eq.s32	%p302, %r747, 0;
	selp.b32	%r2206, %r757, %r1842, %p302;
	clz.b32 	%r2205, %r2201;
	setp.eq.s32	%p303, %r2205, 0;
	shl.b32 	%r1843, %r2201, %r2205;
	mov.u32 	%r1844, 32;
	sub.s32 	%r1845, %r1844, %r2205;
	shr.u32 	%r1846, %r2203, %r1845;
	add.s32 	%r1847, %r1846, %r1843;
	selp.b32	%r766, %r2201, %r1847, %p303;
	mov.u32 	%r1848, -921707870;
	mul.hi.u32 	%r2204, %r766, %r1848;
	setp.lt.s32	%p304, %r2204, 1;
	@%p304 bra 	BB12_471;

	mul.lo.s32 	%r1849, %r766, -921707870;
	shr.u32 	%r1850, %r1849, 31;
	shl.b32 	%r1851, %r2204, 1;
	add.s32 	%r2204, %r1850, %r1851;
	add.s32 	%r2205, %r2205, 1;

BB12_471:
	mov.u32 	%r1852, 126;
	sub.s32 	%r1853, %r1852, %r2205;
	shl.b32 	%r1854, %r1853, 23;
	add.s32 	%r1855, %r2204, 1;
	shr.u32 	%r1856, %r1855, 7;
	add.s32 	%r1857, %r1856, 1;
	shr.u32 	%r1858, %r1857, 1;
	add.s32 	%r1859, %r1858, %r1854;
	or.b32  	%r1860, %r1859, %r762;
	mov.b32 	 %f1275, %r1860;

BB12_472:
	mul.rn.f32 	%f398, %f1275, %f1275;
	and.b32  	%r773, %r2206, 1;
	setp.eq.s32	%p305, %r773, 0;
	@%p305 bra 	BB12_474;

	mov.f32 	%f1086, 0fBAB6061A;
	mov.f32 	%f1087, 0f37CCF5CE;
	fma.rn.f32 	%f1276, %f1087, %f398, %f1086;
	bra.uni 	BB12_475;

BB12_474:
	mov.f32 	%f1088, 0f3C08839E;
	mov.f32 	%f1089, 0fB94CA1F9;
	fma.rn.f32 	%f1276, %f1089, %f398, %f1088;

BB12_475:
	@%p305 bra 	BB12_477;

	mov.f32 	%f1090, 0f3D2AAAA5;
	fma.rn.f32 	%f1091, %f1276, %f398, %f1090;
	mov.f32 	%f1092, 0fBF000000;
	fma.rn.f32 	%f1277, %f1091, %f398, %f1092;
	bra.uni 	BB12_478;

BB12_477:
	mov.f32 	%f1093, 0fBE2AAAA3;
	fma.rn.f32 	%f1094, %f1276, %f398, %f1093;
	mov.f32 	%f1095, 0f00000000;
	fma.rn.f32 	%f1277, %f1094, %f398, %f1095;

BB12_478:
	fma.rn.f32 	%f1278, %f1277, %f1275, %f1275;
	@%p305 bra 	BB12_480;

	mov.f32 	%f1096, 0f3F800000;
	fma.rn.f32 	%f1278, %f1277, %f398, %f1096;

BB12_480:
	and.b32  	%r1861, %r2206, 2;
	setp.eq.s32	%p308, %r1861, 0;
	@%p308 bra 	BB12_482;

	mov.f32 	%f1097, 0f00000000;
	mov.f32 	%f1098, 0fBF800000;
	fma.rn.f32 	%f1278, %f1278, %f1098, %f1097;

BB12_482:
	mul.f32 	%f410, %f392, %f1278;
	mov.f32 	%f1336, %f34;
	@%p36 bra 	BB12_484;

	mov.f32 	%f1099, 0f00000000;
	mul.rn.f32 	%f1336, %f34, %f1099;

BB12_484:
	mul.f32 	%f1100, %f1336, 0f3F22F983;
	cvt.rni.s32.f32	%r2216, %f1100;
	cvt.rn.f32.s32	%f1101, %r2216;
	neg.f32 	%f1102, %f1101;
	fma.rn.f32 	%f1104, %f1102, %f618, %f1336;
	fma.rn.f32 	%f1106, %f1102, %f620, %f1104;
	fma.rn.f32 	%f1279, %f1102, %f622, %f1106;
	abs.f32 	%f1108, %f1336;
	setp.leu.f32	%p310, %f1108, 0f47CE4780;
	@%p310 bra 	BB12_494;

	mov.b32 	 %r775, %f1336;
	shr.u32 	%r776, %r775, 23;
	bfe.u32 	%r1864, %r775, 23, 8;
	add.s32 	%r1865, %r1864, -128;
	shl.b32 	%r1866, %r775, 8;
	or.b32  	%r777, %r1866, -2147483648;
	shr.u32 	%r778, %r1865, 5;
	mov.u32 	%r2208, 0;
	mov.u64 	%rd193, __cudart_i2opi_f;
	mov.u32 	%r2207, -6;
	mov.u64 	%rd221, %rd1;

BB12_486:
	.pragma "nounroll";
	ld.const.u32 	%r1869, [%rd193];
	// inline asm
	{
	mad.lo.cc.u32   %r1867, %r1869, %r777, %r2208;
	madc.hi.u32     %r2208, %r1869, %r777,  0;
	}
	// inline asm
	st.local.u32 	[%rd221], %r1867;
	add.s64 	%rd221, %rd221, 4;
	add.s64 	%rd193, %rd193, 4;
	add.s32 	%r2207, %r2207, 1;
	setp.ne.s32	%p311, %r2207, 0;
	@%p311 bra 	BB12_486;

	and.b32  	%r783, %r775, -2147483648;
	st.local.u32 	[%rd2], %r2208;
	mov.u32 	%r1872, 6;
	sub.s32 	%r1873, %r1872, %r778;
	mul.wide.s32 	%rd167, %r1873, 4;
	add.s64 	%rd112, %rd1, %rd167;
	ld.local.u32 	%r2209, [%rd112];
	ld.local.u32 	%r2210, [%rd112+-4];
	and.b32  	%r786, %r776, 31;
	setp.eq.s32	%p312, %r786, 0;
	@%p312 bra 	BB12_489;

	mov.u32 	%r1874, 32;
	sub.s32 	%r1875, %r1874, %r786;
	shr.u32 	%r1876, %r2210, %r1875;
	shl.b32 	%r1877, %r2209, %r786;
	add.s32 	%r2209, %r1876, %r1877;
	ld.local.u32 	%r1878, [%rd112+-8];
	shr.u32 	%r1879, %r1878, %r1875;
	shl.b32 	%r1880, %r2210, %r786;
	add.s32 	%r2210, %r1879, %r1880;

BB12_489:
	shr.u32 	%r1881, %r2210, 30;
	shl.b32 	%r1882, %r2209, 2;
	add.s32 	%r2211, %r1881, %r1882;
	shl.b32 	%r792, %r2210, 2;
	shr.u32 	%r1883, %r2211, 31;
	shr.u32 	%r1884, %r2209, 30;
	add.s32 	%r793, %r1883, %r1884;
	setp.eq.s32	%p313, %r1883, 0;
	mov.u32 	%r2212, %r783;
	mov.u32 	%r2213, %r792;
	@%p313 bra 	BB12_491;

	not.b32 	%r1885, %r2211;
	neg.s32 	%r794, %r792;
	setp.eq.s32	%p314, %r792, 0;
	selp.u32	%r1886, 1, 0, %p314;
	add.s32 	%r2211, %r1886, %r1885;
	xor.b32  	%r796, %r783, -2147483648;
	mov.u32 	%r2212, %r796;
	mov.u32 	%r2213, %r794;

BB12_491:
	mov.u32 	%r798, %r2212;
	neg.s32 	%r1887, %r793;
	setp.eq.s32	%p315, %r783, 0;
	selp.b32	%r2216, %r793, %r1887, %p315;
	clz.b32 	%r2215, %r2211;
	setp.eq.s32	%p316, %r2215, 0;
	shl.b32 	%r1888, %r2211, %r2215;
	mov.u32 	%r1889, 32;
	sub.s32 	%r1890, %r1889, %r2215;
	shr.u32 	%r1891, %r2213, %r1890;
	add.s32 	%r1892, %r1891, %r1888;
	selp.b32	%r802, %r2211, %r1892, %p316;
	mov.u32 	%r1893, -921707870;
	mul.hi.u32 	%r2214, %r802, %r1893;
	setp.lt.s32	%p317, %r2214, 1;
	@%p317 bra 	BB12_493;

	mul.lo.s32 	%r1894, %r802, -921707870;
	shr.u32 	%r1895, %r1894, 31;
	shl.b32 	%r1896, %r2214, 1;
	add.s32 	%r2214, %r1895, %r1896;
	add.s32 	%r2215, %r2215, 1;

BB12_493:
	mov.u32 	%r1897, 126;
	sub.s32 	%r1898, %r1897, %r2215;
	shl.b32 	%r1899, %r1898, 23;
	add.s32 	%r1900, %r2214, 1;
	shr.u32 	%r1901, %r1900, 7;
	add.s32 	%r1902, %r1901, 1;
	shr.u32 	%r1903, %r1902, 1;
	add.s32 	%r1904, %r1903, %r1899;
	or.b32  	%r1905, %r1904, %r798;
	mov.b32 	 %f1279, %r1905;

BB12_494:
	mul.rn.f32 	%f416, %f1279, %f1279;
	and.b32  	%r809, %r2216, 1;
	setp.eq.s32	%p318, %r809, 0;
	@%p318 bra 	BB12_496;

	mov.f32 	%f1109, 0fBAB6061A;
	mov.f32 	%f1110, 0f37CCF5CE;
	fma.rn.f32 	%f1280, %f1110, %f416, %f1109;
	bra.uni 	BB12_497;

BB12_496:
	mov.f32 	%f1111, 0f3C08839E;
	mov.f32 	%f1112, 0fB94CA1F9;
	fma.rn.f32 	%f1280, %f1112, %f416, %f1111;

BB12_497:
	@%p318 bra 	BB12_499;

	mov.f32 	%f1113, 0f3D2AAAA5;
	fma.rn.f32 	%f1114, %f1280, %f416, %f1113;
	mov.f32 	%f1115, 0fBF000000;
	fma.rn.f32 	%f1281, %f1114, %f416, %f1115;
	bra.uni 	BB12_500;

BB12_499:
	mov.f32 	%f1116, 0fBE2AAAA3;
	fma.rn.f32 	%f1117, %f1280, %f416, %f1116;
	mov.f32 	%f1118, 0f00000000;
	fma.rn.f32 	%f1281, %f1117, %f416, %f1118;

BB12_500:
	fma.rn.f32 	%f1282, %f1281, %f1279, %f1279;
	@%p318 bra 	BB12_502;

	mov.f32 	%f1119, 0f3F800000;
	fma.rn.f32 	%f1282, %f1281, %f416, %f1119;

BB12_502:
	and.b32  	%r1906, %r2216, 2;
	setp.eq.s32	%p321, %r1906, 0;
	@%p321 bra 	BB12_504;

	mov.f32 	%f1120, 0f00000000;
	mov.f32 	%f1121, 0fBF800000;
	fma.rn.f32 	%f1282, %f1282, %f1121, %f1120;

BB12_504:
	mov.f32 	%f1290, %f36;
	@%p49 bra 	BB12_506;

	mov.f32 	%f1122, 0f00000000;
	mul.rn.f32 	%f1290, %f36, %f1122;

BB12_506:
	mul.f32 	%f1123, %f1290, 0f3F22F983;
	cvt.rni.s32.f32	%r2226, %f1123;
	cvt.rn.f32.s32	%f1124, %r2226;
	neg.f32 	%f1125, %f1124;
	fma.rn.f32 	%f1127, %f1125, %f618, %f1290;
	fma.rn.f32 	%f1129, %f1125, %f620, %f1127;
	fma.rn.f32 	%f1298, %f1125, %f622, %f1129;
	abs.f32 	%f1131, %f1290;
	setp.leu.f32	%p323, %f1131, 0f47CE4780;
	@%p323 bra 	BB12_516;

	mov.b32 	 %r811, %f1290;
	shr.u32 	%r812, %r811, 23;
	bfe.u32 	%r1909, %r811, 23, 8;
	add.s32 	%r1910, %r1909, -128;
	shl.b32 	%r1911, %r811, 8;
	or.b32  	%r813, %r1911, -2147483648;
	shr.u32 	%r814, %r1910, 5;
	mov.u32 	%r2218, 0;
	mov.u64 	%rd194, __cudart_i2opi_f;
	mov.u32 	%r2217, -6;
	mov.u64 	%rd220, %rd1;

BB12_508:
	.pragma "nounroll";
	ld.const.u32 	%r1914, [%rd194];
	// inline asm
	{
	mad.lo.cc.u32   %r1912, %r1914, %r813, %r2218;
	madc.hi.u32     %r2218, %r1914, %r813,  0;
	}
	// inline asm
	st.local.u32 	[%rd220], %r1912;
	add.s64 	%rd220, %rd220, 4;
	add.s64 	%rd194, %rd194, 4;
	add.s32 	%r2217, %r2217, 1;
	setp.ne.s32	%p324, %r2217, 0;
	@%p324 bra 	BB12_508;

	and.b32  	%r819, %r811, -2147483648;
	st.local.u32 	[%rd2], %r2218;
	mov.u32 	%r1917, 6;
	sub.s32 	%r1918, %r1917, %r814;
	mul.wide.s32 	%rd169, %r1918, 4;
	add.s64 	%rd117, %rd1, %rd169;
	ld.local.u32 	%r2219, [%rd117];
	ld.local.u32 	%r2220, [%rd117+-4];
	and.b32  	%r822, %r812, 31;
	setp.eq.s32	%p325, %r822, 0;
	@%p325 bra 	BB12_511;

	mov.u32 	%r1919, 32;
	sub.s32 	%r1920, %r1919, %r822;
	shr.u32 	%r1921, %r2220, %r1920;
	shl.b32 	%r1922, %r2219, %r822;
	add.s32 	%r2219, %r1921, %r1922;
	ld.local.u32 	%r1923, [%rd117+-8];
	shr.u32 	%r1924, %r1923, %r1920;
	shl.b32 	%r1925, %r2220, %r822;
	add.s32 	%r2220, %r1924, %r1925;

BB12_511:
	shr.u32 	%r1926, %r2220, 30;
	shl.b32 	%r1927, %r2219, 2;
	add.s32 	%r2221, %r1926, %r1927;
	shl.b32 	%r828, %r2220, 2;
	shr.u32 	%r1928, %r2221, 31;
	shr.u32 	%r1929, %r2219, 30;
	add.s32 	%r829, %r1928, %r1929;
	setp.eq.s32	%p326, %r1928, 0;
	mov.u32 	%r2222, %r819;
	mov.u32 	%r2223, %r828;
	@%p326 bra 	BB12_513;

	not.b32 	%r1930, %r2221;
	neg.s32 	%r830, %r828;
	setp.eq.s32	%p327, %r828, 0;
	selp.u32	%r1931, 1, 0, %p327;
	add.s32 	%r2221, %r1931, %r1930;
	xor.b32  	%r832, %r819, -2147483648;
	mov.u32 	%r2222, %r832;
	mov.u32 	%r2223, %r830;

BB12_513:
	mov.u32 	%r834, %r2222;
	neg.s32 	%r1932, %r829;
	setp.eq.s32	%p328, %r819, 0;
	selp.b32	%r2226, %r829, %r1932, %p328;
	clz.b32 	%r2225, %r2221;
	setp.eq.s32	%p329, %r2225, 0;
	shl.b32 	%r1933, %r2221, %r2225;
	mov.u32 	%r1934, 32;
	sub.s32 	%r1935, %r1934, %r2225;
	shr.u32 	%r1936, %r2223, %r1935;
	add.s32 	%r1937, %r1936, %r1933;
	selp.b32	%r838, %r2221, %r1937, %p329;
	mov.u32 	%r1938, -921707870;
	mul.hi.u32 	%r2224, %r838, %r1938;
	setp.lt.s32	%p330, %r2224, 1;
	@%p330 bra 	BB12_515;

	mul.lo.s32 	%r1939, %r838, -921707870;
	shr.u32 	%r1940, %r1939, 31;
	shl.b32 	%r1941, %r2224, 1;
	add.s32 	%r2224, %r1940, %r1941;
	add.s32 	%r2225, %r2225, 1;

BB12_515:
	mov.u32 	%r1942, 126;
	sub.s32 	%r1943, %r1942, %r2225;
	shl.b32 	%r1944, %r1943, 23;
	add.s32 	%r1945, %r2224, 1;
	shr.u32 	%r1946, %r1945, 7;
	add.s32 	%r1947, %r1946, 1;
	shr.u32 	%r1948, %r1947, 1;
	add.s32 	%r1949, %r1948, %r1944;
	or.b32  	%r1950, %r1949, %r834;
	mov.b32 	 %f1298, %r1950;

BB12_516:
	mul.rn.f32 	%f433, %f1298, %f1298;
	and.b32  	%r845, %r2226, 1;
	setp.eq.s32	%p331, %r845, 0;
	@%p331 bra 	BB12_518;

	mov.f32 	%f1132, 0fBAB6061A;
	mov.f32 	%f1133, 0f37CCF5CE;
	fma.rn.f32 	%f1299, %f1133, %f433, %f1132;
	bra.uni 	BB12_519;

BB12_518:
	mov.f32 	%f1134, 0f3C08839E;
	mov.f32 	%f1135, 0fB94CA1F9;
	fma.rn.f32 	%f1299, %f1135, %f433, %f1134;

BB12_519:
	@%p331 bra 	BB12_521;

	mov.f32 	%f1136, 0f3D2AAAA5;
	fma.rn.f32 	%f1137, %f1299, %f433, %f1136;
	mov.f32 	%f1138, 0fBF000000;
	fma.rn.f32 	%f1300, %f1137, %f433, %f1138;
	bra.uni 	BB12_522;

BB12_521:
	mov.f32 	%f1139, 0fBE2AAAA3;
	fma.rn.f32 	%f1140, %f1299, %f433, %f1139;
	mov.f32 	%f1141, 0f00000000;
	fma.rn.f32 	%f1300, %f1140, %f433, %f1141;

BB12_522:
	fma.rn.f32 	%f1301, %f1300, %f1298, %f1298;
	@%p331 bra 	BB12_524;

	mov.f32 	%f1142, 0f3F800000;
	fma.rn.f32 	%f1301, %f1300, %f433, %f1142;

BB12_524:
	and.b32  	%r1951, %r2226, 2;
	setp.eq.s32	%p334, %r1951, 0;
	@%p334 bra 	BB12_526;

	mov.f32 	%f1143, 0f00000000;
	mov.f32 	%f1144, 0fBF800000;
	fma.rn.f32 	%f1301, %f1301, %f1144, %f1143;

BB12_526:
	mul.f32 	%f445, %f1282, %f1301;
	mov.f32 	%f1309, %f13;
	@%p62 bra 	BB12_528;

	mov.f32 	%f1145, 0f00000000;
	mul.rn.f32 	%f1309, %f13, %f1145;

BB12_528:
	mul.f32 	%f1146, %f1309, 0f3F22F983;
	cvt.rni.s32.f32	%r2236, %f1146;
	cvt.rn.f32.s32	%f1147, %r2236;
	neg.f32 	%f1148, %f1147;
	fma.rn.f32 	%f1150, %f1148, %f618, %f1309;
	fma.rn.f32 	%f1152, %f1148, %f620, %f1150;
	fma.rn.f32 	%f1317, %f1148, %f622, %f1152;
	abs.f32 	%f1154, %f1309;
	setp.leu.f32	%p336, %f1154, 0f47CE4780;
	@%p336 bra 	BB12_538;

	mov.b32 	 %r847, %f1309;
	shr.u32 	%r848, %r847, 23;
	bfe.u32 	%r1954, %r847, 23, 8;
	add.s32 	%r1955, %r1954, -128;
	shl.b32 	%r1956, %r847, 8;
	or.b32  	%r849, %r1956, -2147483648;
	shr.u32 	%r850, %r1955, 5;
	mov.u32 	%r2228, 0;
	mov.u64 	%rd195, __cudart_i2opi_f;
	mov.u32 	%r2227, -6;
	mov.u64 	%rd219, %rd1;

BB12_530:
	.pragma "nounroll";
	ld.const.u32 	%r1959, [%rd195];
	// inline asm
	{
	mad.lo.cc.u32   %r1957, %r1959, %r849, %r2228;
	madc.hi.u32     %r2228, %r1959, %r849,  0;
	}
	// inline asm
	st.local.u32 	[%rd219], %r1957;
	add.s64 	%rd219, %rd219, 4;
	add.s64 	%rd195, %rd195, 4;
	add.s32 	%r2227, %r2227, 1;
	setp.ne.s32	%p337, %r2227, 0;
	@%p337 bra 	BB12_530;

	and.b32  	%r855, %r847, -2147483648;
	st.local.u32 	[%rd2], %r2228;
	mov.u32 	%r1962, 6;
	sub.s32 	%r1963, %r1962, %r850;
	mul.wide.s32 	%rd171, %r1963, 4;
	add.s64 	%rd122, %rd1, %rd171;
	ld.local.u32 	%r2229, [%rd122];
	ld.local.u32 	%r2230, [%rd122+-4];
	and.b32  	%r858, %r848, 31;
	setp.eq.s32	%p338, %r858, 0;
	@%p338 bra 	BB12_533;

	mov.u32 	%r1964, 32;
	sub.s32 	%r1965, %r1964, %r858;
	shr.u32 	%r1966, %r2230, %r1965;
	shl.b32 	%r1967, %r2229, %r858;
	add.s32 	%r2229, %r1966, %r1967;
	ld.local.u32 	%r1968, [%rd122+-8];
	shr.u32 	%r1969, %r1968, %r1965;
	shl.b32 	%r1970, %r2230, %r858;
	add.s32 	%r2230, %r1969, %r1970;

BB12_533:
	shr.u32 	%r1971, %r2230, 30;
	shl.b32 	%r1972, %r2229, 2;
	add.s32 	%r2231, %r1971, %r1972;
	shl.b32 	%r864, %r2230, 2;
	shr.u32 	%r1973, %r2231, 31;
	shr.u32 	%r1974, %r2229, 30;
	add.s32 	%r865, %r1973, %r1974;
	setp.eq.s32	%p339, %r1973, 0;
	mov.u32 	%r2232, %r855;
	mov.u32 	%r2233, %r864;
	@%p339 bra 	BB12_535;

	not.b32 	%r1975, %r2231;
	neg.s32 	%r866, %r864;
	setp.eq.s32	%p340, %r864, 0;
	selp.u32	%r1976, 1, 0, %p340;
	add.s32 	%r2231, %r1976, %r1975;
	xor.b32  	%r868, %r855, -2147483648;
	mov.u32 	%r2232, %r868;
	mov.u32 	%r2233, %r866;

BB12_535:
	mov.u32 	%r870, %r2232;
	neg.s32 	%r1977, %r865;
	setp.eq.s32	%p341, %r855, 0;
	selp.b32	%r2236, %r865, %r1977, %p341;
	clz.b32 	%r2235, %r2231;
	setp.eq.s32	%p342, %r2235, 0;
	shl.b32 	%r1978, %r2231, %r2235;
	mov.u32 	%r1979, 32;
	sub.s32 	%r1980, %r1979, %r2235;
	shr.u32 	%r1981, %r2233, %r1980;
	add.s32 	%r1982, %r1981, %r1978;
	selp.b32	%r874, %r2231, %r1982, %p342;
	mov.u32 	%r1983, -921707870;
	mul.hi.u32 	%r2234, %r874, %r1983;
	setp.lt.s32	%p343, %r2234, 1;
	@%p343 bra 	BB12_537;

	mul.lo.s32 	%r1984, %r874, -921707870;
	shr.u32 	%r1985, %r1984, 31;
	shl.b32 	%r1986, %r2234, 1;
	add.s32 	%r2234, %r1985, %r1986;
	add.s32 	%r2235, %r2235, 1;

BB12_537:
	mov.u32 	%r1987, 126;
	sub.s32 	%r1988, %r1987, %r2235;
	shl.b32 	%r1989, %r1988, 23;
	add.s32 	%r1990, %r2234, 1;
	shr.u32 	%r1991, %r1990, 7;
	add.s32 	%r1992, %r1991, 1;
	shr.u32 	%r1993, %r1992, 1;
	add.s32 	%r1994, %r1993, %r1989;
	or.b32  	%r1995, %r1994, %r870;
	mov.b32 	 %f1317, %r1995;

BB12_538:
	mul.rn.f32 	%f451, %f1317, %f1317;
	add.s32 	%r881, %r2236, 1;
	and.b32  	%r882, %r881, 1;
	setp.eq.s32	%p344, %r882, 0;
	@%p344 bra 	BB12_540;

	mov.f32 	%f1155, 0fBAB6061A;
	mov.f32 	%f1156, 0f37CCF5CE;
	fma.rn.f32 	%f1318, %f1156, %f451, %f1155;
	bra.uni 	BB12_541;

BB12_540:
	mov.f32 	%f1157, 0f3C08839E;
	mov.f32 	%f1158, 0fB94CA1F9;
	fma.rn.f32 	%f1318, %f1158, %f451, %f1157;

BB12_541:
	@%p344 bra 	BB12_543;

	mov.f32 	%f1159, 0f3D2AAAA5;
	fma.rn.f32 	%f1160, %f1318, %f451, %f1159;
	mov.f32 	%f1161, 0fBF000000;
	fma.rn.f32 	%f1319, %f1160, %f451, %f1161;
	bra.uni 	BB12_544;

BB12_543:
	mov.f32 	%f1162, 0fBE2AAAA3;
	fma.rn.f32 	%f1163, %f1318, %f451, %f1162;
	mov.f32 	%f1164, 0f00000000;
	fma.rn.f32 	%f1319, %f1163, %f451, %f1164;

BB12_544:
	fma.rn.f32 	%f1320, %f1319, %f1317, %f1317;
	@%p344 bra 	BB12_546;

	mov.f32 	%f1165, 0f3F800000;
	fma.rn.f32 	%f1320, %f1319, %f451, %f1165;

BB12_546:
	and.b32  	%r1996, %r881, 2;
	setp.eq.s32	%p347, %r1996, 0;
	@%p347 bra 	BB12_548;

	mov.f32 	%f1166, 0f00000000;
	mov.f32 	%f1167, 0fBF800000;
	fma.rn.f32 	%f1320, %f1320, %f1167, %f1166;

BB12_548:
	fma.rn.f32 	%f463, %f445, %f1320, %f410;
	neg.f32 	%f1168, %f6;
	add.f32 	%f1169, %f145, %f6;
	abs.f32 	%f1170, %f1169;
	cvt.f64.f32	%fd2, %f1170;
	abs.f32 	%f1171, %f1168;
	abs.f32 	%f1172, %f145;
	max.f32 	%f1173, %f1172, %f1171;
	cvt.f64.f32	%fd3, %f1173;
	mul.f64 	%fd4, %fd3, 0d3EE9000000000000;
	setp.geu.f64	%p348, %fd2, %fd4;
	mov.f32 	%f1335, %f34;
	@%p348 bra 	BB12_553;

	neg.f32 	%f1174, %f8;
	add.f32 	%f1175, %f251, %f8;
	abs.f32 	%f1176, %f1175;
	cvt.f64.f32	%fd5, %f1176;
	abs.f32 	%f1177, %f1174;
	abs.f32 	%f1178, %f251;
	max.f32 	%f1179, %f1178, %f1177;
	cvt.f64.f32	%fd6, %f1179;
	mul.f64 	%fd7, %fd6, 0d3EE9000000000000;
	setp.geu.f64	%p349, %fd5, %fd7;
	mov.f32 	%f1335, %f34;
	@%p349 bra 	BB12_553;

	neg.f32 	%f1180, %f7;
	add.f32 	%f1181, %f357, %f7;
	abs.f32 	%f1182, %f1181;
	cvt.f64.f32	%fd8, %f1182;
	abs.f32 	%f1183, %f1180;
	abs.f32 	%f1184, %f357;
	max.f32 	%f1185, %f1184, %f1183;
	cvt.f64.f32	%fd9, %f1185;
	mul.f64 	%fd10, %fd9, 0d3EE9000000000000;
	setp.geu.f64	%p350, %fd8, %fd10;
	mov.f32 	%f1335, %f34;
	@%p350 bra 	BB12_553;

	neg.f32 	%f1186, %f9;
	add.f32 	%f1187, %f463, %f9;
	abs.f32 	%f1188, %f1187;
	cvt.f64.f32	%fd11, %f1188;
	abs.f32 	%f1189, %f1186;
	abs.f32 	%f1190, %f463;
	max.f32 	%f1191, %f1190, %f1189;
	cvt.f64.f32	%fd12, %f1191;
	mul.f64 	%fd13, %fd12, 0d3EE9000000000000;
	setp.geu.f64	%p351, %fd11, %fd13;
	mov.f32 	%f1335, %f34;
	@%p351 bra 	BB12_553;

	cvt.f64.f32	%fd14, %f34;
	setp.ltu.f32	%p352, %f34, 0f00000000;
	selp.f64	%fd15, 0dC00921FB54442D18, 0d400921FB54442D18, %p352;
	sub.f64 	%fd16, %fd14, %fd15;
	cvt.rn.f32.f64	%f1335, %fd16;

BB12_553:
	mov.f32 	%f1348, %f13;
	mov.f32 	%f1347, %f36;
	mov.f32 	%f1346, %f1335;
	mov.f32 	%f1345, %f5;

BB12_554:
	st.param.f32	[func_retval0+0], %f1345;
	st.param.f32	[func_retval0+4], %f1346;
	st.param.f32	[func_retval0+8], %f1347;
	st.param.f32	[func_retval0+12], %f1348;
	ret;
}

	// .globl	_Z5qnorm6float4
.visible .func  (.param .b32 func_retval0) _Z5qnorm6float4(
	.param .align 16 .b8 _Z5qnorm6float4_param_0[16]
)
{
	.reg .f32 	%f<10>;


	ld.param.f32 	%f1, [_Z5qnorm6float4_param_0+12];
	ld.param.f32 	%f2, [_Z5qnorm6float4_param_0+8];
	ld.param.f32 	%f3, [_Z5qnorm6float4_param_0];
	ld.param.f32 	%f4, [_Z5qnorm6float4_param_0+4];
	mul.f32 	%f5, %f4, %f4;
	fma.rn.f32 	%f6, %f3, %f3, %f5;
	fma.rn.f32 	%f7, %f2, %f2, %f6;
	fma.rn.f32 	%f8, %f1, %f1, %f7;
	sqrt.rn.f32 	%f9, %f8;
	st.param.f32	[func_retval0+0], %f9;
	ret;
}

	// .globl	_Z4qdiv6float4f
.visible .func  (.param .align 16 .b8 func_retval0[16]) _Z4qdiv6float4f(
	.param .align 16 .b8 _Z4qdiv6float4f_param_0[16],
	.param .b32 _Z4qdiv6float4f_param_1
)
{
	.reg .f32 	%f<10>;


	ld.param.f32 	%f1, [_Z4qdiv6float4f_param_0+12];
	ld.param.f32 	%f2, [_Z4qdiv6float4f_param_0+8];
	ld.param.f32 	%f3, [_Z4qdiv6float4f_param_0+4];
	ld.param.f32 	%f4, [_Z4qdiv6float4f_param_0];
	ld.param.f32 	%f5, [_Z4qdiv6float4f_param_1];
	div.rn.f32 	%f6, %f4, %f5;
	div.rn.f32 	%f7, %f3, %f5;
	div.rn.f32 	%f8, %f2, %f5;
	div.rn.f32 	%f9, %f1, %f5;
	st.param.f32	[func_retval0+0], %f6;
	st.param.f32	[func_retval0+4], %f7;
	st.param.f32	[func_retval0+8], %f8;
	st.param.f32	[func_retval0+12], %f9;
	ret;
}

	// .globl	_Z4qmul6float4S_
.visible .func  (.param .align 16 .b8 func_retval0[16]) _Z4qmul6float4S_(
	.param .align 16 .b8 _Z4qmul6float4S__param_0[16],
	.param .align 16 .b8 _Z4qmul6float4S__param_1[16]
)
{
	.reg .f32 	%f<31>;


	ld.param.f32 	%f1, [_Z4qmul6float4S__param_0+12];
	ld.param.f32 	%f2, [_Z4qmul6float4S__param_0+8];
	ld.param.f32 	%f3, [_Z4qmul6float4S__param_0+4];
	ld.param.f32 	%f4, [_Z4qmul6float4S__param_0];
	ld.param.f32 	%f5, [_Z4qmul6float4S__param_1+12];
	ld.param.f32 	%f6, [_Z4qmul6float4S__param_1+8];
	ld.param.f32 	%f7, [_Z4qmul6float4S__param_1+4];
	ld.param.f32 	%f8, [_Z4qmul6float4S__param_1];
	mul.f32 	%f9, %f4, %f8;
	mul.f32 	%f10, %f3, %f7;
	sub.f32 	%f11, %f9, %f10;
	mul.f32 	%f12, %f2, %f6;
	sub.f32 	%f13, %f11, %f12;
	mul.f32 	%f14, %f1, %f5;
	sub.f32 	%f15, %f13, %f14;
	mul.f32 	%f16, %f4, %f7;
	fma.rn.f32 	%f17, %f3, %f8, %f16;
	mul.f32 	%f18, %f1, %f6;
	sub.f32 	%f19, %f17, %f18;
	fma.rn.f32 	%f20, %f2, %f5, %f19;
	mul.f32 	%f21, %f1, %f7;
	fma.rn.f32 	%f22, %f2, %f8, %f21;
	fma.rn.f32 	%f23, %f4, %f6, %f22;
	mul.f32 	%f24, %f3, %f5;
	sub.f32 	%f25, %f23, %f24;
	mul.f32 	%f26, %f1, %f8;
	mul.f32 	%f27, %f2, %f7;
	sub.f32 	%f28, %f26, %f27;
	fma.rn.f32 	%f29, %f3, %f6, %f28;
	fma.rn.f32 	%f30, %f4, %f5, %f29;
	st.param.f32	[func_retval0+0], %f15;
	st.param.f32	[func_retval0+4], %f20;
	st.param.f32	[func_retval0+8], %f25;
	st.param.f32	[func_retval0+12], %f30;
	ret;
}

	// .globl	_Z4comp6float4S_
.visible .func  (.param .b32 func_retval0) _Z4comp6float4S_(
	.param .align 16 .b8 _Z4comp6float4S__param_0[16],
	.param .align 16 .b8 _Z4comp6float4S__param_1[16]
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<13>;


	ld.param.f32 	%f4, [_Z4comp6float4S__param_0+12];
	ld.param.f32 	%f3, [_Z4comp6float4S__param_0+8];
	ld.param.f32 	%f2, [_Z4comp6float4S__param_0+4];
	ld.param.f32 	%f1, [_Z4comp6float4S__param_0];
	ld.param.f32 	%f8, [_Z4comp6float4S__param_1+12];
	ld.param.f32 	%f7, [_Z4comp6float4S__param_1+8];
	ld.param.f32 	%f6, [_Z4comp6float4S__param_1+4];
	ld.param.f32 	%f5, [_Z4comp6float4S__param_1];
	sub.f32 	%f9, %f1, %f5;
	abs.f32 	%f10, %f9;
	cvt.f64.f32	%fd1, %f10;
	abs.f32 	%f11, %f1;
	abs.f32 	%f12, %f5;
	max.f32 	%f13, %f11, %f12;
	cvt.f64.f32	%fd2, %f13;
	mul.f64 	%fd3, %fd2, 0d3EE9000000000000;
	mov.u16 	%rs6, 0;
	setp.geu.f64	%p1, %fd1, %fd3;
	@%p1 bra 	BB16_4;

	sub.f32 	%f14, %f2, %f6;
	abs.f32 	%f15, %f14;
	cvt.f64.f32	%fd4, %f15;
	abs.f32 	%f16, %f6;
	abs.f32 	%f17, %f2;
	max.f32 	%f18, %f17, %f16;
	cvt.f64.f32	%fd5, %f18;
	mul.f64 	%fd6, %fd5, 0d3EE9000000000000;
	setp.geu.f64	%p2, %fd4, %fd6;
	@%p2 bra 	BB16_4;

	sub.f32 	%f19, %f3, %f7;
	abs.f32 	%f20, %f19;
	cvt.f64.f32	%fd7, %f20;
	abs.f32 	%f21, %f7;
	abs.f32 	%f22, %f3;
	max.f32 	%f23, %f22, %f21;
	cvt.f64.f32	%fd8, %f23;
	mul.f64 	%fd9, %fd8, 0d3EE9000000000000;
	setp.geu.f64	%p3, %fd7, %fd9;
	@%p3 bra 	BB16_4;

	sub.f32 	%f24, %f4, %f8;
	abs.f32 	%f25, %f24;
	cvt.f64.f32	%fd10, %f25;
	abs.f32 	%f26, %f8;
	abs.f32 	%f27, %f4;
	max.f32 	%f28, %f27, %f26;
	cvt.f64.f32	%fd11, %f28;
	mul.f64 	%fd12, %fd11, 0d3EE9000000000000;
	setp.lt.f64	%p4, %fd10, %fd12;
	selp.u16	%rs6, 1, 0, %p4;

BB16_4:
	cvt.u32.u16	%r1, %rs6;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .globl	_Z4qmul6float4f
.visible .func  (.param .align 16 .b8 func_retval0[16]) _Z4qmul6float4f(
	.param .align 16 .b8 _Z4qmul6float4f_param_0[16],
	.param .b32 _Z4qmul6float4f_param_1
)
{
	.reg .f32 	%f<10>;


	ld.param.f32 	%f1, [_Z4qmul6float4f_param_0+12];
	ld.param.f32 	%f2, [_Z4qmul6float4f_param_0+8];
	ld.param.f32 	%f3, [_Z4qmul6float4f_param_0+4];
	ld.param.f32 	%f4, [_Z4qmul6float4f_param_0];
	ld.param.f32 	%f5, [_Z4qmul6float4f_param_1];
	mul.f32 	%f6, %f4, %f5;
	mul.f32 	%f7, %f3, %f5;
	mul.f32 	%f8, %f2, %f5;
	mul.f32 	%f9, %f1, %f5;
	st.param.f32	[func_retval0+0], %f6;
	st.param.f32	[func_retval0+4], %f7;
	st.param.f32	[func_retval0+8], %f8;
	st.param.f32	[func_retval0+12], %f9;
	ret;
}

	// .globl	_Z4fillIfEvPT_S0_i
.visible .func _Z4fillIfEvPT_S0_i(
	.param .b64 _Z4fillIfEvPT_S0_i_param_0,
	.param .b32 _Z4fillIfEvPT_S0_i_param_1,
	.param .b32 _Z4fillIfEvPT_S0_i_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z4fillIfEvPT_S0_i_param_0];
	ld.param.f32 	%f1, [_Z4fillIfEvPT_S0_i_param_1];
	ld.param.u32 	%r2, [_Z4fillIfEvPT_S0_i_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB18_2;

	mul.wide.s32 	%rd2, %r1, 4;
	add.s64 	%rd3, %rd1, %rd2;
	st.f32 	[%rd3], %f1;

BB18_2:
	ret;
}

	// .globl	_Z4fillIdEvPT_S0_i
.visible .func _Z4fillIdEvPT_S0_i(
	.param .b64 _Z4fillIdEvPT_S0_i_param_0,
	.param .b64 _Z4fillIdEvPT_S0_i_param_1,
	.param .b32 _Z4fillIdEvPT_S0_i_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z4fillIdEvPT_S0_i_param_0];
	ld.param.f64 	%fd1, [_Z4fillIdEvPT_S0_i_param_1];
	ld.param.u32 	%r2, [_Z4fillIdEvPT_S0_i_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB19_2;

	mul.wide.s32 	%rd2, %r1, 8;
	add.s64 	%rd3, %rd1, %rd2;
	st.f64 	[%rd3], %fd1;

BB19_2:
	ret;
}

	// .globl	_Z4fillI6float2EvPT_S1_i
.visible .func _Z4fillI6float2EvPT_S1_i(
	.param .b64 _Z4fillI6float2EvPT_S1_i_param_0,
	.param .align 8 .b8 _Z4fillI6float2EvPT_S1_i_param_1[8],
	.param .b32 _Z4fillI6float2EvPT_S1_i_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z4fillI6float2EvPT_S1_i_param_0];
	ld.param.f32 	%f2, [_Z4fillI6float2EvPT_S1_i_param_1+4];
	ld.param.f32 	%f1, [_Z4fillI6float2EvPT_S1_i_param_1];
	ld.param.u32 	%r2, [_Z4fillI6float2EvPT_S1_i_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB20_2;

	mul.wide.s32 	%rd2, %r1, 8;
	add.s64 	%rd3, %rd1, %rd2;
	st.v2.f32 	[%rd3], {%f1, %f2};

BB20_2:
	ret;
}

	// .globl	_Z4fillI7double2EvPT_S1_i
.visible .func _Z4fillI7double2EvPT_S1_i(
	.param .b64 _Z4fillI7double2EvPT_S1_i_param_0,
	.param .align 16 .b8 _Z4fillI7double2EvPT_S1_i_param_1[16],
	.param .b32 _Z4fillI7double2EvPT_S1_i_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z4fillI7double2EvPT_S1_i_param_0];
	ld.param.f64 	%fd2, [_Z4fillI7double2EvPT_S1_i_param_1+8];
	ld.param.f64 	%fd1, [_Z4fillI7double2EvPT_S1_i_param_1];
	ld.param.u32 	%r2, [_Z4fillI7double2EvPT_S1_i_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB21_2;

	mul.wide.s32 	%rd2, %r1, 16;
	add.s64 	%rd3, %rd1, %rd2;
	st.v2.f64 	[%rd3], {%fd1, %fd2};

BB21_2:
	ret;
}

	// .globl	_Z4fillI6float4EvPT_S1_i
.visible .func _Z4fillI6float4EvPT_S1_i(
	.param .b64 _Z4fillI6float4EvPT_S1_i_param_0,
	.param .align 16 .b8 _Z4fillI6float4EvPT_S1_i_param_1[16],
	.param .b32 _Z4fillI6float4EvPT_S1_i_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z4fillI6float4EvPT_S1_i_param_0];
	ld.param.f32 	%f4, [_Z4fillI6float4EvPT_S1_i_param_1+12];
	ld.param.f32 	%f3, [_Z4fillI6float4EvPT_S1_i_param_1+8];
	ld.param.f32 	%f2, [_Z4fillI6float4EvPT_S1_i_param_1+4];
	ld.param.f32 	%f1, [_Z4fillI6float4EvPT_S1_i_param_1];
	ld.param.u32 	%r2, [_Z4fillI6float4EvPT_S1_i_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB22_2;

	mul.wide.s32 	%rd2, %r1, 16;
	add.s64 	%rd3, %rd1, %rd2;
	st.v4.f32 	[%rd3], {%f1, %f2, %f3, %f4};

BB22_2:
	ret;
}

	// .globl	_Z4fillI7double4EvPT_S1_i
.visible .func _Z4fillI7double4EvPT_S1_i(
	.param .b64 _Z4fillI7double4EvPT_S1_i_param_0,
	.param .align 16 .b8 _Z4fillI7double4EvPT_S1_i_param_1[32],
	.param .b32 _Z4fillI7double4EvPT_S1_i_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z4fillI7double4EvPT_S1_i_param_0];
	ld.param.f64 	%fd4, [_Z4fillI7double4EvPT_S1_i_param_1+24];
	ld.param.f64 	%fd3, [_Z4fillI7double4EvPT_S1_i_param_1+16];
	ld.param.f64 	%fd2, [_Z4fillI7double4EvPT_S1_i_param_1+8];
	ld.param.f64 	%fd1, [_Z4fillI7double4EvPT_S1_i_param_1];
	ld.param.u32 	%r2, [_Z4fillI7double4EvPT_S1_i_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB23_2;

	mul.wide.s32 	%rd2, %r1, 32;
	add.s64 	%rd3, %rd1, %rd2;
	st.v2.f64 	[%rd3], {%fd1, %fd2};
	st.v2.f64 	[%rd3+16], {%fd3, %fd4};

BB23_2:
	ret;
}

	// .globl	_Z12compare_realIiEvPiPKT_S3_i
.visible .func _Z12compare_realIiEvPiPKT_S3_i(
	.param .b64 _Z12compare_realIiEvPiPKT_S3_i_param_0,
	.param .b64 _Z12compare_realIiEvPiPKT_S3_i_param_1,
	.param .b64 _Z12compare_realIiEvPiPKT_S3_i_param_2,
	.param .b32 _Z12compare_realIiEvPiPKT_S3_i_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [_Z12compare_realIiEvPiPKT_S3_i_param_0];
	ld.param.u64 	%rd2, [_Z12compare_realIiEvPiPKT_S3_i_param_1];
	ld.param.u64 	%rd3, [_Z12compare_realIiEvPiPKT_S3_i_param_2];
	ld.param.u32 	%r2, [_Z12compare_realIiEvPiPKT_S3_i_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB24_2;

	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd2, %rd4;
	add.s64 	%rd6, %rd3, %rd4;
	ld.u32 	%r6, [%rd6];
	ld.u32 	%r7, [%rd5];
	setp.ne.s32	%p2, %r7, %r6;
	selp.u32	%r8, 1, 0, %p2;
	add.s64 	%rd7, %rd1, %rd4;
	st.u32 	[%rd7], %r8;

BB24_2:
	ret;
}

	// .globl	_Z12compare_realIfEvPiPKT_S3_i
.visible .func _Z12compare_realIfEvPiPKT_S3_i(
	.param .b64 _Z12compare_realIfEvPiPKT_S3_i_param_0,
	.param .b64 _Z12compare_realIfEvPiPKT_S3_i_param_1,
	.param .b64 _Z12compare_realIfEvPiPKT_S3_i_param_2,
	.param .b32 _Z12compare_realIfEvPiPKT_S3_i_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [_Z12compare_realIfEvPiPKT_S3_i_param_0];
	ld.param.u64 	%rd2, [_Z12compare_realIfEvPiPKT_S3_i_param_1];
	ld.param.u64 	%rd3, [_Z12compare_realIfEvPiPKT_S3_i_param_2];
	ld.param.u32 	%r2, [_Z12compare_realIfEvPiPKT_S3_i_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB25_2;

	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd2, %rd4;
	add.s64 	%rd6, %rd3, %rd4;
	ld.f32 	%f1, [%rd6];
	ld.f32 	%f2, [%rd5];
	sub.f32 	%f3, %f2, %f1;
	abs.f32 	%f4, %f3;
	cvt.f64.f32	%fd1, %f4;
	abs.f32 	%f5, %f2;
	abs.f32 	%f6, %f1;
	max.f32 	%f7, %f5, %f6;
	cvt.f64.f32	%fd2, %f7;
	mul.f64 	%fd3, %fd2, 0d3EE9000000000000;
	setp.geu.f64	%p2, %fd1, %fd3;
	selp.u32	%r6, 1, 0, %p2;
	add.s64 	%rd7, %rd1, %rd4;
	st.u32 	[%rd7], %r6;

BB25_2:
	ret;
}

	// .globl	_Z12compare_realIdEvPiPKT_S3_i
.visible .func _Z12compare_realIdEvPiPKT_S3_i(
	.param .b64 _Z12compare_realIdEvPiPKT_S3_i_param_0,
	.param .b64 _Z12compare_realIdEvPiPKT_S3_i_param_1,
	.param .b64 _Z12compare_realIdEvPiPKT_S3_i_param_2,
	.param .b32 _Z12compare_realIdEvPiPKT_S3_i_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z12compare_realIdEvPiPKT_S3_i_param_0];
	ld.param.u64 	%rd2, [_Z12compare_realIdEvPiPKT_S3_i_param_1];
	ld.param.u64 	%rd3, [_Z12compare_realIdEvPiPKT_S3_i_param_2];
	ld.param.u32 	%r2, [_Z12compare_realIdEvPiPKT_S3_i_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB26_2;

	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd2, %rd4;
	add.s64 	%rd6, %rd3, %rd4;
	ld.f64 	%fd1, [%rd6];
	ld.f64 	%fd2, [%rd5];
	sub.f64 	%fd3, %fd2, %fd1;
	abs.f64 	%fd4, %fd3;
	abs.f64 	%fd5, %fd2;
	abs.f64 	%fd6, %fd1;
	max.f64 	%fd7, %fd5, %fd6;
	mul.f64 	%fd8, %fd7, 0d3D83880000000000;
	setp.geu.f64	%p2, %fd4, %fd8;
	selp.u32	%r6, 1, 0, %p2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd1, %rd7;
	st.u32 	[%rd8], %r6;

BB26_2:
	ret;
}

	// .globl	_Z12compare_compI6float2EvPiPKT_S4_i
.visible .func _Z12compare_compI6float2EvPiPKT_S4_i(
	.param .b64 _Z12compare_compI6float2EvPiPKT_S4_i_param_0,
	.param .b64 _Z12compare_compI6float2EvPiPKT_S4_i_param_1,
	.param .b64 _Z12compare_compI6float2EvPiPKT_S4_i_param_2,
	.param .b32 _Z12compare_compI6float2EvPiPKT_S4_i_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [_Z12compare_compI6float2EvPiPKT_S4_i_param_0];
	ld.param.u64 	%rd3, [_Z12compare_compI6float2EvPiPKT_S4_i_param_1];
	ld.param.u64 	%rd4, [_Z12compare_compI6float2EvPiPKT_S4_i_param_2];
	ld.param.u32 	%r1, [_Z12compare_compI6float2EvPiPKT_S4_i_param_3];
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r5, %r3, %r4, %r2;
	cvt.s64.s32	%rd1, %r5;
	setp.ge.s32	%p3, %r5, %r1;
	@%p3 bra 	BB27_4;

	shl.b64 	%rd5, %rd1, 3;
	add.s64 	%rd6, %rd3, %rd5;
	ld.v2.f32 	{%f5, %f6}, [%rd6];
	add.s64 	%rd7, %rd4, %rd5;
	ld.v2.f32 	{%f7, %f8}, [%rd7];
	sub.f32 	%f9, %f5, %f7;
	abs.f32 	%f10, %f9;
	cvt.f64.f32	%fd1, %f10;
	abs.f32 	%f11, %f7;
	abs.f32 	%f12, %f5;
	max.f32 	%f13, %f12, %f11;
	cvt.f64.f32	%fd2, %f13;
	mul.f64 	%fd3, %fd2, 0d3EE9000000000000;
	mov.pred 	%p6, 0;
	setp.geu.f64	%p5, %fd1, %fd3;
	@%p5 bra 	BB27_3;

	sub.f32 	%f14, %f6, %f8;
	abs.f32 	%f15, %f14;
	cvt.f64.f32	%fd4, %f15;
	abs.f32 	%f16, %f8;
	abs.f32 	%f17, %f6;
	max.f32 	%f18, %f17, %f16;
	cvt.f64.f32	%fd5, %f18;
	mul.f64 	%fd6, %fd5, 0d3EE9000000000000;
	setp.lt.f64	%p6, %fd4, %fd6;

BB27_3:
	selp.u32	%r6, 1, 0, %p6;
	xor.b32  	%r7, %r6, 1;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd2, %rd8;
	st.u32 	[%rd9], %r7;

BB27_4:
	ret;
}

	// .globl	_Z12compare_compI7double2EvPiPKT_S4_i
.visible .func _Z12compare_compI7double2EvPiPKT_S4_i(
	.param .b64 _Z12compare_compI7double2EvPiPKT_S4_i_param_0,
	.param .b64 _Z12compare_compI7double2EvPiPKT_S4_i_param_1,
	.param .b64 _Z12compare_compI7double2EvPiPKT_S4_i_param_2,
	.param .b32 _Z12compare_compI7double2EvPiPKT_S4_i_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<21>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [_Z12compare_compI7double2EvPiPKT_S4_i_param_0];
	ld.param.u64 	%rd3, [_Z12compare_compI7double2EvPiPKT_S4_i_param_1];
	ld.param.u64 	%rd4, [_Z12compare_compI7double2EvPiPKT_S4_i_param_2];
	ld.param.u32 	%r1, [_Z12compare_compI7double2EvPiPKT_S4_i_param_3];
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r5, %r3, %r4, %r2;
	cvt.s64.s32	%rd1, %r5;
	setp.ge.s32	%p3, %r5, %r1;
	@%p3 bra 	BB28_4;

	shl.b64 	%rd5, %rd1, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.v2.f64 	{%fd5, %fd6}, [%rd6];
	add.s64 	%rd7, %rd4, %rd5;
	ld.v2.f64 	{%fd7, %fd8}, [%rd7];
	sub.f64 	%fd9, %fd5, %fd7;
	abs.f64 	%fd10, %fd9;
	abs.f64 	%fd11, %fd7;
	abs.f64 	%fd12, %fd5;
	max.f64 	%fd13, %fd12, %fd11;
	mul.f64 	%fd14, %fd13, 0d3D83880000000000;
	mov.pred 	%p6, 0;
	setp.geu.f64	%p5, %fd10, %fd14;
	@%p5 bra 	BB28_3;

	sub.f64 	%fd15, %fd6, %fd8;
	abs.f64 	%fd16, %fd15;
	abs.f64 	%fd17, %fd8;
	abs.f64 	%fd18, %fd6;
	max.f64 	%fd19, %fd18, %fd17;
	mul.f64 	%fd20, %fd19, 0d3D83880000000000;
	setp.lt.f64	%p6, %fd16, %fd20;

BB28_3:
	selp.u32	%r6, 1, 0, %p6;
	xor.b32  	%r7, %r6, 1;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd2, %rd8;
	st.u32 	[%rd9], %r7;

BB28_4:
	ret;
}

	// .globl	_Z12compare_quatI6float4EvPiPKT_S4_i
.visible .func _Z12compare_quatI6float4EvPiPKT_S4_i(
	.param .b64 _Z12compare_quatI6float4EvPiPKT_S4_i_param_0,
	.param .b64 _Z12compare_quatI6float4EvPiPKT_S4_i_param_1,
	.param .b64 _Z12compare_quatI6float4EvPiPKT_S4_i_param_2,
	.param .b32 _Z12compare_quatI6float4EvPiPKT_S4_i_param_3
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<13>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [_Z12compare_quatI6float4EvPiPKT_S4_i_param_0];
	ld.param.u64 	%rd3, [_Z12compare_quatI6float4EvPiPKT_S4_i_param_1];
	ld.param.u64 	%rd4, [_Z12compare_quatI6float4EvPiPKT_S4_i_param_2];
	ld.param.u32 	%r1, [_Z12compare_quatI6float4EvPiPKT_S4_i_param_3];
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r5, %r3, %r4, %r2;
	cvt.s64.s32	%rd1, %r5;
	setp.ge.s32	%p3, %r5, %r1;
	@%p3 bra 	BB29_6;

	shl.b64 	%rd5, %rd1, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd6];
	add.s64 	%rd7, %rd4, %rd5;
	ld.v4.f32 	{%f13, %f14, %f15, %f16}, [%rd7];
	sub.f32 	%f17, %f9, %f13;
	abs.f32 	%f18, %f17;
	cvt.f64.f32	%fd1, %f18;
	abs.f32 	%f19, %f13;
	abs.f32 	%f20, %f9;
	max.f32 	%f21, %f20, %f19;
	cvt.f64.f32	%fd2, %f21;
	mul.f64 	%fd3, %fd2, 0d3EE9000000000000;
	mov.pred 	%p10, 0;
	setp.geu.f64	%p5, %fd1, %fd3;
	@%p5 bra 	BB29_5;

	sub.f32 	%f22, %f10, %f14;
	abs.f32 	%f23, %f22;
	cvt.f64.f32	%fd4, %f23;
	abs.f32 	%f24, %f14;
	abs.f32 	%f25, %f10;
	max.f32 	%f26, %f25, %f24;
	cvt.f64.f32	%fd5, %f26;
	mul.f64 	%fd6, %fd5, 0d3EE9000000000000;
	setp.geu.f64	%p7, %fd4, %fd6;
	@%p7 bra 	BB29_5;

	sub.f32 	%f27, %f11, %f15;
	abs.f32 	%f28, %f27;
	cvt.f64.f32	%fd7, %f28;
	abs.f32 	%f29, %f15;
	abs.f32 	%f30, %f11;
	max.f32 	%f31, %f30, %f29;
	cvt.f64.f32	%fd8, %f31;
	mul.f64 	%fd9, %fd8, 0d3EE9000000000000;
	setp.geu.f64	%p9, %fd7, %fd9;
	@%p9 bra 	BB29_5;

	sub.f32 	%f32, %f12, %f16;
	abs.f32 	%f33, %f32;
	cvt.f64.f32	%fd10, %f33;
	abs.f32 	%f34, %f16;
	abs.f32 	%f35, %f12;
	max.f32 	%f36, %f35, %f34;
	cvt.f64.f32	%fd11, %f36;
	mul.f64 	%fd12, %fd11, 0d3EE9000000000000;
	setp.lt.f64	%p10, %fd10, %fd12;

BB29_5:
	selp.u32	%r6, 1, 0, %p10;
	xor.b32  	%r7, %r6, 1;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd2, %rd8;
	st.u32 	[%rd9], %r7;

BB29_6:
	ret;
}

	// .globl	_Z12compare_quatI7double4EvPiPKT_S4_i
.visible .func _Z12compare_quatI7double4EvPiPKT_S4_i(
	.param .b64 _Z12compare_quatI7double4EvPiPKT_S4_i_param_0,
	.param .b64 _Z12compare_quatI7double4EvPiPKT_S4_i_param_1,
	.param .b64 _Z12compare_quatI7double4EvPiPKT_S4_i_param_2,
	.param .b32 _Z12compare_quatI7double4EvPiPKT_S4_i_param_3
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [_Z12compare_quatI7double4EvPiPKT_S4_i_param_0];
	ld.param.u64 	%rd3, [_Z12compare_quatI7double4EvPiPKT_S4_i_param_1];
	ld.param.u64 	%rd4, [_Z12compare_quatI7double4EvPiPKT_S4_i_param_2];
	ld.param.u32 	%r1, [_Z12compare_quatI7double4EvPiPKT_S4_i_param_3];
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r5, %r3, %r4, %r2;
	cvt.s64.s32	%rd1, %r5;
	setp.ge.s32	%p3, %r5, %r1;
	@%p3 bra 	BB30_6;

	shl.b64 	%rd5, %rd1, 5;
	add.s64 	%rd6, %rd3, %rd5;
	ld.v2.f64 	{%fd9, %fd10}, [%rd6+16];
	ld.v2.f64 	{%fd11, %fd12}, [%rd6];
	add.s64 	%rd7, %rd4, %rd5;
	ld.v2.f64 	{%fd13, %fd14}, [%rd7+16];
	ld.v2.f64 	{%fd15, %fd16}, [%rd7];
	sub.f64 	%fd17, %fd11, %fd15;
	abs.f64 	%fd18, %fd17;
	abs.f64 	%fd19, %fd15;
	abs.f64 	%fd20, %fd11;
	max.f64 	%fd21, %fd20, %fd19;
	mul.f64 	%fd22, %fd21, 0d3D83880000000000;
	mov.pred 	%p10, 0;
	setp.geu.f64	%p5, %fd18, %fd22;
	@%p5 bra 	BB30_5;

	sub.f64 	%fd23, %fd12, %fd16;
	abs.f64 	%fd24, %fd23;
	abs.f64 	%fd25, %fd16;
	abs.f64 	%fd26, %fd12;
	max.f64 	%fd27, %fd26, %fd25;
	mul.f64 	%fd28, %fd27, 0d3D83880000000000;
	setp.geu.f64	%p7, %fd24, %fd28;
	@%p7 bra 	BB30_5;

	sub.f64 	%fd29, %fd9, %fd13;
	abs.f64 	%fd30, %fd29;
	abs.f64 	%fd31, %fd13;
	abs.f64 	%fd32, %fd9;
	max.f64 	%fd33, %fd32, %fd31;
	mul.f64 	%fd34, %fd33, 0d3D83880000000000;
	setp.geu.f64	%p9, %fd30, %fd34;
	@%p9 bra 	BB30_5;

	sub.f64 	%fd35, %fd10, %fd14;
	abs.f64 	%fd36, %fd35;
	abs.f64 	%fd37, %fd14;
	abs.f64 	%fd38, %fd10;
	max.f64 	%fd39, %fd38, %fd37;
	mul.f64 	%fd40, %fd39, 0d3D83880000000000;
	setp.lt.f64	%p10, %fd36, %fd40;

BB30_5:
	selp.u32	%r6, 1, 0, %p10;
	xor.b32  	%r7, %r6, 1;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd2, %rd8;
	st.u32 	[%rd9], %r7;

BB30_6:
	ret;
}

	// .globl	_Z5equivP7double2PK7double4ii
.visible .func _Z5equivP7double2PK7double4ii(
	.param .b64 _Z5equivP7double2PK7double4ii_param_0,
	.param .b64 _Z5equivP7double2PK7double4ii_param_1,
	.param .b32 _Z5equivP7double2PK7double4ii_param_2,
	.param .b32 _Z5equivP7double2PK7double4ii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<18>;
	.reg .f64 	%fd<11>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [_Z5equivP7double2PK7double4ii_param_0];
	ld.param.u64 	%rd2, [_Z5equivP7double2PK7double4ii_param_1];
	ld.param.u32 	%r3, [_Z5equivP7double2PK7double4ii_param_2];
	ld.param.u32 	%r4, [_Z5equivP7double2PK7double4ii_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB31_2;
	bra.uni 	BB31_1;

BB31_1:
	shl.b32 	%r11, %r3, 1;
	mad.lo.s32 	%r12, %r11, %r2, %r1;
	add.s32 	%r13, %r12, %r3;
	add.s32 	%r14, %r2, %r4;
	mad.lo.s32 	%r15, %r11, %r14, %r1;
	add.s32 	%r16, %r15, %r3;
	mad.lo.s32 	%r17, %r2, %r3, %r1;
	mul.wide.s32 	%rd3, %r17, 32;
	add.s64 	%rd4, %rd2, %rd3;
	ld.v2.f64 	{%fd1, %fd2}, [%rd4+16];
	ld.v2.f64 	{%fd3, %fd4}, [%rd4];
	mul.wide.s32 	%rd5, %r12, 16;
	add.s64 	%rd6, %rd1, %rd5;
	st.v2.f64 	[%rd6], {%fd3, %fd4};
	mul.wide.s32 	%rd7, %r13, 16;
	add.s64 	%rd8, %rd1, %rd7;
	neg.f64 	%fd8, %fd1;
	st.v2.f64 	[%rd8], {%fd8, %fd2};
	mul.wide.s32 	%rd9, %r15, 16;
	add.s64 	%rd10, %rd1, %rd9;
	st.v2.f64 	[%rd10], {%fd1, %fd2};
	mul.wide.s32 	%rd11, %r16, 16;
	add.s64 	%rd12, %rd1, %rd11;
	neg.f64 	%fd10, %fd4;
	st.v2.f64 	[%rd12], {%fd3, %fd10};

BB31_2:
	ret;
}

	// .globl	_Z5equivP6float2PK6float4ii
.visible .func _Z5equivP6float2PK6float4ii(
	.param .b64 _Z5equivP6float2PK6float4ii_param_0,
	.param .b64 _Z5equivP6float2PK6float4ii_param_1,
	.param .b32 _Z5equivP6float2PK6float4ii_param_2,
	.param .b32 _Z5equivP6float2PK6float4ii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [_Z5equivP6float2PK6float4ii_param_0];
	ld.param.u64 	%rd2, [_Z5equivP6float2PK6float4ii_param_1];
	ld.param.u32 	%r3, [_Z5equivP6float2PK6float4ii_param_2];
	ld.param.u32 	%r4, [_Z5equivP6float2PK6float4ii_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB32_2;
	bra.uni 	BB32_1;

BB32_1:
	shl.b32 	%r11, %r3, 1;
	mad.lo.s32 	%r12, %r11, %r2, %r1;
	add.s32 	%r13, %r12, %r3;
	add.s32 	%r14, %r2, %r4;
	mad.lo.s32 	%r15, %r11, %r14, %r1;
	add.s32 	%r16, %r15, %r3;
	mad.lo.s32 	%r17, %r2, %r3, %r1;
	mul.wide.s32 	%rd3, %r17, 16;
	add.s64 	%rd4, %rd2, %rd3;
	ld.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	mul.wide.s32 	%rd5, %r12, 8;
	add.s64 	%rd6, %rd1, %rd5;
	st.v2.f32 	[%rd6], {%f1, %f2};
	mul.wide.s32 	%rd7, %r13, 8;
	add.s64 	%rd8, %rd1, %rd7;
	neg.f32 	%f8, %f3;
	st.v2.f32 	[%rd8], {%f8, %f4};
	mul.wide.s32 	%rd9, %r15, 8;
	add.s64 	%rd10, %rd1, %rd9;
	st.v2.f32 	[%rd10], {%f3, %f4};
	mul.wide.s32 	%rd11, %r16, 8;
	add.s64 	%rd12, %rd1, %rd11;
	neg.f32 	%f10, %f2;
	st.v2.f32 	[%rd12], {%f1, %f10};

BB32_2:
	ret;
}

	// .globl	_Z5equivP7double4PK7double2ii
.visible .func _Z5equivP7double4PK7double2ii(
	.param .b64 _Z5equivP7double4PK7double2ii_param_0,
	.param .b64 _Z5equivP7double4PK7double2ii_param_1,
	.param .b32 _Z5equivP7double4PK7double2ii_param_2,
	.param .b32 _Z5equivP7double4PK7double2ii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z5equivP7double4PK7double2ii_param_0];
	ld.param.u64 	%rd2, [_Z5equivP7double4PK7double2ii_param_1];
	ld.param.u32 	%r3, [_Z5equivP7double4PK7double2ii_param_2];
	ld.param.u32 	%r4, [_Z5equivP7double4PK7double2ii_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB33_2;
	bra.uni 	BB33_1;

BB33_1:
	shl.b32 	%r11, %r3, 1;
	mad.lo.s32 	%r12, %r11, %r2, %r1;
	add.s32 	%r13, %r2, %r4;
	mad.lo.s32 	%r14, %r11, %r13, %r1;
	mad.lo.s32 	%r15, %r2, %r3, %r1;
	mul.wide.s32 	%rd3, %r12, 16;
	add.s64 	%rd4, %rd2, %rd3;
	mul.wide.s32 	%rd5, %r14, 16;
	add.s64 	%rd6, %rd2, %rd5;
	ld.v2.f64 	{%fd1, %fd2}, [%rd4];
	ld.v2.f64 	{%fd3, %fd4}, [%rd6];
	mul.wide.s32 	%rd7, %r15, 32;
	add.s64 	%rd8, %rd1, %rd7;
	st.v2.f64 	[%rd8], {%fd1, %fd2};
	st.v2.f64 	[%rd8+16], {%fd3, %fd4};

BB33_2:
	ret;
}

	// .globl	_Z5equivP6float4PK6float2ii
.visible .func _Z5equivP6float4PK6float2ii(
	.param .b64 _Z5equivP6float4PK6float2ii_param_0,
	.param .b64 _Z5equivP6float4PK6float2ii_param_1,
	.param .b32 _Z5equivP6float4PK6float2ii_param_2,
	.param .b32 _Z5equivP6float4PK6float2ii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z5equivP6float4PK6float2ii_param_0];
	ld.param.u64 	%rd2, [_Z5equivP6float4PK6float2ii_param_1];
	ld.param.u32 	%r3, [_Z5equivP6float4PK6float2ii_param_2];
	ld.param.u32 	%r4, [_Z5equivP6float4PK6float2ii_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB34_2;
	bra.uni 	BB34_1;

BB34_1:
	shl.b32 	%r11, %r3, 1;
	mad.lo.s32 	%r12, %r11, %r2, %r1;
	add.s32 	%r13, %r2, %r4;
	mad.lo.s32 	%r14, %r11, %r13, %r1;
	mad.lo.s32 	%r15, %r2, %r3, %r1;
	mul.wide.s32 	%rd3, %r12, 8;
	add.s64 	%rd4, %rd2, %rd3;
	mul.wide.s32 	%rd5, %r14, 8;
	add.s64 	%rd6, %rd2, %rd5;
	ld.v2.f32 	{%f1, %f2}, [%rd4];
	ld.v2.f32 	{%f3, %f4}, [%rd6];
	mul.wide.s32 	%rd7, %r15, 16;
	add.s64 	%rd8, %rd1, %rd7;
	st.v4.f32 	[%rd8], {%f1, %f2, %f3, %f4};

BB34_2:
	ret;
}

	// .globl	_Z13transpose_preP7double2ii
.visible .func _Z13transpose_preP7double2ii(
	.param .b64 _Z13transpose_preP7double2ii_param_0,
	.param .b32 _Z13transpose_preP7double2ii_param_1,
	.param .b32 _Z13transpose_preP7double2ii_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z13transpose_preP7double2ii_param_0];
	ld.param.u32 	%r4, [_Z13transpose_preP7double2ii_param_1];
	ld.param.u32 	%r5, [_Z13transpose_preP7double2ii_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r9, %r10, %r11;
	setp.lt.s32	%p1, %r1, %r4;
	setp.lt.s32	%p2, %r2, %r5;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB35_3;
	bra.uni 	BB35_1;

BB35_1:
	shr.s32 	%r12, %r4, 1;
	shr.s32 	%r13, %r5, 1;
	mad.lo.s32 	%r3, %r2, %r4, %r1;
	setp.lt.s32	%p4, %r2, %r13;
	setp.ge.s32	%p5, %r1, %r12;
	and.pred  	%p6, %p4, %p5;
	setp.ge.s32	%p7, %r2, %r13;
	setp.lt.s32	%p8, %r1, %r12;
	and.pred  	%p9, %p8, %p7;
	or.pred  	%p10, %p6, %p9;
	@!%p10 bra 	BB35_3;
	bra.uni 	BB35_2;

BB35_2:
	mul.wide.s32 	%rd2, %r3, 16;
	add.s64 	%rd3, %rd1, %rd2;
	ld.v2.f64 	{%fd1, %fd2}, [%rd3];
	neg.f64 	%fd5, %fd1;
	st.v2.f64 	[%rd3], {%fd5, %fd2};

BB35_3:
	ret;
}

	// .globl	_Z13transpose_preP6float2ii
.visible .func _Z13transpose_preP6float2ii(
	.param .b64 _Z13transpose_preP6float2ii_param_0,
	.param .b32 _Z13transpose_preP6float2ii_param_1,
	.param .b32 _Z13transpose_preP6float2ii_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z13transpose_preP6float2ii_param_0];
	ld.param.u32 	%r4, [_Z13transpose_preP6float2ii_param_1];
	ld.param.u32 	%r5, [_Z13transpose_preP6float2ii_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r6;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r9, %r10, %r11;
	setp.lt.s32	%p1, %r1, %r4;
	setp.lt.s32	%p2, %r2, %r5;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB36_3;
	bra.uni 	BB36_1;

BB36_1:
	shr.s32 	%r12, %r4, 1;
	shr.s32 	%r13, %r5, 1;
	mad.lo.s32 	%r3, %r2, %r4, %r1;
	setp.lt.s32	%p4, %r2, %r13;
	setp.ge.s32	%p5, %r1, %r12;
	and.pred  	%p6, %p4, %p5;
	setp.ge.s32	%p7, %r2, %r13;
	setp.lt.s32	%p8, %r1, %r12;
	and.pred  	%p9, %p8, %p7;
	or.pred  	%p10, %p6, %p9;
	@!%p10 bra 	BB36_3;
	bra.uni 	BB36_2;

BB36_2:
	mul.wide.s32 	%rd2, %r3, 8;
	add.s64 	%rd3, %rd1, %rd2;
	ld.v2.f32 	{%f1, %f2}, [%rd3];
	neg.f32 	%f5, %f1;
	st.v2.f32 	[%rd3], {%f5, %f2};

BB36_3:
	ret;
}

	// .globl	_Z9transposeI6float4EvPT_PKS1_ii
.visible .func _Z9transposeI6float4EvPT_PKS1_ii(
	.param .b64 _Z9transposeI6float4EvPT_PKS1_ii_param_0,
	.param .b64 _Z9transposeI6float4EvPT_PKS1_ii_param_1,
	.param .b32 _Z9transposeI6float4EvPT_PKS1_ii_param_2,
	.param .b32 _Z9transposeI6float4EvPT_PKS1_ii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [_Z9transposeI6float4EvPT_PKS1_ii_param_0];
	ld.param.u64 	%rd2, [_Z9transposeI6float4EvPT_PKS1_ii_param_1];
	ld.param.u32 	%r3, [_Z9transposeI6float4EvPT_PKS1_ii_param_2];
	ld.param.u32 	%r4, [_Z9transposeI6float4EvPT_PKS1_ii_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB37_2;
	bra.uni 	BB37_1;

BB37_1:
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	mul.wide.s32 	%rd3, %r11, 16;
	add.s64 	%rd4, %rd1, %rd3;
	mul.wide.s32 	%rd5, %r12, 16;
	add.s64 	%rd6, %rd2, %rd5;
	ld.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd6];
	st.v4.f32 	[%rd4], {%f1, %f2, %f3, %f4};

BB37_2:
	ret;
}

	// .globl	_Z9transposeI7double4EvPT_PKS1_ii
.visible .func _Z9transposeI7double4EvPT_PKS1_ii(
	.param .b64 _Z9transposeI7double4EvPT_PKS1_ii_param_0,
	.param .b64 _Z9transposeI7double4EvPT_PKS1_ii_param_1,
	.param .b32 _Z9transposeI7double4EvPT_PKS1_ii_param_2,
	.param .b32 _Z9transposeI7double4EvPT_PKS1_ii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [_Z9transposeI7double4EvPT_PKS1_ii_param_0];
	ld.param.u64 	%rd2, [_Z9transposeI7double4EvPT_PKS1_ii_param_1];
	ld.param.u32 	%r3, [_Z9transposeI7double4EvPT_PKS1_ii_param_2];
	ld.param.u32 	%r4, [_Z9transposeI7double4EvPT_PKS1_ii_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB38_2;
	bra.uni 	BB38_1;

BB38_1:
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	mul.wide.s32 	%rd3, %r11, 32;
	add.s64 	%rd4, %rd1, %rd3;
	mul.wide.s32 	%rd5, %r12, 32;
	add.s64 	%rd6, %rd2, %rd5;
	ld.v2.f64 	{%fd1, %fd2}, [%rd6];
	ld.v2.f64 	{%fd3, %fd4}, [%rd6+16];
	st.v2.f64 	[%rd4+16], {%fd3, %fd4};
	st.v2.f64 	[%rd4], {%fd1, %fd2};

BB38_2:
	ret;
}

	// .globl	_Z10ctransposeI6float4EvPT_PKS1_ii
.visible .func _Z10ctransposeI6float4EvPT_PKS1_ii(
	.param .b64 _Z10ctransposeI6float4EvPT_PKS1_ii_param_0,
	.param .b64 _Z10ctransposeI6float4EvPT_PKS1_ii_param_1,
	.param .b32 _Z10ctransposeI6float4EvPT_PKS1_ii_param_2,
	.param .b32 _Z10ctransposeI6float4EvPT_PKS1_ii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [_Z10ctransposeI6float4EvPT_PKS1_ii_param_0];
	ld.param.u64 	%rd2, [_Z10ctransposeI6float4EvPT_PKS1_ii_param_1];
	ld.param.u32 	%r3, [_Z10ctransposeI6float4EvPT_PKS1_ii_param_2];
	ld.param.u32 	%r4, [_Z10ctransposeI6float4EvPT_PKS1_ii_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB39_2;
	bra.uni 	BB39_1;

BB39_1:
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	mul.wide.s32 	%rd3, %r12, 16;
	add.s64 	%rd4, %rd2, %rd3;
	ld.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	mul.wide.s32 	%rd5, %r11, 16;
	add.s64 	%rd6, %rd1, %rd5;
	neg.f32 	%f9, %f4;
	neg.f32 	%f10, %f3;
	neg.f32 	%f11, %f2;
	st.v4.f32 	[%rd6], {%f1, %f11, %f10, %f9};

BB39_2:
	ret;
}

	// .globl	_Z10ctransposeI7double4EvPT_PKS1_ii
.visible .func _Z10ctransposeI7double4EvPT_PKS1_ii(
	.param .b64 _Z10ctransposeI7double4EvPT_PKS1_ii_param_0,
	.param .b64 _Z10ctransposeI7double4EvPT_PKS1_ii_param_1,
	.param .b32 _Z10ctransposeI7double4EvPT_PKS1_ii_param_2,
	.param .b32 _Z10ctransposeI7double4EvPT_PKS1_ii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [_Z10ctransposeI7double4EvPT_PKS1_ii_param_0];
	ld.param.u64 	%rd2, [_Z10ctransposeI7double4EvPT_PKS1_ii_param_1];
	ld.param.u32 	%r3, [_Z10ctransposeI7double4EvPT_PKS1_ii_param_2];
	ld.param.u32 	%r4, [_Z10ctransposeI7double4EvPT_PKS1_ii_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB40_2;
	bra.uni 	BB40_1;

BB40_1:
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	mul.wide.s32 	%rd3, %r12, 32;
	add.s64 	%rd4, %rd2, %rd3;
	ld.v2.f64 	{%fd1, %fd2}, [%rd4+16];
	ld.v2.f64 	{%fd5, %fd6}, [%rd4];
	mul.wide.s32 	%rd5, %r11, 32;
	add.s64 	%rd6, %rd1, %rd5;
	neg.f64 	%fd9, %fd6;
	st.v2.f64 	[%rd6], {%fd5, %fd9};
	neg.f64 	%fd10, %fd2;
	neg.f64 	%fd11, %fd1;
	st.v2.f64 	[%rd6+16], {%fd11, %fd10};

BB40_2:
	ret;
}

	// .globl	_Z12actfunc_compP6float2i
.visible .func _Z12actfunc_compP6float2i(
	.param .b64 _Z12actfunc_compP6float2i_param_0,
	.param .b32 _Z12actfunc_compP6float2i_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z12actfunc_compP6float2i_param_0];
	ld.param.u32 	%r2, [_Z12actfunc_compP6float2i_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB41_2;

	mul.wide.s32 	%rd2, %r1, 8;
	add.s64 	%rd3, %rd1, %rd2;
	ld.v2.f32 	{%f1, %f2}, [%rd3];
	setp.ltu.f32	%p2, %f1, 0f00000000;
	setp.ltu.f32	%p3, %f2, 0f00000000;
	selp.f32	%f5, 0fBF800000, 0f3F800000, %p3;
	selp.f32	%f6, 0fBF800000, 0f3F800000, %p2;
	st.v2.f32 	[%rd3], {%f6, %f5};

BB41_2:
	ret;
}

	// .globl	_Z12actfunc_compP7double2i
.visible .func _Z12actfunc_compP7double2i(
	.param .b64 _Z12actfunc_compP7double2i_param_0,
	.param .b32 _Z12actfunc_compP7double2i_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z12actfunc_compP7double2i_param_0];
	ld.param.u32 	%r2, [_Z12actfunc_compP7double2i_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB42_2;

	mul.wide.s32 	%rd2, %r1, 16;
	add.s64 	%rd3, %rd1, %rd2;
	ld.v2.f64 	{%fd1, %fd2}, [%rd3];
	setp.ltu.f64	%p2, %fd1, 0d0000000000000000;
	setp.ltu.f64	%p3, %fd2, 0d0000000000000000;
	selp.f64	%fd5, 0dBFF0000000000000, 0d3FF0000000000000, %p3;
	selp.f64	%fd6, 0dBFF0000000000000, 0d3FF0000000000000, %p2;
	st.v2.f64 	[%rd3], {%fd6, %fd5};

BB42_2:
	ret;
}

	// .globl	_Z12actfunc_quatP6float4i
.visible .func _Z12actfunc_quatP6float4i(
	.param .b64 _Z12actfunc_quatP6float4i_param_0,
	.param .b32 _Z12actfunc_quatP6float4i_param_1
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z12actfunc_quatP6float4i_param_0];
	ld.param.u32 	%r2, [_Z12actfunc_quatP6float4i_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB43_2;

	mul.wide.s32 	%rd2, %r1, 16;
	add.s64 	%rd3, %rd1, %rd2;
	ld.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd3];
	setp.ltu.f32	%p2, %f1, 0f00000000;
	setp.ltu.f32	%p3, %f2, 0f00000000;
	setp.ltu.f32	%p4, %f3, 0f00000000;
	setp.ltu.f32	%p5, %f4, 0f00000000;
	selp.f32	%f9, 0fBF800000, 0f3F800000, %p5;
	selp.f32	%f10, 0fBF800000, 0f3F800000, %p4;
	selp.f32	%f11, 0fBF800000, 0f3F800000, %p3;
	selp.f32	%f12, 0fBF800000, 0f3F800000, %p2;
	st.v4.f32 	[%rd3], {%f12, %f11, %f10, %f9};

BB43_2:
	ret;
}

	// .globl	_Z12actfunc_quatP7double4i
.visible .func _Z12actfunc_quatP7double4i(
	.param .b64 _Z12actfunc_quatP7double4i_param_0,
	.param .b32 _Z12actfunc_quatP7double4i_param_1
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<13>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z12actfunc_quatP7double4i_param_0];
	ld.param.u32 	%r2, [_Z12actfunc_quatP7double4i_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB44_2;

	mul.wide.s32 	%rd2, %r1, 32;
	add.s64 	%rd3, %rd1, %rd2;
	ld.v2.f64 	{%fd1, %fd2}, [%rd3];
	setp.ltu.f64	%p2, %fd1, 0d0000000000000000;
	setp.ltu.f64	%p3, %fd2, 0d0000000000000000;
	ld.v2.f64 	{%fd5, %fd6}, [%rd3+16];
	setp.ltu.f64	%p4, %fd5, 0d0000000000000000;
	setp.ltu.f64	%p5, %fd6, 0d0000000000000000;
	selp.f64	%fd9, 0dBFF0000000000000, 0d3FF0000000000000, %p3;
	selp.f64	%fd10, 0dBFF0000000000000, 0d3FF0000000000000, %p2;
	st.v2.f64 	[%rd3], {%fd10, %fd9};
	selp.f64	%fd11, 0dBFF0000000000000, 0d3FF0000000000000, %p5;
	selp.f64	%fd12, 0dBFF0000000000000, 0d3FF0000000000000, %p4;
	st.v2.f64 	[%rd3+16], {%fd12, %fd11};

BB44_2:
	ret;
}

	// .globl	_Z18actfunc_comp_multiP6float2ii
.visible .func _Z18actfunc_comp_multiP6float2ii(
	.param .b64 _Z18actfunc_comp_multiP6float2ii_param_0,
	.param .b32 _Z18actfunc_comp_multiP6float2ii_param_1,
	.param .b32 _Z18actfunc_comp_multiP6float2ii_param_2
)
{
	.local .align 4 .b8 	__local_depot45[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<37>;
	.reg .f32 	%f<136>;
	.reg .b32 	%r<202>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<27>;


	mov.u64 	%rd26, __local_depot45;
	cvta.local.u64 	%SP, %rd26;
	ld.param.u64 	%rd14, [_Z18actfunc_comp_multiP6float2ii_param_0];
	ld.param.u32 	%r77, [_Z18actfunc_comp_multiP6float2ii_param_1];
	ld.param.u32 	%r78, [_Z18actfunc_comp_multiP6float2ii_param_2];
	add.u64 	%rd15, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd15;
	mov.u32 	%r79, %ntid.x;
	mov.u32 	%r80, %ctaid.x;
	mov.u32 	%r81, %tid.x;
	mad.lo.s32 	%r1, %r79, %r80, %r81;
	setp.ge.s32	%p1, %r1, %r78;
	@%p1 bra 	BB45_51;

	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd2, %rd14, %rd16;
	ld.v2.f32 	{%f43, %f44}, [%rd2];
	abs.f32 	%f1, %f43;
	abs.f32 	%f2, %f44;
	setp.eq.f32	%p2, %f1, 0f00000000;
	setp.eq.f32	%p3, %f2, 0f00000000;
	and.pred  	%p4, %p2, %p3;
	mov.b32 	 %r2, %f43;
	mov.b32 	 %r82, %f44;
	and.b32  	%r3, %r82, -2147483648;
	@%p4 bra 	BB45_5;
	bra.uni 	BB45_2;

BB45_5:
	shr.s32 	%r89, %r2, 31;
	and.b32  	%r90, %r89, 1078530011;
	or.b32  	%r91, %r90, %r3;
	mov.b32 	 %f124, %r91;
	bra.uni 	BB45_6;

BB45_2:
	setp.eq.f32	%p5, %f1, 0f7F800000;
	setp.eq.f32	%p6, %f2, 0f7F800000;
	and.pred  	%p7, %p5, %p6;
	@%p7 bra 	BB45_4;
	bra.uni 	BB45_3;

BB45_4:
	shr.s32 	%r85, %r2, 31;
	and.b32  	%r86, %r85, 13483017;
	add.s32 	%r87, %r86, 1061752795;
	or.b32  	%r88, %r87, %r3;
	mov.b32 	 %f124, %r88;
	bra.uni 	BB45_6;

BB45_3:
	max.f32 	%f47, %f2, %f1;
	min.f32 	%f48, %f2, %f1;
	div.rn.f32 	%f49, %f48, %f47;
	mul.rn.f32 	%f50, %f49, %f49;
	mov.f32 	%f51, 0fC0B59883;
	mov.f32 	%f52, 0fBF52C7EA;
	fma.rn.f32 	%f53, %f50, %f52, %f51;
	mov.f32 	%f54, 0fC0D21907;
	fma.rn.f32 	%f55, %f53, %f50, %f54;
	mul.f32 	%f56, %f50, %f55;
	mul.f32 	%f57, %f49, %f56;
	add.f32 	%f58, %f50, 0f41355DC0;
	mov.f32 	%f59, 0f41E6BD60;
	fma.rn.f32 	%f60, %f58, %f50, %f59;
	mov.f32 	%f61, 0f419D92C8;
	fma.rn.f32 	%f62, %f60, %f50, %f61;
	rcp.rn.f32 	%f63, %f62;
	fma.rn.f32 	%f64, %f57, %f63, %f49;
	mov.f32 	%f65, 0f3FC90FDB;
	sub.f32 	%f66, %f65, %f64;
	setp.gt.f32	%p8, %f2, %f1;
	selp.f32	%f67, %f66, %f64, %p8;
	mov.f32 	%f68, 0f40490FDB;
	sub.f32 	%f69, %f68, %f67;
	setp.lt.s32	%p9, %r2, 0;
	selp.f32	%f70, %f69, %f67, %p9;
	mov.b32 	 %r83, %f70;
	or.b32  	%r84, %r83, %r3;
	mov.b32 	 %f71, %r84;
	add.f32 	%f72, %f1, %f2;
	setp.gtu.f32	%p10, %f72, 0f7F800000;
	selp.f32	%f124, %f72, %f71, %p10;

BB45_6:
	cvt.rn.f64.s32	%fd1, %r77;
	mov.f64 	%fd2, 0d400921FB54442D18;
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvt.rn.f32.f64	%f73, %fd3;
	add.f32 	%f74, %f73, %f73;
	add.f32 	%f75, %f73, %f124;
	div.rn.f32 	%f76, %f75, %f74;
	cvt.rmi.f32.f32	%f77, %f76;
	mul.f32 	%f7, %f74, %f77;
	abs.f32 	%f8, %f7;
	setp.neu.f32	%p11, %f8, 0f7F800000;
	mov.f32 	%f131, %f7;
	@%p11 bra 	BB45_8;

	mov.f32 	%f78, 0f00000000;
	mul.rn.f32 	%f9, %f7, %f78;
	mov.f32 	%f131, %f9;

BB45_8:
	mov.f32 	%f10, %f131;
	mul.f32 	%f79, %f10, 0f3F22F983;
	cvt.rni.s32.f32	%r191, %f79;
	cvt.rn.f32.s32	%f80, %r191;
	neg.f32 	%f81, %f80;
	mov.f32 	%f82, 0f3FC90FDA;
	fma.rn.f32 	%f83, %f81, %f82, %f10;
	mov.f32 	%f84, 0f33A22168;
	fma.rn.f32 	%f85, %f81, %f84, %f83;
	mov.f32 	%f86, 0f27C234C5;
	fma.rn.f32 	%f125, %f81, %f86, %f85;
	abs.f32 	%f87, %f10;
	add.s64 	%rd3, %rd1, 24;
	setp.leu.f32	%p12, %f87, 0f47CE4780;
	@%p12 bra 	BB45_18;

	mov.b32 	 %r5, %f10;
	shr.u32 	%r6, %r5, 23;
	bfe.u32 	%r94, %r5, 23, 8;
	add.s32 	%r95, %r94, -128;
	shl.b32 	%r96, %r5, 8;
	or.b32  	%r7, %r96, -2147483648;
	shr.u32 	%r8, %r95, 5;
	mov.u32 	%r183, 0;
	mov.u64 	%rd21, __cudart_i2opi_f;
	mov.u32 	%r182, -6;
	mov.u64 	%rd25, %rd1;

BB45_10:
	.pragma "nounroll";
	mov.u64 	%rd5, %rd25;
	ld.const.u32 	%r99, [%rd21];
	// inline asm
	{
	mad.lo.cc.u32   %r97, %r99, %r7, %r183;
	madc.hi.u32     %r183, %r99, %r7,  0;
	}
	// inline asm
	st.local.u32 	[%rd5], %r97;
	add.s64 	%rd6, %rd5, 4;
	add.s64 	%rd21, %rd21, 4;
	add.s32 	%r182, %r182, 1;
	setp.ne.s32	%p13, %r182, 0;
	mov.u64 	%rd25, %rd6;
	@%p13 bra 	BB45_10;

	and.b32  	%r13, %r5, -2147483648;
	st.local.u32 	[%rd3], %r183;
	mov.u32 	%r102, 6;
	sub.s32 	%r103, %r102, %r8;
	mul.wide.s32 	%rd18, %r103, 4;
	add.s64 	%rd8, %rd1, %rd18;
	ld.local.u32 	%r184, [%rd8];
	ld.local.u32 	%r185, [%rd8+-4];
	and.b32  	%r16, %r6, 31;
	setp.eq.s32	%p14, %r16, 0;
	@%p14 bra 	BB45_13;

	mov.u32 	%r104, 32;
	sub.s32 	%r105, %r104, %r16;
	shr.u32 	%r106, %r185, %r105;
	shl.b32 	%r107, %r184, %r16;
	add.s32 	%r184, %r106, %r107;
	ld.local.u32 	%r108, [%rd8+-8];
	shr.u32 	%r109, %r108, %r105;
	shl.b32 	%r110, %r185, %r16;
	add.s32 	%r185, %r109, %r110;

BB45_13:
	shr.u32 	%r111, %r185, 30;
	shl.b32 	%r112, %r184, 2;
	add.s32 	%r186, %r111, %r112;
	shl.b32 	%r22, %r185, 2;
	shr.u32 	%r113, %r186, 31;
	shr.u32 	%r114, %r184, 30;
	add.s32 	%r23, %r113, %r114;
	setp.eq.s32	%p15, %r113, 0;
	mov.u32 	%r187, %r13;
	mov.u32 	%r188, %r22;
	@%p15 bra 	BB45_15;

	not.b32 	%r115, %r186;
	neg.s32 	%r24, %r22;
	setp.eq.s32	%p16, %r22, 0;
	selp.u32	%r116, 1, 0, %p16;
	add.s32 	%r186, %r116, %r115;
	xor.b32  	%r26, %r13, -2147483648;
	mov.u32 	%r187, %r26;
	mov.u32 	%r188, %r24;

BB45_15:
	mov.u32 	%r28, %r187;
	neg.s32 	%r117, %r23;
	setp.eq.s32	%p17, %r13, 0;
	selp.b32	%r191, %r23, %r117, %p17;
	clz.b32 	%r190, %r186;
	setp.eq.s32	%p18, %r190, 0;
	shl.b32 	%r118, %r186, %r190;
	mov.u32 	%r119, 32;
	sub.s32 	%r120, %r119, %r190;
	shr.u32 	%r121, %r188, %r120;
	add.s32 	%r122, %r121, %r118;
	selp.b32	%r32, %r186, %r122, %p18;
	mov.u32 	%r123, -921707870;
	mul.hi.u32 	%r189, %r32, %r123;
	setp.lt.s32	%p19, %r189, 1;
	@%p19 bra 	BB45_17;

	mul.lo.s32 	%r124, %r32, -921707870;
	shr.u32 	%r125, %r124, 31;
	shl.b32 	%r126, %r189, 1;
	add.s32 	%r189, %r125, %r126;
	add.s32 	%r190, %r190, 1;

BB45_17:
	mov.u32 	%r127, 126;
	sub.s32 	%r128, %r127, %r190;
	shl.b32 	%r129, %r128, 23;
	add.s32 	%r130, %r189, 1;
	shr.u32 	%r131, %r130, 7;
	add.s32 	%r132, %r131, 1;
	shr.u32 	%r133, %r132, 1;
	add.s32 	%r134, %r133, %r129;
	or.b32  	%r135, %r134, %r28;
	mov.b32 	 %f125, %r135;

BB45_18:
	mul.rn.f32 	%f14, %f125, %f125;
	add.s32 	%r39, %r191, 1;
	and.b32  	%r40, %r39, 1;
	setp.eq.s32	%p20, %r40, 0;
	@%p20 bra 	BB45_20;

	mov.f32 	%f88, 0fBAB6061A;
	mov.f32 	%f89, 0f37CCF5CE;
	fma.rn.f32 	%f126, %f89, %f14, %f88;
	bra.uni 	BB45_21;

BB45_20:
	mov.f32 	%f90, 0f3C08839E;
	mov.f32 	%f91, 0fB94CA1F9;
	fma.rn.f32 	%f126, %f91, %f14, %f90;

BB45_21:
	@%p20 bra 	BB45_23;

	mov.f32 	%f92, 0f3D2AAAA5;
	fma.rn.f32 	%f93, %f126, %f14, %f92;
	mov.f32 	%f94, 0fBF000000;
	fma.rn.f32 	%f127, %f93, %f14, %f94;
	bra.uni 	BB45_24;

BB45_23:
	mov.f32 	%f95, 0fBE2AAAA3;
	fma.rn.f32 	%f96, %f126, %f14, %f95;
	mov.f32 	%f97, 0f00000000;
	fma.rn.f32 	%f127, %f96, %f14, %f97;

BB45_24:
	fma.rn.f32 	%f128, %f127, %f125, %f125;
	@%p20 bra 	BB45_26;

	mov.f32 	%f98, 0f3F800000;
	fma.rn.f32 	%f128, %f127, %f14, %f98;

BB45_26:
	and.b32  	%r136, %r39, 2;
	setp.eq.s32	%p23, %r136, 0;
	@%p23 bra 	BB45_28;

	mov.f32 	%f99, 0f00000000;
	mov.f32 	%f100, 0fBF800000;
	fma.rn.f32 	%f128, %f128, %f100, %f99;

BB45_28:
	mov.f32 	%f130, %f7;
	@%p11 bra 	BB45_30;

	mov.f32 	%f101, 0f00000000;
	mul.rn.f32 	%f130, %f7, %f101;

BB45_30:
	mul.f32 	%f102, %f130, 0f3F22F983;
	cvt.rni.s32.f32	%r201, %f102;
	cvt.rn.f32.s32	%f103, %r201;
	neg.f32 	%f104, %f103;
	fma.rn.f32 	%f106, %f104, %f82, %f130;
	fma.rn.f32 	%f108, %f104, %f84, %f106;
	fma.rn.f32 	%f132, %f104, %f86, %f108;
	abs.f32 	%f110, %f130;
	setp.leu.f32	%p25, %f110, 0f47CE4780;
	@%p25 bra 	BB45_40;

	mov.b32 	 %r42, %f130;
	shr.u32 	%r43, %r42, 23;
	bfe.u32 	%r139, %r42, 23, 8;
	add.s32 	%r140, %r139, -128;
	shl.b32 	%r141, %r42, 8;
	or.b32  	%r44, %r141, -2147483648;
	shr.u32 	%r45, %r140, 5;
	mov.u32 	%r193, 0;
	mov.u64 	%rd22, __cudart_i2opi_f;
	mov.u32 	%r192, -6;
	mov.u64 	%rd24, %rd1;

BB45_32:
	.pragma "nounroll";
	ld.const.u32 	%r144, [%rd22];
	// inline asm
	{
	mad.lo.cc.u32   %r142, %r144, %r44, %r193;
	madc.hi.u32     %r193, %r144, %r44,  0;
	}
	// inline asm
	st.local.u32 	[%rd24], %r142;
	add.s64 	%rd24, %rd24, 4;
	add.s64 	%rd22, %rd22, 4;
	add.s32 	%r192, %r192, 1;
	setp.ne.s32	%p26, %r192, 0;
	@%p26 bra 	BB45_32;

	and.b32  	%r50, %r42, -2147483648;
	st.local.u32 	[%rd3], %r193;
	mov.u32 	%r147, 6;
	sub.s32 	%r148, %r147, %r45;
	mul.wide.s32 	%rd20, %r148, 4;
	add.s64 	%rd13, %rd1, %rd20;
	ld.local.u32 	%r194, [%rd13];
	ld.local.u32 	%r195, [%rd13+-4];
	and.b32  	%r53, %r43, 31;
	setp.eq.s32	%p27, %r53, 0;
	@%p27 bra 	BB45_35;

	mov.u32 	%r149, 32;
	sub.s32 	%r150, %r149, %r53;
	shr.u32 	%r151, %r195, %r150;
	shl.b32 	%r152, %r194, %r53;
	add.s32 	%r194, %r151, %r152;
	ld.local.u32 	%r153, [%rd13+-8];
	shr.u32 	%r154, %r153, %r150;
	shl.b32 	%r155, %r195, %r53;
	add.s32 	%r195, %r154, %r155;

BB45_35:
	shr.u32 	%r156, %r195, 30;
	shl.b32 	%r157, %r194, 2;
	add.s32 	%r196, %r156, %r157;
	shl.b32 	%r59, %r195, 2;
	shr.u32 	%r158, %r196, 31;
	shr.u32 	%r159, %r194, 30;
	add.s32 	%r60, %r158, %r159;
	setp.eq.s32	%p28, %r158, 0;
	mov.u32 	%r197, %r50;
	mov.u32 	%r198, %r59;
	@%p28 bra 	BB45_37;

	not.b32 	%r160, %r196;
	neg.s32 	%r61, %r59;
	setp.eq.s32	%p29, %r59, 0;
	selp.u32	%r161, 1, 0, %p29;
	add.s32 	%r196, %r161, %r160;
	xor.b32  	%r63, %r50, -2147483648;
	mov.u32 	%r197, %r63;
	mov.u32 	%r198, %r61;

BB45_37:
	mov.u32 	%r65, %r197;
	neg.s32 	%r162, %r60;
	setp.eq.s32	%p30, %r50, 0;
	selp.b32	%r201, %r60, %r162, %p30;
	clz.b32 	%r200, %r196;
	setp.eq.s32	%p31, %r200, 0;
	shl.b32 	%r163, %r196, %r200;
	mov.u32 	%r164, 32;
	sub.s32 	%r165, %r164, %r200;
	shr.u32 	%r166, %r198, %r165;
	add.s32 	%r167, %r166, %r163;
	selp.b32	%r69, %r196, %r167, %p31;
	mov.u32 	%r168, -921707870;
	mul.hi.u32 	%r199, %r69, %r168;
	setp.lt.s32	%p32, %r199, 1;
	@%p32 bra 	BB45_39;

	mul.lo.s32 	%r169, %r69, -921707870;
	shr.u32 	%r170, %r169, 31;
	shl.b32 	%r171, %r199, 1;
	add.s32 	%r199, %r170, %r171;
	add.s32 	%r200, %r200, 1;

BB45_39:
	mov.u32 	%r172, 126;
	sub.s32 	%r173, %r172, %r200;
	shl.b32 	%r174, %r173, 23;
	add.s32 	%r175, %r199, 1;
	shr.u32 	%r176, %r175, 7;
	add.s32 	%r177, %r176, 1;
	shr.u32 	%r178, %r177, 1;
	add.s32 	%r179, %r178, %r174;
	or.b32  	%r180, %r179, %r65;
	mov.b32 	 %f132, %r180;

BB45_40:
	mul.rn.f32 	%f31, %f132, %f132;
	and.b32  	%r76, %r201, 1;
	setp.eq.s32	%p33, %r76, 0;
	@%p33 bra 	BB45_42;

	mov.f32 	%f111, 0fBAB6061A;
	mov.f32 	%f112, 0f37CCF5CE;
	fma.rn.f32 	%f133, %f112, %f31, %f111;
	bra.uni 	BB45_43;

BB45_42:
	mov.f32 	%f113, 0f3C08839E;
	mov.f32 	%f114, 0fB94CA1F9;
	fma.rn.f32 	%f133, %f114, %f31, %f113;

BB45_43:
	@%p33 bra 	BB45_45;

	mov.f32 	%f115, 0f3D2AAAA5;
	fma.rn.f32 	%f116, %f133, %f31, %f115;
	mov.f32 	%f117, 0fBF000000;
	fma.rn.f32 	%f134, %f116, %f31, %f117;
	bra.uni 	BB45_46;

BB45_45:
	mov.f32 	%f118, 0fBE2AAAA3;
	fma.rn.f32 	%f119, %f133, %f31, %f118;
	mov.f32 	%f120, 0f00000000;
	fma.rn.f32 	%f134, %f119, %f31, %f120;

BB45_46:
	fma.rn.f32 	%f135, %f134, %f132, %f132;
	@%p33 bra 	BB45_48;

	mov.f32 	%f121, 0f3F800000;
	fma.rn.f32 	%f135, %f134, %f31, %f121;

BB45_48:
	and.b32  	%r181, %r201, 2;
	setp.eq.s32	%p36, %r181, 0;
	@%p36 bra 	BB45_50;

	mov.f32 	%f122, 0f00000000;
	mov.f32 	%f123, 0fBF800000;
	fma.rn.f32 	%f135, %f135, %f123, %f122;

BB45_50:
	st.v2.f32 	[%rd2], {%f128, %f135};

BB45_51:
	ret;
}

	// .globl	_Z18actfunc_comp_multiP7double2ii
.visible .func _Z18actfunc_comp_multiP7double2ii(
	.param .b64 _Z18actfunc_comp_multiP7double2ii_param_0,
	.param .b32 _Z18actfunc_comp_multiP7double2ii_param_1,
	.param .b32 _Z18actfunc_comp_multiP7double2ii_param_2
)
{
	.local .align 4 .b8 	__local_depot46[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<23>;
	.reg .b32 	%r<49>;
	.reg .f64 	%fd<182>;
	.reg .b64 	%rd<16>;


	mov.u64 	%rd15, __local_depot46;
	cvta.local.u64 	%SP, %rd15;
	ld.param.u64 	%rd4, [_Z18actfunc_comp_multiP7double2ii_param_0];
	ld.param.u32 	%r11, [_Z18actfunc_comp_multiP7double2ii_param_2];
	add.u64 	%rd5, %SP, 4;
	cvta.to.local.u64 	%rd1, %rd5;
	add.u64 	%rd6, %SP, 0;
	cvta.to.local.u64 	%rd2, %rd6;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r14;
	setp.ge.s32	%p1, %r1, %r11;
	@%p1 bra 	BB46_19;

	mul.wide.s32 	%rd7, %r1, 16;
	add.s64 	%rd3, %rd4, %rd7;
	ld.v2.f64 	{%fd25, %fd26}, [%rd3];
	abs.f64 	%fd1, %fd25;
	abs.f64 	%fd2, %fd26;
	setp.eq.f64	%p2, %fd1, 0d0000000000000000;
	setp.eq.f64	%p3, %fd2, 0d0000000000000000;
	and.pred  	%p4, %p2, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd25;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd26;
	}
	and.b32  	%r3, %r15, -2147483648;
	@%p4 bra 	BB46_5;
	bra.uni 	BB46_2;

BB46_5:
	setp.lt.s32	%p12, %r2, 0;
	selp.f64	%fd81, 0d400921FB54442D18, 0d0000000000000000, %p12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd81;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd81;
	}
	or.b32  	%r24, %r23, %r3;
	mov.b64 	%fd174, {%r22, %r24};
	bra.uni 	BB46_6;

BB46_2:
	setp.eq.f64	%p5, %fd1, 0d7FF0000000000000;
	setp.eq.f64	%p6, %fd2, 0d7FF0000000000000;
	and.pred  	%p7, %p5, %p6;
	@%p7 bra 	BB46_4;
	bra.uni 	BB46_3;

BB46_4:
	setp.lt.s32	%p11, %r2, 0;
	selp.f64	%fd80, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd80;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd80;
	}
	or.b32  	%r21, %r20, %r3;
	mov.b64 	%fd174, {%r19, %r21};
	bra.uni 	BB46_6;

BB46_3:
	setp.lt.s32	%p8, %r2, 0;
	min.f64 	%fd29, %fd2, %fd1;
	max.f64 	%fd30, %fd2, %fd1;
	div.rn.f64 	%fd31, %fd29, %fd30;
	mul.f64 	%fd32, %fd31, %fd31;
	mov.f64 	%fd33, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd34, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd35, %fd34, %fd32, %fd33;
	mov.f64 	%fd36, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd37, %fd35, %fd32, %fd36;
	mov.f64 	%fd38, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd39, %fd37, %fd32, %fd38;
	mov.f64 	%fd40, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd41, %fd39, %fd32, %fd40;
	mov.f64 	%fd42, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd43, %fd41, %fd32, %fd42;
	mov.f64 	%fd44, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd45, %fd43, %fd32, %fd44;
	mov.f64 	%fd46, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd47, %fd45, %fd32, %fd46;
	mov.f64 	%fd48, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd49, %fd47, %fd32, %fd48;
	mov.f64 	%fd50, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd51, %fd49, %fd32, %fd50;
	mov.f64 	%fd52, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd53, %fd51, %fd32, %fd52;
	mov.f64 	%fd54, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd55, %fd53, %fd32, %fd54;
	mov.f64 	%fd56, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd57, %fd55, %fd32, %fd56;
	mov.f64 	%fd58, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd59, %fd57, %fd32, %fd58;
	mov.f64 	%fd60, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd61, %fd59, %fd32, %fd60;
	mov.f64 	%fd62, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd63, %fd61, %fd32, %fd62;
	mov.f64 	%fd64, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd65, %fd63, %fd32, %fd64;
	mov.f64 	%fd66, 0d3FC99999999840D2;
	fma.rn.f64 	%fd67, %fd65, %fd32, %fd66;
	mov.f64 	%fd68, 0dBFD555555555544C;
	fma.rn.f64 	%fd69, %fd67, %fd32, %fd68;
	mul.f64 	%fd70, %fd32, %fd69;
	fma.rn.f64 	%fd71, %fd70, %fd31, %fd31;
	mov.f64 	%fd72, 0d3FF921FB54442D18;
	sub.f64 	%fd73, %fd72, %fd71;
	setp.gt.f64	%p9, %fd2, %fd1;
	selp.f64	%fd74, %fd73, %fd71, %p9;
	mov.f64 	%fd75, 0d400921FB54442D18;
	sub.f64 	%fd76, %fd75, %fd74;
	selp.f64	%fd77, %fd76, %fd74, %p8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd77;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd77;
	}
	or.b32  	%r18, %r17, %r3;
	mov.b64 	%fd78, {%r16, %r18};
	add.f64 	%fd79, %fd1, %fd2;
	setp.gtu.f64	%p10, %fd79, 0d7FF0000000000000;
	selp.f64	%fd174, %fd79, %fd78, %p10;

BB46_6:
	ld.param.u32 	%r42, [_Z18actfunc_comp_multiP7double2ii_param_1];
	cvt.rn.f64.s32	%fd82, %r42;
	mov.f64 	%fd83, 0d400921FB54442D18;
	div.rn.f64 	%fd84, %fd83, %fd82;
	add.f64 	%fd85, %fd84, %fd84;
	add.f64 	%fd86, %fd84, %fd174;
	div.rn.f64 	%fd87, %fd86, %fd85;
	cvt.rmi.f64.f64	%fd88, %fd87;
	mul.f64 	%fd7, %fd85, %fd88;
	abs.f64 	%fd8, %fd7;
	setp.neu.f64	%p13, %fd8, 0d7FF0000000000000;
	mov.f64 	%fd179, %fd7;
	@%p13 bra 	BB46_8;

	mov.f64 	%fd89, 0d0000000000000000;
	mul.rn.f64 	%fd9, %fd7, %fd89;
	mov.f64 	%fd179, %fd9;

BB46_8:
	mov.f64 	%fd10, %fd179;
	mul.f64 	%fd90, %fd10, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r47, %fd90;
	st.local.u32 	[%rd2], %r47;
	cvt.rn.f64.s32	%fd91, %r47;
	neg.f64 	%fd92, %fd91;
	mov.f64 	%fd93, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd94, %fd92, %fd93, %fd10;
	mov.f64 	%fd95, 0d3C91A62633145C00;
	fma.rn.f64 	%fd96, %fd92, %fd95, %fd94;
	mov.f64 	%fd97, 0d397B839A252049C0;
	fma.rn.f64 	%fd175, %fd92, %fd97, %fd96;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd10;
	}
	and.b32  	%r26, %r25, 2145386496;
	setp.lt.u32	%p14, %r26, 1105199104;
	@%p14 bra 	BB46_10;

	add.u64 	%rd14, %SP, 0;
	// Callseq Start 24
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd10;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd14;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd175, [retval0+0];
	
	//{
	}// Callseq End 24
	ld.local.u32 	%r47, [%rd2];

BB46_10:
	mul.rn.f64 	%fd98, %fd175, %fd175;
	mov.f64 	%fd99, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd100, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd101, %fd100, %fd98, %fd99;
	mov.f64 	%fd102, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd103, %fd101, %fd98, %fd102;
	mov.f64 	%fd104, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd105, %fd103, %fd98, %fd104;
	mov.f64 	%fd106, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd107, %fd105, %fd98, %fd106;
	mov.f64 	%fd108, 0d3FA5555555555551;
	fma.rn.f64 	%fd109, %fd107, %fd98, %fd108;
	mov.f64 	%fd110, 0dBFE0000000000000;
	fma.rn.f64 	%fd111, %fd109, %fd98, %fd110;
	mov.f64 	%fd112, 0d3FF0000000000000;
	fma.rn.f64 	%fd113, %fd111, %fd98, %fd112;
	mov.f64 	%fd114, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd115, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd116, %fd115, %fd98, %fd114;
	mov.f64 	%fd117, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd118, %fd116, %fd98, %fd117;
	mov.f64 	%fd119, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd120, %fd118, %fd98, %fd119;
	mov.f64 	%fd121, 0d3F81111111110818;
	fma.rn.f64 	%fd122, %fd120, %fd98, %fd121;
	mov.f64 	%fd123, 0dBFC5555555555554;
	fma.rn.f64 	%fd124, %fd122, %fd98, %fd123;
	mov.f64 	%fd125, 0d0000000000000000;
	fma.rn.f64 	%fd126, %fd124, %fd98, %fd125;
	fma.rn.f64 	%fd127, %fd126, %fd175, %fd175;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd127;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd127;
	}
	xor.b32  	%r29, %r27, -2147483648;
	mov.b64 	%fd128, {%r28, %r29};
	and.b32  	%r30, %r47, 1;
	setp.eq.b32	%p15, %r30, 1;
	not.pred 	%p16, %p15;
	selp.f64	%fd176, %fd113, %fd128, %p16;
	and.b32  	%r31, %r47, 2;
	setp.eq.s32	%p17, %r31, 0;
	@%p17 bra 	BB46_12;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd176;
	}
	xor.b32  	%r33, %r32, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd176;
	}
	mov.b64 	%fd176, {%r34, %r33};

BB46_12:
	mov.f64 	%fd178, %fd7;
	@%p13 bra 	BB46_14;

	mul.rn.f64 	%fd178, %fd7, %fd125;

BB46_14:
	mov.f64 	%fd170, 0d397B839A252049C0;
	mov.f64 	%fd169, 0d3C91A62633145C00;
	mov.f64 	%fd168, 0d3FF921FB54442D18;
	mul.f64 	%fd130, %fd178, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r48, %fd130;
	st.local.u32 	[%rd1], %r48;
	cvt.rn.f64.s32	%fd131, %r48;
	neg.f64 	%fd132, %fd131;
	fma.rn.f64 	%fd134, %fd132, %fd168, %fd178;
	fma.rn.f64 	%fd136, %fd132, %fd169, %fd134;
	fma.rn.f64 	%fd180, %fd132, %fd170, %fd136;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd178;
	}
	and.b32  	%r36, %r35, 2145386496;
	setp.lt.u32	%p19, %r36, 1105199104;
	@%p19 bra 	BB46_16;

	add.u64 	%rd10, %SP, 4;
	// Callseq Start 25
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd178;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd10;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd180, [retval0+0];
	
	//{
	}// Callseq End 25
	ld.local.u32 	%r48, [%rd1];

BB46_16:
	mov.f64 	%fd173, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd172, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd171, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd138, %fd180, %fd180;
	fma.rn.f64 	%fd141, %fd171, %fd138, %fd172;
	fma.rn.f64 	%fd143, %fd141, %fd138, %fd173;
	fma.rn.f64 	%fd145, %fd143, %fd138, %fd104;
	fma.rn.f64 	%fd147, %fd145, %fd138, %fd106;
	fma.rn.f64 	%fd149, %fd147, %fd138, %fd108;
	fma.rn.f64 	%fd151, %fd149, %fd138, %fd110;
	fma.rn.f64 	%fd153, %fd151, %fd138, %fd112;
	fma.rn.f64 	%fd156, %fd115, %fd138, %fd114;
	fma.rn.f64 	%fd158, %fd156, %fd138, %fd117;
	fma.rn.f64 	%fd160, %fd158, %fd138, %fd119;
	fma.rn.f64 	%fd162, %fd160, %fd138, %fd121;
	fma.rn.f64 	%fd164, %fd162, %fd138, %fd123;
	fma.rn.f64 	%fd166, %fd164, %fd138, %fd125;
	fma.rn.f64 	%fd167, %fd166, %fd180, %fd180;
	and.b32  	%r37, %r48, 1;
	setp.eq.b32	%p20, %r37, 1;
	not.pred 	%p21, %p20;
	selp.f64	%fd181, %fd167, %fd153, %p21;
	and.b32  	%r38, %r48, 2;
	setp.eq.s32	%p22, %r38, 0;
	@%p22 bra 	BB46_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd181;
	}
	xor.b32  	%r40, %r39, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd181;
	}
	mov.b64 	%fd181, {%r41, %r40};

BB46_18:
	mov.u32 	%r46, %tid.x;
	mov.u32 	%r45, %ctaid.x;
	mov.u32 	%r44, %ntid.x;
	mad.lo.s32 	%r43, %r44, %r45, %r46;
	mul.wide.s32 	%rd13, %r43, 16;
	ld.param.u64 	%rd12, [_Z18actfunc_comp_multiP7double2ii_param_0];
	add.s64 	%rd11, %rd12, %rd13;
	st.v2.f64 	[%rd11], {%fd176, %fd181};

BB46_19:
	ret;
}

	// .globl	_Z5qsignffi
.visible .func  (.param .b32 func_retval0) _Z5qsignffi(
	.param .b32 _Z5qsignffi_param_0,
	.param .b32 _Z5qsignffi_param_1,
	.param .b32 _Z5qsignffi_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<7>;


	ld.param.f32 	%f8, [_Z5qsignffi_param_0];
	ld.param.f32 	%f9, [_Z5qsignffi_param_1];
	ld.param.u32 	%r1, [_Z5qsignffi_param_2];
	cvt.rn.f64.s32	%fd1, %r1;
	mov.f64 	%fd2, 0d401921FB54442D18;
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvt.f64.f32	%fd4, %f9;
	mul.f64 	%fd5, %fd4, %fd3;
	cvt.rn.f32.f64	%f1, %fd5;
	mul.f64 	%fd6, %fd4, 0d400921FB54442D18;
	cvt.rn.f32.f64	%f2, %fd6;
	mul.f32 	%f3, %f1, 0f3F000000;
	sub.f32 	%f10, %f8, %f3;
	add.f32 	%f11, %f2, %f10;
	div.rn.f32 	%f4, %f11, %f1;
	abs.f32 	%f12, %f4;
	mov.b32 	 %r2, %f4;
	and.b32  	%r3, %r2, -2147483648;
	or.b32  	%r4, %r3, 1056964608;
	mov.b32 	 %f13, %r4;
	add.f32 	%f14, %f4, %f13;
	cvt.rzi.f32.f32	%f15, %f14;
	setp.gt.f32	%p1, %f12, 0f4B000000;
	selp.f32	%f19, %f4, %f15, %p1;
	setp.geu.f32	%p2, %f12, 0f3F000000;
	@%p2 bra 	BB47_2;

	cvt.rzi.f32.f32	%f19, %f4;

BB47_2:
	mul.f32 	%f16, %f1, %f19;
	sub.f32 	%f17, %f16, %f2;
	add.f32 	%f18, %f3, %f17;
	st.param.f32	[func_retval0+0], %f18;
	ret;
}

	// .globl	_Z18actfunc_quat_multiP6float4iiii
.visible .func _Z18actfunc_quat_multiP6float4iiii(
	.param .b64 _Z18actfunc_quat_multiP6float4iiii_param_0,
	.param .b32 _Z18actfunc_quat_multiP6float4iiii_param_1,
	.param .b32 _Z18actfunc_quat_multiP6float4iiii_param_2,
	.param .b32 _Z18actfunc_quat_multiP6float4iiii_param_3,
	.param .b32 _Z18actfunc_quat_multiP6float4iiii_param_4
)
{
	.local .align 4 .b8 	__local_depot48[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<672>;
	.reg .f32 	%f<2534>;
	.reg .b32 	%r<4457>;
	.reg .f64 	%fd<28>;
	.reg .b64 	%rd<490>;


	mov.u64 	%rd489, __local_depot48;
	cvta.local.u64 	%SP, %rd489;
	ld.param.u64 	%rd244, [_Z18actfunc_quat_multiP6float4iiii_param_0];
	ld.param.u32 	%r1763, [_Z18actfunc_quat_multiP6float4iiii_param_4];
	add.u64 	%rd245, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd245;
	mov.u32 	%r1764, %ntid.x;
	mov.u32 	%r1765, %ctaid.x;
	mov.u32 	%r1766, %tid.x;
	mad.lo.s32 	%r1, %r1764, %r1765, %r1766;
	setp.ge.s32	%p1, %r1, %r1763;
	@%p1 bra 	BB48_1090;

	mul.wide.s32 	%rd246, %r1, 16;
	add.s64 	%rd2, %rd244, %rd246;
	ld.v4.f32 	{%f925, %f926, %f927, %f928}, [%rd2];
	mul.f32 	%f929, %f926, %f926;
	fma.rn.f32 	%f930, %f925, %f925, %f929;
	fma.rn.f32 	%f931, %f927, %f927, %f930;
	fma.rn.f32 	%f932, %f928, %f928, %f931;
	sqrt.rn.f32 	%f5, %f932;
	setp.eq.f32	%p2, %f5, 0f00000000;
	add.s64 	%rd3, %rd1, 24;
	mov.f32 	%f2389, 0f00000000;
	mov.f32 	%f2388, %f2389;
	mov.f32 	%f2387, %f2389;
	@%p2 bra 	BB48_555;

	div.rn.f32 	%f6, %f925, %f5;
	div.rn.f32 	%f7, %f927, %f5;
	div.rn.f32 	%f8, %f926, %f5;
	mul.f32 	%f934, %f8, %f7;
	div.rn.f32 	%f9, %f928, %f5;
	mul.f32 	%f935, %f6, %f9;
	sub.f32 	%f936, %f934, %f935;
	add.f32 	%f10, %f936, %f936;
	setp.gt.f32	%p3, %f10, 0f3F800000;
	mov.f32 	%f933, 0f3F800000;
	mov.f32 	%f2234, %f933;
	@%p3 bra 	BB48_4;

	setp.lt.f32	%p4, %f10, 0fBF800000;
	selp.f32	%f11, 0fBF800000, %f10, %p4;
	mov.f32 	%f2234, %f11;

BB48_4:
	mov.f32 	%f12, %f2234;
	abs.f32 	%f937, %f12;
	sub.f32 	%f939, %f933, %f937;
	mul.f32 	%f940, %f939, 0f3F000000;
	sqrt.rn.f32 	%f941, %f940;
	setp.gt.f32	%p5, %f937, 0f3F11EB85;
	selp.f32	%f942, %f941, %f937, %p5;
	mul.f32 	%f943, %f942, %f942;
	mov.f32 	%f944, 0f3C94D2E9;
	mov.f32 	%f945, 0f3D53F941;
	fma.rn.f32 	%f946, %f945, %f943, %f944;
	mov.f32 	%f947, 0f3D3F841F;
	fma.rn.f32 	%f948, %f946, %f943, %f947;
	mov.f32 	%f949, 0f3D994929;
	fma.rn.f32 	%f950, %f948, %f943, %f949;
	mov.f32 	%f951, 0f3E2AAB94;
	fma.rn.f32 	%f952, %f950, %f943, %f951;
	mul.f32 	%f953, %f943, %f952;
	fma.rn.f32 	%f954, %f953, %f942, %f942;
	mov.f32 	%f955, 0f3FC90FDB;
	mov.f32 	%f956, 0fC0000000;
	fma.rn.f32 	%f957, %f956, %f954, %f955;
	selp.f32	%f958, %f957, %f954, %p5;
	setp.gtu.f32	%p6, %f958, 0f7F800000;
	mov.b32 	 %r1767, %f958;
	mov.b32 	 %r1768, %f12;
	and.b32  	%r1769, %r1768, -2147483648;
	or.b32  	%r1770, %r1767, %r1769;
	mov.b32 	 %f959, %r1770;
	selp.f32	%f960, %f958, %f959, %p6;
	mul.f32 	%f13, %f960, 0fBF000000;
	cvt.f64.f32	%fd1, %f13;
	setp.neu.f64	%p7, %fd1, 0d3FE921FB54442D18;
	setp.neu.f64	%p8, %fd1, 0dBFE921FB54442D18;
	and.pred  	%p9, %p7, %p8;
	mul.f32 	%f961, %f8, %f8;
	mul.f32 	%f14, %f6, %f6;
	sub.f32 	%f15, %f14, %f961;
	@%p9 bra 	BB48_10;
	bra.uni 	BB48_5;

BB48_10:
	fma.rn.f32 	%f999, %f7, %f7, %f15;
	mul.f32 	%f21, %f9, %f9;
	sub.f32 	%f1000, %f999, %f21;
	mul.f32 	%f1001, %f6, %f8;
	fma.rn.f32 	%f1002, %f6, %f8, %f1001;
	fma.rn.f32 	%f1003, %f9, %f7, %f1002;
	fma.rn.f32 	%f1004, %f7, %f9, %f1003;
	abs.f32 	%f22, %f1000;
	abs.f32 	%f23, %f1004;
	setp.eq.f32	%p19, %f22, 0f00000000;
	setp.eq.f32	%p20, %f23, 0f00000000;
	and.pred  	%p21, %p19, %p20;
	mov.b32 	 %r4, %f1000;
	mov.b32 	 %r1781, %f1004;
	and.b32  	%r5, %r1781, -2147483648;
	@%p21 bra 	BB48_14;
	bra.uni 	BB48_11;

BB48_14:
	shr.s32 	%r1788, %r4, 31;
	and.b32  	%r1789, %r1788, 1078530011;
	or.b32  	%r1790, %r1789, %r5;
	mov.b32 	 %f2235, %r1790;
	bra.uni 	BB48_15;

BB48_5:
	mul.f32 	%f962, %f7, %f7;
	sub.f32 	%f963, %f15, %f962;
	fma.rn.f32 	%f964, %f9, %f9, %f963;
	mul.f32 	%f965, %f6, %f7;
	mul.f32 	%f966, %f8, %f9;
	sub.f32 	%f967, %f965, %f966;
	fma.rn.f32 	%f968, %f6, %f7, %f967;
	sub.f32 	%f969, %f968, %f966;
	abs.f32 	%f16, %f964;
	abs.f32 	%f17, %f969;
	setp.eq.f32	%p10, %f16, 0f00000000;
	setp.eq.f32	%p11, %f17, 0f00000000;
	and.pred  	%p12, %p10, %p11;
	mov.b32 	 %r2, %f964;
	mov.b32 	 %r1771, %f969;
	and.b32  	%r3, %r1771, -2147483648;
	@%p12 bra 	BB48_9;
	bra.uni 	BB48_6;

BB48_9:
	shr.s32 	%r1778, %r2, 31;
	and.b32  	%r1779, %r1778, 1078530011;
	or.b32  	%r1780, %r3, %r1779;
	mov.b32 	 %f2236, %r1780;
	mov.f32 	%f2386, 0f00000000;
	bra.uni 	BB48_21;

BB48_11:
	setp.eq.f32	%p22, %f22, 0f7F800000;
	setp.eq.f32	%p23, %f23, 0f7F800000;
	and.pred  	%p24, %p22, %p23;
	@%p24 bra 	BB48_13;
	bra.uni 	BB48_12;

BB48_13:
	shr.s32 	%r1784, %r4, 31;
	and.b32  	%r1785, %r1784, 13483017;
	add.s32 	%r1786, %r1785, 1061752795;
	or.b32  	%r1787, %r1786, %r5;
	mov.b32 	 %f2235, %r1787;
	bra.uni 	BB48_15;

BB48_6:
	setp.eq.f32	%p13, %f16, 0f7F800000;
	setp.eq.f32	%p14, %f17, 0f7F800000;
	and.pred  	%p15, %p13, %p14;
	@%p15 bra 	BB48_8;
	bra.uni 	BB48_7;

BB48_8:
	shr.s32 	%r1774, %r2, 31;
	and.b32  	%r1775, %r1774, 13483017;
	add.s32 	%r1776, %r1775, 1061752795;
	or.b32  	%r1777, %r1776, %r3;
	mov.b32 	 %f2236, %r1777;
	mov.f32 	%f2386, 0f00000000;
	bra.uni 	BB48_21;

BB48_12:
	mov.f32 	%f2220, 0f3FC90FDB;
	max.f32 	%f1005, %f23, %f22;
	min.f32 	%f1006, %f23, %f22;
	div.rn.f32 	%f1007, %f1006, %f1005;
	mul.rn.f32 	%f1008, %f1007, %f1007;
	mov.f32 	%f1009, 0fC0B59883;
	mov.f32 	%f1010, 0fBF52C7EA;
	fma.rn.f32 	%f1011, %f1008, %f1010, %f1009;
	mov.f32 	%f1012, 0fC0D21907;
	fma.rn.f32 	%f1013, %f1011, %f1008, %f1012;
	mul.f32 	%f1014, %f1008, %f1013;
	mul.f32 	%f1015, %f1007, %f1014;
	add.f32 	%f1016, %f1008, 0f41355DC0;
	mov.f32 	%f1017, 0f41E6BD60;
	fma.rn.f32 	%f1018, %f1016, %f1008, %f1017;
	mov.f32 	%f1019, 0f419D92C8;
	fma.rn.f32 	%f1020, %f1018, %f1008, %f1019;
	rcp.rn.f32 	%f1021, %f1020;
	fma.rn.f32 	%f1022, %f1015, %f1021, %f1007;
	sub.f32 	%f1024, %f2220, %f1022;
	setp.gt.f32	%p25, %f23, %f22;
	selp.f32	%f1025, %f1024, %f1022, %p25;
	mov.f32 	%f1026, 0f40490FDB;
	sub.f32 	%f1027, %f1026, %f1025;
	setp.lt.s32	%p26, %r4, 0;
	selp.f32	%f1028, %f1027, %f1025, %p26;
	mov.b32 	 %r1782, %f1028;
	or.b32  	%r1783, %r1782, %r5;
	mov.b32 	 %f1029, %r1783;
	add.f32 	%f1030, %f22, %f23;
	setp.gtu.f32	%p27, %f1030, 0f7F800000;
	selp.f32	%f2235, %f1030, %f1029, %p27;

BB48_15:
	mul.f32 	%f2223, %f9, %f9;
	mul.f32 	%f2222, %f6, %f6;
	mul.f32 	%f2386, %f2235, 0f3F000000;
	fma.rn.f32 	%f1031, %f8, %f8, %f2222;
	mul.f32 	%f1032, %f7, %f7;
	sub.f32 	%f1033, %f1031, %f1032;
	sub.f32 	%f1034, %f1033, %f2223;
	mul.f32 	%f1035, %f9, %f8;
	fma.rn.f32 	%f1036, %f6, %f7, %f1035;
	fma.rn.f32 	%f1037, %f6, %f7, %f1036;
	add.f32 	%f1038, %f1037, %f1035;
	abs.f32 	%f29, %f1034;
	abs.f32 	%f30, %f1038;
	setp.eq.f32	%p28, %f29, 0f00000000;
	setp.eq.f32	%p29, %f30, 0f00000000;
	and.pred  	%p30, %p28, %p29;
	mov.b32 	 %r6, %f1034;
	mov.b32 	 %r1791, %f1038;
	and.b32  	%r7, %r1791, -2147483648;
	@%p30 bra 	BB48_19;
	bra.uni 	BB48_16;

BB48_19:
	shr.s32 	%r1798, %r6, 31;
	and.b32  	%r1799, %r1798, 1078530011;
	or.b32  	%r1800, %r1799, %r7;
	mov.b32 	 %f2236, %r1800;
	bra.uni 	BB48_20;

BB48_16:
	setp.eq.f32	%p31, %f29, 0f7F800000;
	setp.eq.f32	%p32, %f30, 0f7F800000;
	and.pred  	%p33, %p31, %p32;
	@%p33 bra 	BB48_18;
	bra.uni 	BB48_17;

BB48_18:
	shr.s32 	%r1794, %r6, 31;
	and.b32  	%r1795, %r1794, 13483017;
	add.s32 	%r1796, %r1795, 1061752795;
	or.b32  	%r1797, %r1796, %r7;
	mov.b32 	 %f2236, %r1797;

BB48_20:
	bra.uni 	BB48_21;

BB48_7:
	mov.f32 	%f2219, 0f3FC90FDB;
	max.f32 	%f971, %f17, %f16;
	min.f32 	%f972, %f17, %f16;
	div.rn.f32 	%f973, %f972, %f971;
	mul.rn.f32 	%f974, %f973, %f973;
	mov.f32 	%f975, 0fC0B59883;
	mov.f32 	%f976, 0fBF52C7EA;
	fma.rn.f32 	%f977, %f974, %f976, %f975;
	mov.f32 	%f978, 0fC0D21907;
	fma.rn.f32 	%f979, %f977, %f974, %f978;
	mul.f32 	%f980, %f974, %f979;
	mul.f32 	%f981, %f973, %f980;
	add.f32 	%f982, %f974, 0f41355DC0;
	mov.f32 	%f983, 0f41E6BD60;
	fma.rn.f32 	%f984, %f982, %f974, %f983;
	mov.f32 	%f985, 0f419D92C8;
	fma.rn.f32 	%f986, %f984, %f974, %f985;
	rcp.rn.f32 	%f987, %f986;
	fma.rn.f32 	%f988, %f981, %f987, %f973;
	sub.f32 	%f990, %f2219, %f988;
	setp.gt.f32	%p16, %f17, %f16;
	selp.f32	%f991, %f990, %f988, %p16;
	mov.f32 	%f992, 0f40490FDB;
	sub.f32 	%f993, %f992, %f991;
	setp.lt.s32	%p17, %r2, 0;
	selp.f32	%f994, %f993, %f991, %p17;
	mov.b32 	 %r1772, %f994;
	or.b32  	%r1773, %r1772, %r3;
	mov.b32 	 %f995, %r1773;
	add.f32 	%f996, %f16, %f17;
	setp.gtu.f32	%p18, %f996, 0f7F800000;
	selp.f32	%f2236, %f996, %f995, %p18;
	mov.f32 	%f2386, 0f00000000;
	bra.uni 	BB48_21;

BB48_17:
	mov.f32 	%f2221, 0f3FC90FDB;
	max.f32 	%f1039, %f30, %f29;
	min.f32 	%f1040, %f30, %f29;
	div.rn.f32 	%f1041, %f1040, %f1039;
	mul.rn.f32 	%f1042, %f1041, %f1041;
	mov.f32 	%f1043, 0fC0B59883;
	mov.f32 	%f1044, 0fBF52C7EA;
	fma.rn.f32 	%f1045, %f1042, %f1044, %f1043;
	mov.f32 	%f1046, 0fC0D21907;
	fma.rn.f32 	%f1047, %f1045, %f1042, %f1046;
	mul.f32 	%f1048, %f1042, %f1047;
	mul.f32 	%f1049, %f1041, %f1048;
	add.f32 	%f1050, %f1042, 0f41355DC0;
	mov.f32 	%f1051, 0f41E6BD60;
	fma.rn.f32 	%f1052, %f1050, %f1042, %f1051;
	mov.f32 	%f1053, 0f419D92C8;
	fma.rn.f32 	%f1054, %f1052, %f1042, %f1053;
	rcp.rn.f32 	%f1055, %f1054;
	fma.rn.f32 	%f1056, %f1049, %f1055, %f1041;
	sub.f32 	%f1058, %f2221, %f1056;
	setp.gt.f32	%p34, %f30, %f29;
	selp.f32	%f1059, %f1058, %f1056, %p34;
	mov.f32 	%f1060, 0f40490FDB;
	sub.f32 	%f1061, %f1060, %f1059;
	setp.lt.s32	%p35, %r6, 0;
	selp.f32	%f1062, %f1061, %f1059, %p35;
	mov.b32 	 %r1792, %f1062;
	or.b32  	%r1793, %r1792, %r7;
	mov.b32 	 %f1063, %r1793;
	add.f32 	%f1064, %f29, %f30;
	setp.gtu.f32	%p36, %f1064, 0f7F800000;
	selp.f32	%f2236, %f1064, %f1063, %p36;

BB48_21:
	mov.f32 	%f34, %f2386;
	mul.f32 	%f36, %f2236, 0f3F000000;
	abs.f32 	%f37, %f34;
	setp.neu.f32	%p37, %f37, 0f7F800000;
	mov.f32 	%f2385, %f34;
	@%p37 bra 	BB48_23;

	mov.f32 	%f1065, 0f00000000;
	mul.rn.f32 	%f38, %f34, %f1065;
	mov.f32 	%f2385, %f38;

BB48_23:
	mov.f32 	%f39, %f2385;
	mul.f32 	%f1066, %f39, 0f3F22F983;
	cvt.rni.s32.f32	%r3986, %f1066;
	cvt.rn.f32.s32	%f1067, %r3986;
	neg.f32 	%f1068, %f1067;
	mov.f32 	%f1069, 0f3FC90FDA;
	fma.rn.f32 	%f1070, %f1068, %f1069, %f39;
	mov.f32 	%f1071, 0f33A22168;
	fma.rn.f32 	%f1072, %f1068, %f1071, %f1070;
	mov.f32 	%f1073, 0f27C234C5;
	fma.rn.f32 	%f2237, %f1068, %f1073, %f1072;
	abs.f32 	%f1074, %f39;
	setp.leu.f32	%p38, %f1074, 0f47CE4780;
	@%p38 bra 	BB48_33;

	mov.b32 	 %r9, %f39;
	shr.u32 	%r10, %r9, 23;
	bfe.u32 	%r1803, %r9, 23, 8;
	add.s32 	%r1804, %r1803, -128;
	shl.b32 	%r1805, %r9, 8;
	or.b32  	%r11, %r1805, -2147483648;
	shr.u32 	%r12, %r1804, 5;
	mov.u32 	%r3978, 0;
	mov.u64 	%rd346, __cudart_i2opi_f;
	mov.u32 	%r3977, -6;
	mov.u64 	%rd488, %rd1;

BB48_25:
	.pragma "nounroll";
	mov.u64 	%rd5, %rd488;
	ld.const.u32 	%r1808, [%rd346];
	// inline asm
	{
	mad.lo.cc.u32   %r1806, %r1808, %r11, %r3978;
	madc.hi.u32     %r3978, %r1808, %r11,  0;
	}
	// inline asm
	st.local.u32 	[%rd5], %r1806;
	add.s64 	%rd6, %rd5, 4;
	add.s64 	%rd346, %rd346, 4;
	add.s32 	%r3977, %r3977, 1;
	setp.ne.s32	%p39, %r3977, 0;
	mov.u64 	%rd488, %rd6;
	@%p39 bra 	BB48_25;

	and.b32  	%r17, %r9, -2147483648;
	st.local.u32 	[%rd3], %r3978;
	mov.u32 	%r1811, 6;
	sub.s32 	%r1812, %r1811, %r12;
	mul.wide.s32 	%rd248, %r1812, 4;
	add.s64 	%rd8, %rd1, %rd248;
	ld.local.u32 	%r3979, [%rd8];
	ld.local.u32 	%r3980, [%rd8+-4];
	and.b32  	%r20, %r10, 31;
	setp.eq.s32	%p40, %r20, 0;
	@%p40 bra 	BB48_28;

	mov.u32 	%r1813, 32;
	sub.s32 	%r1814, %r1813, %r20;
	shr.u32 	%r1815, %r3980, %r1814;
	shl.b32 	%r1816, %r3979, %r20;
	add.s32 	%r3979, %r1815, %r1816;
	ld.local.u32 	%r1817, [%rd8+-8];
	shr.u32 	%r1818, %r1817, %r1814;
	shl.b32 	%r1819, %r3980, %r20;
	add.s32 	%r3980, %r1818, %r1819;

BB48_28:
	shr.u32 	%r1820, %r3980, 30;
	shl.b32 	%r1821, %r3979, 2;
	add.s32 	%r3981, %r1820, %r1821;
	shl.b32 	%r26, %r3980, 2;
	shr.u32 	%r1822, %r3981, 31;
	shr.u32 	%r1823, %r3979, 30;
	add.s32 	%r27, %r1822, %r1823;
	setp.eq.s32	%p41, %r1822, 0;
	mov.u32 	%r3982, %r17;
	mov.u32 	%r3983, %r26;
	@%p41 bra 	BB48_30;

	not.b32 	%r1824, %r3981;
	neg.s32 	%r28, %r26;
	setp.eq.s32	%p42, %r26, 0;
	selp.u32	%r1825, 1, 0, %p42;
	add.s32 	%r3981, %r1825, %r1824;
	xor.b32  	%r30, %r17, -2147483648;
	mov.u32 	%r3982, %r30;
	mov.u32 	%r3983, %r28;

BB48_30:
	mov.u32 	%r32, %r3982;
	neg.s32 	%r1826, %r27;
	setp.eq.s32	%p43, %r17, 0;
	selp.b32	%r3986, %r27, %r1826, %p43;
	clz.b32 	%r3985, %r3981;
	setp.eq.s32	%p44, %r3985, 0;
	shl.b32 	%r1827, %r3981, %r3985;
	mov.u32 	%r1828, 32;
	sub.s32 	%r1829, %r1828, %r3985;
	shr.u32 	%r1830, %r3983, %r1829;
	add.s32 	%r1831, %r1830, %r1827;
	selp.b32	%r36, %r3981, %r1831, %p44;
	mov.u32 	%r1832, -921707870;
	mul.hi.u32 	%r3984, %r36, %r1832;
	setp.lt.s32	%p45, %r3984, 1;
	@%p45 bra 	BB48_32;

	mul.lo.s32 	%r1833, %r36, -921707870;
	shr.u32 	%r1834, %r1833, 31;
	shl.b32 	%r1835, %r3984, 1;
	add.s32 	%r3984, %r1834, %r1835;
	add.s32 	%r3985, %r3985, 1;

BB48_32:
	mov.u32 	%r1836, 126;
	sub.s32 	%r1837, %r1836, %r3985;
	shl.b32 	%r1838, %r1837, 23;
	add.s32 	%r1839, %r3984, 1;
	shr.u32 	%r1840, %r1839, 7;
	add.s32 	%r1841, %r1840, 1;
	shr.u32 	%r1842, %r1841, 1;
	add.s32 	%r1843, %r1842, %r1838;
	or.b32  	%r1844, %r1843, %r32;
	mov.b32 	 %f2237, %r1844;

BB48_33:
	mul.rn.f32 	%f43, %f2237, %f2237;
	add.s32 	%r43, %r3986, 1;
	and.b32  	%r44, %r43, 1;
	setp.eq.s32	%p46, %r44, 0;
	@%p46 bra 	BB48_35;

	mov.f32 	%f1075, 0fBAB6061A;
	mov.f32 	%f1076, 0f37CCF5CE;
	fma.rn.f32 	%f2238, %f1076, %f43, %f1075;
	bra.uni 	BB48_36;

BB48_35:
	mov.f32 	%f1077, 0f3C08839E;
	mov.f32 	%f1078, 0fB94CA1F9;
	fma.rn.f32 	%f2238, %f1078, %f43, %f1077;

BB48_36:
	@%p46 bra 	BB48_38;

	mov.f32 	%f1079, 0f3D2AAAA5;
	fma.rn.f32 	%f1080, %f2238, %f43, %f1079;
	mov.f32 	%f1081, 0fBF000000;
	fma.rn.f32 	%f2239, %f1080, %f43, %f1081;
	bra.uni 	BB48_39;

BB48_38:
	mov.f32 	%f1082, 0fBE2AAAA3;
	fma.rn.f32 	%f1083, %f2238, %f43, %f1082;
	mov.f32 	%f1084, 0f00000000;
	fma.rn.f32 	%f2239, %f1083, %f43, %f1084;

BB48_39:
	fma.rn.f32 	%f2240, %f2239, %f2237, %f2237;
	@%p46 bra 	BB48_41;

	mul.rn.f32 	%f2224, %f2237, %f2237;
	mov.f32 	%f1085, 0f3F800000;
	fma.rn.f32 	%f2240, %f2239, %f2224, %f1085;

BB48_41:
	and.b32  	%r1845, %r43, 2;
	setp.eq.s32	%p49, %r1845, 0;
	@%p49 bra 	BB48_43;

	mov.f32 	%f1086, 0f00000000;
	mov.f32 	%f1087, 0fBF800000;
	fma.rn.f32 	%f2240, %f2240, %f1087, %f1086;

BB48_43:
	abs.f32 	%f55, %f36;
	setp.neu.f32	%p50, %f55, 0f7F800000;
	mov.f32 	%f2339, %f36;
	@%p50 bra 	BB48_45;

	mov.f32 	%f1088, 0f00000000;
	mul.rn.f32 	%f56, %f36, %f1088;
	mov.f32 	%f2339, %f56;

BB48_45:
	mov.f32 	%f57, %f2339;
	mul.f32 	%f1089, %f57, 0f3F22F983;
	cvt.rni.s32.f32	%r3996, %f1089;
	cvt.rn.f32.s32	%f1090, %r3996;
	neg.f32 	%f1091, %f1090;
	fma.rn.f32 	%f1093, %f1091, %f1069, %f57;
	fma.rn.f32 	%f1095, %f1091, %f1071, %f1093;
	fma.rn.f32 	%f2241, %f1091, %f1073, %f1095;
	abs.f32 	%f1097, %f57;
	setp.leu.f32	%p51, %f1097, 0f47CE4780;
	@%p51 bra 	BB48_55;

	mov.b32 	 %r46, %f57;
	shr.u32 	%r47, %r46, 23;
	bfe.u32 	%r1848, %r46, 23, 8;
	add.s32 	%r1849, %r1848, -128;
	shl.b32 	%r1850, %r46, 8;
	or.b32  	%r48, %r1850, -2147483648;
	shr.u32 	%r49, %r1849, 5;
	mov.u32 	%r3988, 0;
	mov.u64 	%rd347, __cudart_i2opi_f;
	mov.u32 	%r3987, -6;
	mov.u64 	%rd487, %rd1;

BB48_47:
	.pragma "nounroll";
	ld.const.u32 	%r1853, [%rd347];
	// inline asm
	{
	mad.lo.cc.u32   %r1851, %r1853, %r48, %r3988;
	madc.hi.u32     %r3988, %r1853, %r48,  0;
	}
	// inline asm
	st.local.u32 	[%rd487], %r1851;
	add.s64 	%rd487, %rd487, 4;
	add.s64 	%rd347, %rd347, 4;
	add.s32 	%r3987, %r3987, 1;
	setp.ne.s32	%p52, %r3987, 0;
	@%p52 bra 	BB48_47;

	and.b32  	%r54, %r46, -2147483648;
	st.local.u32 	[%rd3], %r3988;
	mov.u32 	%r1856, 6;
	sub.s32 	%r1857, %r1856, %r49;
	mul.wide.s32 	%rd250, %r1857, 4;
	add.s64 	%rd13, %rd1, %rd250;
	ld.local.u32 	%r3989, [%rd13];
	ld.local.u32 	%r3990, [%rd13+-4];
	and.b32  	%r57, %r47, 31;
	setp.eq.s32	%p53, %r57, 0;
	@%p53 bra 	BB48_50;

	mov.u32 	%r1858, 32;
	sub.s32 	%r1859, %r1858, %r57;
	shr.u32 	%r1860, %r3990, %r1859;
	shl.b32 	%r1861, %r3989, %r57;
	add.s32 	%r3989, %r1860, %r1861;
	ld.local.u32 	%r1862, [%rd13+-8];
	shr.u32 	%r1863, %r1862, %r1859;
	shl.b32 	%r1864, %r3990, %r57;
	add.s32 	%r3990, %r1863, %r1864;

BB48_50:
	shr.u32 	%r1865, %r3990, 30;
	shl.b32 	%r1866, %r3989, 2;
	add.s32 	%r3991, %r1865, %r1866;
	shl.b32 	%r63, %r3990, 2;
	shr.u32 	%r1867, %r3991, 31;
	shr.u32 	%r1868, %r3989, 30;
	add.s32 	%r64, %r1867, %r1868;
	setp.eq.s32	%p54, %r1867, 0;
	mov.u32 	%r3992, %r54;
	mov.u32 	%r3993, %r63;
	@%p54 bra 	BB48_52;

	not.b32 	%r1869, %r3991;
	neg.s32 	%r65, %r63;
	setp.eq.s32	%p55, %r63, 0;
	selp.u32	%r1870, 1, 0, %p55;
	add.s32 	%r3991, %r1870, %r1869;
	xor.b32  	%r67, %r54, -2147483648;
	mov.u32 	%r3992, %r67;
	mov.u32 	%r3993, %r65;

BB48_52:
	mov.u32 	%r69, %r3992;
	neg.s32 	%r1871, %r64;
	setp.eq.s32	%p56, %r54, 0;
	selp.b32	%r3996, %r64, %r1871, %p56;
	clz.b32 	%r3995, %r3991;
	setp.eq.s32	%p57, %r3995, 0;
	shl.b32 	%r1872, %r3991, %r3995;
	mov.u32 	%r1873, 32;
	sub.s32 	%r1874, %r1873, %r3995;
	shr.u32 	%r1875, %r3993, %r1874;
	add.s32 	%r1876, %r1875, %r1872;
	selp.b32	%r73, %r3991, %r1876, %p57;
	mov.u32 	%r1877, -921707870;
	mul.hi.u32 	%r3994, %r73, %r1877;
	setp.lt.s32	%p58, %r3994, 1;
	@%p58 bra 	BB48_54;

	mul.lo.s32 	%r1878, %r73, -921707870;
	shr.u32 	%r1879, %r1878, 31;
	shl.b32 	%r1880, %r3994, 1;
	add.s32 	%r3994, %r1879, %r1880;
	add.s32 	%r3995, %r3995, 1;

BB48_54:
	mov.u32 	%r1881, 126;
	sub.s32 	%r1882, %r1881, %r3995;
	shl.b32 	%r1883, %r1882, 23;
	add.s32 	%r1884, %r3994, 1;
	shr.u32 	%r1885, %r1884, 7;
	add.s32 	%r1886, %r1885, 1;
	shr.u32 	%r1887, %r1886, 1;
	add.s32 	%r1888, %r1887, %r1883;
	or.b32  	%r1889, %r1888, %r69;
	mov.b32 	 %f2241, %r1889;

BB48_55:
	mul.rn.f32 	%f61, %f2241, %f2241;
	add.s32 	%r80, %r3996, 1;
	and.b32  	%r81, %r80, 1;
	setp.eq.s32	%p59, %r81, 0;
	@%p59 bra 	BB48_57;

	mov.f32 	%f1098, 0fBAB6061A;
	mov.f32 	%f1099, 0f37CCF5CE;
	fma.rn.f32 	%f2242, %f1099, %f61, %f1098;
	bra.uni 	BB48_58;

BB48_57:
	mov.f32 	%f1100, 0f3C08839E;
	mov.f32 	%f1101, 0fB94CA1F9;
	fma.rn.f32 	%f2242, %f1101, %f61, %f1100;

BB48_58:
	@%p59 bra 	BB48_60;

	mul.rn.f32 	%f2225, %f2241, %f2241;
	mov.f32 	%f1102, 0f3D2AAAA5;
	fma.rn.f32 	%f1103, %f2242, %f2225, %f1102;
	mov.f32 	%f1104, 0fBF000000;
	fma.rn.f32 	%f2243, %f1103, %f2225, %f1104;
	bra.uni 	BB48_61;

BB48_60:
	mul.rn.f32 	%f2227, %f2241, %f2241;
	mov.f32 	%f1105, 0fBE2AAAA3;
	fma.rn.f32 	%f1106, %f2242, %f2227, %f1105;
	mov.f32 	%f1107, 0f00000000;
	fma.rn.f32 	%f2243, %f1106, %f2227, %f1107;

BB48_61:
	fma.rn.f32 	%f2244, %f2243, %f2241, %f2241;
	@%p59 bra 	BB48_63;

	mul.rn.f32 	%f2226, %f2241, %f2241;
	mov.f32 	%f1108, 0f3F800000;
	fma.rn.f32 	%f2244, %f2243, %f2226, %f1108;

BB48_63:
	and.b32  	%r1890, %r80, 2;
	setp.eq.s32	%p62, %r1890, 0;
	@%p62 bra 	BB48_65;

	mov.f32 	%f1109, 0f00000000;
	mov.f32 	%f1110, 0fBF800000;
	fma.rn.f32 	%f2244, %f2244, %f1110, %f1109;

BB48_65:
	mul.f32 	%f73, %f2240, %f2244;
	abs.f32 	%f74, %f13;
	setp.neu.f32	%p63, %f74, 0f7F800000;
	mov.f32 	%f2358, %f13;
	@%p63 bra 	BB48_67;

	mov.f32 	%f1111, 0f00000000;
	mul.rn.f32 	%f75, %f13, %f1111;
	mov.f32 	%f2358, %f75;

BB48_67:
	mov.f32 	%f76, %f2358;
	mul.f32 	%f1112, %f76, 0f3F22F983;
	cvt.rni.s32.f32	%r4006, %f1112;
	cvt.rn.f32.s32	%f1113, %r4006;
	neg.f32 	%f1114, %f1113;
	fma.rn.f32 	%f1116, %f1114, %f1069, %f76;
	fma.rn.f32 	%f1118, %f1114, %f1071, %f1116;
	fma.rn.f32 	%f2245, %f1114, %f1073, %f1118;
	abs.f32 	%f1120, %f76;
	setp.leu.f32	%p64, %f1120, 0f47CE4780;
	@%p64 bra 	BB48_77;

	mov.b32 	 %r83, %f76;
	shr.u32 	%r84, %r83, 23;
	bfe.u32 	%r1893, %r83, 23, 8;
	add.s32 	%r1894, %r1893, -128;
	shl.b32 	%r1895, %r83, 8;
	or.b32  	%r85, %r1895, -2147483648;
	shr.u32 	%r86, %r1894, 5;
	mov.u32 	%r3998, 0;
	mov.u64 	%rd348, __cudart_i2opi_f;
	mov.u32 	%r3997, -6;
	mov.u64 	%rd486, %rd1;

BB48_69:
	.pragma "nounroll";
	ld.const.u32 	%r1898, [%rd348];
	// inline asm
	{
	mad.lo.cc.u32   %r1896, %r1898, %r85, %r3998;
	madc.hi.u32     %r3998, %r1898, %r85,  0;
	}
	// inline asm
	st.local.u32 	[%rd486], %r1896;
	add.s64 	%rd486, %rd486, 4;
	add.s64 	%rd348, %rd348, 4;
	add.s32 	%r3997, %r3997, 1;
	setp.ne.s32	%p65, %r3997, 0;
	@%p65 bra 	BB48_69;

	and.b32  	%r91, %r83, -2147483648;
	st.local.u32 	[%rd3], %r3998;
	mov.u32 	%r1901, 6;
	sub.s32 	%r1902, %r1901, %r86;
	mul.wide.s32 	%rd252, %r1902, 4;
	add.s64 	%rd18, %rd1, %rd252;
	ld.local.u32 	%r3999, [%rd18];
	ld.local.u32 	%r4000, [%rd18+-4];
	and.b32  	%r94, %r84, 31;
	setp.eq.s32	%p66, %r94, 0;
	@%p66 bra 	BB48_72;

	mov.u32 	%r1903, 32;
	sub.s32 	%r1904, %r1903, %r94;
	shr.u32 	%r1905, %r4000, %r1904;
	shl.b32 	%r1906, %r3999, %r94;
	add.s32 	%r3999, %r1905, %r1906;
	ld.local.u32 	%r1907, [%rd18+-8];
	shr.u32 	%r1908, %r1907, %r1904;
	shl.b32 	%r1909, %r4000, %r94;
	add.s32 	%r4000, %r1908, %r1909;

BB48_72:
	shr.u32 	%r1910, %r4000, 30;
	shl.b32 	%r1911, %r3999, 2;
	add.s32 	%r4001, %r1910, %r1911;
	shl.b32 	%r100, %r4000, 2;
	shr.u32 	%r1912, %r4001, 31;
	shr.u32 	%r1913, %r3999, 30;
	add.s32 	%r101, %r1912, %r1913;
	setp.eq.s32	%p67, %r1912, 0;
	mov.u32 	%r4002, %r91;
	mov.u32 	%r4003, %r100;
	@%p67 bra 	BB48_74;

	not.b32 	%r1914, %r4001;
	neg.s32 	%r102, %r100;
	setp.eq.s32	%p68, %r100, 0;
	selp.u32	%r1915, 1, 0, %p68;
	add.s32 	%r4001, %r1915, %r1914;
	xor.b32  	%r104, %r91, -2147483648;
	mov.u32 	%r4002, %r104;
	mov.u32 	%r4003, %r102;

BB48_74:
	mov.u32 	%r106, %r4002;
	neg.s32 	%r1916, %r101;
	setp.eq.s32	%p69, %r91, 0;
	selp.b32	%r4006, %r101, %r1916, %p69;
	clz.b32 	%r4005, %r4001;
	setp.eq.s32	%p70, %r4005, 0;
	shl.b32 	%r1917, %r4001, %r4005;
	mov.u32 	%r1918, 32;
	sub.s32 	%r1919, %r1918, %r4005;
	shr.u32 	%r1920, %r4003, %r1919;
	add.s32 	%r1921, %r1920, %r1917;
	selp.b32	%r110, %r4001, %r1921, %p70;
	mov.u32 	%r1922, -921707870;
	mul.hi.u32 	%r4004, %r110, %r1922;
	setp.lt.s32	%p71, %r4004, 1;
	@%p71 bra 	BB48_76;

	mul.lo.s32 	%r1923, %r110, -921707870;
	shr.u32 	%r1924, %r1923, 31;
	shl.b32 	%r1925, %r4004, 1;
	add.s32 	%r4004, %r1924, %r1925;
	add.s32 	%r4005, %r4005, 1;

BB48_76:
	mov.u32 	%r1926, 126;
	sub.s32 	%r1927, %r1926, %r4005;
	shl.b32 	%r1928, %r1927, 23;
	add.s32 	%r1929, %r4004, 1;
	shr.u32 	%r1930, %r1929, 7;
	add.s32 	%r1931, %r1930, 1;
	shr.u32 	%r1932, %r1931, 1;
	add.s32 	%r1933, %r1932, %r1928;
	or.b32  	%r1934, %r1933, %r106;
	mov.b32 	 %f2245, %r1934;

BB48_77:
	mul.rn.f32 	%f80, %f2245, %f2245;
	add.s32 	%r117, %r4006, 1;
	and.b32  	%r118, %r117, 1;
	setp.eq.s32	%p72, %r118, 0;
	@%p72 bra 	BB48_79;

	mov.f32 	%f1121, 0fBAB6061A;
	mov.f32 	%f1122, 0f37CCF5CE;
	fma.rn.f32 	%f2246, %f1122, %f80, %f1121;
	bra.uni 	BB48_80;

BB48_79:
	mov.f32 	%f1123, 0f3C08839E;
	mov.f32 	%f1124, 0fB94CA1F9;
	fma.rn.f32 	%f2246, %f1124, %f80, %f1123;

BB48_80:
	@%p72 bra 	BB48_82;

	mul.rn.f32 	%f2228, %f2245, %f2245;
	mov.f32 	%f1125, 0f3D2AAAA5;
	fma.rn.f32 	%f1126, %f2246, %f2228, %f1125;
	mov.f32 	%f1127, 0fBF000000;
	fma.rn.f32 	%f2247, %f1126, %f2228, %f1127;
	bra.uni 	BB48_83;

BB48_82:
	mul.rn.f32 	%f2230, %f2245, %f2245;
	mov.f32 	%f1128, 0fBE2AAAA3;
	fma.rn.f32 	%f1129, %f2246, %f2230, %f1128;
	mov.f32 	%f1130, 0f00000000;
	fma.rn.f32 	%f2247, %f1129, %f2230, %f1130;

BB48_83:
	fma.rn.f32 	%f2248, %f2247, %f2245, %f2245;
	@%p72 bra 	BB48_85;

	mul.rn.f32 	%f2229, %f2245, %f2245;
	mov.f32 	%f1131, 0f3F800000;
	fma.rn.f32 	%f2248, %f2247, %f2229, %f1131;

BB48_85:
	and.b32  	%r1935, %r117, 2;
	setp.eq.s32	%p75, %r1935, 0;
	@%p75 bra 	BB48_87;

	mov.f32 	%f1132, 0f00000000;
	mov.f32 	%f1133, 0fBF800000;
	fma.rn.f32 	%f2248, %f2248, %f1133, %f1132;

BB48_87:
	mul.f32 	%f92, %f73, %f2248;
	mov.f32 	%f2384, %f34;
	@%p37 bra 	BB48_89;

	mov.f32 	%f1134, 0f00000000;
	mul.rn.f32 	%f2384, %f34, %f1134;

BB48_89:
	mul.f32 	%f1135, %f2384, 0f3F22F983;
	cvt.rni.s32.f32	%r4016, %f1135;
	cvt.rn.f32.s32	%f1136, %r4016;
	neg.f32 	%f1137, %f1136;
	fma.rn.f32 	%f1139, %f1137, %f1069, %f2384;
	fma.rn.f32 	%f1141, %f1137, %f1071, %f1139;
	fma.rn.f32 	%f2249, %f1137, %f1073, %f1141;
	abs.f32 	%f1143, %f2384;
	setp.leu.f32	%p77, %f1143, 0f47CE4780;
	@%p77 bra 	BB48_99;

	mov.b32 	 %r120, %f2384;
	shr.u32 	%r121, %r120, 23;
	bfe.u32 	%r1938, %r120, 23, 8;
	add.s32 	%r1939, %r1938, -128;
	shl.b32 	%r1940, %r120, 8;
	or.b32  	%r122, %r1940, -2147483648;
	shr.u32 	%r123, %r1939, 5;
	mov.u32 	%r4008, 0;
	mov.u64 	%rd349, __cudart_i2opi_f;
	mov.u32 	%r4007, -6;
	mov.u64 	%rd485, %rd1;

BB48_91:
	.pragma "nounroll";
	ld.const.u32 	%r1943, [%rd349];
	// inline asm
	{
	mad.lo.cc.u32   %r1941, %r1943, %r122, %r4008;
	madc.hi.u32     %r4008, %r1943, %r122,  0;
	}
	// inline asm
	st.local.u32 	[%rd485], %r1941;
	add.s64 	%rd485, %rd485, 4;
	add.s64 	%rd349, %rd349, 4;
	add.s32 	%r4007, %r4007, 1;
	setp.ne.s32	%p78, %r4007, 0;
	@%p78 bra 	BB48_91;

	and.b32  	%r128, %r120, -2147483648;
	st.local.u32 	[%rd3], %r4008;
	mov.u32 	%r1946, 6;
	sub.s32 	%r1947, %r1946, %r123;
	mul.wide.s32 	%rd254, %r1947, 4;
	add.s64 	%rd23, %rd1, %rd254;
	ld.local.u32 	%r4009, [%rd23];
	ld.local.u32 	%r4010, [%rd23+-4];
	and.b32  	%r131, %r121, 31;
	setp.eq.s32	%p79, %r131, 0;
	@%p79 bra 	BB48_94;

	mov.u32 	%r1948, 32;
	sub.s32 	%r1949, %r1948, %r131;
	shr.u32 	%r1950, %r4010, %r1949;
	shl.b32 	%r1951, %r4009, %r131;
	add.s32 	%r4009, %r1950, %r1951;
	ld.local.u32 	%r1952, [%rd23+-8];
	shr.u32 	%r1953, %r1952, %r1949;
	shl.b32 	%r1954, %r4010, %r131;
	add.s32 	%r4010, %r1953, %r1954;

BB48_94:
	shr.u32 	%r1955, %r4010, 30;
	shl.b32 	%r1956, %r4009, 2;
	add.s32 	%r4011, %r1955, %r1956;
	shl.b32 	%r137, %r4010, 2;
	shr.u32 	%r1957, %r4011, 31;
	shr.u32 	%r1958, %r4009, 30;
	add.s32 	%r138, %r1957, %r1958;
	setp.eq.s32	%p80, %r1957, 0;
	mov.u32 	%r4012, %r128;
	mov.u32 	%r4013, %r137;
	@%p80 bra 	BB48_96;

	not.b32 	%r1959, %r4011;
	neg.s32 	%r139, %r137;
	setp.eq.s32	%p81, %r137, 0;
	selp.u32	%r1960, 1, 0, %p81;
	add.s32 	%r4011, %r1960, %r1959;
	xor.b32  	%r141, %r128, -2147483648;
	mov.u32 	%r4012, %r141;
	mov.u32 	%r4013, %r139;

BB48_96:
	mov.u32 	%r143, %r4012;
	neg.s32 	%r1961, %r138;
	setp.eq.s32	%p82, %r128, 0;
	selp.b32	%r4016, %r138, %r1961, %p82;
	clz.b32 	%r4015, %r4011;
	setp.eq.s32	%p83, %r4015, 0;
	shl.b32 	%r1962, %r4011, %r4015;
	mov.u32 	%r1963, 32;
	sub.s32 	%r1964, %r1963, %r4015;
	shr.u32 	%r1965, %r4013, %r1964;
	add.s32 	%r1966, %r1965, %r1962;
	selp.b32	%r147, %r4011, %r1966, %p83;
	mov.u32 	%r1967, -921707870;
	mul.hi.u32 	%r4014, %r147, %r1967;
	setp.lt.s32	%p84, %r4014, 1;
	@%p84 bra 	BB48_98;

	mul.lo.s32 	%r1968, %r147, -921707870;
	shr.u32 	%r1969, %r1968, 31;
	shl.b32 	%r1970, %r4014, 1;
	add.s32 	%r4014, %r1969, %r1970;
	add.s32 	%r4015, %r4015, 1;

BB48_98:
	mov.u32 	%r1971, 126;
	sub.s32 	%r1972, %r1971, %r4015;
	shl.b32 	%r1973, %r1972, 23;
	add.s32 	%r1974, %r4014, 1;
	shr.u32 	%r1975, %r1974, 7;
	add.s32 	%r1976, %r1975, 1;
	shr.u32 	%r1977, %r1976, 1;
	add.s32 	%r1978, %r1977, %r1973;
	or.b32  	%r1979, %r1978, %r143;
	mov.b32 	 %f2249, %r1979;

BB48_99:
	mul.rn.f32 	%f98, %f2249, %f2249;
	and.b32  	%r154, %r4016, 1;
	setp.eq.s32	%p85, %r154, 0;
	@%p85 bra 	BB48_101;

	mov.f32 	%f1144, 0fBAB6061A;
	mov.f32 	%f1145, 0f37CCF5CE;
	fma.rn.f32 	%f2250, %f1145, %f98, %f1144;
	bra.uni 	BB48_102;

BB48_101:
	mov.f32 	%f1146, 0f3C08839E;
	mov.f32 	%f1147, 0fB94CA1F9;
	fma.rn.f32 	%f2250, %f1147, %f98, %f1146;

BB48_102:
	@%p85 bra 	BB48_104;

	mov.f32 	%f1148, 0f3D2AAAA5;
	fma.rn.f32 	%f1149, %f2250, %f98, %f1148;
	mov.f32 	%f1150, 0fBF000000;
	fma.rn.f32 	%f2251, %f1149, %f98, %f1150;
	bra.uni 	BB48_105;

BB48_104:
	mov.f32 	%f1151, 0fBE2AAAA3;
	fma.rn.f32 	%f1152, %f2250, %f98, %f1151;
	mov.f32 	%f1153, 0f00000000;
	fma.rn.f32 	%f2251, %f1152, %f98, %f1153;

BB48_105:
	fma.rn.f32 	%f2252, %f2251, %f2249, %f2249;
	@%p85 bra 	BB48_107;

	mul.rn.f32 	%f2231, %f2249, %f2249;
	mov.f32 	%f1154, 0f3F800000;
	fma.rn.f32 	%f2252, %f2251, %f2231, %f1154;

BB48_107:
	and.b32  	%r1980, %r4016, 2;
	setp.eq.s32	%p88, %r1980, 0;
	@%p88 bra 	BB48_109;

	mov.f32 	%f1155, 0f00000000;
	mov.f32 	%f1156, 0fBF800000;
	fma.rn.f32 	%f2252, %f2252, %f1156, %f1155;

BB48_109:
	mov.f32 	%f2338, %f36;
	@%p50 bra 	BB48_111;

	mov.f32 	%f1157, 0f00000000;
	mul.rn.f32 	%f2338, %f36, %f1157;

BB48_111:
	mul.f32 	%f1158, %f2338, 0f3F22F983;
	cvt.rni.s32.f32	%r4026, %f1158;
	cvt.rn.f32.s32	%f1159, %r4026;
	neg.f32 	%f1160, %f1159;
	fma.rn.f32 	%f1162, %f1160, %f1069, %f2338;
	fma.rn.f32 	%f1164, %f1160, %f1071, %f1162;
	fma.rn.f32 	%f2253, %f1160, %f1073, %f1164;
	abs.f32 	%f1166, %f2338;
	setp.leu.f32	%p90, %f1166, 0f47CE4780;
	@%p90 bra 	BB48_121;

	mov.b32 	 %r156, %f2338;
	shr.u32 	%r157, %r156, 23;
	bfe.u32 	%r1983, %r156, 23, 8;
	add.s32 	%r1984, %r1983, -128;
	shl.b32 	%r1985, %r156, 8;
	or.b32  	%r158, %r1985, -2147483648;
	shr.u32 	%r159, %r1984, 5;
	mov.u32 	%r4018, 0;
	mov.u64 	%rd350, __cudart_i2opi_f;
	mov.u32 	%r4017, -6;
	mov.u64 	%rd484, %rd1;

BB48_113:
	.pragma "nounroll";
	ld.const.u32 	%r1988, [%rd350];
	// inline asm
	{
	mad.lo.cc.u32   %r1986, %r1988, %r158, %r4018;
	madc.hi.u32     %r4018, %r1988, %r158,  0;
	}
	// inline asm
	st.local.u32 	[%rd484], %r1986;
	add.s64 	%rd484, %rd484, 4;
	add.s64 	%rd350, %rd350, 4;
	add.s32 	%r4017, %r4017, 1;
	setp.ne.s32	%p91, %r4017, 0;
	@%p91 bra 	BB48_113;

	and.b32  	%r164, %r156, -2147483648;
	st.local.u32 	[%rd3], %r4018;
	mov.u32 	%r1991, 6;
	sub.s32 	%r1992, %r1991, %r159;
	mul.wide.s32 	%rd256, %r1992, 4;
	add.s64 	%rd28, %rd1, %rd256;
	ld.local.u32 	%r4019, [%rd28];
	ld.local.u32 	%r4020, [%rd28+-4];
	and.b32  	%r167, %r157, 31;
	setp.eq.s32	%p92, %r167, 0;
	@%p92 bra 	BB48_116;

	mov.u32 	%r1993, 32;
	sub.s32 	%r1994, %r1993, %r167;
	shr.u32 	%r1995, %r4020, %r1994;
	shl.b32 	%r1996, %r4019, %r167;
	add.s32 	%r4019, %r1995, %r1996;
	ld.local.u32 	%r1997, [%rd28+-8];
	shr.u32 	%r1998, %r1997, %r1994;
	shl.b32 	%r1999, %r4020, %r167;
	add.s32 	%r4020, %r1998, %r1999;

BB48_116:
	shr.u32 	%r2000, %r4020, 30;
	shl.b32 	%r2001, %r4019, 2;
	add.s32 	%r4021, %r2000, %r2001;
	shl.b32 	%r173, %r4020, 2;
	shr.u32 	%r2002, %r4021, 31;
	shr.u32 	%r2003, %r4019, 30;
	add.s32 	%r174, %r2002, %r2003;
	setp.eq.s32	%p93, %r2002, 0;
	mov.u32 	%r4022, %r164;
	mov.u32 	%r4023, %r173;
	@%p93 bra 	BB48_118;

	not.b32 	%r2004, %r4021;
	neg.s32 	%r175, %r173;
	setp.eq.s32	%p94, %r173, 0;
	selp.u32	%r2005, 1, 0, %p94;
	add.s32 	%r4021, %r2005, %r2004;
	xor.b32  	%r177, %r164, -2147483648;
	mov.u32 	%r4022, %r177;
	mov.u32 	%r4023, %r175;

BB48_118:
	mov.u32 	%r179, %r4022;
	neg.s32 	%r2006, %r174;
	setp.eq.s32	%p95, %r164, 0;
	selp.b32	%r4026, %r174, %r2006, %p95;
	clz.b32 	%r4025, %r4021;
	setp.eq.s32	%p96, %r4025, 0;
	shl.b32 	%r2007, %r4021, %r4025;
	mov.u32 	%r2008, 32;
	sub.s32 	%r2009, %r2008, %r4025;
	shr.u32 	%r2010, %r4023, %r2009;
	add.s32 	%r2011, %r2010, %r2007;
	selp.b32	%r183, %r4021, %r2011, %p96;
	mov.u32 	%r2012, -921707870;
	mul.hi.u32 	%r4024, %r183, %r2012;
	setp.lt.s32	%p97, %r4024, 1;
	@%p97 bra 	BB48_120;

	mul.lo.s32 	%r2013, %r183, -921707870;
	shr.u32 	%r2014, %r2013, 31;
	shl.b32 	%r2015, %r4024, 1;
	add.s32 	%r4024, %r2014, %r2015;
	add.s32 	%r4025, %r4025, 1;

BB48_120:
	mov.u32 	%r2016, 126;
	sub.s32 	%r2017, %r2016, %r4025;
	shl.b32 	%r2018, %r2017, 23;
	add.s32 	%r2019, %r4024, 1;
	shr.u32 	%r2020, %r2019, 7;
	add.s32 	%r2021, %r2020, 1;
	shr.u32 	%r2022, %r2021, 1;
	add.s32 	%r2023, %r2022, %r2018;
	or.b32  	%r2024, %r2023, %r179;
	mov.b32 	 %f2253, %r2024;

BB48_121:
	mul.rn.f32 	%f115, %f2253, %f2253;
	and.b32  	%r190, %r4026, 1;
	setp.eq.s32	%p98, %r190, 0;
	@%p98 bra 	BB48_123;

	mov.f32 	%f1167, 0fBAB6061A;
	mov.f32 	%f1168, 0f37CCF5CE;
	fma.rn.f32 	%f2254, %f1168, %f115, %f1167;
	bra.uni 	BB48_124;

BB48_123:
	mov.f32 	%f1169, 0f3C08839E;
	mov.f32 	%f1170, 0fB94CA1F9;
	fma.rn.f32 	%f2254, %f1170, %f115, %f1169;

BB48_124:
	@%p98 bra 	BB48_126;

	mov.f32 	%f1171, 0f3D2AAAA5;
	fma.rn.f32 	%f1172, %f2254, %f115, %f1171;
	mov.f32 	%f1173, 0fBF000000;
	fma.rn.f32 	%f2255, %f1172, %f115, %f1173;
	bra.uni 	BB48_127;

BB48_126:
	mov.f32 	%f1174, 0fBE2AAAA3;
	fma.rn.f32 	%f1175, %f2254, %f115, %f1174;
	mov.f32 	%f1176, 0f00000000;
	fma.rn.f32 	%f2255, %f1175, %f115, %f1176;

BB48_127:
	fma.rn.f32 	%f2256, %f2255, %f2253, %f2253;
	@%p98 bra 	BB48_129;

	mul.rn.f32 	%f2232, %f2253, %f2253;
	mov.f32 	%f1177, 0f3F800000;
	fma.rn.f32 	%f2256, %f2255, %f2232, %f1177;

BB48_129:
	and.b32  	%r2025, %r4026, 2;
	setp.eq.s32	%p101, %r2025, 0;
	@%p101 bra 	BB48_131;

	mov.f32 	%f1178, 0f00000000;
	mov.f32 	%f1179, 0fBF800000;
	fma.rn.f32 	%f2256, %f2256, %f1179, %f1178;

BB48_131:
	mul.f32 	%f127, %f2252, %f2256;
	mov.f32 	%f2357, %f13;
	@%p63 bra 	BB48_133;

	mov.f32 	%f1180, 0f00000000;
	mul.rn.f32 	%f2357, %f13, %f1180;

BB48_133:
	mul.f32 	%f1181, %f2357, 0f3F22F983;
	cvt.rni.s32.f32	%r4036, %f1181;
	cvt.rn.f32.s32	%f1182, %r4036;
	neg.f32 	%f1183, %f1182;
	fma.rn.f32 	%f1185, %f1183, %f1069, %f2357;
	fma.rn.f32 	%f1187, %f1183, %f1071, %f1185;
	fma.rn.f32 	%f2257, %f1183, %f1073, %f1187;
	abs.f32 	%f1189, %f2357;
	setp.leu.f32	%p103, %f1189, 0f47CE4780;
	@%p103 bra 	BB48_143;

	mov.b32 	 %r192, %f2357;
	shr.u32 	%r193, %r192, 23;
	bfe.u32 	%r2028, %r192, 23, 8;
	add.s32 	%r2029, %r2028, -128;
	shl.b32 	%r2030, %r192, 8;
	or.b32  	%r194, %r2030, -2147483648;
	shr.u32 	%r195, %r2029, 5;
	mov.u32 	%r4028, 0;
	mov.u64 	%rd351, __cudart_i2opi_f;
	mov.u32 	%r4027, -6;
	mov.u64 	%rd483, %rd1;

BB48_135:
	.pragma "nounroll";
	ld.const.u32 	%r2033, [%rd351];
	// inline asm
	{
	mad.lo.cc.u32   %r2031, %r2033, %r194, %r4028;
	madc.hi.u32     %r4028, %r2033, %r194,  0;
	}
	// inline asm
	st.local.u32 	[%rd483], %r2031;
	add.s64 	%rd483, %rd483, 4;
	add.s64 	%rd351, %rd351, 4;
	add.s32 	%r4027, %r4027, 1;
	setp.ne.s32	%p104, %r4027, 0;
	@%p104 bra 	BB48_135;

	and.b32  	%r200, %r192, -2147483648;
	st.local.u32 	[%rd3], %r4028;
	mov.u32 	%r2036, 6;
	sub.s32 	%r2037, %r2036, %r195;
	mul.wide.s32 	%rd258, %r2037, 4;
	add.s64 	%rd33, %rd1, %rd258;
	ld.local.u32 	%r4029, [%rd33];
	ld.local.u32 	%r4030, [%rd33+-4];
	and.b32  	%r203, %r193, 31;
	setp.eq.s32	%p105, %r203, 0;
	@%p105 bra 	BB48_138;

	mov.u32 	%r2038, 32;
	sub.s32 	%r2039, %r2038, %r203;
	shr.u32 	%r2040, %r4030, %r2039;
	shl.b32 	%r2041, %r4029, %r203;
	add.s32 	%r4029, %r2040, %r2041;
	ld.local.u32 	%r2042, [%rd33+-8];
	shr.u32 	%r2043, %r2042, %r2039;
	shl.b32 	%r2044, %r4030, %r203;
	add.s32 	%r4030, %r2043, %r2044;

BB48_138:
	shr.u32 	%r2045, %r4030, 30;
	shl.b32 	%r2046, %r4029, 2;
	add.s32 	%r4031, %r2045, %r2046;
	shl.b32 	%r209, %r4030, 2;
	shr.u32 	%r2047, %r4031, 31;
	shr.u32 	%r2048, %r4029, 30;
	add.s32 	%r210, %r2047, %r2048;
	setp.eq.s32	%p106, %r2047, 0;
	mov.u32 	%r4032, %r200;
	mov.u32 	%r4033, %r209;
	@%p106 bra 	BB48_140;

	not.b32 	%r2049, %r4031;
	neg.s32 	%r211, %r209;
	setp.eq.s32	%p107, %r209, 0;
	selp.u32	%r2050, 1, 0, %p107;
	add.s32 	%r4031, %r2050, %r2049;
	xor.b32  	%r213, %r200, -2147483648;
	mov.u32 	%r4032, %r213;
	mov.u32 	%r4033, %r211;

BB48_140:
	mov.u32 	%r215, %r4032;
	neg.s32 	%r2051, %r210;
	setp.eq.s32	%p108, %r200, 0;
	selp.b32	%r4036, %r210, %r2051, %p108;
	clz.b32 	%r4035, %r4031;
	setp.eq.s32	%p109, %r4035, 0;
	shl.b32 	%r2052, %r4031, %r4035;
	mov.u32 	%r2053, 32;
	sub.s32 	%r2054, %r2053, %r4035;
	shr.u32 	%r2055, %r4033, %r2054;
	add.s32 	%r2056, %r2055, %r2052;
	selp.b32	%r219, %r4031, %r2056, %p109;
	mov.u32 	%r2057, -921707870;
	mul.hi.u32 	%r4034, %r219, %r2057;
	setp.lt.s32	%p110, %r4034, 1;
	@%p110 bra 	BB48_142;

	mul.lo.s32 	%r2058, %r219, -921707870;
	shr.u32 	%r2059, %r2058, 31;
	shl.b32 	%r2060, %r4034, 1;
	add.s32 	%r4034, %r2059, %r2060;
	add.s32 	%r4035, %r4035, 1;

BB48_142:
	mov.u32 	%r2061, 126;
	sub.s32 	%r2062, %r2061, %r4035;
	shl.b32 	%r2063, %r2062, 23;
	add.s32 	%r2064, %r4034, 1;
	shr.u32 	%r2065, %r2064, 7;
	add.s32 	%r2066, %r2065, 1;
	shr.u32 	%r2067, %r2066, 1;
	add.s32 	%r2068, %r2067, %r2063;
	or.b32  	%r2069, %r2068, %r215;
	mov.b32 	 %f2257, %r2069;

BB48_143:
	mul.rn.f32 	%f133, %f2257, %f2257;
	and.b32  	%r226, %r4036, 1;
	setp.eq.s32	%p111, %r226, 0;
	@%p111 bra 	BB48_145;

	mov.f32 	%f1190, 0fBAB6061A;
	mov.f32 	%f1191, 0f37CCF5CE;
	fma.rn.f32 	%f2258, %f1191, %f133, %f1190;
	bra.uni 	BB48_146;

BB48_145:
	mov.f32 	%f1192, 0f3C08839E;
	mov.f32 	%f1193, 0fB94CA1F9;
	fma.rn.f32 	%f2258, %f1193, %f133, %f1192;

BB48_146:
	@%p111 bra 	BB48_148;

	mov.f32 	%f1194, 0f3D2AAAA5;
	fma.rn.f32 	%f1195, %f2258, %f133, %f1194;
	mov.f32 	%f1196, 0fBF000000;
	fma.rn.f32 	%f2259, %f1195, %f133, %f1196;
	bra.uni 	BB48_149;

BB48_148:
	mov.f32 	%f1197, 0fBE2AAAA3;
	fma.rn.f32 	%f1198, %f2258, %f133, %f1197;
	mov.f32 	%f1199, 0f00000000;
	fma.rn.f32 	%f2259, %f1198, %f133, %f1199;

BB48_149:
	fma.rn.f32 	%f2260, %f2259, %f2257, %f2257;
	@%p111 bra 	BB48_151;

	mul.rn.f32 	%f2233, %f2257, %f2257;
	mov.f32 	%f1200, 0f3F800000;
	fma.rn.f32 	%f2260, %f2259, %f2233, %f1200;

BB48_151:
	and.b32  	%r2070, %r4036, 2;
	setp.eq.s32	%p114, %r2070, 0;
	@%p114 bra 	BB48_153;

	mov.f32 	%f1201, 0f00000000;
	mov.f32 	%f1202, 0fBF800000;
	fma.rn.f32 	%f2260, %f2260, %f1202, %f1201;

BB48_153:
	fma.rn.f32 	%f145, %f127, %f2260, %f92;
	mov.f32 	%f2383, %f34;
	@%p37 bra 	BB48_155;

	mov.f32 	%f1203, 0f00000000;
	mul.rn.f32 	%f2383, %f34, %f1203;

BB48_155:
	mul.f32 	%f1204, %f2383, 0f3F22F983;
	cvt.rni.s32.f32	%r4046, %f1204;
	cvt.rn.f32.s32	%f1205, %r4046;
	neg.f32 	%f1206, %f1205;
	fma.rn.f32 	%f1208, %f1206, %f1069, %f2383;
	fma.rn.f32 	%f1210, %f1206, %f1071, %f1208;
	fma.rn.f32 	%f2261, %f1206, %f1073, %f1210;
	abs.f32 	%f1212, %f2383;
	setp.leu.f32	%p116, %f1212, 0f47CE4780;
	@%p116 bra 	BB48_165;

	mov.b32 	 %r228, %f2383;
	shr.u32 	%r229, %r228, 23;
	bfe.u32 	%r2073, %r228, 23, 8;
	add.s32 	%r2074, %r2073, -128;
	shl.b32 	%r2075, %r228, 8;
	or.b32  	%r230, %r2075, -2147483648;
	shr.u32 	%r231, %r2074, 5;
	mov.u32 	%r4038, 0;
	mov.u64 	%rd352, __cudart_i2opi_f;
	mov.u32 	%r4037, -6;
	mov.u64 	%rd482, %rd1;

BB48_157:
	.pragma "nounroll";
	ld.const.u32 	%r2078, [%rd352];
	// inline asm
	{
	mad.lo.cc.u32   %r2076, %r2078, %r230, %r4038;
	madc.hi.u32     %r4038, %r2078, %r230,  0;
	}
	// inline asm
	st.local.u32 	[%rd482], %r2076;
	add.s64 	%rd482, %rd482, 4;
	add.s64 	%rd352, %rd352, 4;
	add.s32 	%r4037, %r4037, 1;
	setp.ne.s32	%p117, %r4037, 0;
	@%p117 bra 	BB48_157;

	and.b32  	%r236, %r228, -2147483648;
	st.local.u32 	[%rd3], %r4038;
	mov.u32 	%r2081, 6;
	sub.s32 	%r2082, %r2081, %r231;
	mul.wide.s32 	%rd260, %r2082, 4;
	add.s64 	%rd38, %rd1, %rd260;
	ld.local.u32 	%r4039, [%rd38];
	ld.local.u32 	%r4040, [%rd38+-4];
	and.b32  	%r239, %r229, 31;
	setp.eq.s32	%p118, %r239, 0;
	@%p118 bra 	BB48_160;

	mov.u32 	%r2083, 32;
	sub.s32 	%r2084, %r2083, %r239;
	shr.u32 	%r2085, %r4040, %r2084;
	shl.b32 	%r2086, %r4039, %r239;
	add.s32 	%r4039, %r2085, %r2086;
	ld.local.u32 	%r2087, [%rd38+-8];
	shr.u32 	%r2088, %r2087, %r2084;
	shl.b32 	%r2089, %r4040, %r239;
	add.s32 	%r4040, %r2088, %r2089;

BB48_160:
	shr.u32 	%r2090, %r4040, 30;
	shl.b32 	%r2091, %r4039, 2;
	add.s32 	%r4041, %r2090, %r2091;
	shl.b32 	%r245, %r4040, 2;
	shr.u32 	%r2092, %r4041, 31;
	shr.u32 	%r2093, %r4039, 30;
	add.s32 	%r246, %r2092, %r2093;
	setp.eq.s32	%p119, %r2092, 0;
	mov.u32 	%r4042, %r236;
	mov.u32 	%r4043, %r245;
	@%p119 bra 	BB48_162;

	not.b32 	%r2094, %r4041;
	neg.s32 	%r247, %r245;
	setp.eq.s32	%p120, %r245, 0;
	selp.u32	%r2095, 1, 0, %p120;
	add.s32 	%r4041, %r2095, %r2094;
	xor.b32  	%r249, %r236, -2147483648;
	mov.u32 	%r4042, %r249;
	mov.u32 	%r4043, %r247;

BB48_162:
	mov.u32 	%r251, %r4042;
	neg.s32 	%r2096, %r246;
	setp.eq.s32	%p121, %r236, 0;
	selp.b32	%r4046, %r246, %r2096, %p121;
	clz.b32 	%r4045, %r4041;
	setp.eq.s32	%p122, %r4045, 0;
	shl.b32 	%r2097, %r4041, %r4045;
	mov.u32 	%r2098, 32;
	sub.s32 	%r2099, %r2098, %r4045;
	shr.u32 	%r2100, %r4043, %r2099;
	add.s32 	%r2101, %r2100, %r2097;
	selp.b32	%r255, %r4041, %r2101, %p122;
	mov.u32 	%r2102, -921707870;
	mul.hi.u32 	%r4044, %r255, %r2102;
	setp.lt.s32	%p123, %r4044, 1;
	@%p123 bra 	BB48_164;

	mul.lo.s32 	%r2103, %r255, -921707870;
	shr.u32 	%r2104, %r2103, 31;
	shl.b32 	%r2105, %r4044, 1;
	add.s32 	%r4044, %r2104, %r2105;
	add.s32 	%r4045, %r4045, 1;

BB48_164:
	mov.u32 	%r2106, 126;
	sub.s32 	%r2107, %r2106, %r4045;
	shl.b32 	%r2108, %r2107, 23;
	add.s32 	%r2109, %r4044, 1;
	shr.u32 	%r2110, %r2109, 7;
	add.s32 	%r2111, %r2110, 1;
	shr.u32 	%r2112, %r2111, 1;
	add.s32 	%r2113, %r2112, %r2108;
	or.b32  	%r2114, %r2113, %r251;
	mov.b32 	 %f2261, %r2114;

BB48_165:
	mul.rn.f32 	%f151, %f2261, %f2261;
	and.b32  	%r262, %r4046, 1;
	setp.eq.s32	%p124, %r262, 0;
	@%p124 bra 	BB48_167;

	mov.f32 	%f1213, 0fBAB6061A;
	mov.f32 	%f1214, 0f37CCF5CE;
	fma.rn.f32 	%f2262, %f1214, %f151, %f1213;
	bra.uni 	BB48_168;

BB48_167:
	mov.f32 	%f1215, 0f3C08839E;
	mov.f32 	%f1216, 0fB94CA1F9;
	fma.rn.f32 	%f2262, %f1216, %f151, %f1215;

BB48_168:
	@%p124 bra 	BB48_170;

	mov.f32 	%f1217, 0f3D2AAAA5;
	fma.rn.f32 	%f1218, %f2262, %f151, %f1217;
	mov.f32 	%f1219, 0fBF000000;
	fma.rn.f32 	%f2263, %f1218, %f151, %f1219;
	bra.uni 	BB48_171;

BB48_170:
	mov.f32 	%f1220, 0fBE2AAAA3;
	fma.rn.f32 	%f1221, %f2262, %f151, %f1220;
	mov.f32 	%f1222, 0f00000000;
	fma.rn.f32 	%f2263, %f1221, %f151, %f1222;

BB48_171:
	fma.rn.f32 	%f2264, %f2263, %f2261, %f2261;
	@%p124 bra 	BB48_173;

	mov.f32 	%f1223, 0f3F800000;
	fma.rn.f32 	%f2264, %f2263, %f151, %f1223;

BB48_173:
	and.b32  	%r2115, %r4046, 2;
	setp.eq.s32	%p127, %r2115, 0;
	@%p127 bra 	BB48_175;

	mov.f32 	%f1224, 0f00000000;
	mov.f32 	%f1225, 0fBF800000;
	fma.rn.f32 	%f2264, %f2264, %f1225, %f1224;

BB48_175:
	mov.f32 	%f2337, %f36;
	@%p50 bra 	BB48_177;

	mov.f32 	%f1226, 0f00000000;
	mul.rn.f32 	%f2337, %f36, %f1226;

BB48_177:
	mul.f32 	%f1227, %f2337, 0f3F22F983;
	cvt.rni.s32.f32	%r4056, %f1227;
	cvt.rn.f32.s32	%f1228, %r4056;
	neg.f32 	%f1229, %f1228;
	fma.rn.f32 	%f1231, %f1229, %f1069, %f2337;
	fma.rn.f32 	%f1233, %f1229, %f1071, %f1231;
	fma.rn.f32 	%f2265, %f1229, %f1073, %f1233;
	abs.f32 	%f1235, %f2337;
	setp.leu.f32	%p129, %f1235, 0f47CE4780;
	@%p129 bra 	BB48_187;

	mov.b32 	 %r264, %f2337;
	shr.u32 	%r265, %r264, 23;
	bfe.u32 	%r2118, %r264, 23, 8;
	add.s32 	%r2119, %r2118, -128;
	shl.b32 	%r2120, %r264, 8;
	or.b32  	%r266, %r2120, -2147483648;
	shr.u32 	%r267, %r2119, 5;
	mov.u32 	%r4048, 0;
	mov.u64 	%rd353, __cudart_i2opi_f;
	mov.u32 	%r4047, -6;
	mov.u64 	%rd481, %rd1;

BB48_179:
	.pragma "nounroll";
	ld.const.u32 	%r2123, [%rd353];
	// inline asm
	{
	mad.lo.cc.u32   %r2121, %r2123, %r266, %r4048;
	madc.hi.u32     %r4048, %r2123, %r266,  0;
	}
	// inline asm
	st.local.u32 	[%rd481], %r2121;
	add.s64 	%rd481, %rd481, 4;
	add.s64 	%rd353, %rd353, 4;
	add.s32 	%r4047, %r4047, 1;
	setp.ne.s32	%p130, %r4047, 0;
	@%p130 bra 	BB48_179;

	and.b32  	%r272, %r264, -2147483648;
	st.local.u32 	[%rd3], %r4048;
	mov.u32 	%r2126, 6;
	sub.s32 	%r2127, %r2126, %r267;
	mul.wide.s32 	%rd262, %r2127, 4;
	add.s64 	%rd43, %rd1, %rd262;
	ld.local.u32 	%r4049, [%rd43];
	ld.local.u32 	%r4050, [%rd43+-4];
	and.b32  	%r275, %r265, 31;
	setp.eq.s32	%p131, %r275, 0;
	@%p131 bra 	BB48_182;

	mov.u32 	%r2128, 32;
	sub.s32 	%r2129, %r2128, %r275;
	shr.u32 	%r2130, %r4050, %r2129;
	shl.b32 	%r2131, %r4049, %r275;
	add.s32 	%r4049, %r2130, %r2131;
	ld.local.u32 	%r2132, [%rd43+-8];
	shr.u32 	%r2133, %r2132, %r2129;
	shl.b32 	%r2134, %r4050, %r275;
	add.s32 	%r4050, %r2133, %r2134;

BB48_182:
	shr.u32 	%r2135, %r4050, 30;
	shl.b32 	%r2136, %r4049, 2;
	add.s32 	%r4051, %r2135, %r2136;
	shl.b32 	%r281, %r4050, 2;
	shr.u32 	%r2137, %r4051, 31;
	shr.u32 	%r2138, %r4049, 30;
	add.s32 	%r282, %r2137, %r2138;
	setp.eq.s32	%p132, %r2137, 0;
	mov.u32 	%r4052, %r272;
	mov.u32 	%r4053, %r281;
	@%p132 bra 	BB48_184;

	not.b32 	%r2139, %r4051;
	neg.s32 	%r283, %r281;
	setp.eq.s32	%p133, %r281, 0;
	selp.u32	%r2140, 1, 0, %p133;
	add.s32 	%r4051, %r2140, %r2139;
	xor.b32  	%r285, %r272, -2147483648;
	mov.u32 	%r4052, %r285;
	mov.u32 	%r4053, %r283;

BB48_184:
	mov.u32 	%r287, %r4052;
	neg.s32 	%r2141, %r282;
	setp.eq.s32	%p134, %r272, 0;
	selp.b32	%r4056, %r282, %r2141, %p134;
	clz.b32 	%r4055, %r4051;
	setp.eq.s32	%p135, %r4055, 0;
	shl.b32 	%r2142, %r4051, %r4055;
	mov.u32 	%r2143, 32;
	sub.s32 	%r2144, %r2143, %r4055;
	shr.u32 	%r2145, %r4053, %r2144;
	add.s32 	%r2146, %r2145, %r2142;
	selp.b32	%r291, %r4051, %r2146, %p135;
	mov.u32 	%r2147, -921707870;
	mul.hi.u32 	%r4054, %r291, %r2147;
	setp.lt.s32	%p136, %r4054, 1;
	@%p136 bra 	BB48_186;

	mul.lo.s32 	%r2148, %r291, -921707870;
	shr.u32 	%r2149, %r2148, 31;
	shl.b32 	%r2150, %r4054, 1;
	add.s32 	%r4054, %r2149, %r2150;
	add.s32 	%r4055, %r4055, 1;

BB48_186:
	mov.u32 	%r2151, 126;
	sub.s32 	%r2152, %r2151, %r4055;
	shl.b32 	%r2153, %r2152, 23;
	add.s32 	%r2154, %r4054, 1;
	shr.u32 	%r2155, %r2154, 7;
	add.s32 	%r2156, %r2155, 1;
	shr.u32 	%r2157, %r2156, 1;
	add.s32 	%r2158, %r2157, %r2153;
	or.b32  	%r2159, %r2158, %r287;
	mov.b32 	 %f2265, %r2159;

BB48_187:
	mul.rn.f32 	%f168, %f2265, %f2265;
	add.s32 	%r298, %r4056, 1;
	and.b32  	%r299, %r298, 1;
	setp.eq.s32	%p137, %r299, 0;
	@%p137 bra 	BB48_189;

	mov.f32 	%f1236, 0fBAB6061A;
	mov.f32 	%f1237, 0f37CCF5CE;
	fma.rn.f32 	%f2266, %f1237, %f168, %f1236;
	bra.uni 	BB48_190;

BB48_189:
	mov.f32 	%f1238, 0f3C08839E;
	mov.f32 	%f1239, 0fB94CA1F9;
	fma.rn.f32 	%f2266, %f1239, %f168, %f1238;

BB48_190:
	@%p137 bra 	BB48_192;

	mov.f32 	%f1240, 0f3D2AAAA5;
	fma.rn.f32 	%f1241, %f2266, %f168, %f1240;
	mov.f32 	%f1242, 0fBF000000;
	fma.rn.f32 	%f2267, %f1241, %f168, %f1242;
	bra.uni 	BB48_193;

BB48_192:
	mov.f32 	%f1243, 0fBE2AAAA3;
	fma.rn.f32 	%f1244, %f2266, %f168, %f1243;
	mov.f32 	%f1245, 0f00000000;
	fma.rn.f32 	%f2267, %f1244, %f168, %f1245;

BB48_193:
	fma.rn.f32 	%f2268, %f2267, %f2265, %f2265;
	@%p137 bra 	BB48_195;

	mov.f32 	%f1246, 0f3F800000;
	fma.rn.f32 	%f2268, %f2267, %f168, %f1246;

BB48_195:
	and.b32  	%r2160, %r298, 2;
	setp.eq.s32	%p140, %r2160, 0;
	@%p140 bra 	BB48_197;

	mov.f32 	%f1247, 0f00000000;
	mov.f32 	%f1248, 0fBF800000;
	fma.rn.f32 	%f2268, %f2268, %f1248, %f1247;

BB48_197:
	mul.f32 	%f180, %f2264, %f2268;
	mov.f32 	%f2356, %f13;
	@%p63 bra 	BB48_199;

	mov.f32 	%f1249, 0f00000000;
	mul.rn.f32 	%f2356, %f13, %f1249;

BB48_199:
	mul.f32 	%f1250, %f2356, 0f3F22F983;
	cvt.rni.s32.f32	%r4066, %f1250;
	cvt.rn.f32.s32	%f1251, %r4066;
	neg.f32 	%f1252, %f1251;
	fma.rn.f32 	%f1254, %f1252, %f1069, %f2356;
	fma.rn.f32 	%f1256, %f1252, %f1071, %f1254;
	fma.rn.f32 	%f2269, %f1252, %f1073, %f1256;
	abs.f32 	%f1258, %f2356;
	setp.leu.f32	%p142, %f1258, 0f47CE4780;
	@%p142 bra 	BB48_209;

	mov.b32 	 %r301, %f2356;
	shr.u32 	%r302, %r301, 23;
	bfe.u32 	%r2163, %r301, 23, 8;
	add.s32 	%r2164, %r2163, -128;
	shl.b32 	%r2165, %r301, 8;
	or.b32  	%r303, %r2165, -2147483648;
	shr.u32 	%r304, %r2164, 5;
	mov.u32 	%r4058, 0;
	mov.u64 	%rd354, __cudart_i2opi_f;
	mov.u32 	%r4057, -6;
	mov.u64 	%rd480, %rd1;

BB48_201:
	.pragma "nounroll";
	ld.const.u32 	%r2168, [%rd354];
	// inline asm
	{
	mad.lo.cc.u32   %r2166, %r2168, %r303, %r4058;
	madc.hi.u32     %r4058, %r2168, %r303,  0;
	}
	// inline asm
	st.local.u32 	[%rd480], %r2166;
	add.s64 	%rd480, %rd480, 4;
	add.s64 	%rd354, %rd354, 4;
	add.s32 	%r4057, %r4057, 1;
	setp.ne.s32	%p143, %r4057, 0;
	@%p143 bra 	BB48_201;

	and.b32  	%r309, %r301, -2147483648;
	st.local.u32 	[%rd3], %r4058;
	mov.u32 	%r2171, 6;
	sub.s32 	%r2172, %r2171, %r304;
	mul.wide.s32 	%rd264, %r2172, 4;
	add.s64 	%rd48, %rd1, %rd264;
	ld.local.u32 	%r4059, [%rd48];
	ld.local.u32 	%r4060, [%rd48+-4];
	and.b32  	%r312, %r302, 31;
	setp.eq.s32	%p144, %r312, 0;
	@%p144 bra 	BB48_204;

	mov.u32 	%r2173, 32;
	sub.s32 	%r2174, %r2173, %r312;
	shr.u32 	%r2175, %r4060, %r2174;
	shl.b32 	%r2176, %r4059, %r312;
	add.s32 	%r4059, %r2175, %r2176;
	ld.local.u32 	%r2177, [%rd48+-8];
	shr.u32 	%r2178, %r2177, %r2174;
	shl.b32 	%r2179, %r4060, %r312;
	add.s32 	%r4060, %r2178, %r2179;

BB48_204:
	shr.u32 	%r2180, %r4060, 30;
	shl.b32 	%r2181, %r4059, 2;
	add.s32 	%r4061, %r2180, %r2181;
	shl.b32 	%r318, %r4060, 2;
	shr.u32 	%r2182, %r4061, 31;
	shr.u32 	%r2183, %r4059, 30;
	add.s32 	%r319, %r2182, %r2183;
	setp.eq.s32	%p145, %r2182, 0;
	mov.u32 	%r4062, %r309;
	mov.u32 	%r4063, %r318;
	@%p145 bra 	BB48_206;

	not.b32 	%r2184, %r4061;
	neg.s32 	%r320, %r318;
	setp.eq.s32	%p146, %r318, 0;
	selp.u32	%r2185, 1, 0, %p146;
	add.s32 	%r4061, %r2185, %r2184;
	xor.b32  	%r322, %r309, -2147483648;
	mov.u32 	%r4062, %r322;
	mov.u32 	%r4063, %r320;

BB48_206:
	mov.u32 	%r324, %r4062;
	neg.s32 	%r2186, %r319;
	setp.eq.s32	%p147, %r309, 0;
	selp.b32	%r4066, %r319, %r2186, %p147;
	clz.b32 	%r4065, %r4061;
	setp.eq.s32	%p148, %r4065, 0;
	shl.b32 	%r2187, %r4061, %r4065;
	mov.u32 	%r2188, 32;
	sub.s32 	%r2189, %r2188, %r4065;
	shr.u32 	%r2190, %r4063, %r2189;
	add.s32 	%r2191, %r2190, %r2187;
	selp.b32	%r328, %r4061, %r2191, %p148;
	mov.u32 	%r2192, -921707870;
	mul.hi.u32 	%r4064, %r328, %r2192;
	setp.lt.s32	%p149, %r4064, 1;
	@%p149 bra 	BB48_208;

	mul.lo.s32 	%r2193, %r328, -921707870;
	shr.u32 	%r2194, %r2193, 31;
	shl.b32 	%r2195, %r4064, 1;
	add.s32 	%r4064, %r2194, %r2195;
	add.s32 	%r4065, %r4065, 1;

BB48_208:
	mov.u32 	%r2196, 126;
	sub.s32 	%r2197, %r2196, %r4065;
	shl.b32 	%r2198, %r2197, 23;
	add.s32 	%r2199, %r4064, 1;
	shr.u32 	%r2200, %r2199, 7;
	add.s32 	%r2201, %r2200, 1;
	shr.u32 	%r2202, %r2201, 1;
	add.s32 	%r2203, %r2202, %r2198;
	or.b32  	%r2204, %r2203, %r324;
	mov.b32 	 %f2269, %r2204;

BB48_209:
	mul.rn.f32 	%f186, %f2269, %f2269;
	add.s32 	%r335, %r4066, 1;
	and.b32  	%r336, %r335, 1;
	setp.eq.s32	%p150, %r336, 0;
	@%p150 bra 	BB48_211;

	mov.f32 	%f1259, 0fBAB6061A;
	mov.f32 	%f1260, 0f37CCF5CE;
	fma.rn.f32 	%f2270, %f1260, %f186, %f1259;
	bra.uni 	BB48_212;

BB48_211:
	mov.f32 	%f1261, 0f3C08839E;
	mov.f32 	%f1262, 0fB94CA1F9;
	fma.rn.f32 	%f2270, %f1262, %f186, %f1261;

BB48_212:
	@%p150 bra 	BB48_214;

	mov.f32 	%f1263, 0f3D2AAAA5;
	fma.rn.f32 	%f1264, %f2270, %f186, %f1263;
	mov.f32 	%f1265, 0fBF000000;
	fma.rn.f32 	%f2271, %f1264, %f186, %f1265;
	bra.uni 	BB48_215;

BB48_214:
	mov.f32 	%f1266, 0fBE2AAAA3;
	fma.rn.f32 	%f1267, %f2270, %f186, %f1266;
	mov.f32 	%f1268, 0f00000000;
	fma.rn.f32 	%f2271, %f1267, %f186, %f1268;

BB48_215:
	fma.rn.f32 	%f2272, %f2271, %f2269, %f2269;
	@%p150 bra 	BB48_217;

	mov.f32 	%f1269, 0f3F800000;
	fma.rn.f32 	%f2272, %f2271, %f186, %f1269;

BB48_217:
	and.b32  	%r2205, %r335, 2;
	setp.eq.s32	%p153, %r2205, 0;
	@%p153 bra 	BB48_219;

	mov.f32 	%f1270, 0f00000000;
	mov.f32 	%f1271, 0fBF800000;
	fma.rn.f32 	%f2272, %f2272, %f1271, %f1270;

BB48_219:
	mul.f32 	%f198, %f180, %f2272;
	mov.f32 	%f2382, %f34;
	@%p37 bra 	BB48_221;

	mov.f32 	%f1272, 0f00000000;
	mul.rn.f32 	%f2382, %f34, %f1272;

BB48_221:
	mul.f32 	%f1273, %f2382, 0f3F22F983;
	cvt.rni.s32.f32	%r4076, %f1273;
	cvt.rn.f32.s32	%f1274, %r4076;
	neg.f32 	%f1275, %f1274;
	fma.rn.f32 	%f1277, %f1275, %f1069, %f2382;
	fma.rn.f32 	%f1279, %f1275, %f1071, %f1277;
	fma.rn.f32 	%f2273, %f1275, %f1073, %f1279;
	abs.f32 	%f1281, %f2382;
	setp.leu.f32	%p155, %f1281, 0f47CE4780;
	@%p155 bra 	BB48_231;

	mov.b32 	 %r338, %f2382;
	shr.u32 	%r339, %r338, 23;
	bfe.u32 	%r2208, %r338, 23, 8;
	add.s32 	%r2209, %r2208, -128;
	shl.b32 	%r2210, %r338, 8;
	or.b32  	%r340, %r2210, -2147483648;
	shr.u32 	%r341, %r2209, 5;
	mov.u32 	%r4068, 0;
	mov.u64 	%rd355, __cudart_i2opi_f;
	mov.u32 	%r4067, -6;
	mov.u64 	%rd479, %rd1;

BB48_223:
	.pragma "nounroll";
	ld.const.u32 	%r2213, [%rd355];
	// inline asm
	{
	mad.lo.cc.u32   %r2211, %r2213, %r340, %r4068;
	madc.hi.u32     %r4068, %r2213, %r340,  0;
	}
	// inline asm
	st.local.u32 	[%rd479], %r2211;
	add.s64 	%rd479, %rd479, 4;
	add.s64 	%rd355, %rd355, 4;
	add.s32 	%r4067, %r4067, 1;
	setp.ne.s32	%p156, %r4067, 0;
	@%p156 bra 	BB48_223;

	and.b32  	%r346, %r338, -2147483648;
	st.local.u32 	[%rd3], %r4068;
	mov.u32 	%r2216, 6;
	sub.s32 	%r2217, %r2216, %r341;
	mul.wide.s32 	%rd266, %r2217, 4;
	add.s64 	%rd53, %rd1, %rd266;
	ld.local.u32 	%r4069, [%rd53];
	ld.local.u32 	%r4070, [%rd53+-4];
	and.b32  	%r349, %r339, 31;
	setp.eq.s32	%p157, %r349, 0;
	@%p157 bra 	BB48_226;

	mov.u32 	%r2218, 32;
	sub.s32 	%r2219, %r2218, %r349;
	shr.u32 	%r2220, %r4070, %r2219;
	shl.b32 	%r2221, %r4069, %r349;
	add.s32 	%r4069, %r2220, %r2221;
	ld.local.u32 	%r2222, [%rd53+-8];
	shr.u32 	%r2223, %r2222, %r2219;
	shl.b32 	%r2224, %r4070, %r349;
	add.s32 	%r4070, %r2223, %r2224;

BB48_226:
	shr.u32 	%r2225, %r4070, 30;
	shl.b32 	%r2226, %r4069, 2;
	add.s32 	%r4071, %r2225, %r2226;
	shl.b32 	%r355, %r4070, 2;
	shr.u32 	%r2227, %r4071, 31;
	shr.u32 	%r2228, %r4069, 30;
	add.s32 	%r356, %r2227, %r2228;
	setp.eq.s32	%p158, %r2227, 0;
	mov.u32 	%r4072, %r346;
	mov.u32 	%r4073, %r355;
	@%p158 bra 	BB48_228;

	not.b32 	%r2229, %r4071;
	neg.s32 	%r357, %r355;
	setp.eq.s32	%p159, %r355, 0;
	selp.u32	%r2230, 1, 0, %p159;
	add.s32 	%r4071, %r2230, %r2229;
	xor.b32  	%r359, %r346, -2147483648;
	mov.u32 	%r4072, %r359;
	mov.u32 	%r4073, %r357;

BB48_228:
	mov.u32 	%r361, %r4072;
	neg.s32 	%r2231, %r356;
	setp.eq.s32	%p160, %r346, 0;
	selp.b32	%r4076, %r356, %r2231, %p160;
	clz.b32 	%r4075, %r4071;
	setp.eq.s32	%p161, %r4075, 0;
	shl.b32 	%r2232, %r4071, %r4075;
	mov.u32 	%r2233, 32;
	sub.s32 	%r2234, %r2233, %r4075;
	shr.u32 	%r2235, %r4073, %r2234;
	add.s32 	%r2236, %r2235, %r2232;
	selp.b32	%r365, %r4071, %r2236, %p161;
	mov.u32 	%r2237, -921707870;
	mul.hi.u32 	%r4074, %r365, %r2237;
	setp.lt.s32	%p162, %r4074, 1;
	@%p162 bra 	BB48_230;

	mul.lo.s32 	%r2238, %r365, -921707870;
	shr.u32 	%r2239, %r2238, 31;
	shl.b32 	%r2240, %r4074, 1;
	add.s32 	%r4074, %r2239, %r2240;
	add.s32 	%r4075, %r4075, 1;

BB48_230:
	mov.u32 	%r2241, 126;
	sub.s32 	%r2242, %r2241, %r4075;
	shl.b32 	%r2243, %r2242, 23;
	add.s32 	%r2244, %r4074, 1;
	shr.u32 	%r2245, %r2244, 7;
	add.s32 	%r2246, %r2245, 1;
	shr.u32 	%r2247, %r2246, 1;
	add.s32 	%r2248, %r2247, %r2243;
	or.b32  	%r2249, %r2248, %r361;
	mov.b32 	 %f2273, %r2249;

BB48_231:
	mul.rn.f32 	%f204, %f2273, %f2273;
	add.s32 	%r372, %r4076, 1;
	and.b32  	%r373, %r372, 1;
	setp.eq.s32	%p163, %r373, 0;
	@%p163 bra 	BB48_233;

	mov.f32 	%f1282, 0fBAB6061A;
	mov.f32 	%f1283, 0f37CCF5CE;
	fma.rn.f32 	%f2274, %f1283, %f204, %f1282;
	bra.uni 	BB48_234;

BB48_233:
	mov.f32 	%f1284, 0f3C08839E;
	mov.f32 	%f1285, 0fB94CA1F9;
	fma.rn.f32 	%f2274, %f1285, %f204, %f1284;

BB48_234:
	@%p163 bra 	BB48_236;

	mov.f32 	%f1286, 0f3D2AAAA5;
	fma.rn.f32 	%f1287, %f2274, %f204, %f1286;
	mov.f32 	%f1288, 0fBF000000;
	fma.rn.f32 	%f2275, %f1287, %f204, %f1288;
	bra.uni 	BB48_237;

BB48_236:
	mov.f32 	%f1289, 0fBE2AAAA3;
	fma.rn.f32 	%f1290, %f2274, %f204, %f1289;
	mov.f32 	%f1291, 0f00000000;
	fma.rn.f32 	%f2275, %f1290, %f204, %f1291;

BB48_237:
	fma.rn.f32 	%f2276, %f2275, %f2273, %f2273;
	@%p163 bra 	BB48_239;

	mov.f32 	%f1292, 0f3F800000;
	fma.rn.f32 	%f2276, %f2275, %f204, %f1292;

BB48_239:
	and.b32  	%r2250, %r372, 2;
	setp.eq.s32	%p166, %r2250, 0;
	@%p166 bra 	BB48_241;

	mov.f32 	%f1293, 0f00000000;
	mov.f32 	%f1294, 0fBF800000;
	fma.rn.f32 	%f2276, %f2276, %f1294, %f1293;

BB48_241:
	mov.f32 	%f2336, %f36;
	@%p50 bra 	BB48_243;

	mov.f32 	%f1295, 0f00000000;
	mul.rn.f32 	%f2336, %f36, %f1295;

BB48_243:
	mul.f32 	%f1296, %f2336, 0f3F22F983;
	cvt.rni.s32.f32	%r4086, %f1296;
	cvt.rn.f32.s32	%f1297, %r4086;
	neg.f32 	%f1298, %f1297;
	fma.rn.f32 	%f1300, %f1298, %f1069, %f2336;
	fma.rn.f32 	%f1302, %f1298, %f1071, %f1300;
	fma.rn.f32 	%f2277, %f1298, %f1073, %f1302;
	abs.f32 	%f1304, %f2336;
	setp.leu.f32	%p168, %f1304, 0f47CE4780;
	@%p168 bra 	BB48_253;

	mov.b32 	 %r375, %f2336;
	shr.u32 	%r376, %r375, 23;
	bfe.u32 	%r2253, %r375, 23, 8;
	add.s32 	%r2254, %r2253, -128;
	shl.b32 	%r2255, %r375, 8;
	or.b32  	%r377, %r2255, -2147483648;
	shr.u32 	%r378, %r2254, 5;
	mov.u32 	%r4078, 0;
	mov.u64 	%rd356, __cudart_i2opi_f;
	mov.u32 	%r4077, -6;
	mov.u64 	%rd478, %rd1;

BB48_245:
	.pragma "nounroll";
	ld.const.u32 	%r2258, [%rd356];
	// inline asm
	{
	mad.lo.cc.u32   %r2256, %r2258, %r377, %r4078;
	madc.hi.u32     %r4078, %r2258, %r377,  0;
	}
	// inline asm
	st.local.u32 	[%rd478], %r2256;
	add.s64 	%rd478, %rd478, 4;
	add.s64 	%rd356, %rd356, 4;
	add.s32 	%r4077, %r4077, 1;
	setp.ne.s32	%p169, %r4077, 0;
	@%p169 bra 	BB48_245;

	and.b32  	%r383, %r375, -2147483648;
	st.local.u32 	[%rd3], %r4078;
	mov.u32 	%r2261, 6;
	sub.s32 	%r2262, %r2261, %r378;
	mul.wide.s32 	%rd268, %r2262, 4;
	add.s64 	%rd58, %rd1, %rd268;
	ld.local.u32 	%r4079, [%rd58];
	ld.local.u32 	%r4080, [%rd58+-4];
	and.b32  	%r386, %r376, 31;
	setp.eq.s32	%p170, %r386, 0;
	@%p170 bra 	BB48_248;

	mov.u32 	%r2263, 32;
	sub.s32 	%r2264, %r2263, %r386;
	shr.u32 	%r2265, %r4080, %r2264;
	shl.b32 	%r2266, %r4079, %r386;
	add.s32 	%r4079, %r2265, %r2266;
	ld.local.u32 	%r2267, [%rd58+-8];
	shr.u32 	%r2268, %r2267, %r2264;
	shl.b32 	%r2269, %r4080, %r386;
	add.s32 	%r4080, %r2268, %r2269;

BB48_248:
	shr.u32 	%r2270, %r4080, 30;
	shl.b32 	%r2271, %r4079, 2;
	add.s32 	%r4081, %r2270, %r2271;
	shl.b32 	%r392, %r4080, 2;
	shr.u32 	%r2272, %r4081, 31;
	shr.u32 	%r2273, %r4079, 30;
	add.s32 	%r393, %r2272, %r2273;
	setp.eq.s32	%p171, %r2272, 0;
	mov.u32 	%r4082, %r383;
	mov.u32 	%r4083, %r392;
	@%p171 bra 	BB48_250;

	not.b32 	%r2274, %r4081;
	neg.s32 	%r394, %r392;
	setp.eq.s32	%p172, %r392, 0;
	selp.u32	%r2275, 1, 0, %p172;
	add.s32 	%r4081, %r2275, %r2274;
	xor.b32  	%r396, %r383, -2147483648;
	mov.u32 	%r4082, %r396;
	mov.u32 	%r4083, %r394;

BB48_250:
	mov.u32 	%r398, %r4082;
	neg.s32 	%r2276, %r393;
	setp.eq.s32	%p173, %r383, 0;
	selp.b32	%r4086, %r393, %r2276, %p173;
	clz.b32 	%r4085, %r4081;
	setp.eq.s32	%p174, %r4085, 0;
	shl.b32 	%r2277, %r4081, %r4085;
	mov.u32 	%r2278, 32;
	sub.s32 	%r2279, %r2278, %r4085;
	shr.u32 	%r2280, %r4083, %r2279;
	add.s32 	%r2281, %r2280, %r2277;
	selp.b32	%r402, %r4081, %r2281, %p174;
	mov.u32 	%r2282, -921707870;
	mul.hi.u32 	%r4084, %r402, %r2282;
	setp.lt.s32	%p175, %r4084, 1;
	@%p175 bra 	BB48_252;

	mul.lo.s32 	%r2283, %r402, -921707870;
	shr.u32 	%r2284, %r2283, 31;
	shl.b32 	%r2285, %r4084, 1;
	add.s32 	%r4084, %r2284, %r2285;
	add.s32 	%r4085, %r4085, 1;

BB48_252:
	mov.u32 	%r2286, 126;
	sub.s32 	%r2287, %r2286, %r4085;
	shl.b32 	%r2288, %r2287, 23;
	add.s32 	%r2289, %r4084, 1;
	shr.u32 	%r2290, %r2289, 7;
	add.s32 	%r2291, %r2290, 1;
	shr.u32 	%r2292, %r2291, 1;
	add.s32 	%r2293, %r2292, %r2288;
	or.b32  	%r2294, %r2293, %r398;
	mov.b32 	 %f2277, %r2294;

BB48_253:
	mul.rn.f32 	%f221, %f2277, %f2277;
	and.b32  	%r409, %r4086, 1;
	setp.eq.s32	%p176, %r409, 0;
	@%p176 bra 	BB48_255;

	mov.f32 	%f1305, 0fBAB6061A;
	mov.f32 	%f1306, 0f37CCF5CE;
	fma.rn.f32 	%f2278, %f1306, %f221, %f1305;
	bra.uni 	BB48_256;

BB48_255:
	mov.f32 	%f1307, 0f3C08839E;
	mov.f32 	%f1308, 0fB94CA1F9;
	fma.rn.f32 	%f2278, %f1308, %f221, %f1307;

BB48_256:
	@%p176 bra 	BB48_258;

	mov.f32 	%f1309, 0f3D2AAAA5;
	fma.rn.f32 	%f1310, %f2278, %f221, %f1309;
	mov.f32 	%f1311, 0fBF000000;
	fma.rn.f32 	%f2279, %f1310, %f221, %f1311;
	bra.uni 	BB48_259;

BB48_258:
	mov.f32 	%f1312, 0fBE2AAAA3;
	fma.rn.f32 	%f1313, %f2278, %f221, %f1312;
	mov.f32 	%f1314, 0f00000000;
	fma.rn.f32 	%f2279, %f1313, %f221, %f1314;

BB48_259:
	fma.rn.f32 	%f2280, %f2279, %f2277, %f2277;
	@%p176 bra 	BB48_261;

	mov.f32 	%f1315, 0f3F800000;
	fma.rn.f32 	%f2280, %f2279, %f221, %f1315;

BB48_261:
	and.b32  	%r2295, %r4086, 2;
	setp.eq.s32	%p179, %r2295, 0;
	@%p179 bra 	BB48_263;

	mov.f32 	%f1316, 0f00000000;
	mov.f32 	%f1317, 0fBF800000;
	fma.rn.f32 	%f2280, %f2280, %f1317, %f1316;

BB48_263:
	mul.f32 	%f233, %f2276, %f2280;
	mov.f32 	%f2355, %f13;
	@%p63 bra 	BB48_265;

	mov.f32 	%f1318, 0f00000000;
	mul.rn.f32 	%f2355, %f13, %f1318;

BB48_265:
	mul.f32 	%f1319, %f2355, 0f3F22F983;
	cvt.rni.s32.f32	%r4096, %f1319;
	cvt.rn.f32.s32	%f1320, %r4096;
	neg.f32 	%f1321, %f1320;
	fma.rn.f32 	%f1323, %f1321, %f1069, %f2355;
	fma.rn.f32 	%f1325, %f1321, %f1071, %f1323;
	fma.rn.f32 	%f2281, %f1321, %f1073, %f1325;
	abs.f32 	%f1327, %f2355;
	setp.leu.f32	%p181, %f1327, 0f47CE4780;
	@%p181 bra 	BB48_275;

	mov.b32 	 %r411, %f2355;
	shr.u32 	%r412, %r411, 23;
	bfe.u32 	%r2298, %r411, 23, 8;
	add.s32 	%r2299, %r2298, -128;
	shl.b32 	%r2300, %r411, 8;
	or.b32  	%r413, %r2300, -2147483648;
	shr.u32 	%r414, %r2299, 5;
	mov.u32 	%r4088, 0;
	mov.u64 	%rd357, __cudart_i2opi_f;
	mov.u32 	%r4087, -6;
	mov.u64 	%rd477, %rd1;

BB48_267:
	.pragma "nounroll";
	ld.const.u32 	%r2303, [%rd357];
	// inline asm
	{
	mad.lo.cc.u32   %r2301, %r2303, %r413, %r4088;
	madc.hi.u32     %r4088, %r2303, %r413,  0;
	}
	// inline asm
	st.local.u32 	[%rd477], %r2301;
	add.s64 	%rd477, %rd477, 4;
	add.s64 	%rd357, %rd357, 4;
	add.s32 	%r4087, %r4087, 1;
	setp.ne.s32	%p182, %r4087, 0;
	@%p182 bra 	BB48_267;

	and.b32  	%r419, %r411, -2147483648;
	st.local.u32 	[%rd3], %r4088;
	mov.u32 	%r2306, 6;
	sub.s32 	%r2307, %r2306, %r414;
	mul.wide.s32 	%rd270, %r2307, 4;
	add.s64 	%rd63, %rd1, %rd270;
	ld.local.u32 	%r4089, [%rd63];
	ld.local.u32 	%r4090, [%rd63+-4];
	and.b32  	%r422, %r412, 31;
	setp.eq.s32	%p183, %r422, 0;
	@%p183 bra 	BB48_270;

	mov.u32 	%r2308, 32;
	sub.s32 	%r2309, %r2308, %r422;
	shr.u32 	%r2310, %r4090, %r2309;
	shl.b32 	%r2311, %r4089, %r422;
	add.s32 	%r4089, %r2310, %r2311;
	ld.local.u32 	%r2312, [%rd63+-8];
	shr.u32 	%r2313, %r2312, %r2309;
	shl.b32 	%r2314, %r4090, %r422;
	add.s32 	%r4090, %r2313, %r2314;

BB48_270:
	shr.u32 	%r2315, %r4090, 30;
	shl.b32 	%r2316, %r4089, 2;
	add.s32 	%r4091, %r2315, %r2316;
	shl.b32 	%r428, %r4090, 2;
	shr.u32 	%r2317, %r4091, 31;
	shr.u32 	%r2318, %r4089, 30;
	add.s32 	%r429, %r2317, %r2318;
	setp.eq.s32	%p184, %r2317, 0;
	mov.u32 	%r4092, %r419;
	mov.u32 	%r4093, %r428;
	@%p184 bra 	BB48_272;

	not.b32 	%r2319, %r4091;
	neg.s32 	%r430, %r428;
	setp.eq.s32	%p185, %r428, 0;
	selp.u32	%r2320, 1, 0, %p185;
	add.s32 	%r4091, %r2320, %r2319;
	xor.b32  	%r432, %r419, -2147483648;
	mov.u32 	%r4092, %r432;
	mov.u32 	%r4093, %r430;

BB48_272:
	mov.u32 	%r434, %r4092;
	neg.s32 	%r2321, %r429;
	setp.eq.s32	%p186, %r419, 0;
	selp.b32	%r4096, %r429, %r2321, %p186;
	clz.b32 	%r4095, %r4091;
	setp.eq.s32	%p187, %r4095, 0;
	shl.b32 	%r2322, %r4091, %r4095;
	mov.u32 	%r2323, 32;
	sub.s32 	%r2324, %r2323, %r4095;
	shr.u32 	%r2325, %r4093, %r2324;
	add.s32 	%r2326, %r2325, %r2322;
	selp.b32	%r438, %r4091, %r2326, %p187;
	mov.u32 	%r2327, -921707870;
	mul.hi.u32 	%r4094, %r438, %r2327;
	setp.lt.s32	%p188, %r4094, 1;
	@%p188 bra 	BB48_274;

	mul.lo.s32 	%r2328, %r438, -921707870;
	shr.u32 	%r2329, %r2328, 31;
	shl.b32 	%r2330, %r4094, 1;
	add.s32 	%r4094, %r2329, %r2330;
	add.s32 	%r4095, %r4095, 1;

BB48_274:
	mov.u32 	%r2331, 126;
	sub.s32 	%r2332, %r2331, %r4095;
	shl.b32 	%r2333, %r2332, 23;
	add.s32 	%r2334, %r4094, 1;
	shr.u32 	%r2335, %r2334, 7;
	add.s32 	%r2336, %r2335, 1;
	shr.u32 	%r2337, %r2336, 1;
	add.s32 	%r2338, %r2337, %r2333;
	or.b32  	%r2339, %r2338, %r434;
	mov.b32 	 %f2281, %r2339;

BB48_275:
	mul.rn.f32 	%f239, %f2281, %f2281;
	and.b32  	%r445, %r4096, 1;
	setp.eq.s32	%p189, %r445, 0;
	@%p189 bra 	BB48_277;

	mov.f32 	%f1328, 0fBAB6061A;
	mov.f32 	%f1329, 0f37CCF5CE;
	fma.rn.f32 	%f2282, %f1329, %f239, %f1328;
	bra.uni 	BB48_278;

BB48_277:
	mov.f32 	%f1330, 0f3C08839E;
	mov.f32 	%f1331, 0fB94CA1F9;
	fma.rn.f32 	%f2282, %f1331, %f239, %f1330;

BB48_278:
	@%p189 bra 	BB48_280;

	mov.f32 	%f1332, 0f3D2AAAA5;
	fma.rn.f32 	%f1333, %f2282, %f239, %f1332;
	mov.f32 	%f1334, 0fBF000000;
	fma.rn.f32 	%f2283, %f1333, %f239, %f1334;
	bra.uni 	BB48_281;

BB48_280:
	mov.f32 	%f1335, 0fBE2AAAA3;
	fma.rn.f32 	%f1336, %f2282, %f239, %f1335;
	mov.f32 	%f1337, 0f00000000;
	fma.rn.f32 	%f2283, %f1336, %f239, %f1337;

BB48_281:
	fma.rn.f32 	%f2284, %f2283, %f2281, %f2281;
	@%p189 bra 	BB48_283;

	mov.f32 	%f1338, 0f3F800000;
	fma.rn.f32 	%f2284, %f2283, %f239, %f1338;

BB48_283:
	and.b32  	%r2340, %r4096, 2;
	setp.eq.s32	%p192, %r2340, 0;
	@%p192 bra 	BB48_285;

	mov.f32 	%f1339, 0f00000000;
	mov.f32 	%f1340, 0fBF800000;
	fma.rn.f32 	%f2284, %f2284, %f1340, %f1339;

BB48_285:
	mul.f32 	%f1341, %f233, %f2284;
	sub.f32 	%f251, %f198, %f1341;
	mov.f32 	%f2381, %f34;
	@%p37 bra 	BB48_287;

	mov.f32 	%f1342, 0f00000000;
	mul.rn.f32 	%f2381, %f34, %f1342;

BB48_287:
	mul.f32 	%f1343, %f2381, 0f3F22F983;
	cvt.rni.s32.f32	%r4106, %f1343;
	cvt.rn.f32.s32	%f1344, %r4106;
	neg.f32 	%f1345, %f1344;
	fma.rn.f32 	%f1347, %f1345, %f1069, %f2381;
	fma.rn.f32 	%f1349, %f1345, %f1071, %f1347;
	fma.rn.f32 	%f2285, %f1345, %f1073, %f1349;
	abs.f32 	%f1351, %f2381;
	setp.leu.f32	%p194, %f1351, 0f47CE4780;
	@%p194 bra 	BB48_297;

	mov.b32 	 %r447, %f2381;
	shr.u32 	%r448, %r447, 23;
	bfe.u32 	%r2343, %r447, 23, 8;
	add.s32 	%r2344, %r2343, -128;
	shl.b32 	%r2345, %r447, 8;
	or.b32  	%r449, %r2345, -2147483648;
	shr.u32 	%r450, %r2344, 5;
	mov.u32 	%r4098, 0;
	mov.u64 	%rd358, __cudart_i2opi_f;
	mov.u32 	%r4097, -6;
	mov.u64 	%rd476, %rd1;

BB48_289:
	.pragma "nounroll";
	ld.const.u32 	%r2348, [%rd358];
	// inline asm
	{
	mad.lo.cc.u32   %r2346, %r2348, %r449, %r4098;
	madc.hi.u32     %r4098, %r2348, %r449,  0;
	}
	// inline asm
	st.local.u32 	[%rd476], %r2346;
	add.s64 	%rd476, %rd476, 4;
	add.s64 	%rd358, %rd358, 4;
	add.s32 	%r4097, %r4097, 1;
	setp.ne.s32	%p195, %r4097, 0;
	@%p195 bra 	BB48_289;

	and.b32  	%r455, %r447, -2147483648;
	st.local.u32 	[%rd3], %r4098;
	mov.u32 	%r2351, 6;
	sub.s32 	%r2352, %r2351, %r450;
	mul.wide.s32 	%rd272, %r2352, 4;
	add.s64 	%rd68, %rd1, %rd272;
	ld.local.u32 	%r4099, [%rd68];
	ld.local.u32 	%r4100, [%rd68+-4];
	and.b32  	%r458, %r448, 31;
	setp.eq.s32	%p196, %r458, 0;
	@%p196 bra 	BB48_292;

	mov.u32 	%r2353, 32;
	sub.s32 	%r2354, %r2353, %r458;
	shr.u32 	%r2355, %r4100, %r2354;
	shl.b32 	%r2356, %r4099, %r458;
	add.s32 	%r4099, %r2355, %r2356;
	ld.local.u32 	%r2357, [%rd68+-8];
	shr.u32 	%r2358, %r2357, %r2354;
	shl.b32 	%r2359, %r4100, %r458;
	add.s32 	%r4100, %r2358, %r2359;

BB48_292:
	shr.u32 	%r2360, %r4100, 30;
	shl.b32 	%r2361, %r4099, 2;
	add.s32 	%r4101, %r2360, %r2361;
	shl.b32 	%r464, %r4100, 2;
	shr.u32 	%r2362, %r4101, 31;
	shr.u32 	%r2363, %r4099, 30;
	add.s32 	%r465, %r2362, %r2363;
	setp.eq.s32	%p197, %r2362, 0;
	mov.u32 	%r4102, %r455;
	mov.u32 	%r4103, %r464;
	@%p197 bra 	BB48_294;

	not.b32 	%r2364, %r4101;
	neg.s32 	%r466, %r464;
	setp.eq.s32	%p198, %r464, 0;
	selp.u32	%r2365, 1, 0, %p198;
	add.s32 	%r4101, %r2365, %r2364;
	xor.b32  	%r468, %r455, -2147483648;
	mov.u32 	%r4102, %r468;
	mov.u32 	%r4103, %r466;

BB48_294:
	mov.u32 	%r470, %r4102;
	neg.s32 	%r2366, %r465;
	setp.eq.s32	%p199, %r455, 0;
	selp.b32	%r4106, %r465, %r2366, %p199;
	clz.b32 	%r4105, %r4101;
	setp.eq.s32	%p200, %r4105, 0;
	shl.b32 	%r2367, %r4101, %r4105;
	mov.u32 	%r2368, 32;
	sub.s32 	%r2369, %r2368, %r4105;
	shr.u32 	%r2370, %r4103, %r2369;
	add.s32 	%r2371, %r2370, %r2367;
	selp.b32	%r474, %r4101, %r2371, %p200;
	mov.u32 	%r2372, -921707870;
	mul.hi.u32 	%r4104, %r474, %r2372;
	setp.lt.s32	%p201, %r4104, 1;
	@%p201 bra 	BB48_296;

	mul.lo.s32 	%r2373, %r474, -921707870;
	shr.u32 	%r2374, %r2373, 31;
	shl.b32 	%r2375, %r4104, 1;
	add.s32 	%r4104, %r2374, %r2375;
	add.s32 	%r4105, %r4105, 1;

BB48_296:
	mov.u32 	%r2376, 126;
	sub.s32 	%r2377, %r2376, %r4105;
	shl.b32 	%r2378, %r2377, 23;
	add.s32 	%r2379, %r4104, 1;
	shr.u32 	%r2380, %r2379, 7;
	add.s32 	%r2381, %r2380, 1;
	shr.u32 	%r2382, %r2381, 1;
	add.s32 	%r2383, %r2382, %r2378;
	or.b32  	%r2384, %r2383, %r470;
	mov.b32 	 %f2285, %r2384;

BB48_297:
	mul.rn.f32 	%f257, %f2285, %f2285;
	add.s32 	%r481, %r4106, 1;
	and.b32  	%r482, %r481, 1;
	setp.eq.s32	%p202, %r482, 0;
	@%p202 bra 	BB48_299;

	mov.f32 	%f1352, 0fBAB6061A;
	mov.f32 	%f1353, 0f37CCF5CE;
	fma.rn.f32 	%f2286, %f1353, %f257, %f1352;
	bra.uni 	BB48_300;

BB48_299:
	mov.f32 	%f1354, 0f3C08839E;
	mov.f32 	%f1355, 0fB94CA1F9;
	fma.rn.f32 	%f2286, %f1355, %f257, %f1354;

BB48_300:
	@%p202 bra 	BB48_302;

	mov.f32 	%f1356, 0f3D2AAAA5;
	fma.rn.f32 	%f1357, %f2286, %f257, %f1356;
	mov.f32 	%f1358, 0fBF000000;
	fma.rn.f32 	%f2287, %f1357, %f257, %f1358;
	bra.uni 	BB48_303;

BB48_302:
	mov.f32 	%f1359, 0fBE2AAAA3;
	fma.rn.f32 	%f1360, %f2286, %f257, %f1359;
	mov.f32 	%f1361, 0f00000000;
	fma.rn.f32 	%f2287, %f1360, %f257, %f1361;

BB48_303:
	fma.rn.f32 	%f2288, %f2287, %f2285, %f2285;
	@%p202 bra 	BB48_305;

	mov.f32 	%f1362, 0f3F800000;
	fma.rn.f32 	%f2288, %f2287, %f257, %f1362;

BB48_305:
	and.b32  	%r2385, %r481, 2;
	setp.eq.s32	%p205, %r2385, 0;
	@%p205 bra 	BB48_307;

	mov.f32 	%f1363, 0f00000000;
	mov.f32 	%f1364, 0fBF800000;
	fma.rn.f32 	%f2288, %f2288, %f1364, %f1363;

BB48_307:
	mov.f32 	%f2335, %f36;
	@%p50 bra 	BB48_309;

	mov.f32 	%f1365, 0f00000000;
	mul.rn.f32 	%f2335, %f36, %f1365;

BB48_309:
	mul.f32 	%f1366, %f2335, 0f3F22F983;
	cvt.rni.s32.f32	%r4116, %f1366;
	cvt.rn.f32.s32	%f1367, %r4116;
	neg.f32 	%f1368, %f1367;
	fma.rn.f32 	%f1370, %f1368, %f1069, %f2335;
	fma.rn.f32 	%f1372, %f1368, %f1071, %f1370;
	fma.rn.f32 	%f2289, %f1368, %f1073, %f1372;
	abs.f32 	%f1374, %f2335;
	setp.leu.f32	%p207, %f1374, 0f47CE4780;
	@%p207 bra 	BB48_319;

	mov.b32 	 %r484, %f2335;
	shr.u32 	%r485, %r484, 23;
	bfe.u32 	%r2388, %r484, 23, 8;
	add.s32 	%r2389, %r2388, -128;
	shl.b32 	%r2390, %r484, 8;
	or.b32  	%r486, %r2390, -2147483648;
	shr.u32 	%r487, %r2389, 5;
	mov.u32 	%r4108, 0;
	mov.u64 	%rd359, __cudart_i2opi_f;
	mov.u32 	%r4107, -6;
	mov.u64 	%rd475, %rd1;

BB48_311:
	.pragma "nounroll";
	ld.const.u32 	%r2393, [%rd359];
	// inline asm
	{
	mad.lo.cc.u32   %r2391, %r2393, %r486, %r4108;
	madc.hi.u32     %r4108, %r2393, %r486,  0;
	}
	// inline asm
	st.local.u32 	[%rd475], %r2391;
	add.s64 	%rd475, %rd475, 4;
	add.s64 	%rd359, %rd359, 4;
	add.s32 	%r4107, %r4107, 1;
	setp.ne.s32	%p208, %r4107, 0;
	@%p208 bra 	BB48_311;

	and.b32  	%r492, %r484, -2147483648;
	st.local.u32 	[%rd3], %r4108;
	mov.u32 	%r2396, 6;
	sub.s32 	%r2397, %r2396, %r487;
	mul.wide.s32 	%rd274, %r2397, 4;
	add.s64 	%rd73, %rd1, %rd274;
	ld.local.u32 	%r4109, [%rd73];
	ld.local.u32 	%r4110, [%rd73+-4];
	and.b32  	%r495, %r485, 31;
	setp.eq.s32	%p209, %r495, 0;
	@%p209 bra 	BB48_314;

	mov.u32 	%r2398, 32;
	sub.s32 	%r2399, %r2398, %r495;
	shr.u32 	%r2400, %r4110, %r2399;
	shl.b32 	%r2401, %r4109, %r495;
	add.s32 	%r4109, %r2400, %r2401;
	ld.local.u32 	%r2402, [%rd73+-8];
	shr.u32 	%r2403, %r2402, %r2399;
	shl.b32 	%r2404, %r4110, %r495;
	add.s32 	%r4110, %r2403, %r2404;

BB48_314:
	shr.u32 	%r2405, %r4110, 30;
	shl.b32 	%r2406, %r4109, 2;
	add.s32 	%r4111, %r2405, %r2406;
	shl.b32 	%r501, %r4110, 2;
	shr.u32 	%r2407, %r4111, 31;
	shr.u32 	%r2408, %r4109, 30;
	add.s32 	%r502, %r2407, %r2408;
	setp.eq.s32	%p210, %r2407, 0;
	mov.u32 	%r4112, %r492;
	mov.u32 	%r4113, %r501;
	@%p210 bra 	BB48_316;

	not.b32 	%r2409, %r4111;
	neg.s32 	%r503, %r501;
	setp.eq.s32	%p211, %r501, 0;
	selp.u32	%r2410, 1, 0, %p211;
	add.s32 	%r4111, %r2410, %r2409;
	xor.b32  	%r505, %r492, -2147483648;
	mov.u32 	%r4112, %r505;
	mov.u32 	%r4113, %r503;

BB48_316:
	mov.u32 	%r507, %r4112;
	neg.s32 	%r2411, %r502;
	setp.eq.s32	%p212, %r492, 0;
	selp.b32	%r4116, %r502, %r2411, %p212;
	clz.b32 	%r4115, %r4111;
	setp.eq.s32	%p213, %r4115, 0;
	shl.b32 	%r2412, %r4111, %r4115;
	mov.u32 	%r2413, 32;
	sub.s32 	%r2414, %r2413, %r4115;
	shr.u32 	%r2415, %r4113, %r2414;
	add.s32 	%r2416, %r2415, %r2412;
	selp.b32	%r511, %r4111, %r2416, %p213;
	mov.u32 	%r2417, -921707870;
	mul.hi.u32 	%r4114, %r511, %r2417;
	setp.lt.s32	%p214, %r4114, 1;
	@%p214 bra 	BB48_318;

	mul.lo.s32 	%r2418, %r511, -921707870;
	shr.u32 	%r2419, %r2418, 31;
	shl.b32 	%r2420, %r4114, 1;
	add.s32 	%r4114, %r2419, %r2420;
	add.s32 	%r4115, %r4115, 1;

BB48_318:
	mov.u32 	%r2421, 126;
	sub.s32 	%r2422, %r2421, %r4115;
	shl.b32 	%r2423, %r2422, 23;
	add.s32 	%r2424, %r4114, 1;
	shr.u32 	%r2425, %r2424, 7;
	add.s32 	%r2426, %r2425, 1;
	shr.u32 	%r2427, %r2426, 1;
	add.s32 	%r2428, %r2427, %r2423;
	or.b32  	%r2429, %r2428, %r507;
	mov.b32 	 %f2289, %r2429;

BB48_319:
	mul.rn.f32 	%f274, %f2289, %f2289;
	and.b32  	%r518, %r4116, 1;
	setp.eq.s32	%p215, %r518, 0;
	@%p215 bra 	BB48_321;

	mov.f32 	%f1375, 0fBAB6061A;
	mov.f32 	%f1376, 0f37CCF5CE;
	fma.rn.f32 	%f2290, %f1376, %f274, %f1375;
	bra.uni 	BB48_322;

BB48_321:
	mov.f32 	%f1377, 0f3C08839E;
	mov.f32 	%f1378, 0fB94CA1F9;
	fma.rn.f32 	%f2290, %f1378, %f274, %f1377;

BB48_322:
	@%p215 bra 	BB48_324;

	mov.f32 	%f1379, 0f3D2AAAA5;
	fma.rn.f32 	%f1380, %f2290, %f274, %f1379;
	mov.f32 	%f1381, 0fBF000000;
	fma.rn.f32 	%f2291, %f1380, %f274, %f1381;
	bra.uni 	BB48_325;

BB48_324:
	mov.f32 	%f1382, 0fBE2AAAA3;
	fma.rn.f32 	%f1383, %f2290, %f274, %f1382;
	mov.f32 	%f1384, 0f00000000;
	fma.rn.f32 	%f2291, %f1383, %f274, %f1384;

BB48_325:
	fma.rn.f32 	%f2292, %f2291, %f2289, %f2289;
	@%p215 bra 	BB48_327;

	mov.f32 	%f1385, 0f3F800000;
	fma.rn.f32 	%f2292, %f2291, %f274, %f1385;

BB48_327:
	and.b32  	%r2430, %r4116, 2;
	setp.eq.s32	%p218, %r2430, 0;
	@%p218 bra 	BB48_329;

	mov.f32 	%f1386, 0f00000000;
	mov.f32 	%f1387, 0fBF800000;
	fma.rn.f32 	%f2292, %f2292, %f1387, %f1386;

BB48_329:
	mul.f32 	%f286, %f2288, %f2292;
	mov.f32 	%f2354, %f13;
	@%p63 bra 	BB48_331;

	mov.f32 	%f1388, 0f00000000;
	mul.rn.f32 	%f2354, %f13, %f1388;

BB48_331:
	mul.f32 	%f1389, %f2354, 0f3F22F983;
	cvt.rni.s32.f32	%r4126, %f1389;
	cvt.rn.f32.s32	%f1390, %r4126;
	neg.f32 	%f1391, %f1390;
	fma.rn.f32 	%f1393, %f1391, %f1069, %f2354;
	fma.rn.f32 	%f1395, %f1391, %f1071, %f1393;
	fma.rn.f32 	%f2293, %f1391, %f1073, %f1395;
	abs.f32 	%f1397, %f2354;
	setp.leu.f32	%p220, %f1397, 0f47CE4780;
	@%p220 bra 	BB48_341;

	mov.b32 	 %r520, %f2354;
	shr.u32 	%r521, %r520, 23;
	bfe.u32 	%r2433, %r520, 23, 8;
	add.s32 	%r2434, %r2433, -128;
	shl.b32 	%r2435, %r520, 8;
	or.b32  	%r522, %r2435, -2147483648;
	shr.u32 	%r523, %r2434, 5;
	mov.u32 	%r4118, 0;
	mov.u64 	%rd360, __cudart_i2opi_f;
	mov.u32 	%r4117, -6;
	mov.u64 	%rd474, %rd1;

BB48_333:
	.pragma "nounroll";
	ld.const.u32 	%r2438, [%rd360];
	// inline asm
	{
	mad.lo.cc.u32   %r2436, %r2438, %r522, %r4118;
	madc.hi.u32     %r4118, %r2438, %r522,  0;
	}
	// inline asm
	st.local.u32 	[%rd474], %r2436;
	add.s64 	%rd474, %rd474, 4;
	add.s64 	%rd360, %rd360, 4;
	add.s32 	%r4117, %r4117, 1;
	setp.ne.s32	%p221, %r4117, 0;
	@%p221 bra 	BB48_333;

	and.b32  	%r528, %r520, -2147483648;
	st.local.u32 	[%rd3], %r4118;
	mov.u32 	%r2441, 6;
	sub.s32 	%r2442, %r2441, %r523;
	mul.wide.s32 	%rd276, %r2442, 4;
	add.s64 	%rd78, %rd1, %rd276;
	ld.local.u32 	%r4119, [%rd78];
	ld.local.u32 	%r4120, [%rd78+-4];
	and.b32  	%r531, %r521, 31;
	setp.eq.s32	%p222, %r531, 0;
	@%p222 bra 	BB48_336;

	mov.u32 	%r2443, 32;
	sub.s32 	%r2444, %r2443, %r531;
	shr.u32 	%r2445, %r4120, %r2444;
	shl.b32 	%r2446, %r4119, %r531;
	add.s32 	%r4119, %r2445, %r2446;
	ld.local.u32 	%r2447, [%rd78+-8];
	shr.u32 	%r2448, %r2447, %r2444;
	shl.b32 	%r2449, %r4120, %r531;
	add.s32 	%r4120, %r2448, %r2449;

BB48_336:
	shr.u32 	%r2450, %r4120, 30;
	shl.b32 	%r2451, %r4119, 2;
	add.s32 	%r4121, %r2450, %r2451;
	shl.b32 	%r537, %r4120, 2;
	shr.u32 	%r2452, %r4121, 31;
	shr.u32 	%r2453, %r4119, 30;
	add.s32 	%r538, %r2452, %r2453;
	setp.eq.s32	%p223, %r2452, 0;
	mov.u32 	%r4122, %r528;
	mov.u32 	%r4123, %r537;
	@%p223 bra 	BB48_338;

	not.b32 	%r2454, %r4121;
	neg.s32 	%r539, %r537;
	setp.eq.s32	%p224, %r537, 0;
	selp.u32	%r2455, 1, 0, %p224;
	add.s32 	%r4121, %r2455, %r2454;
	xor.b32  	%r541, %r528, -2147483648;
	mov.u32 	%r4122, %r541;
	mov.u32 	%r4123, %r539;

BB48_338:
	mov.u32 	%r543, %r4122;
	neg.s32 	%r2456, %r538;
	setp.eq.s32	%p225, %r528, 0;
	selp.b32	%r4126, %r538, %r2456, %p225;
	clz.b32 	%r4125, %r4121;
	setp.eq.s32	%p226, %r4125, 0;
	shl.b32 	%r2457, %r4121, %r4125;
	mov.u32 	%r2458, 32;
	sub.s32 	%r2459, %r2458, %r4125;
	shr.u32 	%r2460, %r4123, %r2459;
	add.s32 	%r2461, %r2460, %r2457;
	selp.b32	%r547, %r4121, %r2461, %p226;
	mov.u32 	%r2462, -921707870;
	mul.hi.u32 	%r4124, %r547, %r2462;
	setp.lt.s32	%p227, %r4124, 1;
	@%p227 bra 	BB48_340;

	mul.lo.s32 	%r2463, %r547, -921707870;
	shr.u32 	%r2464, %r2463, 31;
	shl.b32 	%r2465, %r4124, 1;
	add.s32 	%r4124, %r2464, %r2465;
	add.s32 	%r4125, %r4125, 1;

BB48_340:
	mov.u32 	%r2466, 126;
	sub.s32 	%r2467, %r2466, %r4125;
	shl.b32 	%r2468, %r2467, 23;
	add.s32 	%r2469, %r4124, 1;
	shr.u32 	%r2470, %r2469, 7;
	add.s32 	%r2471, %r2470, 1;
	shr.u32 	%r2472, %r2471, 1;
	add.s32 	%r2473, %r2472, %r2468;
	or.b32  	%r2474, %r2473, %r543;
	mov.b32 	 %f2293, %r2474;

BB48_341:
	mul.rn.f32 	%f292, %f2293, %f2293;
	add.s32 	%r554, %r4126, 1;
	and.b32  	%r555, %r554, 1;
	setp.eq.s32	%p228, %r555, 0;
	@%p228 bra 	BB48_343;

	mov.f32 	%f1398, 0fBAB6061A;
	mov.f32 	%f1399, 0f37CCF5CE;
	fma.rn.f32 	%f2294, %f1399, %f292, %f1398;
	bra.uni 	BB48_344;

BB48_343:
	mov.f32 	%f1400, 0f3C08839E;
	mov.f32 	%f1401, 0fB94CA1F9;
	fma.rn.f32 	%f2294, %f1401, %f292, %f1400;

BB48_344:
	@%p228 bra 	BB48_346;

	mov.f32 	%f1402, 0f3D2AAAA5;
	fma.rn.f32 	%f1403, %f2294, %f292, %f1402;
	mov.f32 	%f1404, 0fBF000000;
	fma.rn.f32 	%f2295, %f1403, %f292, %f1404;
	bra.uni 	BB48_347;

BB48_346:
	mov.f32 	%f1405, 0fBE2AAAA3;
	fma.rn.f32 	%f1406, %f2294, %f292, %f1405;
	mov.f32 	%f1407, 0f00000000;
	fma.rn.f32 	%f2295, %f1406, %f292, %f1407;

BB48_347:
	fma.rn.f32 	%f2296, %f2295, %f2293, %f2293;
	@%p228 bra 	BB48_349;

	mov.f32 	%f1408, 0f3F800000;
	fma.rn.f32 	%f2296, %f2295, %f292, %f1408;

BB48_349:
	and.b32  	%r2475, %r554, 2;
	setp.eq.s32	%p231, %r2475, 0;
	@%p231 bra 	BB48_351;

	mov.f32 	%f1409, 0f00000000;
	mov.f32 	%f1410, 0fBF800000;
	fma.rn.f32 	%f2296, %f2296, %f1410, %f1409;

BB48_351:
	mul.f32 	%f304, %f286, %f2296;
	mov.f32 	%f2380, %f34;
	@%p37 bra 	BB48_353;

	mov.f32 	%f1411, 0f00000000;
	mul.rn.f32 	%f2380, %f34, %f1411;

BB48_353:
	mul.f32 	%f1412, %f2380, 0f3F22F983;
	cvt.rni.s32.f32	%r4136, %f1412;
	cvt.rn.f32.s32	%f1413, %r4136;
	neg.f32 	%f1414, %f1413;
	fma.rn.f32 	%f1416, %f1414, %f1069, %f2380;
	fma.rn.f32 	%f1418, %f1414, %f1071, %f1416;
	fma.rn.f32 	%f2297, %f1414, %f1073, %f1418;
	abs.f32 	%f1420, %f2380;
	setp.leu.f32	%p233, %f1420, 0f47CE4780;
	@%p233 bra 	BB48_363;

	mov.b32 	 %r557, %f2380;
	shr.u32 	%r558, %r557, 23;
	bfe.u32 	%r2478, %r557, 23, 8;
	add.s32 	%r2479, %r2478, -128;
	shl.b32 	%r2480, %r557, 8;
	or.b32  	%r559, %r2480, -2147483648;
	shr.u32 	%r560, %r2479, 5;
	mov.u32 	%r4128, 0;
	mov.u64 	%rd361, __cudart_i2opi_f;
	mov.u32 	%r4127, -6;
	mov.u64 	%rd473, %rd1;

BB48_355:
	.pragma "nounroll";
	ld.const.u32 	%r2483, [%rd361];
	// inline asm
	{
	mad.lo.cc.u32   %r2481, %r2483, %r559, %r4128;
	madc.hi.u32     %r4128, %r2483, %r559,  0;
	}
	// inline asm
	st.local.u32 	[%rd473], %r2481;
	add.s64 	%rd473, %rd473, 4;
	add.s64 	%rd361, %rd361, 4;
	add.s32 	%r4127, %r4127, 1;
	setp.ne.s32	%p234, %r4127, 0;
	@%p234 bra 	BB48_355;

	and.b32  	%r565, %r557, -2147483648;
	st.local.u32 	[%rd3], %r4128;
	mov.u32 	%r2486, 6;
	sub.s32 	%r2487, %r2486, %r560;
	mul.wide.s32 	%rd278, %r2487, 4;
	add.s64 	%rd83, %rd1, %rd278;
	ld.local.u32 	%r4129, [%rd83];
	ld.local.u32 	%r4130, [%rd83+-4];
	and.b32  	%r568, %r558, 31;
	setp.eq.s32	%p235, %r568, 0;
	@%p235 bra 	BB48_358;

	mov.u32 	%r2488, 32;
	sub.s32 	%r2489, %r2488, %r568;
	shr.u32 	%r2490, %r4130, %r2489;
	shl.b32 	%r2491, %r4129, %r568;
	add.s32 	%r4129, %r2490, %r2491;
	ld.local.u32 	%r2492, [%rd83+-8];
	shr.u32 	%r2493, %r2492, %r2489;
	shl.b32 	%r2494, %r4130, %r568;
	add.s32 	%r4130, %r2493, %r2494;

BB48_358:
	shr.u32 	%r2495, %r4130, 30;
	shl.b32 	%r2496, %r4129, 2;
	add.s32 	%r4131, %r2495, %r2496;
	shl.b32 	%r574, %r4130, 2;
	shr.u32 	%r2497, %r4131, 31;
	shr.u32 	%r2498, %r4129, 30;
	add.s32 	%r575, %r2497, %r2498;
	setp.eq.s32	%p236, %r2497, 0;
	mov.u32 	%r4132, %r565;
	mov.u32 	%r4133, %r574;
	@%p236 bra 	BB48_360;

	not.b32 	%r2499, %r4131;
	neg.s32 	%r576, %r574;
	setp.eq.s32	%p237, %r574, 0;
	selp.u32	%r2500, 1, 0, %p237;
	add.s32 	%r4131, %r2500, %r2499;
	xor.b32  	%r578, %r565, -2147483648;
	mov.u32 	%r4132, %r578;
	mov.u32 	%r4133, %r576;

BB48_360:
	mov.u32 	%r580, %r4132;
	neg.s32 	%r2501, %r575;
	setp.eq.s32	%p238, %r565, 0;
	selp.b32	%r4136, %r575, %r2501, %p238;
	clz.b32 	%r4135, %r4131;
	setp.eq.s32	%p239, %r4135, 0;
	shl.b32 	%r2502, %r4131, %r4135;
	mov.u32 	%r2503, 32;
	sub.s32 	%r2504, %r2503, %r4135;
	shr.u32 	%r2505, %r4133, %r2504;
	add.s32 	%r2506, %r2505, %r2502;
	selp.b32	%r584, %r4131, %r2506, %p239;
	mov.u32 	%r2507, -921707870;
	mul.hi.u32 	%r4134, %r584, %r2507;
	setp.lt.s32	%p240, %r4134, 1;
	@%p240 bra 	BB48_362;

	mul.lo.s32 	%r2508, %r584, -921707870;
	shr.u32 	%r2509, %r2508, 31;
	shl.b32 	%r2510, %r4134, 1;
	add.s32 	%r4134, %r2509, %r2510;
	add.s32 	%r4135, %r4135, 1;

BB48_362:
	mov.u32 	%r2511, 126;
	sub.s32 	%r2512, %r2511, %r4135;
	shl.b32 	%r2513, %r2512, 23;
	add.s32 	%r2514, %r4134, 1;
	shr.u32 	%r2515, %r2514, 7;
	add.s32 	%r2516, %r2515, 1;
	shr.u32 	%r2517, %r2516, 1;
	add.s32 	%r2518, %r2517, %r2513;
	or.b32  	%r2519, %r2518, %r580;
	mov.b32 	 %f2297, %r2519;

BB48_363:
	mul.rn.f32 	%f310, %f2297, %f2297;
	and.b32  	%r591, %r4136, 1;
	setp.eq.s32	%p241, %r591, 0;
	@%p241 bra 	BB48_365;

	mov.f32 	%f1421, 0fBAB6061A;
	mov.f32 	%f1422, 0f37CCF5CE;
	fma.rn.f32 	%f2298, %f1422, %f310, %f1421;
	bra.uni 	BB48_366;

BB48_365:
	mov.f32 	%f1423, 0f3C08839E;
	mov.f32 	%f1424, 0fB94CA1F9;
	fma.rn.f32 	%f2298, %f1424, %f310, %f1423;

BB48_366:
	@%p241 bra 	BB48_368;

	mov.f32 	%f1425, 0f3D2AAAA5;
	fma.rn.f32 	%f1426, %f2298, %f310, %f1425;
	mov.f32 	%f1427, 0fBF000000;
	fma.rn.f32 	%f2299, %f1426, %f310, %f1427;
	bra.uni 	BB48_369;

BB48_368:
	mov.f32 	%f1428, 0fBE2AAAA3;
	fma.rn.f32 	%f1429, %f2298, %f310, %f1428;
	mov.f32 	%f1430, 0f00000000;
	fma.rn.f32 	%f2299, %f1429, %f310, %f1430;

BB48_369:
	fma.rn.f32 	%f2300, %f2299, %f2297, %f2297;
	@%p241 bra 	BB48_371;

	mov.f32 	%f1431, 0f3F800000;
	fma.rn.f32 	%f2300, %f2299, %f310, %f1431;

BB48_371:
	and.b32  	%r2520, %r4136, 2;
	setp.eq.s32	%p244, %r2520, 0;
	@%p244 bra 	BB48_373;

	mov.f32 	%f1432, 0f00000000;
	mov.f32 	%f1433, 0fBF800000;
	fma.rn.f32 	%f2300, %f2300, %f1433, %f1432;

BB48_373:
	mov.f32 	%f2334, %f36;
	@%p50 bra 	BB48_375;

	mov.f32 	%f1434, 0f00000000;
	mul.rn.f32 	%f2334, %f36, %f1434;

BB48_375:
	mul.f32 	%f1435, %f2334, 0f3F22F983;
	cvt.rni.s32.f32	%r4146, %f1435;
	cvt.rn.f32.s32	%f1436, %r4146;
	neg.f32 	%f1437, %f1436;
	fma.rn.f32 	%f1439, %f1437, %f1069, %f2334;
	fma.rn.f32 	%f1441, %f1437, %f1071, %f1439;
	fma.rn.f32 	%f2301, %f1437, %f1073, %f1441;
	abs.f32 	%f1443, %f2334;
	setp.leu.f32	%p246, %f1443, 0f47CE4780;
	@%p246 bra 	BB48_385;

	mov.b32 	 %r593, %f2334;
	shr.u32 	%r594, %r593, 23;
	bfe.u32 	%r2523, %r593, 23, 8;
	add.s32 	%r2524, %r2523, -128;
	shl.b32 	%r2525, %r593, 8;
	or.b32  	%r595, %r2525, -2147483648;
	shr.u32 	%r596, %r2524, 5;
	mov.u32 	%r4138, 0;
	mov.u64 	%rd362, __cudart_i2opi_f;
	mov.u32 	%r4137, -6;
	mov.u64 	%rd472, %rd1;

BB48_377:
	.pragma "nounroll";
	ld.const.u32 	%r2528, [%rd362];
	// inline asm
	{
	mad.lo.cc.u32   %r2526, %r2528, %r595, %r4138;
	madc.hi.u32     %r4138, %r2528, %r595,  0;
	}
	// inline asm
	st.local.u32 	[%rd472], %r2526;
	add.s64 	%rd472, %rd472, 4;
	add.s64 	%rd362, %rd362, 4;
	add.s32 	%r4137, %r4137, 1;
	setp.ne.s32	%p247, %r4137, 0;
	@%p247 bra 	BB48_377;

	and.b32  	%r601, %r593, -2147483648;
	st.local.u32 	[%rd3], %r4138;
	mov.u32 	%r2531, 6;
	sub.s32 	%r2532, %r2531, %r596;
	mul.wide.s32 	%rd280, %r2532, 4;
	add.s64 	%rd88, %rd1, %rd280;
	ld.local.u32 	%r4139, [%rd88];
	ld.local.u32 	%r4140, [%rd88+-4];
	and.b32  	%r604, %r594, 31;
	setp.eq.s32	%p248, %r604, 0;
	@%p248 bra 	BB48_380;

	mov.u32 	%r2533, 32;
	sub.s32 	%r2534, %r2533, %r604;
	shr.u32 	%r2535, %r4140, %r2534;
	shl.b32 	%r2536, %r4139, %r604;
	add.s32 	%r4139, %r2535, %r2536;
	ld.local.u32 	%r2537, [%rd88+-8];
	shr.u32 	%r2538, %r2537, %r2534;
	shl.b32 	%r2539, %r4140, %r604;
	add.s32 	%r4140, %r2538, %r2539;

BB48_380:
	shr.u32 	%r2540, %r4140, 30;
	shl.b32 	%r2541, %r4139, 2;
	add.s32 	%r4141, %r2540, %r2541;
	shl.b32 	%r610, %r4140, 2;
	shr.u32 	%r2542, %r4141, 31;
	shr.u32 	%r2543, %r4139, 30;
	add.s32 	%r611, %r2542, %r2543;
	setp.eq.s32	%p249, %r2542, 0;
	mov.u32 	%r4142, %r601;
	mov.u32 	%r4143, %r610;
	@%p249 bra 	BB48_382;

	not.b32 	%r2544, %r4141;
	neg.s32 	%r612, %r610;
	setp.eq.s32	%p250, %r610, 0;
	selp.u32	%r2545, 1, 0, %p250;
	add.s32 	%r4141, %r2545, %r2544;
	xor.b32  	%r614, %r601, -2147483648;
	mov.u32 	%r4142, %r614;
	mov.u32 	%r4143, %r612;

BB48_382:
	mov.u32 	%r616, %r4142;
	neg.s32 	%r2546, %r611;
	setp.eq.s32	%p251, %r601, 0;
	selp.b32	%r4146, %r611, %r2546, %p251;
	clz.b32 	%r4145, %r4141;
	setp.eq.s32	%p252, %r4145, 0;
	shl.b32 	%r2547, %r4141, %r4145;
	mov.u32 	%r2548, 32;
	sub.s32 	%r2549, %r2548, %r4145;
	shr.u32 	%r2550, %r4143, %r2549;
	add.s32 	%r2551, %r2550, %r2547;
	selp.b32	%r620, %r4141, %r2551, %p252;
	mov.u32 	%r2552, -921707870;
	mul.hi.u32 	%r4144, %r620, %r2552;
	setp.lt.s32	%p253, %r4144, 1;
	@%p253 bra 	BB48_384;

	mul.lo.s32 	%r2553, %r620, -921707870;
	shr.u32 	%r2554, %r2553, 31;
	shl.b32 	%r2555, %r4144, 1;
	add.s32 	%r4144, %r2554, %r2555;
	add.s32 	%r4145, %r4145, 1;

BB48_384:
	mov.u32 	%r2556, 126;
	sub.s32 	%r2557, %r2556, %r4145;
	shl.b32 	%r2558, %r2557, 23;
	add.s32 	%r2559, %r4144, 1;
	shr.u32 	%r2560, %r2559, 7;
	add.s32 	%r2561, %r2560, 1;
	shr.u32 	%r2562, %r2561, 1;
	add.s32 	%r2563, %r2562, %r2558;
	or.b32  	%r2564, %r2563, %r616;
	mov.b32 	 %f2301, %r2564;

BB48_385:
	mul.rn.f32 	%f327, %f2301, %f2301;
	add.s32 	%r627, %r4146, 1;
	and.b32  	%r628, %r627, 1;
	setp.eq.s32	%p254, %r628, 0;
	@%p254 bra 	BB48_387;

	mov.f32 	%f1444, 0fBAB6061A;
	mov.f32 	%f1445, 0f37CCF5CE;
	fma.rn.f32 	%f2302, %f1445, %f327, %f1444;
	bra.uni 	BB48_388;

BB48_387:
	mov.f32 	%f1446, 0f3C08839E;
	mov.f32 	%f1447, 0fB94CA1F9;
	fma.rn.f32 	%f2302, %f1447, %f327, %f1446;

BB48_388:
	@%p254 bra 	BB48_390;

	mov.f32 	%f1448, 0f3D2AAAA5;
	fma.rn.f32 	%f1449, %f2302, %f327, %f1448;
	mov.f32 	%f1450, 0fBF000000;
	fma.rn.f32 	%f2303, %f1449, %f327, %f1450;
	bra.uni 	BB48_391;

BB48_390:
	mov.f32 	%f1451, 0fBE2AAAA3;
	fma.rn.f32 	%f1452, %f2302, %f327, %f1451;
	mov.f32 	%f1453, 0f00000000;
	fma.rn.f32 	%f2303, %f1452, %f327, %f1453;

BB48_391:
	fma.rn.f32 	%f2304, %f2303, %f2301, %f2301;
	@%p254 bra 	BB48_393;

	mov.f32 	%f1454, 0f3F800000;
	fma.rn.f32 	%f2304, %f2303, %f327, %f1454;

BB48_393:
	and.b32  	%r2565, %r627, 2;
	setp.eq.s32	%p257, %r2565, 0;
	@%p257 bra 	BB48_395;

	mov.f32 	%f1455, 0f00000000;
	mov.f32 	%f1456, 0fBF800000;
	fma.rn.f32 	%f2304, %f2304, %f1456, %f1455;

BB48_395:
	mul.f32 	%f339, %f2300, %f2304;
	mov.f32 	%f2353, %f13;
	@%p63 bra 	BB48_397;

	mov.f32 	%f1457, 0f00000000;
	mul.rn.f32 	%f2353, %f13, %f1457;

BB48_397:
	mul.f32 	%f1458, %f2353, 0f3F22F983;
	cvt.rni.s32.f32	%r4156, %f1458;
	cvt.rn.f32.s32	%f1459, %r4156;
	neg.f32 	%f1460, %f1459;
	fma.rn.f32 	%f1462, %f1460, %f1069, %f2353;
	fma.rn.f32 	%f1464, %f1460, %f1071, %f1462;
	fma.rn.f32 	%f2305, %f1460, %f1073, %f1464;
	abs.f32 	%f1466, %f2353;
	setp.leu.f32	%p259, %f1466, 0f47CE4780;
	@%p259 bra 	BB48_407;

	mov.b32 	 %r630, %f2353;
	shr.u32 	%r631, %r630, 23;
	bfe.u32 	%r2568, %r630, 23, 8;
	add.s32 	%r2569, %r2568, -128;
	shl.b32 	%r2570, %r630, 8;
	or.b32  	%r632, %r2570, -2147483648;
	shr.u32 	%r633, %r2569, 5;
	mov.u32 	%r4148, 0;
	mov.u64 	%rd363, __cudart_i2opi_f;
	mov.u32 	%r4147, -6;
	mov.u64 	%rd471, %rd1;

BB48_399:
	.pragma "nounroll";
	ld.const.u32 	%r2573, [%rd363];
	// inline asm
	{
	mad.lo.cc.u32   %r2571, %r2573, %r632, %r4148;
	madc.hi.u32     %r4148, %r2573, %r632,  0;
	}
	// inline asm
	st.local.u32 	[%rd471], %r2571;
	add.s64 	%rd471, %rd471, 4;
	add.s64 	%rd363, %rd363, 4;
	add.s32 	%r4147, %r4147, 1;
	setp.ne.s32	%p260, %r4147, 0;
	@%p260 bra 	BB48_399;

	and.b32  	%r638, %r630, -2147483648;
	st.local.u32 	[%rd3], %r4148;
	mov.u32 	%r2576, 6;
	sub.s32 	%r2577, %r2576, %r633;
	mul.wide.s32 	%rd282, %r2577, 4;
	add.s64 	%rd93, %rd1, %rd282;
	ld.local.u32 	%r4149, [%rd93];
	ld.local.u32 	%r4150, [%rd93+-4];
	and.b32  	%r641, %r631, 31;
	setp.eq.s32	%p261, %r641, 0;
	@%p261 bra 	BB48_402;

	mov.u32 	%r2578, 32;
	sub.s32 	%r2579, %r2578, %r641;
	shr.u32 	%r2580, %r4150, %r2579;
	shl.b32 	%r2581, %r4149, %r641;
	add.s32 	%r4149, %r2580, %r2581;
	ld.local.u32 	%r2582, [%rd93+-8];
	shr.u32 	%r2583, %r2582, %r2579;
	shl.b32 	%r2584, %r4150, %r641;
	add.s32 	%r4150, %r2583, %r2584;

BB48_402:
	shr.u32 	%r2585, %r4150, 30;
	shl.b32 	%r2586, %r4149, 2;
	add.s32 	%r4151, %r2585, %r2586;
	shl.b32 	%r647, %r4150, 2;
	shr.u32 	%r2587, %r4151, 31;
	shr.u32 	%r2588, %r4149, 30;
	add.s32 	%r648, %r2587, %r2588;
	setp.eq.s32	%p262, %r2587, 0;
	mov.u32 	%r4152, %r638;
	mov.u32 	%r4153, %r647;
	@%p262 bra 	BB48_404;

	not.b32 	%r2589, %r4151;
	neg.s32 	%r649, %r647;
	setp.eq.s32	%p263, %r647, 0;
	selp.u32	%r2590, 1, 0, %p263;
	add.s32 	%r4151, %r2590, %r2589;
	xor.b32  	%r651, %r638, -2147483648;
	mov.u32 	%r4152, %r651;
	mov.u32 	%r4153, %r649;

BB48_404:
	mov.u32 	%r653, %r4152;
	neg.s32 	%r2591, %r648;
	setp.eq.s32	%p264, %r638, 0;
	selp.b32	%r4156, %r648, %r2591, %p264;
	clz.b32 	%r4155, %r4151;
	setp.eq.s32	%p265, %r4155, 0;
	shl.b32 	%r2592, %r4151, %r4155;
	mov.u32 	%r2593, 32;
	sub.s32 	%r2594, %r2593, %r4155;
	shr.u32 	%r2595, %r4153, %r2594;
	add.s32 	%r2596, %r2595, %r2592;
	selp.b32	%r657, %r4151, %r2596, %p265;
	mov.u32 	%r2597, -921707870;
	mul.hi.u32 	%r4154, %r657, %r2597;
	setp.lt.s32	%p266, %r4154, 1;
	@%p266 bra 	BB48_406;

	mul.lo.s32 	%r2598, %r657, -921707870;
	shr.u32 	%r2599, %r2598, 31;
	shl.b32 	%r2600, %r4154, 1;
	add.s32 	%r4154, %r2599, %r2600;
	add.s32 	%r4155, %r4155, 1;

BB48_406:
	mov.u32 	%r2601, 126;
	sub.s32 	%r2602, %r2601, %r4155;
	shl.b32 	%r2603, %r2602, 23;
	add.s32 	%r2604, %r4154, 1;
	shr.u32 	%r2605, %r2604, 7;
	add.s32 	%r2606, %r2605, 1;
	shr.u32 	%r2607, %r2606, 1;
	add.s32 	%r2608, %r2607, %r2603;
	or.b32  	%r2609, %r2608, %r653;
	mov.b32 	 %f2305, %r2609;

BB48_407:
	mul.rn.f32 	%f345, %f2305, %f2305;
	and.b32  	%r664, %r4156, 1;
	setp.eq.s32	%p267, %r664, 0;
	@%p267 bra 	BB48_409;

	mov.f32 	%f1467, 0fBAB6061A;
	mov.f32 	%f1468, 0f37CCF5CE;
	fma.rn.f32 	%f2306, %f1468, %f345, %f1467;
	bra.uni 	BB48_410;

BB48_409:
	mov.f32 	%f1469, 0f3C08839E;
	mov.f32 	%f1470, 0fB94CA1F9;
	fma.rn.f32 	%f2306, %f1470, %f345, %f1469;

BB48_410:
	@%p267 bra 	BB48_412;

	mov.f32 	%f1471, 0f3D2AAAA5;
	fma.rn.f32 	%f1472, %f2306, %f345, %f1471;
	mov.f32 	%f1473, 0fBF000000;
	fma.rn.f32 	%f2307, %f1472, %f345, %f1473;
	bra.uni 	BB48_413;

BB48_412:
	mov.f32 	%f1474, 0fBE2AAAA3;
	fma.rn.f32 	%f1475, %f2306, %f345, %f1474;
	mov.f32 	%f1476, 0f00000000;
	fma.rn.f32 	%f2307, %f1475, %f345, %f1476;

BB48_413:
	fma.rn.f32 	%f2308, %f2307, %f2305, %f2305;
	@%p267 bra 	BB48_415;

	mov.f32 	%f1477, 0f3F800000;
	fma.rn.f32 	%f2308, %f2307, %f345, %f1477;

BB48_415:
	and.b32  	%r2610, %r4156, 2;
	setp.eq.s32	%p270, %r2610, 0;
	@%p270 bra 	BB48_417;

	mov.f32 	%f1478, 0f00000000;
	mov.f32 	%f1479, 0fBF800000;
	fma.rn.f32 	%f2308, %f2308, %f1479, %f1478;

BB48_417:
	mul.f32 	%f1480, %f339, %f2308;
	sub.f32 	%f357, %f304, %f1480;
	mov.f32 	%f2379, %f34;
	@%p37 bra 	BB48_419;

	mov.f32 	%f1481, 0f00000000;
	mul.rn.f32 	%f2379, %f34, %f1481;

BB48_419:
	mul.f32 	%f1482, %f2379, 0f3F22F983;
	cvt.rni.s32.f32	%r4166, %f1482;
	cvt.rn.f32.s32	%f1483, %r4166;
	neg.f32 	%f1484, %f1483;
	fma.rn.f32 	%f1486, %f1484, %f1069, %f2379;
	fma.rn.f32 	%f1488, %f1484, %f1071, %f1486;
	fma.rn.f32 	%f2309, %f1484, %f1073, %f1488;
	abs.f32 	%f1490, %f2379;
	setp.leu.f32	%p272, %f1490, 0f47CE4780;
	@%p272 bra 	BB48_429;

	mov.b32 	 %r666, %f2379;
	shr.u32 	%r667, %r666, 23;
	bfe.u32 	%r2613, %r666, 23, 8;
	add.s32 	%r2614, %r2613, -128;
	shl.b32 	%r2615, %r666, 8;
	or.b32  	%r668, %r2615, -2147483648;
	shr.u32 	%r669, %r2614, 5;
	mov.u32 	%r4158, 0;
	mov.u64 	%rd364, __cudart_i2opi_f;
	mov.u32 	%r4157, -6;
	mov.u64 	%rd470, %rd1;

BB48_421:
	.pragma "nounroll";
	ld.const.u32 	%r2618, [%rd364];
	// inline asm
	{
	mad.lo.cc.u32   %r2616, %r2618, %r668, %r4158;
	madc.hi.u32     %r4158, %r2618, %r668,  0;
	}
	// inline asm
	st.local.u32 	[%rd470], %r2616;
	add.s64 	%rd470, %rd470, 4;
	add.s64 	%rd364, %rd364, 4;
	add.s32 	%r4157, %r4157, 1;
	setp.ne.s32	%p273, %r4157, 0;
	@%p273 bra 	BB48_421;

	and.b32  	%r674, %r666, -2147483648;
	st.local.u32 	[%rd3], %r4158;
	mov.u32 	%r2621, 6;
	sub.s32 	%r2622, %r2621, %r669;
	mul.wide.s32 	%rd284, %r2622, 4;
	add.s64 	%rd98, %rd1, %rd284;
	ld.local.u32 	%r4159, [%rd98];
	ld.local.u32 	%r4160, [%rd98+-4];
	and.b32  	%r677, %r667, 31;
	setp.eq.s32	%p274, %r677, 0;
	@%p274 bra 	BB48_424;

	mov.u32 	%r2623, 32;
	sub.s32 	%r2624, %r2623, %r677;
	shr.u32 	%r2625, %r4160, %r2624;
	shl.b32 	%r2626, %r4159, %r677;
	add.s32 	%r4159, %r2625, %r2626;
	ld.local.u32 	%r2627, [%rd98+-8];
	shr.u32 	%r2628, %r2627, %r2624;
	shl.b32 	%r2629, %r4160, %r677;
	add.s32 	%r4160, %r2628, %r2629;

BB48_424:
	shr.u32 	%r2630, %r4160, 30;
	shl.b32 	%r2631, %r4159, 2;
	add.s32 	%r4161, %r2630, %r2631;
	shl.b32 	%r683, %r4160, 2;
	shr.u32 	%r2632, %r4161, 31;
	shr.u32 	%r2633, %r4159, 30;
	add.s32 	%r684, %r2632, %r2633;
	setp.eq.s32	%p275, %r2632, 0;
	mov.u32 	%r4162, %r674;
	mov.u32 	%r4163, %r683;
	@%p275 bra 	BB48_426;

	not.b32 	%r2634, %r4161;
	neg.s32 	%r685, %r683;
	setp.eq.s32	%p276, %r683, 0;
	selp.u32	%r2635, 1, 0, %p276;
	add.s32 	%r4161, %r2635, %r2634;
	xor.b32  	%r687, %r674, -2147483648;
	mov.u32 	%r4162, %r687;
	mov.u32 	%r4163, %r685;

BB48_426:
	mov.u32 	%r689, %r4162;
	neg.s32 	%r2636, %r684;
	setp.eq.s32	%p277, %r674, 0;
	selp.b32	%r4166, %r684, %r2636, %p277;
	clz.b32 	%r4165, %r4161;
	setp.eq.s32	%p278, %r4165, 0;
	shl.b32 	%r2637, %r4161, %r4165;
	mov.u32 	%r2638, 32;
	sub.s32 	%r2639, %r2638, %r4165;
	shr.u32 	%r2640, %r4163, %r2639;
	add.s32 	%r2641, %r2640, %r2637;
	selp.b32	%r693, %r4161, %r2641, %p278;
	mov.u32 	%r2642, -921707870;
	mul.hi.u32 	%r4164, %r693, %r2642;
	setp.lt.s32	%p279, %r4164, 1;
	@%p279 bra 	BB48_428;

	mul.lo.s32 	%r2643, %r693, -921707870;
	shr.u32 	%r2644, %r2643, 31;
	shl.b32 	%r2645, %r4164, 1;
	add.s32 	%r4164, %r2644, %r2645;
	add.s32 	%r4165, %r4165, 1;

BB48_428:
	mov.u32 	%r2646, 126;
	sub.s32 	%r2647, %r2646, %r4165;
	shl.b32 	%r2648, %r2647, 23;
	add.s32 	%r2649, %r4164, 1;
	shr.u32 	%r2650, %r2649, 7;
	add.s32 	%r2651, %r2650, 1;
	shr.u32 	%r2652, %r2651, 1;
	add.s32 	%r2653, %r2652, %r2648;
	or.b32  	%r2654, %r2653, %r689;
	mov.b32 	 %f2309, %r2654;

BB48_429:
	mul.rn.f32 	%f363, %f2309, %f2309;
	add.s32 	%r700, %r4166, 1;
	and.b32  	%r701, %r700, 1;
	setp.eq.s32	%p280, %r701, 0;
	@%p280 bra 	BB48_431;

	mov.f32 	%f1491, 0fBAB6061A;
	mov.f32 	%f1492, 0f37CCF5CE;
	fma.rn.f32 	%f2310, %f1492, %f363, %f1491;
	bra.uni 	BB48_432;

BB48_431:
	mov.f32 	%f1493, 0f3C08839E;
	mov.f32 	%f1494, 0fB94CA1F9;
	fma.rn.f32 	%f2310, %f1494, %f363, %f1493;

BB48_432:
	@%p280 bra 	BB48_434;

	mov.f32 	%f1495, 0f3D2AAAA5;
	fma.rn.f32 	%f1496, %f2310, %f363, %f1495;
	mov.f32 	%f1497, 0fBF000000;
	fma.rn.f32 	%f2311, %f1496, %f363, %f1497;
	bra.uni 	BB48_435;

BB48_434:
	mov.f32 	%f1498, 0fBE2AAAA3;
	fma.rn.f32 	%f1499, %f2310, %f363, %f1498;
	mov.f32 	%f1500, 0f00000000;
	fma.rn.f32 	%f2311, %f1499, %f363, %f1500;

BB48_435:
	fma.rn.f32 	%f2312, %f2311, %f2309, %f2309;
	@%p280 bra 	BB48_437;

	mov.f32 	%f1501, 0f3F800000;
	fma.rn.f32 	%f2312, %f2311, %f363, %f1501;

BB48_437:
	and.b32  	%r2655, %r700, 2;
	setp.eq.s32	%p283, %r2655, 0;
	@%p283 bra 	BB48_439;

	mov.f32 	%f1502, 0f00000000;
	mov.f32 	%f1503, 0fBF800000;
	fma.rn.f32 	%f2312, %f2312, %f1503, %f1502;

BB48_439:
	mov.f32 	%f2333, %f36;
	@%p50 bra 	BB48_441;

	mov.f32 	%f1504, 0f00000000;
	mul.rn.f32 	%f2333, %f36, %f1504;

BB48_441:
	mul.f32 	%f1505, %f2333, 0f3F22F983;
	cvt.rni.s32.f32	%r4176, %f1505;
	cvt.rn.f32.s32	%f1506, %r4176;
	neg.f32 	%f1507, %f1506;
	fma.rn.f32 	%f1509, %f1507, %f1069, %f2333;
	fma.rn.f32 	%f1511, %f1507, %f1071, %f1509;
	fma.rn.f32 	%f2313, %f1507, %f1073, %f1511;
	abs.f32 	%f1513, %f2333;
	setp.leu.f32	%p285, %f1513, 0f47CE4780;
	@%p285 bra 	BB48_451;

	mov.b32 	 %r703, %f2333;
	shr.u32 	%r704, %r703, 23;
	bfe.u32 	%r2658, %r703, 23, 8;
	add.s32 	%r2659, %r2658, -128;
	shl.b32 	%r2660, %r703, 8;
	or.b32  	%r705, %r2660, -2147483648;
	shr.u32 	%r706, %r2659, 5;
	mov.u32 	%r4168, 0;
	mov.u64 	%rd365, __cudart_i2opi_f;
	mov.u32 	%r4167, -6;
	mov.u64 	%rd469, %rd1;

BB48_443:
	.pragma "nounroll";
	ld.const.u32 	%r2663, [%rd365];
	// inline asm
	{
	mad.lo.cc.u32   %r2661, %r2663, %r705, %r4168;
	madc.hi.u32     %r4168, %r2663, %r705,  0;
	}
	// inline asm
	st.local.u32 	[%rd469], %r2661;
	add.s64 	%rd469, %rd469, 4;
	add.s64 	%rd365, %rd365, 4;
	add.s32 	%r4167, %r4167, 1;
	setp.ne.s32	%p286, %r4167, 0;
	@%p286 bra 	BB48_443;

	and.b32  	%r711, %r703, -2147483648;
	st.local.u32 	[%rd3], %r4168;
	mov.u32 	%r2666, 6;
	sub.s32 	%r2667, %r2666, %r706;
	mul.wide.s32 	%rd286, %r2667, 4;
	add.s64 	%rd103, %rd1, %rd286;
	ld.local.u32 	%r4169, [%rd103];
	ld.local.u32 	%r4170, [%rd103+-4];
	and.b32  	%r714, %r704, 31;
	setp.eq.s32	%p287, %r714, 0;
	@%p287 bra 	BB48_446;

	mov.u32 	%r2668, 32;
	sub.s32 	%r2669, %r2668, %r714;
	shr.u32 	%r2670, %r4170, %r2669;
	shl.b32 	%r2671, %r4169, %r714;
	add.s32 	%r4169, %r2670, %r2671;
	ld.local.u32 	%r2672, [%rd103+-8];
	shr.u32 	%r2673, %r2672, %r2669;
	shl.b32 	%r2674, %r4170, %r714;
	add.s32 	%r4170, %r2673, %r2674;

BB48_446:
	shr.u32 	%r2675, %r4170, 30;
	shl.b32 	%r2676, %r4169, 2;
	add.s32 	%r4171, %r2675, %r2676;
	shl.b32 	%r720, %r4170, 2;
	shr.u32 	%r2677, %r4171, 31;
	shr.u32 	%r2678, %r4169, 30;
	add.s32 	%r721, %r2677, %r2678;
	setp.eq.s32	%p288, %r2677, 0;
	mov.u32 	%r4172, %r711;
	mov.u32 	%r4173, %r720;
	@%p288 bra 	BB48_448;

	not.b32 	%r2679, %r4171;
	neg.s32 	%r722, %r720;
	setp.eq.s32	%p289, %r720, 0;
	selp.u32	%r2680, 1, 0, %p289;
	add.s32 	%r4171, %r2680, %r2679;
	xor.b32  	%r724, %r711, -2147483648;
	mov.u32 	%r4172, %r724;
	mov.u32 	%r4173, %r722;

BB48_448:
	mov.u32 	%r726, %r4172;
	neg.s32 	%r2681, %r721;
	setp.eq.s32	%p290, %r711, 0;
	selp.b32	%r4176, %r721, %r2681, %p290;
	clz.b32 	%r4175, %r4171;
	setp.eq.s32	%p291, %r4175, 0;
	shl.b32 	%r2682, %r4171, %r4175;
	mov.u32 	%r2683, 32;
	sub.s32 	%r2684, %r2683, %r4175;
	shr.u32 	%r2685, %r4173, %r2684;
	add.s32 	%r2686, %r2685, %r2682;
	selp.b32	%r730, %r4171, %r2686, %p291;
	mov.u32 	%r2687, -921707870;
	mul.hi.u32 	%r4174, %r730, %r2687;
	setp.lt.s32	%p292, %r4174, 1;
	@%p292 bra 	BB48_450;

	mul.lo.s32 	%r2688, %r730, -921707870;
	shr.u32 	%r2689, %r2688, 31;
	shl.b32 	%r2690, %r4174, 1;
	add.s32 	%r4174, %r2689, %r2690;
	add.s32 	%r4175, %r4175, 1;

BB48_450:
	mov.u32 	%r2691, 126;
	sub.s32 	%r2692, %r2691, %r4175;
	shl.b32 	%r2693, %r2692, 23;
	add.s32 	%r2694, %r4174, 1;
	shr.u32 	%r2695, %r2694, 7;
	add.s32 	%r2696, %r2695, 1;
	shr.u32 	%r2697, %r2696, 1;
	add.s32 	%r2698, %r2697, %r2693;
	or.b32  	%r2699, %r2698, %r726;
	mov.b32 	 %f2313, %r2699;

BB48_451:
	mul.rn.f32 	%f380, %f2313, %f2313;
	add.s32 	%r737, %r4176, 1;
	and.b32  	%r738, %r737, 1;
	setp.eq.s32	%p293, %r738, 0;
	@%p293 bra 	BB48_453;

	mov.f32 	%f1514, 0fBAB6061A;
	mov.f32 	%f1515, 0f37CCF5CE;
	fma.rn.f32 	%f2314, %f1515, %f380, %f1514;
	bra.uni 	BB48_454;

BB48_453:
	mov.f32 	%f1516, 0f3C08839E;
	mov.f32 	%f1517, 0fB94CA1F9;
	fma.rn.f32 	%f2314, %f1517, %f380, %f1516;

BB48_454:
	@%p293 bra 	BB48_456;

	mov.f32 	%f1518, 0f3D2AAAA5;
	fma.rn.f32 	%f1519, %f2314, %f380, %f1518;
	mov.f32 	%f1520, 0fBF000000;
	fma.rn.f32 	%f2315, %f1519, %f380, %f1520;
	bra.uni 	BB48_457;

BB48_456:
	mov.f32 	%f1521, 0fBE2AAAA3;
	fma.rn.f32 	%f1522, %f2314, %f380, %f1521;
	mov.f32 	%f1523, 0f00000000;
	fma.rn.f32 	%f2315, %f1522, %f380, %f1523;

BB48_457:
	fma.rn.f32 	%f2316, %f2315, %f2313, %f2313;
	@%p293 bra 	BB48_459;

	mov.f32 	%f1524, 0f3F800000;
	fma.rn.f32 	%f2316, %f2315, %f380, %f1524;

BB48_459:
	and.b32  	%r2700, %r737, 2;
	setp.eq.s32	%p296, %r2700, 0;
	@%p296 bra 	BB48_461;

	mov.f32 	%f1525, 0f00000000;
	mov.f32 	%f1526, 0fBF800000;
	fma.rn.f32 	%f2316, %f2316, %f1526, %f1525;

BB48_461:
	mul.f32 	%f392, %f2312, %f2316;
	mov.f32 	%f2352, %f13;
	@%p63 bra 	BB48_463;

	mov.f32 	%f1527, 0f00000000;
	mul.rn.f32 	%f2352, %f13, %f1527;

BB48_463:
	mul.f32 	%f1528, %f2352, 0f3F22F983;
	cvt.rni.s32.f32	%r4186, %f1528;
	cvt.rn.f32.s32	%f1529, %r4186;
	neg.f32 	%f1530, %f1529;
	fma.rn.f32 	%f1532, %f1530, %f1069, %f2352;
	fma.rn.f32 	%f1534, %f1530, %f1071, %f1532;
	fma.rn.f32 	%f2317, %f1530, %f1073, %f1534;
	abs.f32 	%f1536, %f2352;
	setp.leu.f32	%p298, %f1536, 0f47CE4780;
	@%p298 bra 	BB48_473;

	mov.b32 	 %r740, %f2352;
	shr.u32 	%r741, %r740, 23;
	bfe.u32 	%r2703, %r740, 23, 8;
	add.s32 	%r2704, %r2703, -128;
	shl.b32 	%r2705, %r740, 8;
	or.b32  	%r742, %r2705, -2147483648;
	shr.u32 	%r743, %r2704, 5;
	mov.u32 	%r4178, 0;
	mov.u64 	%rd366, __cudart_i2opi_f;
	mov.u32 	%r4177, -6;
	mov.u64 	%rd468, %rd1;

BB48_465:
	.pragma "nounroll";
	ld.const.u32 	%r2708, [%rd366];
	// inline asm
	{
	mad.lo.cc.u32   %r2706, %r2708, %r742, %r4178;
	madc.hi.u32     %r4178, %r2708, %r742,  0;
	}
	// inline asm
	st.local.u32 	[%rd468], %r2706;
	add.s64 	%rd468, %rd468, 4;
	add.s64 	%rd366, %rd366, 4;
	add.s32 	%r4177, %r4177, 1;
	setp.ne.s32	%p299, %r4177, 0;
	@%p299 bra 	BB48_465;

	and.b32  	%r748, %r740, -2147483648;
	st.local.u32 	[%rd3], %r4178;
	mov.u32 	%r2711, 6;
	sub.s32 	%r2712, %r2711, %r743;
	mul.wide.s32 	%rd288, %r2712, 4;
	add.s64 	%rd108, %rd1, %rd288;
	ld.local.u32 	%r4179, [%rd108];
	ld.local.u32 	%r4180, [%rd108+-4];
	and.b32  	%r751, %r741, 31;
	setp.eq.s32	%p300, %r751, 0;
	@%p300 bra 	BB48_468;

	mov.u32 	%r2713, 32;
	sub.s32 	%r2714, %r2713, %r751;
	shr.u32 	%r2715, %r4180, %r2714;
	shl.b32 	%r2716, %r4179, %r751;
	add.s32 	%r4179, %r2715, %r2716;
	ld.local.u32 	%r2717, [%rd108+-8];
	shr.u32 	%r2718, %r2717, %r2714;
	shl.b32 	%r2719, %r4180, %r751;
	add.s32 	%r4180, %r2718, %r2719;

BB48_468:
	shr.u32 	%r2720, %r4180, 30;
	shl.b32 	%r2721, %r4179, 2;
	add.s32 	%r4181, %r2720, %r2721;
	shl.b32 	%r757, %r4180, 2;
	shr.u32 	%r2722, %r4181, 31;
	shr.u32 	%r2723, %r4179, 30;
	add.s32 	%r758, %r2722, %r2723;
	setp.eq.s32	%p301, %r2722, 0;
	mov.u32 	%r4182, %r748;
	mov.u32 	%r4183, %r757;
	@%p301 bra 	BB48_470;

	not.b32 	%r2724, %r4181;
	neg.s32 	%r759, %r757;
	setp.eq.s32	%p302, %r757, 0;
	selp.u32	%r2725, 1, 0, %p302;
	add.s32 	%r4181, %r2725, %r2724;
	xor.b32  	%r761, %r748, -2147483648;
	mov.u32 	%r4182, %r761;
	mov.u32 	%r4183, %r759;

BB48_470:
	mov.u32 	%r763, %r4182;
	neg.s32 	%r2726, %r758;
	setp.eq.s32	%p303, %r748, 0;
	selp.b32	%r4186, %r758, %r2726, %p303;
	clz.b32 	%r4185, %r4181;
	setp.eq.s32	%p304, %r4185, 0;
	shl.b32 	%r2727, %r4181, %r4185;
	mov.u32 	%r2728, 32;
	sub.s32 	%r2729, %r2728, %r4185;
	shr.u32 	%r2730, %r4183, %r2729;
	add.s32 	%r2731, %r2730, %r2727;
	selp.b32	%r767, %r4181, %r2731, %p304;
	mov.u32 	%r2732, -921707870;
	mul.hi.u32 	%r4184, %r767, %r2732;
	setp.lt.s32	%p305, %r4184, 1;
	@%p305 bra 	BB48_472;

	mul.lo.s32 	%r2733, %r767, -921707870;
	shr.u32 	%r2734, %r2733, 31;
	shl.b32 	%r2735, %r4184, 1;
	add.s32 	%r4184, %r2734, %r2735;
	add.s32 	%r4185, %r4185, 1;

BB48_472:
	mov.u32 	%r2736, 126;
	sub.s32 	%r2737, %r2736, %r4185;
	shl.b32 	%r2738, %r2737, 23;
	add.s32 	%r2739, %r4184, 1;
	shr.u32 	%r2740, %r2739, 7;
	add.s32 	%r2741, %r2740, 1;
	shr.u32 	%r2742, %r2741, 1;
	add.s32 	%r2743, %r2742, %r2738;
	or.b32  	%r2744, %r2743, %r763;
	mov.b32 	 %f2317, %r2744;

BB48_473:
	mul.rn.f32 	%f398, %f2317, %f2317;
	and.b32  	%r774, %r4186, 1;
	setp.eq.s32	%p306, %r774, 0;
	@%p306 bra 	BB48_475;

	mov.f32 	%f1537, 0fBAB6061A;
	mov.f32 	%f1538, 0f37CCF5CE;
	fma.rn.f32 	%f2318, %f1538, %f398, %f1537;
	bra.uni 	BB48_476;

BB48_475:
	mov.f32 	%f1539, 0f3C08839E;
	mov.f32 	%f1540, 0fB94CA1F9;
	fma.rn.f32 	%f2318, %f1540, %f398, %f1539;

BB48_476:
	@%p306 bra 	BB48_478;

	mov.f32 	%f1541, 0f3D2AAAA5;
	fma.rn.f32 	%f1542, %f2318, %f398, %f1541;
	mov.f32 	%f1543, 0fBF000000;
	fma.rn.f32 	%f2319, %f1542, %f398, %f1543;
	bra.uni 	BB48_479;

BB48_478:
	mov.f32 	%f1544, 0fBE2AAAA3;
	fma.rn.f32 	%f1545, %f2318, %f398, %f1544;
	mov.f32 	%f1546, 0f00000000;
	fma.rn.f32 	%f2319, %f1545, %f398, %f1546;

BB48_479:
	fma.rn.f32 	%f2320, %f2319, %f2317, %f2317;
	@%p306 bra 	BB48_481;

	mov.f32 	%f1547, 0f3F800000;
	fma.rn.f32 	%f2320, %f2319, %f398, %f1547;

BB48_481:
	and.b32  	%r2745, %r4186, 2;
	setp.eq.s32	%p309, %r2745, 0;
	@%p309 bra 	BB48_483;

	mov.f32 	%f1548, 0f00000000;
	mov.f32 	%f1549, 0fBF800000;
	fma.rn.f32 	%f2320, %f2320, %f1549, %f1548;

BB48_483:
	mul.f32 	%f410, %f392, %f2320;
	mov.f32 	%f2378, %f34;
	@%p37 bra 	BB48_485;

	mov.f32 	%f1550, 0f00000000;
	mul.rn.f32 	%f2378, %f34, %f1550;

BB48_485:
	mul.f32 	%f1551, %f2378, 0f3F22F983;
	cvt.rni.s32.f32	%r4196, %f1551;
	cvt.rn.f32.s32	%f1552, %r4196;
	neg.f32 	%f1553, %f1552;
	fma.rn.f32 	%f1555, %f1553, %f1069, %f2378;
	fma.rn.f32 	%f1557, %f1553, %f1071, %f1555;
	fma.rn.f32 	%f2321, %f1553, %f1073, %f1557;
	abs.f32 	%f1559, %f2378;
	setp.leu.f32	%p311, %f1559, 0f47CE4780;
	@%p311 bra 	BB48_495;

	mov.b32 	 %r776, %f2378;
	shr.u32 	%r777, %r776, 23;
	bfe.u32 	%r2748, %r776, 23, 8;
	add.s32 	%r2749, %r2748, -128;
	shl.b32 	%r2750, %r776, 8;
	or.b32  	%r778, %r2750, -2147483648;
	shr.u32 	%r779, %r2749, 5;
	mov.u32 	%r4188, 0;
	mov.u64 	%rd367, __cudart_i2opi_f;
	mov.u32 	%r4187, -6;
	mov.u64 	%rd467, %rd1;

BB48_487:
	.pragma "nounroll";
	ld.const.u32 	%r2753, [%rd367];
	// inline asm
	{
	mad.lo.cc.u32   %r2751, %r2753, %r778, %r4188;
	madc.hi.u32     %r4188, %r2753, %r778,  0;
	}
	// inline asm
	st.local.u32 	[%rd467], %r2751;
	add.s64 	%rd467, %rd467, 4;
	add.s64 	%rd367, %rd367, 4;
	add.s32 	%r4187, %r4187, 1;
	setp.ne.s32	%p312, %r4187, 0;
	@%p312 bra 	BB48_487;

	and.b32  	%r784, %r776, -2147483648;
	st.local.u32 	[%rd3], %r4188;
	mov.u32 	%r2756, 6;
	sub.s32 	%r2757, %r2756, %r779;
	mul.wide.s32 	%rd290, %r2757, 4;
	add.s64 	%rd113, %rd1, %rd290;
	ld.local.u32 	%r4189, [%rd113];
	ld.local.u32 	%r4190, [%rd113+-4];
	and.b32  	%r787, %r777, 31;
	setp.eq.s32	%p313, %r787, 0;
	@%p313 bra 	BB48_490;

	mov.u32 	%r2758, 32;
	sub.s32 	%r2759, %r2758, %r787;
	shr.u32 	%r2760, %r4190, %r2759;
	shl.b32 	%r2761, %r4189, %r787;
	add.s32 	%r4189, %r2760, %r2761;
	ld.local.u32 	%r2762, [%rd113+-8];
	shr.u32 	%r2763, %r2762, %r2759;
	shl.b32 	%r2764, %r4190, %r787;
	add.s32 	%r4190, %r2763, %r2764;

BB48_490:
	shr.u32 	%r2765, %r4190, 30;
	shl.b32 	%r2766, %r4189, 2;
	add.s32 	%r4191, %r2765, %r2766;
	shl.b32 	%r793, %r4190, 2;
	shr.u32 	%r2767, %r4191, 31;
	shr.u32 	%r2768, %r4189, 30;
	add.s32 	%r794, %r2767, %r2768;
	setp.eq.s32	%p314, %r2767, 0;
	mov.u32 	%r4192, %r784;
	mov.u32 	%r4193, %r793;
	@%p314 bra 	BB48_492;

	not.b32 	%r2769, %r4191;
	neg.s32 	%r795, %r793;
	setp.eq.s32	%p315, %r793, 0;
	selp.u32	%r2770, 1, 0, %p315;
	add.s32 	%r4191, %r2770, %r2769;
	xor.b32  	%r797, %r784, -2147483648;
	mov.u32 	%r4192, %r797;
	mov.u32 	%r4193, %r795;

BB48_492:
	mov.u32 	%r799, %r4192;
	neg.s32 	%r2771, %r794;
	setp.eq.s32	%p316, %r784, 0;
	selp.b32	%r4196, %r794, %r2771, %p316;
	clz.b32 	%r4195, %r4191;
	setp.eq.s32	%p317, %r4195, 0;
	shl.b32 	%r2772, %r4191, %r4195;
	mov.u32 	%r2773, 32;
	sub.s32 	%r2774, %r2773, %r4195;
	shr.u32 	%r2775, %r4193, %r2774;
	add.s32 	%r2776, %r2775, %r2772;
	selp.b32	%r803, %r4191, %r2776, %p317;
	mov.u32 	%r2777, -921707870;
	mul.hi.u32 	%r4194, %r803, %r2777;
	setp.lt.s32	%p318, %r4194, 1;
	@%p318 bra 	BB48_494;

	mul.lo.s32 	%r2778, %r803, -921707870;
	shr.u32 	%r2779, %r2778, 31;
	shl.b32 	%r2780, %r4194, 1;
	add.s32 	%r4194, %r2779, %r2780;
	add.s32 	%r4195, %r4195, 1;

BB48_494:
	mov.u32 	%r2781, 126;
	sub.s32 	%r2782, %r2781, %r4195;
	shl.b32 	%r2783, %r2782, 23;
	add.s32 	%r2784, %r4194, 1;
	shr.u32 	%r2785, %r2784, 7;
	add.s32 	%r2786, %r2785, 1;
	shr.u32 	%r2787, %r2786, 1;
	add.s32 	%r2788, %r2787, %r2783;
	or.b32  	%r2789, %r2788, %r799;
	mov.b32 	 %f2321, %r2789;

BB48_495:
	mul.rn.f32 	%f416, %f2321, %f2321;
	and.b32  	%r810, %r4196, 1;
	setp.eq.s32	%p319, %r810, 0;
	@%p319 bra 	BB48_497;

	mov.f32 	%f1560, 0fBAB6061A;
	mov.f32 	%f1561, 0f37CCF5CE;
	fma.rn.f32 	%f2322, %f1561, %f416, %f1560;
	bra.uni 	BB48_498;

BB48_497:
	mov.f32 	%f1562, 0f3C08839E;
	mov.f32 	%f1563, 0fB94CA1F9;
	fma.rn.f32 	%f2322, %f1563, %f416, %f1562;

BB48_498:
	@%p319 bra 	BB48_500;

	mov.f32 	%f1564, 0f3D2AAAA5;
	fma.rn.f32 	%f1565, %f2322, %f416, %f1564;
	mov.f32 	%f1566, 0fBF000000;
	fma.rn.f32 	%f2323, %f1565, %f416, %f1566;
	bra.uni 	BB48_501;

BB48_500:
	mov.f32 	%f1567, 0fBE2AAAA3;
	fma.rn.f32 	%f1568, %f2322, %f416, %f1567;
	mov.f32 	%f1569, 0f00000000;
	fma.rn.f32 	%f2323, %f1568, %f416, %f1569;

BB48_501:
	fma.rn.f32 	%f2324, %f2323, %f2321, %f2321;
	@%p319 bra 	BB48_503;

	mov.f32 	%f1570, 0f3F800000;
	fma.rn.f32 	%f2324, %f2323, %f416, %f1570;

BB48_503:
	and.b32  	%r2790, %r4196, 2;
	setp.eq.s32	%p322, %r2790, 0;
	@%p322 bra 	BB48_505;

	mov.f32 	%f1571, 0f00000000;
	mov.f32 	%f1572, 0fBF800000;
	fma.rn.f32 	%f2324, %f2324, %f1572, %f1571;

BB48_505:
	mov.f32 	%f2332, %f36;
	@%p50 bra 	BB48_507;

	mov.f32 	%f1573, 0f00000000;
	mul.rn.f32 	%f2332, %f36, %f1573;

BB48_507:
	mul.f32 	%f1574, %f2332, 0f3F22F983;
	cvt.rni.s32.f32	%r4206, %f1574;
	cvt.rn.f32.s32	%f1575, %r4206;
	neg.f32 	%f1576, %f1575;
	fma.rn.f32 	%f1578, %f1576, %f1069, %f2332;
	fma.rn.f32 	%f1580, %f1576, %f1071, %f1578;
	fma.rn.f32 	%f2340, %f1576, %f1073, %f1580;
	abs.f32 	%f1582, %f2332;
	setp.leu.f32	%p324, %f1582, 0f47CE4780;
	@%p324 bra 	BB48_517;

	mov.b32 	 %r812, %f2332;
	shr.u32 	%r813, %r812, 23;
	bfe.u32 	%r2793, %r812, 23, 8;
	add.s32 	%r2794, %r2793, -128;
	shl.b32 	%r2795, %r812, 8;
	or.b32  	%r814, %r2795, -2147483648;
	shr.u32 	%r815, %r2794, 5;
	mov.u32 	%r4198, 0;
	mov.u64 	%rd368, __cudart_i2opi_f;
	mov.u32 	%r4197, -6;
	mov.u64 	%rd466, %rd1;

BB48_509:
	.pragma "nounroll";
	ld.const.u32 	%r2798, [%rd368];
	// inline asm
	{
	mad.lo.cc.u32   %r2796, %r2798, %r814, %r4198;
	madc.hi.u32     %r4198, %r2798, %r814,  0;
	}
	// inline asm
	st.local.u32 	[%rd466], %r2796;
	add.s64 	%rd466, %rd466, 4;
	add.s64 	%rd368, %rd368, 4;
	add.s32 	%r4197, %r4197, 1;
	setp.ne.s32	%p325, %r4197, 0;
	@%p325 bra 	BB48_509;

	and.b32  	%r820, %r812, -2147483648;
	st.local.u32 	[%rd3], %r4198;
	mov.u32 	%r2801, 6;
	sub.s32 	%r2802, %r2801, %r815;
	mul.wide.s32 	%rd292, %r2802, 4;
	add.s64 	%rd118, %rd1, %rd292;
	ld.local.u32 	%r4199, [%rd118];
	ld.local.u32 	%r4200, [%rd118+-4];
	and.b32  	%r823, %r813, 31;
	setp.eq.s32	%p326, %r823, 0;
	@%p326 bra 	BB48_512;

	mov.u32 	%r2803, 32;
	sub.s32 	%r2804, %r2803, %r823;
	shr.u32 	%r2805, %r4200, %r2804;
	shl.b32 	%r2806, %r4199, %r823;
	add.s32 	%r4199, %r2805, %r2806;
	ld.local.u32 	%r2807, [%rd118+-8];
	shr.u32 	%r2808, %r2807, %r2804;
	shl.b32 	%r2809, %r4200, %r823;
	add.s32 	%r4200, %r2808, %r2809;

BB48_512:
	shr.u32 	%r2810, %r4200, 30;
	shl.b32 	%r2811, %r4199, 2;
	add.s32 	%r4201, %r2810, %r2811;
	shl.b32 	%r829, %r4200, 2;
	shr.u32 	%r2812, %r4201, 31;
	shr.u32 	%r2813, %r4199, 30;
	add.s32 	%r830, %r2812, %r2813;
	setp.eq.s32	%p327, %r2812, 0;
	mov.u32 	%r4202, %r820;
	mov.u32 	%r4203, %r829;
	@%p327 bra 	BB48_514;

	not.b32 	%r2814, %r4201;
	neg.s32 	%r831, %r829;
	setp.eq.s32	%p328, %r829, 0;
	selp.u32	%r2815, 1, 0, %p328;
	add.s32 	%r4201, %r2815, %r2814;
	xor.b32  	%r833, %r820, -2147483648;
	mov.u32 	%r4202, %r833;
	mov.u32 	%r4203, %r831;

BB48_514:
	mov.u32 	%r835, %r4202;
	neg.s32 	%r2816, %r830;
	setp.eq.s32	%p329, %r820, 0;
	selp.b32	%r4206, %r830, %r2816, %p329;
	clz.b32 	%r4205, %r4201;
	setp.eq.s32	%p330, %r4205, 0;
	shl.b32 	%r2817, %r4201, %r4205;
	mov.u32 	%r2818, 32;
	sub.s32 	%r2819, %r2818, %r4205;
	shr.u32 	%r2820, %r4203, %r2819;
	add.s32 	%r2821, %r2820, %r2817;
	selp.b32	%r839, %r4201, %r2821, %p330;
	mov.u32 	%r2822, -921707870;
	mul.hi.u32 	%r4204, %r839, %r2822;
	setp.lt.s32	%p331, %r4204, 1;
	@%p331 bra 	BB48_516;

	mul.lo.s32 	%r2823, %r839, -921707870;
	shr.u32 	%r2824, %r2823, 31;
	shl.b32 	%r2825, %r4204, 1;
	add.s32 	%r4204, %r2824, %r2825;
	add.s32 	%r4205, %r4205, 1;

BB48_516:
	mov.u32 	%r2826, 126;
	sub.s32 	%r2827, %r2826, %r4205;
	shl.b32 	%r2828, %r2827, 23;
	add.s32 	%r2829, %r4204, 1;
	shr.u32 	%r2830, %r2829, 7;
	add.s32 	%r2831, %r2830, 1;
	shr.u32 	%r2832, %r2831, 1;
	add.s32 	%r2833, %r2832, %r2828;
	or.b32  	%r2834, %r2833, %r835;
	mov.b32 	 %f2340, %r2834;

BB48_517:
	mul.rn.f32 	%f433, %f2340, %f2340;
	and.b32  	%r846, %r4206, 1;
	setp.eq.s32	%p332, %r846, 0;
	@%p332 bra 	BB48_519;

	mov.f32 	%f1583, 0fBAB6061A;
	mov.f32 	%f1584, 0f37CCF5CE;
	fma.rn.f32 	%f2341, %f1584, %f433, %f1583;
	bra.uni 	BB48_520;

BB48_519:
	mov.f32 	%f1585, 0f3C08839E;
	mov.f32 	%f1586, 0fB94CA1F9;
	fma.rn.f32 	%f2341, %f1586, %f433, %f1585;

BB48_520:
	@%p332 bra 	BB48_522;

	mov.f32 	%f1587, 0f3D2AAAA5;
	fma.rn.f32 	%f1588, %f2341, %f433, %f1587;
	mov.f32 	%f1589, 0fBF000000;
	fma.rn.f32 	%f2342, %f1588, %f433, %f1589;
	bra.uni 	BB48_523;

BB48_522:
	mov.f32 	%f1590, 0fBE2AAAA3;
	fma.rn.f32 	%f1591, %f2341, %f433, %f1590;
	mov.f32 	%f1592, 0f00000000;
	fma.rn.f32 	%f2342, %f1591, %f433, %f1592;

BB48_523:
	fma.rn.f32 	%f2343, %f2342, %f2340, %f2340;
	@%p332 bra 	BB48_525;

	mov.f32 	%f1593, 0f3F800000;
	fma.rn.f32 	%f2343, %f2342, %f433, %f1593;

BB48_525:
	and.b32  	%r2835, %r4206, 2;
	setp.eq.s32	%p335, %r2835, 0;
	@%p335 bra 	BB48_527;

	mov.f32 	%f1594, 0f00000000;
	mov.f32 	%f1595, 0fBF800000;
	fma.rn.f32 	%f2343, %f2343, %f1595, %f1594;

BB48_527:
	mul.f32 	%f445, %f2324, %f2343;
	mov.f32 	%f2351, %f13;
	@%p63 bra 	BB48_529;

	mov.f32 	%f1596, 0f00000000;
	mul.rn.f32 	%f2351, %f13, %f1596;

BB48_529:
	mul.f32 	%f1597, %f2351, 0f3F22F983;
	cvt.rni.s32.f32	%r4216, %f1597;
	cvt.rn.f32.s32	%f1598, %r4216;
	neg.f32 	%f1599, %f1598;
	fma.rn.f32 	%f1601, %f1599, %f1069, %f2351;
	fma.rn.f32 	%f1603, %f1599, %f1071, %f1601;
	fma.rn.f32 	%f2359, %f1599, %f1073, %f1603;
	abs.f32 	%f1605, %f2351;
	setp.leu.f32	%p337, %f1605, 0f47CE4780;
	@%p337 bra 	BB48_539;

	mov.b32 	 %r848, %f2351;
	shr.u32 	%r849, %r848, 23;
	bfe.u32 	%r2838, %r848, 23, 8;
	add.s32 	%r2839, %r2838, -128;
	shl.b32 	%r2840, %r848, 8;
	or.b32  	%r850, %r2840, -2147483648;
	shr.u32 	%r851, %r2839, 5;
	mov.u32 	%r4208, 0;
	mov.u64 	%rd369, __cudart_i2opi_f;
	mov.u32 	%r4207, -6;
	mov.u64 	%rd465, %rd1;

BB48_531:
	.pragma "nounroll";
	ld.const.u32 	%r2843, [%rd369];
	// inline asm
	{
	mad.lo.cc.u32   %r2841, %r2843, %r850, %r4208;
	madc.hi.u32     %r4208, %r2843, %r850,  0;
	}
	// inline asm
	st.local.u32 	[%rd465], %r2841;
	add.s64 	%rd465, %rd465, 4;
	add.s64 	%rd369, %rd369, 4;
	add.s32 	%r4207, %r4207, 1;
	setp.ne.s32	%p338, %r4207, 0;
	@%p338 bra 	BB48_531;

	and.b32  	%r856, %r848, -2147483648;
	st.local.u32 	[%rd3], %r4208;
	mov.u32 	%r2846, 6;
	sub.s32 	%r2847, %r2846, %r851;
	mul.wide.s32 	%rd294, %r2847, 4;
	add.s64 	%rd123, %rd1, %rd294;
	ld.local.u32 	%r4209, [%rd123];
	ld.local.u32 	%r4210, [%rd123+-4];
	and.b32  	%r859, %r849, 31;
	setp.eq.s32	%p339, %r859, 0;
	@%p339 bra 	BB48_534;

	mov.u32 	%r2848, 32;
	sub.s32 	%r2849, %r2848, %r859;
	shr.u32 	%r2850, %r4210, %r2849;
	shl.b32 	%r2851, %r4209, %r859;
	add.s32 	%r4209, %r2850, %r2851;
	ld.local.u32 	%r2852, [%rd123+-8];
	shr.u32 	%r2853, %r2852, %r2849;
	shl.b32 	%r2854, %r4210, %r859;
	add.s32 	%r4210, %r2853, %r2854;

BB48_534:
	shr.u32 	%r2855, %r4210, 30;
	shl.b32 	%r2856, %r4209, 2;
	add.s32 	%r4211, %r2855, %r2856;
	shl.b32 	%r865, %r4210, 2;
	shr.u32 	%r2857, %r4211, 31;
	shr.u32 	%r2858, %r4209, 30;
	add.s32 	%r866, %r2857, %r2858;
	setp.eq.s32	%p340, %r2857, 0;
	mov.u32 	%r4212, %r856;
	mov.u32 	%r4213, %r865;
	@%p340 bra 	BB48_536;

	not.b32 	%r2859, %r4211;
	neg.s32 	%r867, %r865;
	setp.eq.s32	%p341, %r865, 0;
	selp.u32	%r2860, 1, 0, %p341;
	add.s32 	%r4211, %r2860, %r2859;
	xor.b32  	%r869, %r856, -2147483648;
	mov.u32 	%r4212, %r869;
	mov.u32 	%r4213, %r867;

BB48_536:
	mov.u32 	%r871, %r4212;
	neg.s32 	%r2861, %r866;
	setp.eq.s32	%p342, %r856, 0;
	selp.b32	%r4216, %r866, %r2861, %p342;
	clz.b32 	%r4215, %r4211;
	setp.eq.s32	%p343, %r4215, 0;
	shl.b32 	%r2862, %r4211, %r4215;
	mov.u32 	%r2863, 32;
	sub.s32 	%r2864, %r2863, %r4215;
	shr.u32 	%r2865, %r4213, %r2864;
	add.s32 	%r2866, %r2865, %r2862;
	selp.b32	%r875, %r4211, %r2866, %p343;
	mov.u32 	%r2867, -921707870;
	mul.hi.u32 	%r4214, %r875, %r2867;
	setp.lt.s32	%p344, %r4214, 1;
	@%p344 bra 	BB48_538;

	mul.lo.s32 	%r2868, %r875, -921707870;
	shr.u32 	%r2869, %r2868, 31;
	shl.b32 	%r2870, %r4214, 1;
	add.s32 	%r4214, %r2869, %r2870;
	add.s32 	%r4215, %r4215, 1;

BB48_538:
	mov.u32 	%r2871, 126;
	sub.s32 	%r2872, %r2871, %r4215;
	shl.b32 	%r2873, %r2872, 23;
	add.s32 	%r2874, %r4214, 1;
	shr.u32 	%r2875, %r2874, 7;
	add.s32 	%r2876, %r2875, 1;
	shr.u32 	%r2877, %r2876, 1;
	add.s32 	%r2878, %r2877, %r2873;
	or.b32  	%r2879, %r2878, %r871;
	mov.b32 	 %f2359, %r2879;

BB48_539:
	mul.rn.f32 	%f451, %f2359, %f2359;
	add.s32 	%r882, %r4216, 1;
	and.b32  	%r883, %r882, 1;
	setp.eq.s32	%p345, %r883, 0;
	@%p345 bra 	BB48_541;

	mov.f32 	%f1606, 0fBAB6061A;
	mov.f32 	%f1607, 0f37CCF5CE;
	fma.rn.f32 	%f2360, %f1607, %f451, %f1606;
	bra.uni 	BB48_542;

BB48_541:
	mov.f32 	%f1608, 0f3C08839E;
	mov.f32 	%f1609, 0fB94CA1F9;
	fma.rn.f32 	%f2360, %f1609, %f451, %f1608;

BB48_542:
	@%p345 bra 	BB48_544;

	mov.f32 	%f1610, 0f3D2AAAA5;
	fma.rn.f32 	%f1611, %f2360, %f451, %f1610;
	mov.f32 	%f1612, 0fBF000000;
	fma.rn.f32 	%f2361, %f1611, %f451, %f1612;
	bra.uni 	BB48_545;

BB48_544:
	mov.f32 	%f1613, 0fBE2AAAA3;
	fma.rn.f32 	%f1614, %f2360, %f451, %f1613;
	mov.f32 	%f1615, 0f00000000;
	fma.rn.f32 	%f2361, %f1614, %f451, %f1615;

BB48_545:
	fma.rn.f32 	%f2362, %f2361, %f2359, %f2359;
	@%p345 bra 	BB48_547;

	mov.f32 	%f1616, 0f3F800000;
	fma.rn.f32 	%f2362, %f2361, %f451, %f1616;

BB48_547:
	and.b32  	%r2880, %r882, 2;
	setp.eq.s32	%p348, %r2880, 0;
	@%p348 bra 	BB48_549;

	mov.f32 	%f1617, 0f00000000;
	mov.f32 	%f1618, 0fBF800000;
	fma.rn.f32 	%f2362, %f2362, %f1618, %f1617;

BB48_549:
	fma.rn.f32 	%f463, %f445, %f2362, %f410;
	neg.f32 	%f1619, %f6;
	add.f32 	%f1620, %f145, %f6;
	abs.f32 	%f1621, %f1620;
	cvt.f64.f32	%fd2, %f1621;
	abs.f32 	%f1622, %f1619;
	abs.f32 	%f1623, %f145;
	max.f32 	%f1624, %f1623, %f1622;
	cvt.f64.f32	%fd3, %f1624;
	mul.f64 	%fd4, %fd3, 0d3EE9000000000000;
	setp.geu.f64	%p349, %fd2, %fd4;
	mov.f32 	%f2377, %f34;
	@%p349 bra 	BB48_554;

	neg.f32 	%f1625, %f8;
	add.f32 	%f1626, %f251, %f8;
	abs.f32 	%f1627, %f1626;
	cvt.f64.f32	%fd5, %f1627;
	abs.f32 	%f1628, %f1625;
	abs.f32 	%f1629, %f251;
	max.f32 	%f1630, %f1629, %f1628;
	cvt.f64.f32	%fd6, %f1630;
	mul.f64 	%fd7, %fd6, 0d3EE9000000000000;
	setp.geu.f64	%p350, %fd5, %fd7;
	mov.f32 	%f2377, %f34;
	@%p350 bra 	BB48_554;

	neg.f32 	%f1631, %f7;
	add.f32 	%f1632, %f357, %f7;
	abs.f32 	%f1633, %f1632;
	cvt.f64.f32	%fd8, %f1633;
	abs.f32 	%f1634, %f1631;
	abs.f32 	%f1635, %f357;
	max.f32 	%f1636, %f1635, %f1634;
	cvt.f64.f32	%fd9, %f1636;
	mul.f64 	%fd10, %fd9, 0d3EE9000000000000;
	setp.geu.f64	%p351, %fd8, %fd10;
	mov.f32 	%f2377, %f34;
	@%p351 bra 	BB48_554;

	neg.f32 	%f1637, %f9;
	add.f32 	%f1638, %f463, %f9;
	abs.f32 	%f1639, %f1638;
	cvt.f64.f32	%fd11, %f1639;
	abs.f32 	%f1640, %f1637;
	abs.f32 	%f1641, %f463;
	max.f32 	%f1642, %f1641, %f1640;
	cvt.f64.f32	%fd12, %f1642;
	mul.f64 	%fd13, %fd12, 0d3EE9000000000000;
	setp.geu.f64	%p352, %fd11, %fd13;
	mov.f32 	%f2377, %f34;
	@%p352 bra 	BB48_554;

	cvt.f64.f32	%fd14, %f34;
	setp.ltu.f32	%p353, %f34, 0f00000000;
	selp.f64	%fd15, 0dC00921FB54442D18, 0d400921FB54442D18, %p353;
	sub.f64 	%fd16, %fd14, %fd15;
	cvt.rn.f32.f64	%f2377, %fd16;

BB48_554:
	mov.f32 	%f2389, %f13;
	mov.f32 	%f2388, %f36;
	mov.f32 	%f2387, %f2377;

BB48_555:
	ld.param.u32 	%r3970, [_Z18actfunc_quat_multiP6float4iiii_param_1];
	cvt.rn.f64.s32	%fd17, %r3970;
	mov.f64 	%fd18, 0d401921FB54442D18;
	div.rn.f64 	%fd19, %fd18, %fd17;
	cvt.rn.f32.f64	%f474, %fd19;
	mul.f32 	%f475, %f474, 0f3F000000;
	sub.f32 	%f1643, %f2387, %f475;
	add.f32 	%f1644, %f1643, 0f40490FDB;
	div.rn.f32 	%f476, %f1644, %f474;
	abs.f32 	%f1645, %f476;
	mov.b32 	 %r2881, %f476;
	and.b32  	%r2882, %r2881, -2147483648;
	or.b32  	%r2883, %r2882, 1056964608;
	mov.b32 	 %f1646, %r2883;
	add.f32 	%f1647, %f476, %f1646;
	cvt.rzi.f32.f32	%f1648, %f1647;
	setp.gt.f32	%p354, %f1645, 0f4B000000;
	selp.f32	%f2390, %f476, %f1648, %p354;
	setp.geu.f32	%p355, %f1645, 0f3F000000;
	@%p355 bra 	BB48_557;

	cvt.rzi.f32.f32	%f2390, %f476;

BB48_557:
	ld.param.u32 	%r3971, [_Z18actfunc_quat_multiP6float4iiii_param_2];
	fma.rn.f32 	%f1649, %f474, %f2390, 0fC0490FDB;
	add.f32 	%f480, %f475, %f1649;
	cvt.rn.f64.s32	%fd20, %r3971;
	div.rn.f64 	%fd22, %fd18, %fd20;
	mul.f64 	%fd23, %fd22, 0d3FE0000000000000;
	cvt.rn.f32.f64	%f481, %fd23;
	mul.f32 	%f482, %f481, 0f3F000000;
	sub.f32 	%f1650, %f2388, %f482;
	add.f32 	%f1651, %f1650, 0f3FC90FDB;
	div.rn.f32 	%f483, %f1651, %f481;
	abs.f32 	%f1652, %f483;
	mov.b32 	 %r2884, %f483;
	and.b32  	%r2885, %r2884, -2147483648;
	or.b32  	%r2886, %r2885, 1056964608;
	mov.b32 	 %f1653, %r2886;
	add.f32 	%f1654, %f483, %f1653;
	cvt.rzi.f32.f32	%f1655, %f1654;
	setp.gt.f32	%p356, %f1652, 0f4B000000;
	selp.f32	%f2391, %f483, %f1655, %p356;
	setp.geu.f32	%p357, %f1652, 0f3F000000;
	@%p357 bra 	BB48_559;

	cvt.rzi.f32.f32	%f2391, %f483;

BB48_559:
	ld.param.u32 	%r3972, [_Z18actfunc_quat_multiP6float4iiii_param_3];
	fma.rn.f32 	%f1656, %f481, %f2391, 0fBFC90FDB;
	add.f32 	%f487, %f482, %f1656;
	cvt.rn.f64.s32	%fd24, %r3972;
	div.rn.f64 	%fd26, %fd18, %fd24;
	mul.f64 	%fd27, %fd26, 0d3FD0000000000000;
	cvt.rn.f32.f64	%f488, %fd27;
	mul.f32 	%f489, %f488, 0f3F000000;
	sub.f32 	%f1657, %f2389, %f489;
	add.f32 	%f1658, %f1657, 0f3F490FDB;
	div.rn.f32 	%f490, %f1658, %f488;
	abs.f32 	%f1659, %f490;
	mov.b32 	 %r2887, %f490;
	and.b32  	%r2888, %r2887, -2147483648;
	or.b32  	%r2889, %r2888, 1056964608;
	mov.b32 	 %f1660, %r2889;
	add.f32 	%f1661, %f490, %f1660;
	cvt.rzi.f32.f32	%f1662, %f1661;
	setp.gt.f32	%p358, %f1659, 0f4B000000;
	selp.f32	%f2392, %f490, %f1662, %p358;
	setp.geu.f32	%p359, %f1659, 0f3F000000;
	@%p359 bra 	BB48_561;

	cvt.rzi.f32.f32	%f2392, %f490;

BB48_561:
	fma.rn.f32 	%f1663, %f488, %f2392, 0fBF490FDB;
	add.f32 	%f494, %f489, %f1663;
	abs.f32 	%f495, %f480;
	setp.neu.f32	%p360, %f495, 0f7F800000;
	mov.f32 	%f2491, %f480;
	@%p360 bra 	BB48_563;

	mov.f32 	%f1664, 0f00000000;
	mul.rn.f32 	%f496, %f480, %f1664;
	mov.f32 	%f2491, %f496;

BB48_563:
	mov.f32 	%f497, %f2491;
	mul.f32 	%f1665, %f497, 0f3F22F983;
	cvt.rni.s32.f32	%r4226, %f1665;
	cvt.rn.f32.s32	%f1666, %r4226;
	neg.f32 	%f1667, %f1666;
	mov.f32 	%f1668, 0f3FC90FDA;
	fma.rn.f32 	%f1669, %f1667, %f1668, %f497;
	mov.f32 	%f1670, 0f33A22168;
	fma.rn.f32 	%f1671, %f1667, %f1670, %f1669;
	mov.f32 	%f1672, 0f27C234C5;
	fma.rn.f32 	%f2393, %f1667, %f1672, %f1671;
	abs.f32 	%f1673, %f497;
	setp.leu.f32	%p361, %f1673, 0f47CE4780;
	@%p361 bra 	BB48_573;

	mov.b32 	 %r885, %f497;
	shr.u32 	%r886, %r885, 23;
	bfe.u32 	%r2892, %r885, 23, 8;
	add.s32 	%r2893, %r2892, -128;
	shl.b32 	%r2894, %r885, 8;
	or.b32  	%r887, %r2894, -2147483648;
	shr.u32 	%r888, %r2893, 5;
	mov.u32 	%r4218, 0;
	mov.u64 	%rd370, __cudart_i2opi_f;
	mov.u32 	%r4217, -6;
	mov.u64 	%rd464, %rd1;

BB48_565:
	.pragma "nounroll";
	ld.const.u32 	%r2897, [%rd370];
	// inline asm
	{
	mad.lo.cc.u32   %r2895, %r2897, %r887, %r4218;
	madc.hi.u32     %r4218, %r2897, %r887,  0;
	}
	// inline asm
	st.local.u32 	[%rd464], %r2895;
	add.s64 	%rd464, %rd464, 4;
	add.s64 	%rd370, %rd370, 4;
	add.s32 	%r4217, %r4217, 1;
	setp.ne.s32	%p362, %r4217, 0;
	@%p362 bra 	BB48_565;

	and.b32  	%r893, %r885, -2147483648;
	st.local.u32 	[%rd3], %r4218;
	mov.u32 	%r2900, 6;
	sub.s32 	%r2901, %r2900, %r888;
	mul.wide.s32 	%rd296, %r2901, 4;
	add.s64 	%rd128, %rd1, %rd296;
	ld.local.u32 	%r4219, [%rd128];
	ld.local.u32 	%r4220, [%rd128+-4];
	and.b32  	%r896, %r886, 31;
	setp.eq.s32	%p363, %r896, 0;
	@%p363 bra 	BB48_568;

	mov.u32 	%r2902, 32;
	sub.s32 	%r2903, %r2902, %r896;
	shr.u32 	%r2904, %r4220, %r2903;
	shl.b32 	%r2905, %r4219, %r896;
	add.s32 	%r4219, %r2904, %r2905;
	ld.local.u32 	%r2906, [%rd128+-8];
	shr.u32 	%r2907, %r2906, %r2903;
	shl.b32 	%r2908, %r4220, %r896;
	add.s32 	%r4220, %r2907, %r2908;

BB48_568:
	shr.u32 	%r2909, %r4220, 30;
	shl.b32 	%r2910, %r4219, 2;
	add.s32 	%r4221, %r2909, %r2910;
	shl.b32 	%r902, %r4220, 2;
	shr.u32 	%r2911, %r4221, 31;
	shr.u32 	%r2912, %r4219, 30;
	add.s32 	%r903, %r2911, %r2912;
	setp.eq.s32	%p364, %r2911, 0;
	mov.u32 	%r4222, %r893;
	mov.u32 	%r4223, %r902;
	@%p364 bra 	BB48_570;

	not.b32 	%r2913, %r4221;
	neg.s32 	%r904, %r902;
	setp.eq.s32	%p365, %r902, 0;
	selp.u32	%r2914, 1, 0, %p365;
	add.s32 	%r4221, %r2914, %r2913;
	xor.b32  	%r906, %r893, -2147483648;
	mov.u32 	%r4222, %r906;
	mov.u32 	%r4223, %r904;

BB48_570:
	mov.u32 	%r908, %r4222;
	neg.s32 	%r2915, %r903;
	setp.eq.s32	%p366, %r893, 0;
	selp.b32	%r4226, %r903, %r2915, %p366;
	clz.b32 	%r4225, %r4221;
	setp.eq.s32	%p367, %r4225, 0;
	shl.b32 	%r2916, %r4221, %r4225;
	mov.u32 	%r2917, 32;
	sub.s32 	%r2918, %r2917, %r4225;
	shr.u32 	%r2919, %r4223, %r2918;
	add.s32 	%r2920, %r2919, %r2916;
	selp.b32	%r912, %r4221, %r2920, %p367;
	mov.u32 	%r2921, -921707870;
	mul.hi.u32 	%r4224, %r912, %r2921;
	setp.lt.s32	%p368, %r4224, 1;
	@%p368 bra 	BB48_572;

	mul.lo.s32 	%r2922, %r912, -921707870;
	shr.u32 	%r2923, %r2922, 31;
	shl.b32 	%r2924, %r4224, 1;
	add.s32 	%r4224, %r2923, %r2924;
	add.s32 	%r4225, %r4225, 1;

BB48_572:
	mov.u32 	%r2925, 126;
	sub.s32 	%r2926, %r2925, %r4225;
	shl.b32 	%r2927, %r2926, 23;
	add.s32 	%r2928, %r4224, 1;
	shr.u32 	%r2929, %r2928, 7;
	add.s32 	%r2930, %r2929, 1;
	shr.u32 	%r2931, %r2930, 1;
	add.s32 	%r2932, %r2931, %r2927;
	or.b32  	%r2933, %r2932, %r908;
	mov.b32 	 %f2393, %r2933;

BB48_573:
	mul.rn.f32 	%f501, %f2393, %f2393;
	add.s32 	%r919, %r4226, 1;
	and.b32  	%r920, %r919, 1;
	setp.eq.s32	%p369, %r920, 0;
	@%p369 bra 	BB48_575;

	mov.f32 	%f1674, 0fBAB6061A;
	mov.f32 	%f1675, 0f37CCF5CE;
	fma.rn.f32 	%f2394, %f1675, %f501, %f1674;
	bra.uni 	BB48_576;

BB48_575:
	mov.f32 	%f1676, 0f3C08839E;
	mov.f32 	%f1677, 0fB94CA1F9;
	fma.rn.f32 	%f2394, %f1677, %f501, %f1676;

BB48_576:
	@%p369 bra 	BB48_578;

	mov.f32 	%f1678, 0f3D2AAAA5;
	fma.rn.f32 	%f1679, %f2394, %f501, %f1678;
	mov.f32 	%f1680, 0fBF000000;
	fma.rn.f32 	%f2395, %f1679, %f501, %f1680;
	bra.uni 	BB48_579;

BB48_578:
	mov.f32 	%f1681, 0fBE2AAAA3;
	fma.rn.f32 	%f1682, %f2394, %f501, %f1681;
	mov.f32 	%f1683, 0f00000000;
	fma.rn.f32 	%f2395, %f1682, %f501, %f1683;

BB48_579:
	fma.rn.f32 	%f2396, %f2395, %f2393, %f2393;
	@%p369 bra 	BB48_581;

	mov.f32 	%f1684, 0f3F800000;
	fma.rn.f32 	%f2396, %f2395, %f501, %f1684;

BB48_581:
	and.b32  	%r2934, %r919, 2;
	setp.eq.s32	%p372, %r2934, 0;
	@%p372 bra 	BB48_583;

	mov.f32 	%f1685, 0f00000000;
	mov.f32 	%f1686, 0fBF800000;
	fma.rn.f32 	%f2396, %f2396, %f1686, %f1685;

BB48_583:
	abs.f32 	%f513, %f487;
	setp.neu.f32	%p373, %f513, 0f7F800000;
	mov.f32 	%f2510, %f487;
	@%p373 bra 	BB48_585;

	mov.f32 	%f1687, 0f00000000;
	mul.rn.f32 	%f514, %f487, %f1687;
	mov.f32 	%f2510, %f514;

BB48_585:
	mov.f32 	%f515, %f2510;
	mul.f32 	%f1688, %f515, 0f3F22F983;
	cvt.rni.s32.f32	%r4236, %f1688;
	cvt.rn.f32.s32	%f1689, %r4236;
	neg.f32 	%f1690, %f1689;
	fma.rn.f32 	%f1692, %f1690, %f1668, %f515;
	fma.rn.f32 	%f1694, %f1690, %f1670, %f1692;
	fma.rn.f32 	%f2397, %f1690, %f1672, %f1694;
	abs.f32 	%f1696, %f515;
	setp.leu.f32	%p374, %f1696, 0f47CE4780;
	@%p374 bra 	BB48_595;

	mov.b32 	 %r922, %f515;
	shr.u32 	%r923, %r922, 23;
	bfe.u32 	%r2937, %r922, 23, 8;
	add.s32 	%r2938, %r2937, -128;
	shl.b32 	%r2939, %r922, 8;
	or.b32  	%r924, %r2939, -2147483648;
	shr.u32 	%r925, %r2938, 5;
	mov.u32 	%r4228, 0;
	mov.u64 	%rd371, __cudart_i2opi_f;
	mov.u32 	%r4227, -6;
	mov.u64 	%rd463, %rd1;

BB48_587:
	.pragma "nounroll";
	ld.const.u32 	%r2942, [%rd371];
	// inline asm
	{
	mad.lo.cc.u32   %r2940, %r2942, %r924, %r4228;
	madc.hi.u32     %r4228, %r2942, %r924,  0;
	}
	// inline asm
	st.local.u32 	[%rd463], %r2940;
	add.s64 	%rd463, %rd463, 4;
	add.s64 	%rd371, %rd371, 4;
	add.s32 	%r4227, %r4227, 1;
	setp.ne.s32	%p375, %r4227, 0;
	@%p375 bra 	BB48_587;

	and.b32  	%r930, %r922, -2147483648;
	st.local.u32 	[%rd3], %r4228;
	mov.u32 	%r2945, 6;
	sub.s32 	%r2946, %r2945, %r925;
	mul.wide.s32 	%rd298, %r2946, 4;
	add.s64 	%rd133, %rd1, %rd298;
	ld.local.u32 	%r4229, [%rd133];
	ld.local.u32 	%r4230, [%rd133+-4];
	and.b32  	%r933, %r923, 31;
	setp.eq.s32	%p376, %r933, 0;
	@%p376 bra 	BB48_590;

	mov.u32 	%r2947, 32;
	sub.s32 	%r2948, %r2947, %r933;
	shr.u32 	%r2949, %r4230, %r2948;
	shl.b32 	%r2950, %r4229, %r933;
	add.s32 	%r4229, %r2949, %r2950;
	ld.local.u32 	%r2951, [%rd133+-8];
	shr.u32 	%r2952, %r2951, %r2948;
	shl.b32 	%r2953, %r4230, %r933;
	add.s32 	%r4230, %r2952, %r2953;

BB48_590:
	shr.u32 	%r2954, %r4230, 30;
	shl.b32 	%r2955, %r4229, 2;
	add.s32 	%r4231, %r2954, %r2955;
	shl.b32 	%r939, %r4230, 2;
	shr.u32 	%r2956, %r4231, 31;
	shr.u32 	%r2957, %r4229, 30;
	add.s32 	%r940, %r2956, %r2957;
	setp.eq.s32	%p377, %r2956, 0;
	mov.u32 	%r4232, %r930;
	mov.u32 	%r4233, %r939;
	@%p377 bra 	BB48_592;

	not.b32 	%r2958, %r4231;
	neg.s32 	%r941, %r939;
	setp.eq.s32	%p378, %r939, 0;
	selp.u32	%r2959, 1, 0, %p378;
	add.s32 	%r4231, %r2959, %r2958;
	xor.b32  	%r943, %r930, -2147483648;
	mov.u32 	%r4232, %r943;
	mov.u32 	%r4233, %r941;

BB48_592:
	mov.u32 	%r945, %r4232;
	neg.s32 	%r2960, %r940;
	setp.eq.s32	%p379, %r930, 0;
	selp.b32	%r4236, %r940, %r2960, %p379;
	clz.b32 	%r4235, %r4231;
	setp.eq.s32	%p380, %r4235, 0;
	shl.b32 	%r2961, %r4231, %r4235;
	mov.u32 	%r2962, 32;
	sub.s32 	%r2963, %r2962, %r4235;
	shr.u32 	%r2964, %r4233, %r2963;
	add.s32 	%r2965, %r2964, %r2961;
	selp.b32	%r949, %r4231, %r2965, %p380;
	mov.u32 	%r2966, -921707870;
	mul.hi.u32 	%r4234, %r949, %r2966;
	setp.lt.s32	%p381, %r4234, 1;
	@%p381 bra 	BB48_594;

	mul.lo.s32 	%r2967, %r949, -921707870;
	shr.u32 	%r2968, %r2967, 31;
	shl.b32 	%r2969, %r4234, 1;
	add.s32 	%r4234, %r2968, %r2969;
	add.s32 	%r4235, %r4235, 1;

BB48_594:
	mov.u32 	%r2970, 126;
	sub.s32 	%r2971, %r2970, %r4235;
	shl.b32 	%r2972, %r2971, 23;
	add.s32 	%r2973, %r4234, 1;
	shr.u32 	%r2974, %r2973, 7;
	add.s32 	%r2975, %r2974, 1;
	shr.u32 	%r2976, %r2975, 1;
	add.s32 	%r2977, %r2976, %r2972;
	or.b32  	%r2978, %r2977, %r945;
	mov.b32 	 %f2397, %r2978;

BB48_595:
	mul.rn.f32 	%f519, %f2397, %f2397;
	add.s32 	%r956, %r4236, 1;
	and.b32  	%r957, %r956, 1;
	setp.eq.s32	%p382, %r957, 0;
	@%p382 bra 	BB48_597;

	mov.f32 	%f1697, 0fBAB6061A;
	mov.f32 	%f1698, 0f37CCF5CE;
	fma.rn.f32 	%f2398, %f1698, %f519, %f1697;
	bra.uni 	BB48_598;

BB48_597:
	mov.f32 	%f1699, 0f3C08839E;
	mov.f32 	%f1700, 0fB94CA1F9;
	fma.rn.f32 	%f2398, %f1700, %f519, %f1699;

BB48_598:
	@%p382 bra 	BB48_600;

	mov.f32 	%f1701, 0f3D2AAAA5;
	fma.rn.f32 	%f1702, %f2398, %f519, %f1701;
	mov.f32 	%f1703, 0fBF000000;
	fma.rn.f32 	%f2399, %f1702, %f519, %f1703;
	bra.uni 	BB48_601;

BB48_600:
	mov.f32 	%f1704, 0fBE2AAAA3;
	fma.rn.f32 	%f1705, %f2398, %f519, %f1704;
	mov.f32 	%f1706, 0f00000000;
	fma.rn.f32 	%f2399, %f1705, %f519, %f1706;

BB48_601:
	fma.rn.f32 	%f2400, %f2399, %f2397, %f2397;
	@%p382 bra 	BB48_603;

	mov.f32 	%f1707, 0f3F800000;
	fma.rn.f32 	%f2400, %f2399, %f519, %f1707;

BB48_603:
	and.b32  	%r2979, %r956, 2;
	setp.eq.s32	%p385, %r2979, 0;
	@%p385 bra 	BB48_605;

	mov.f32 	%f1708, 0f00000000;
	mov.f32 	%f1709, 0fBF800000;
	fma.rn.f32 	%f2400, %f2400, %f1709, %f1708;

BB48_605:
	mul.f32 	%f531, %f2396, %f2400;
	abs.f32 	%f532, %f494;
	setp.neu.f32	%p386, %f532, 0f7F800000;
	mov.f32 	%f2529, %f494;
	@%p386 bra 	BB48_607;

	mov.f32 	%f1710, 0f00000000;
	mul.rn.f32 	%f533, %f494, %f1710;
	mov.f32 	%f2529, %f533;

BB48_607:
	mov.f32 	%f534, %f2529;
	mul.f32 	%f1711, %f534, 0f3F22F983;
	cvt.rni.s32.f32	%r4246, %f1711;
	cvt.rn.f32.s32	%f1712, %r4246;
	neg.f32 	%f1713, %f1712;
	fma.rn.f32 	%f1715, %f1713, %f1668, %f534;
	fma.rn.f32 	%f1717, %f1713, %f1670, %f1715;
	fma.rn.f32 	%f2401, %f1713, %f1672, %f1717;
	abs.f32 	%f1719, %f534;
	setp.leu.f32	%p387, %f1719, 0f47CE4780;
	@%p387 bra 	BB48_617;

	mov.b32 	 %r959, %f534;
	shr.u32 	%r960, %r959, 23;
	bfe.u32 	%r2982, %r959, 23, 8;
	add.s32 	%r2983, %r2982, -128;
	shl.b32 	%r2984, %r959, 8;
	or.b32  	%r961, %r2984, -2147483648;
	shr.u32 	%r962, %r2983, 5;
	mov.u32 	%r4238, 0;
	mov.u64 	%rd372, __cudart_i2opi_f;
	mov.u32 	%r4237, -6;
	mov.u64 	%rd462, %rd1;

BB48_609:
	.pragma "nounroll";
	ld.const.u32 	%r2987, [%rd372];
	// inline asm
	{
	mad.lo.cc.u32   %r2985, %r2987, %r961, %r4238;
	madc.hi.u32     %r4238, %r2987, %r961,  0;
	}
	// inline asm
	st.local.u32 	[%rd462], %r2985;
	add.s64 	%rd462, %rd462, 4;
	add.s64 	%rd372, %rd372, 4;
	add.s32 	%r4237, %r4237, 1;
	setp.ne.s32	%p388, %r4237, 0;
	@%p388 bra 	BB48_609;

	and.b32  	%r967, %r959, -2147483648;
	st.local.u32 	[%rd3], %r4238;
	mov.u32 	%r2990, 6;
	sub.s32 	%r2991, %r2990, %r962;
	mul.wide.s32 	%rd300, %r2991, 4;
	add.s64 	%rd138, %rd1, %rd300;
	ld.local.u32 	%r4239, [%rd138];
	ld.local.u32 	%r4240, [%rd138+-4];
	and.b32  	%r970, %r960, 31;
	setp.eq.s32	%p389, %r970, 0;
	@%p389 bra 	BB48_612;

	mov.u32 	%r2992, 32;
	sub.s32 	%r2993, %r2992, %r970;
	shr.u32 	%r2994, %r4240, %r2993;
	shl.b32 	%r2995, %r4239, %r970;
	add.s32 	%r4239, %r2994, %r2995;
	ld.local.u32 	%r2996, [%rd138+-8];
	shr.u32 	%r2997, %r2996, %r2993;
	shl.b32 	%r2998, %r4240, %r970;
	add.s32 	%r4240, %r2997, %r2998;

BB48_612:
	shr.u32 	%r2999, %r4240, 30;
	shl.b32 	%r3000, %r4239, 2;
	add.s32 	%r4241, %r2999, %r3000;
	shl.b32 	%r976, %r4240, 2;
	shr.u32 	%r3001, %r4241, 31;
	shr.u32 	%r3002, %r4239, 30;
	add.s32 	%r977, %r3001, %r3002;
	setp.eq.s32	%p390, %r3001, 0;
	mov.u32 	%r4242, %r967;
	mov.u32 	%r4243, %r976;
	@%p390 bra 	BB48_614;

	not.b32 	%r3003, %r4241;
	neg.s32 	%r978, %r976;
	setp.eq.s32	%p391, %r976, 0;
	selp.u32	%r3004, 1, 0, %p391;
	add.s32 	%r4241, %r3004, %r3003;
	xor.b32  	%r980, %r967, -2147483648;
	mov.u32 	%r4242, %r980;
	mov.u32 	%r4243, %r978;

BB48_614:
	mov.u32 	%r982, %r4242;
	neg.s32 	%r3005, %r977;
	setp.eq.s32	%p392, %r967, 0;
	selp.b32	%r4246, %r977, %r3005, %p392;
	clz.b32 	%r4245, %r4241;
	setp.eq.s32	%p393, %r4245, 0;
	shl.b32 	%r3006, %r4241, %r4245;
	mov.u32 	%r3007, 32;
	sub.s32 	%r3008, %r3007, %r4245;
	shr.u32 	%r3009, %r4243, %r3008;
	add.s32 	%r3010, %r3009, %r3006;
	selp.b32	%r986, %r4241, %r3010, %p393;
	mov.u32 	%r3011, -921707870;
	mul.hi.u32 	%r4244, %r986, %r3011;
	setp.lt.s32	%p394, %r4244, 1;
	@%p394 bra 	BB48_616;

	mul.lo.s32 	%r3012, %r986, -921707870;
	shr.u32 	%r3013, %r3012, 31;
	shl.b32 	%r3014, %r4244, 1;
	add.s32 	%r4244, %r3013, %r3014;
	add.s32 	%r4245, %r4245, 1;

BB48_616:
	mov.u32 	%r3015, 126;
	sub.s32 	%r3016, %r3015, %r4245;
	shl.b32 	%r3017, %r3016, 23;
	add.s32 	%r3018, %r4244, 1;
	shr.u32 	%r3019, %r3018, 7;
	add.s32 	%r3020, %r3019, 1;
	shr.u32 	%r3021, %r3020, 1;
	add.s32 	%r3022, %r3021, %r3017;
	or.b32  	%r3023, %r3022, %r982;
	mov.b32 	 %f2401, %r3023;

BB48_617:
	mul.rn.f32 	%f538, %f2401, %f2401;
	add.s32 	%r993, %r4246, 1;
	and.b32  	%r994, %r993, 1;
	setp.eq.s32	%p395, %r994, 0;
	@%p395 bra 	BB48_619;

	mov.f32 	%f1720, 0fBAB6061A;
	mov.f32 	%f1721, 0f37CCF5CE;
	fma.rn.f32 	%f2402, %f1721, %f538, %f1720;
	bra.uni 	BB48_620;

BB48_619:
	mov.f32 	%f1722, 0f3C08839E;
	mov.f32 	%f1723, 0fB94CA1F9;
	fma.rn.f32 	%f2402, %f1723, %f538, %f1722;

BB48_620:
	@%p395 bra 	BB48_622;

	mov.f32 	%f1724, 0f3D2AAAA5;
	fma.rn.f32 	%f1725, %f2402, %f538, %f1724;
	mov.f32 	%f1726, 0fBF000000;
	fma.rn.f32 	%f2403, %f1725, %f538, %f1726;
	bra.uni 	BB48_623;

BB48_622:
	mov.f32 	%f1727, 0fBE2AAAA3;
	fma.rn.f32 	%f1728, %f2402, %f538, %f1727;
	mov.f32 	%f1729, 0f00000000;
	fma.rn.f32 	%f2403, %f1728, %f538, %f1729;

BB48_623:
	fma.rn.f32 	%f2404, %f2403, %f2401, %f2401;
	@%p395 bra 	BB48_625;

	mov.f32 	%f1730, 0f3F800000;
	fma.rn.f32 	%f2404, %f2403, %f538, %f1730;

BB48_625:
	and.b32  	%r3024, %r993, 2;
	setp.eq.s32	%p398, %r3024, 0;
	@%p398 bra 	BB48_627;

	mov.f32 	%f1731, 0f00000000;
	mov.f32 	%f1732, 0fBF800000;
	fma.rn.f32 	%f2404, %f2404, %f1732, %f1731;

BB48_627:
	mul.f32 	%f550, %f531, %f2404;
	mov.f32 	%f2490, %f480;
	@%p360 bra 	BB48_629;

	mov.f32 	%f1733, 0f00000000;
	mul.rn.f32 	%f2490, %f480, %f1733;

BB48_629:
	mul.f32 	%f1734, %f2490, 0f3F22F983;
	cvt.rni.s32.f32	%r4256, %f1734;
	cvt.rn.f32.s32	%f1735, %r4256;
	neg.f32 	%f1736, %f1735;
	fma.rn.f32 	%f1738, %f1736, %f1668, %f2490;
	fma.rn.f32 	%f1740, %f1736, %f1670, %f1738;
	fma.rn.f32 	%f2405, %f1736, %f1672, %f1740;
	abs.f32 	%f1742, %f2490;
	setp.leu.f32	%p400, %f1742, 0f47CE4780;
	@%p400 bra 	BB48_639;

	mov.b32 	 %r996, %f2490;
	shr.u32 	%r997, %r996, 23;
	bfe.u32 	%r3027, %r996, 23, 8;
	add.s32 	%r3028, %r3027, -128;
	shl.b32 	%r3029, %r996, 8;
	or.b32  	%r998, %r3029, -2147483648;
	shr.u32 	%r999, %r3028, 5;
	mov.u32 	%r4248, 0;
	mov.u64 	%rd373, __cudart_i2opi_f;
	mov.u32 	%r4247, -6;
	mov.u64 	%rd461, %rd1;

BB48_631:
	.pragma "nounroll";
	ld.const.u32 	%r3032, [%rd373];
	// inline asm
	{
	mad.lo.cc.u32   %r3030, %r3032, %r998, %r4248;
	madc.hi.u32     %r4248, %r3032, %r998,  0;
	}
	// inline asm
	st.local.u32 	[%rd461], %r3030;
	add.s64 	%rd461, %rd461, 4;
	add.s64 	%rd373, %rd373, 4;
	add.s32 	%r4247, %r4247, 1;
	setp.ne.s32	%p401, %r4247, 0;
	@%p401 bra 	BB48_631;

	and.b32  	%r1004, %r996, -2147483648;
	st.local.u32 	[%rd3], %r4248;
	mov.u32 	%r3035, 6;
	sub.s32 	%r3036, %r3035, %r999;
	mul.wide.s32 	%rd302, %r3036, 4;
	add.s64 	%rd143, %rd1, %rd302;
	ld.local.u32 	%r4249, [%rd143];
	ld.local.u32 	%r4250, [%rd143+-4];
	and.b32  	%r1007, %r997, 31;
	setp.eq.s32	%p402, %r1007, 0;
	@%p402 bra 	BB48_634;

	mov.u32 	%r3037, 32;
	sub.s32 	%r3038, %r3037, %r1007;
	shr.u32 	%r3039, %r4250, %r3038;
	shl.b32 	%r3040, %r4249, %r1007;
	add.s32 	%r4249, %r3039, %r3040;
	ld.local.u32 	%r3041, [%rd143+-8];
	shr.u32 	%r3042, %r3041, %r3038;
	shl.b32 	%r3043, %r4250, %r1007;
	add.s32 	%r4250, %r3042, %r3043;

BB48_634:
	shr.u32 	%r3044, %r4250, 30;
	shl.b32 	%r3045, %r4249, 2;
	add.s32 	%r4251, %r3044, %r3045;
	shl.b32 	%r1013, %r4250, 2;
	shr.u32 	%r3046, %r4251, 31;
	shr.u32 	%r3047, %r4249, 30;
	add.s32 	%r1014, %r3046, %r3047;
	setp.eq.s32	%p403, %r3046, 0;
	mov.u32 	%r4252, %r1004;
	mov.u32 	%r4253, %r1013;
	@%p403 bra 	BB48_636;

	not.b32 	%r3048, %r4251;
	neg.s32 	%r1015, %r1013;
	setp.eq.s32	%p404, %r1013, 0;
	selp.u32	%r3049, 1, 0, %p404;
	add.s32 	%r4251, %r3049, %r3048;
	xor.b32  	%r1017, %r1004, -2147483648;
	mov.u32 	%r4252, %r1017;
	mov.u32 	%r4253, %r1015;

BB48_636:
	mov.u32 	%r1019, %r4252;
	neg.s32 	%r3050, %r1014;
	setp.eq.s32	%p405, %r1004, 0;
	selp.b32	%r4256, %r1014, %r3050, %p405;
	clz.b32 	%r4255, %r4251;
	setp.eq.s32	%p406, %r4255, 0;
	shl.b32 	%r3051, %r4251, %r4255;
	mov.u32 	%r3052, 32;
	sub.s32 	%r3053, %r3052, %r4255;
	shr.u32 	%r3054, %r4253, %r3053;
	add.s32 	%r3055, %r3054, %r3051;
	selp.b32	%r1023, %r4251, %r3055, %p406;
	mov.u32 	%r3056, -921707870;
	mul.hi.u32 	%r4254, %r1023, %r3056;
	setp.lt.s32	%p407, %r4254, 1;
	@%p407 bra 	BB48_638;

	mul.lo.s32 	%r3057, %r1023, -921707870;
	shr.u32 	%r3058, %r3057, 31;
	shl.b32 	%r3059, %r4254, 1;
	add.s32 	%r4254, %r3058, %r3059;
	add.s32 	%r4255, %r4255, 1;

BB48_638:
	mov.u32 	%r3060, 126;
	sub.s32 	%r3061, %r3060, %r4255;
	shl.b32 	%r3062, %r3061, 23;
	add.s32 	%r3063, %r4254, 1;
	shr.u32 	%r3064, %r3063, 7;
	add.s32 	%r3065, %r3064, 1;
	shr.u32 	%r3066, %r3065, 1;
	add.s32 	%r3067, %r3066, %r3062;
	or.b32  	%r3068, %r3067, %r1019;
	mov.b32 	 %f2405, %r3068;

BB48_639:
	mul.rn.f32 	%f556, %f2405, %f2405;
	and.b32  	%r1030, %r4256, 1;
	setp.eq.s32	%p408, %r1030, 0;
	@%p408 bra 	BB48_641;

	mov.f32 	%f1743, 0fBAB6061A;
	mov.f32 	%f1744, 0f37CCF5CE;
	fma.rn.f32 	%f2406, %f1744, %f556, %f1743;
	bra.uni 	BB48_642;

BB48_641:
	mov.f32 	%f1745, 0f3C08839E;
	mov.f32 	%f1746, 0fB94CA1F9;
	fma.rn.f32 	%f2406, %f1746, %f556, %f1745;

BB48_642:
	@%p408 bra 	BB48_644;

	mov.f32 	%f1747, 0f3D2AAAA5;
	fma.rn.f32 	%f1748, %f2406, %f556, %f1747;
	mov.f32 	%f1749, 0fBF000000;
	fma.rn.f32 	%f2407, %f1748, %f556, %f1749;
	bra.uni 	BB48_645;

BB48_644:
	mov.f32 	%f1750, 0fBE2AAAA3;
	fma.rn.f32 	%f1751, %f2406, %f556, %f1750;
	mov.f32 	%f1752, 0f00000000;
	fma.rn.f32 	%f2407, %f1751, %f556, %f1752;

BB48_645:
	fma.rn.f32 	%f2408, %f2407, %f2405, %f2405;
	@%p408 bra 	BB48_647;

	mov.f32 	%f1753, 0f3F800000;
	fma.rn.f32 	%f2408, %f2407, %f556, %f1753;

BB48_647:
	and.b32  	%r3069, %r4256, 2;
	setp.eq.s32	%p411, %r3069, 0;
	@%p411 bra 	BB48_649;

	mov.f32 	%f1754, 0f00000000;
	mov.f32 	%f1755, 0fBF800000;
	fma.rn.f32 	%f2408, %f2408, %f1755, %f1754;

BB48_649:
	mov.f32 	%f2509, %f487;
	@%p373 bra 	BB48_651;

	mov.f32 	%f1756, 0f00000000;
	mul.rn.f32 	%f2509, %f487, %f1756;

BB48_651:
	mul.f32 	%f1757, %f2509, 0f3F22F983;
	cvt.rni.s32.f32	%r4266, %f1757;
	cvt.rn.f32.s32	%f1758, %r4266;
	neg.f32 	%f1759, %f1758;
	fma.rn.f32 	%f1761, %f1759, %f1668, %f2509;
	fma.rn.f32 	%f1763, %f1759, %f1670, %f1761;
	fma.rn.f32 	%f2409, %f1759, %f1672, %f1763;
	abs.f32 	%f1765, %f2509;
	setp.leu.f32	%p413, %f1765, 0f47CE4780;
	@%p413 bra 	BB48_661;

	mov.b32 	 %r1032, %f2509;
	shr.u32 	%r1033, %r1032, 23;
	bfe.u32 	%r3072, %r1032, 23, 8;
	add.s32 	%r3073, %r3072, -128;
	shl.b32 	%r3074, %r1032, 8;
	or.b32  	%r1034, %r3074, -2147483648;
	shr.u32 	%r1035, %r3073, 5;
	mov.u32 	%r4258, 0;
	mov.u64 	%rd374, __cudart_i2opi_f;
	mov.u32 	%r4257, -6;
	mov.u64 	%rd460, %rd1;

BB48_653:
	.pragma "nounroll";
	ld.const.u32 	%r3077, [%rd374];
	// inline asm
	{
	mad.lo.cc.u32   %r3075, %r3077, %r1034, %r4258;
	madc.hi.u32     %r4258, %r3077, %r1034,  0;
	}
	// inline asm
	st.local.u32 	[%rd460], %r3075;
	add.s64 	%rd460, %rd460, 4;
	add.s64 	%rd374, %rd374, 4;
	add.s32 	%r4257, %r4257, 1;
	setp.ne.s32	%p414, %r4257, 0;
	@%p414 bra 	BB48_653;

	and.b32  	%r1040, %r1032, -2147483648;
	st.local.u32 	[%rd3], %r4258;
	mov.u32 	%r3080, 6;
	sub.s32 	%r3081, %r3080, %r1035;
	mul.wide.s32 	%rd304, %r3081, 4;
	add.s64 	%rd148, %rd1, %rd304;
	ld.local.u32 	%r4259, [%rd148];
	ld.local.u32 	%r4260, [%rd148+-4];
	and.b32  	%r1043, %r1033, 31;
	setp.eq.s32	%p415, %r1043, 0;
	@%p415 bra 	BB48_656;

	mov.u32 	%r3082, 32;
	sub.s32 	%r3083, %r3082, %r1043;
	shr.u32 	%r3084, %r4260, %r3083;
	shl.b32 	%r3085, %r4259, %r1043;
	add.s32 	%r4259, %r3084, %r3085;
	ld.local.u32 	%r3086, [%rd148+-8];
	shr.u32 	%r3087, %r3086, %r3083;
	shl.b32 	%r3088, %r4260, %r1043;
	add.s32 	%r4260, %r3087, %r3088;

BB48_656:
	shr.u32 	%r3089, %r4260, 30;
	shl.b32 	%r3090, %r4259, 2;
	add.s32 	%r4261, %r3089, %r3090;
	shl.b32 	%r1049, %r4260, 2;
	shr.u32 	%r3091, %r4261, 31;
	shr.u32 	%r3092, %r4259, 30;
	add.s32 	%r1050, %r3091, %r3092;
	setp.eq.s32	%p416, %r3091, 0;
	mov.u32 	%r4262, %r1040;
	mov.u32 	%r4263, %r1049;
	@%p416 bra 	BB48_658;

	not.b32 	%r3093, %r4261;
	neg.s32 	%r1051, %r1049;
	setp.eq.s32	%p417, %r1049, 0;
	selp.u32	%r3094, 1, 0, %p417;
	add.s32 	%r4261, %r3094, %r3093;
	xor.b32  	%r1053, %r1040, -2147483648;
	mov.u32 	%r4262, %r1053;
	mov.u32 	%r4263, %r1051;

BB48_658:
	mov.u32 	%r1055, %r4262;
	neg.s32 	%r3095, %r1050;
	setp.eq.s32	%p418, %r1040, 0;
	selp.b32	%r4266, %r1050, %r3095, %p418;
	clz.b32 	%r4265, %r4261;
	setp.eq.s32	%p419, %r4265, 0;
	shl.b32 	%r3096, %r4261, %r4265;
	mov.u32 	%r3097, 32;
	sub.s32 	%r3098, %r3097, %r4265;
	shr.u32 	%r3099, %r4263, %r3098;
	add.s32 	%r3100, %r3099, %r3096;
	selp.b32	%r1059, %r4261, %r3100, %p419;
	mov.u32 	%r3101, -921707870;
	mul.hi.u32 	%r4264, %r1059, %r3101;
	setp.lt.s32	%p420, %r4264, 1;
	@%p420 bra 	BB48_660;

	mul.lo.s32 	%r3102, %r1059, -921707870;
	shr.u32 	%r3103, %r3102, 31;
	shl.b32 	%r3104, %r4264, 1;
	add.s32 	%r4264, %r3103, %r3104;
	add.s32 	%r4265, %r4265, 1;

BB48_660:
	mov.u32 	%r3105, 126;
	sub.s32 	%r3106, %r3105, %r4265;
	shl.b32 	%r3107, %r3106, 23;
	add.s32 	%r3108, %r4264, 1;
	shr.u32 	%r3109, %r3108, 7;
	add.s32 	%r3110, %r3109, 1;
	shr.u32 	%r3111, %r3110, 1;
	add.s32 	%r3112, %r3111, %r3107;
	or.b32  	%r3113, %r3112, %r1055;
	mov.b32 	 %f2409, %r3113;

BB48_661:
	mul.rn.f32 	%f573, %f2409, %f2409;
	and.b32  	%r1066, %r4266, 1;
	setp.eq.s32	%p421, %r1066, 0;
	@%p421 bra 	BB48_663;

	mov.f32 	%f1766, 0fBAB6061A;
	mov.f32 	%f1767, 0f37CCF5CE;
	fma.rn.f32 	%f2410, %f1767, %f573, %f1766;
	bra.uni 	BB48_664;

BB48_663:
	mov.f32 	%f1768, 0f3C08839E;
	mov.f32 	%f1769, 0fB94CA1F9;
	fma.rn.f32 	%f2410, %f1769, %f573, %f1768;

BB48_664:
	@%p421 bra 	BB48_666;

	mov.f32 	%f1770, 0f3D2AAAA5;
	fma.rn.f32 	%f1771, %f2410, %f573, %f1770;
	mov.f32 	%f1772, 0fBF000000;
	fma.rn.f32 	%f2411, %f1771, %f573, %f1772;
	bra.uni 	BB48_667;

BB48_666:
	mov.f32 	%f1773, 0fBE2AAAA3;
	fma.rn.f32 	%f1774, %f2410, %f573, %f1773;
	mov.f32 	%f1775, 0f00000000;
	fma.rn.f32 	%f2411, %f1774, %f573, %f1775;

BB48_667:
	fma.rn.f32 	%f2412, %f2411, %f2409, %f2409;
	@%p421 bra 	BB48_669;

	mov.f32 	%f1776, 0f3F800000;
	fma.rn.f32 	%f2412, %f2411, %f573, %f1776;

BB48_669:
	and.b32  	%r3114, %r4266, 2;
	setp.eq.s32	%p424, %r3114, 0;
	@%p424 bra 	BB48_671;

	mov.f32 	%f1777, 0f00000000;
	mov.f32 	%f1778, 0fBF800000;
	fma.rn.f32 	%f2412, %f2412, %f1778, %f1777;

BB48_671:
	mul.f32 	%f585, %f2408, %f2412;
	mov.f32 	%f2528, %f494;
	@%p386 bra 	BB48_673;

	mov.f32 	%f1779, 0f00000000;
	mul.rn.f32 	%f2528, %f494, %f1779;

BB48_673:
	mul.f32 	%f1780, %f2528, 0f3F22F983;
	cvt.rni.s32.f32	%r4276, %f1780;
	cvt.rn.f32.s32	%f1781, %r4276;
	neg.f32 	%f1782, %f1781;
	fma.rn.f32 	%f1784, %f1782, %f1668, %f2528;
	fma.rn.f32 	%f1786, %f1782, %f1670, %f1784;
	fma.rn.f32 	%f2413, %f1782, %f1672, %f1786;
	abs.f32 	%f1788, %f2528;
	setp.leu.f32	%p426, %f1788, 0f47CE4780;
	@%p426 bra 	BB48_683;

	mov.b32 	 %r1068, %f2528;
	shr.u32 	%r1069, %r1068, 23;
	bfe.u32 	%r3117, %r1068, 23, 8;
	add.s32 	%r3118, %r3117, -128;
	shl.b32 	%r3119, %r1068, 8;
	or.b32  	%r1070, %r3119, -2147483648;
	shr.u32 	%r1071, %r3118, 5;
	mov.u32 	%r4268, 0;
	mov.u64 	%rd375, __cudart_i2opi_f;
	mov.u32 	%r4267, -6;
	mov.u64 	%rd459, %rd1;

BB48_675:
	.pragma "nounroll";
	ld.const.u32 	%r3122, [%rd375];
	// inline asm
	{
	mad.lo.cc.u32   %r3120, %r3122, %r1070, %r4268;
	madc.hi.u32     %r4268, %r3122, %r1070,  0;
	}
	// inline asm
	st.local.u32 	[%rd459], %r3120;
	add.s64 	%rd459, %rd459, 4;
	add.s64 	%rd375, %rd375, 4;
	add.s32 	%r4267, %r4267, 1;
	setp.ne.s32	%p427, %r4267, 0;
	@%p427 bra 	BB48_675;

	and.b32  	%r1076, %r1068, -2147483648;
	st.local.u32 	[%rd3], %r4268;
	mov.u32 	%r3125, 6;
	sub.s32 	%r3126, %r3125, %r1071;
	mul.wide.s32 	%rd306, %r3126, 4;
	add.s64 	%rd153, %rd1, %rd306;
	ld.local.u32 	%r4269, [%rd153];
	ld.local.u32 	%r4270, [%rd153+-4];
	and.b32  	%r1079, %r1069, 31;
	setp.eq.s32	%p428, %r1079, 0;
	@%p428 bra 	BB48_678;

	mov.u32 	%r3127, 32;
	sub.s32 	%r3128, %r3127, %r1079;
	shr.u32 	%r3129, %r4270, %r3128;
	shl.b32 	%r3130, %r4269, %r1079;
	add.s32 	%r4269, %r3129, %r3130;
	ld.local.u32 	%r3131, [%rd153+-8];
	shr.u32 	%r3132, %r3131, %r3128;
	shl.b32 	%r3133, %r4270, %r1079;
	add.s32 	%r4270, %r3132, %r3133;

BB48_678:
	shr.u32 	%r3134, %r4270, 30;
	shl.b32 	%r3135, %r4269, 2;
	add.s32 	%r4271, %r3134, %r3135;
	shl.b32 	%r1085, %r4270, 2;
	shr.u32 	%r3136, %r4271, 31;
	shr.u32 	%r3137, %r4269, 30;
	add.s32 	%r1086, %r3136, %r3137;
	setp.eq.s32	%p429, %r3136, 0;
	mov.u32 	%r4272, %r1076;
	mov.u32 	%r4273, %r1085;
	@%p429 bra 	BB48_680;

	not.b32 	%r3138, %r4271;
	neg.s32 	%r1087, %r1085;
	setp.eq.s32	%p430, %r1085, 0;
	selp.u32	%r3139, 1, 0, %p430;
	add.s32 	%r4271, %r3139, %r3138;
	xor.b32  	%r1089, %r1076, -2147483648;
	mov.u32 	%r4272, %r1089;
	mov.u32 	%r4273, %r1087;

BB48_680:
	mov.u32 	%r1091, %r4272;
	neg.s32 	%r3140, %r1086;
	setp.eq.s32	%p431, %r1076, 0;
	selp.b32	%r4276, %r1086, %r3140, %p431;
	clz.b32 	%r4275, %r4271;
	setp.eq.s32	%p432, %r4275, 0;
	shl.b32 	%r3141, %r4271, %r4275;
	mov.u32 	%r3142, 32;
	sub.s32 	%r3143, %r3142, %r4275;
	shr.u32 	%r3144, %r4273, %r3143;
	add.s32 	%r3145, %r3144, %r3141;
	selp.b32	%r1095, %r4271, %r3145, %p432;
	mov.u32 	%r3146, -921707870;
	mul.hi.u32 	%r4274, %r1095, %r3146;
	setp.lt.s32	%p433, %r4274, 1;
	@%p433 bra 	BB48_682;

	mul.lo.s32 	%r3147, %r1095, -921707870;
	shr.u32 	%r3148, %r3147, 31;
	shl.b32 	%r3149, %r4274, 1;
	add.s32 	%r4274, %r3148, %r3149;
	add.s32 	%r4275, %r4275, 1;

BB48_682:
	mov.u32 	%r3150, 126;
	sub.s32 	%r3151, %r3150, %r4275;
	shl.b32 	%r3152, %r3151, 23;
	add.s32 	%r3153, %r4274, 1;
	shr.u32 	%r3154, %r3153, 7;
	add.s32 	%r3155, %r3154, 1;
	shr.u32 	%r3156, %r3155, 1;
	add.s32 	%r3157, %r3156, %r3152;
	or.b32  	%r3158, %r3157, %r1091;
	mov.b32 	 %f2413, %r3158;

BB48_683:
	mul.rn.f32 	%f591, %f2413, %f2413;
	and.b32  	%r1102, %r4276, 1;
	setp.eq.s32	%p434, %r1102, 0;
	@%p434 bra 	BB48_685;

	mov.f32 	%f1789, 0fBAB6061A;
	mov.f32 	%f1790, 0f37CCF5CE;
	fma.rn.f32 	%f2414, %f1790, %f591, %f1789;
	bra.uni 	BB48_686;

BB48_685:
	mov.f32 	%f1791, 0f3C08839E;
	mov.f32 	%f1792, 0fB94CA1F9;
	fma.rn.f32 	%f2414, %f1792, %f591, %f1791;

BB48_686:
	@%p434 bra 	BB48_688;

	mov.f32 	%f1793, 0f3D2AAAA5;
	fma.rn.f32 	%f1794, %f2414, %f591, %f1793;
	mov.f32 	%f1795, 0fBF000000;
	fma.rn.f32 	%f2415, %f1794, %f591, %f1795;
	bra.uni 	BB48_689;

BB48_688:
	mov.f32 	%f1796, 0fBE2AAAA3;
	fma.rn.f32 	%f1797, %f2414, %f591, %f1796;
	mov.f32 	%f1798, 0f00000000;
	fma.rn.f32 	%f2415, %f1797, %f591, %f1798;

BB48_689:
	fma.rn.f32 	%f2416, %f2415, %f2413, %f2413;
	@%p434 bra 	BB48_691;

	mov.f32 	%f1799, 0f3F800000;
	fma.rn.f32 	%f2416, %f2415, %f591, %f1799;

BB48_691:
	and.b32  	%r3159, %r4276, 2;
	setp.eq.s32	%p437, %r3159, 0;
	@%p437 bra 	BB48_693;

	mov.f32 	%f1800, 0f00000000;
	mov.f32 	%f1801, 0fBF800000;
	fma.rn.f32 	%f2416, %f2416, %f1801, %f1800;

BB48_693:
	fma.rn.f32 	%f603, %f585, %f2416, %f550;
	mov.f32 	%f2489, %f480;
	@%p360 bra 	BB48_695;

	mov.f32 	%f1802, 0f00000000;
	mul.rn.f32 	%f2489, %f480, %f1802;

BB48_695:
	mul.f32 	%f1803, %f2489, 0f3F22F983;
	cvt.rni.s32.f32	%r4286, %f1803;
	cvt.rn.f32.s32	%f1804, %r4286;
	neg.f32 	%f1805, %f1804;
	fma.rn.f32 	%f1807, %f1805, %f1668, %f2489;
	fma.rn.f32 	%f1809, %f1805, %f1670, %f1807;
	fma.rn.f32 	%f2417, %f1805, %f1672, %f1809;
	abs.f32 	%f1811, %f2489;
	setp.leu.f32	%p439, %f1811, 0f47CE4780;
	@%p439 bra 	BB48_705;

	mov.b32 	 %r1104, %f2489;
	shr.u32 	%r1105, %r1104, 23;
	bfe.u32 	%r3162, %r1104, 23, 8;
	add.s32 	%r3163, %r3162, -128;
	shl.b32 	%r3164, %r1104, 8;
	or.b32  	%r1106, %r3164, -2147483648;
	shr.u32 	%r1107, %r3163, 5;
	mov.u32 	%r4278, 0;
	mov.u64 	%rd376, __cudart_i2opi_f;
	mov.u32 	%r4277, -6;
	mov.u64 	%rd458, %rd1;

BB48_697:
	.pragma "nounroll";
	ld.const.u32 	%r3167, [%rd376];
	// inline asm
	{
	mad.lo.cc.u32   %r3165, %r3167, %r1106, %r4278;
	madc.hi.u32     %r4278, %r3167, %r1106,  0;
	}
	// inline asm
	st.local.u32 	[%rd458], %r3165;
	add.s64 	%rd458, %rd458, 4;
	add.s64 	%rd376, %rd376, 4;
	add.s32 	%r4277, %r4277, 1;
	setp.ne.s32	%p440, %r4277, 0;
	@%p440 bra 	BB48_697;

	and.b32  	%r1112, %r1104, -2147483648;
	st.local.u32 	[%rd3], %r4278;
	mov.u32 	%r3170, 6;
	sub.s32 	%r3171, %r3170, %r1107;
	mul.wide.s32 	%rd308, %r3171, 4;
	add.s64 	%rd158, %rd1, %rd308;
	ld.local.u32 	%r4279, [%rd158];
	ld.local.u32 	%r4280, [%rd158+-4];
	and.b32  	%r1115, %r1105, 31;
	setp.eq.s32	%p441, %r1115, 0;
	@%p441 bra 	BB48_700;

	mov.u32 	%r3172, 32;
	sub.s32 	%r3173, %r3172, %r1115;
	shr.u32 	%r3174, %r4280, %r3173;
	shl.b32 	%r3175, %r4279, %r1115;
	add.s32 	%r4279, %r3174, %r3175;
	ld.local.u32 	%r3176, [%rd158+-8];
	shr.u32 	%r3177, %r3176, %r3173;
	shl.b32 	%r3178, %r4280, %r1115;
	add.s32 	%r4280, %r3177, %r3178;

BB48_700:
	shr.u32 	%r3179, %r4280, 30;
	shl.b32 	%r3180, %r4279, 2;
	add.s32 	%r4281, %r3179, %r3180;
	shl.b32 	%r1121, %r4280, 2;
	shr.u32 	%r3181, %r4281, 31;
	shr.u32 	%r3182, %r4279, 30;
	add.s32 	%r1122, %r3181, %r3182;
	setp.eq.s32	%p442, %r3181, 0;
	mov.u32 	%r4282, %r1112;
	mov.u32 	%r4283, %r1121;
	@%p442 bra 	BB48_702;

	not.b32 	%r3183, %r4281;
	neg.s32 	%r1123, %r1121;
	setp.eq.s32	%p443, %r1121, 0;
	selp.u32	%r3184, 1, 0, %p443;
	add.s32 	%r4281, %r3184, %r3183;
	xor.b32  	%r1125, %r1112, -2147483648;
	mov.u32 	%r4282, %r1125;
	mov.u32 	%r4283, %r1123;

BB48_702:
	mov.u32 	%r1127, %r4282;
	neg.s32 	%r3185, %r1122;
	setp.eq.s32	%p444, %r1112, 0;
	selp.b32	%r4286, %r1122, %r3185, %p444;
	clz.b32 	%r4285, %r4281;
	setp.eq.s32	%p445, %r4285, 0;
	shl.b32 	%r3186, %r4281, %r4285;
	mov.u32 	%r3187, 32;
	sub.s32 	%r3188, %r3187, %r4285;
	shr.u32 	%r3189, %r4283, %r3188;
	add.s32 	%r3190, %r3189, %r3186;
	selp.b32	%r1131, %r4281, %r3190, %p445;
	mov.u32 	%r3191, -921707870;
	mul.hi.u32 	%r4284, %r1131, %r3191;
	setp.lt.s32	%p446, %r4284, 1;
	@%p446 bra 	BB48_704;

	mul.lo.s32 	%r3192, %r1131, -921707870;
	shr.u32 	%r3193, %r3192, 31;
	shl.b32 	%r3194, %r4284, 1;
	add.s32 	%r4284, %r3193, %r3194;
	add.s32 	%r4285, %r4285, 1;

BB48_704:
	mov.u32 	%r3195, 126;
	sub.s32 	%r3196, %r3195, %r4285;
	shl.b32 	%r3197, %r3196, 23;
	add.s32 	%r3198, %r4284, 1;
	shr.u32 	%r3199, %r3198, 7;
	add.s32 	%r3200, %r3199, 1;
	shr.u32 	%r3201, %r3200, 1;
	add.s32 	%r3202, %r3201, %r3197;
	or.b32  	%r3203, %r3202, %r1127;
	mov.b32 	 %f2417, %r3203;

BB48_705:
	mul.rn.f32 	%f609, %f2417, %f2417;
	and.b32  	%r1138, %r4286, 1;
	setp.eq.s32	%p447, %r1138, 0;
	@%p447 bra 	BB48_707;

	mov.f32 	%f1812, 0fBAB6061A;
	mov.f32 	%f1813, 0f37CCF5CE;
	fma.rn.f32 	%f2418, %f1813, %f609, %f1812;
	bra.uni 	BB48_708;

BB48_707:
	mov.f32 	%f1814, 0f3C08839E;
	mov.f32 	%f1815, 0fB94CA1F9;
	fma.rn.f32 	%f2418, %f1815, %f609, %f1814;

BB48_708:
	@%p447 bra 	BB48_710;

	mov.f32 	%f1816, 0f3D2AAAA5;
	fma.rn.f32 	%f1817, %f2418, %f609, %f1816;
	mov.f32 	%f1818, 0fBF000000;
	fma.rn.f32 	%f2419, %f1817, %f609, %f1818;
	bra.uni 	BB48_711;

BB48_710:
	mov.f32 	%f1819, 0fBE2AAAA3;
	fma.rn.f32 	%f1820, %f2418, %f609, %f1819;
	mov.f32 	%f1821, 0f00000000;
	fma.rn.f32 	%f2419, %f1820, %f609, %f1821;

BB48_711:
	fma.rn.f32 	%f2420, %f2419, %f2417, %f2417;
	@%p447 bra 	BB48_713;

	mov.f32 	%f1822, 0f3F800000;
	fma.rn.f32 	%f2420, %f2419, %f609, %f1822;

BB48_713:
	and.b32  	%r3204, %r4286, 2;
	setp.eq.s32	%p450, %r3204, 0;
	@%p450 bra 	BB48_715;

	mov.f32 	%f1823, 0f00000000;
	mov.f32 	%f1824, 0fBF800000;
	fma.rn.f32 	%f2420, %f2420, %f1824, %f1823;

BB48_715:
	mov.f32 	%f2508, %f487;
	@%p373 bra 	BB48_717;

	mov.f32 	%f1825, 0f00000000;
	mul.rn.f32 	%f2508, %f487, %f1825;

BB48_717:
	mul.f32 	%f1826, %f2508, 0f3F22F983;
	cvt.rni.s32.f32	%r4296, %f1826;
	cvt.rn.f32.s32	%f1827, %r4296;
	neg.f32 	%f1828, %f1827;
	fma.rn.f32 	%f1830, %f1828, %f1668, %f2508;
	fma.rn.f32 	%f1832, %f1828, %f1670, %f1830;
	fma.rn.f32 	%f2421, %f1828, %f1672, %f1832;
	abs.f32 	%f1834, %f2508;
	setp.leu.f32	%p452, %f1834, 0f47CE4780;
	@%p452 bra 	BB48_727;

	mov.b32 	 %r1140, %f2508;
	shr.u32 	%r1141, %r1140, 23;
	bfe.u32 	%r3207, %r1140, 23, 8;
	add.s32 	%r3208, %r3207, -128;
	shl.b32 	%r3209, %r1140, 8;
	or.b32  	%r1142, %r3209, -2147483648;
	shr.u32 	%r1143, %r3208, 5;
	mov.u32 	%r4288, 0;
	mov.u64 	%rd377, __cudart_i2opi_f;
	mov.u32 	%r4287, -6;
	mov.u64 	%rd457, %rd1;

BB48_719:
	.pragma "nounroll";
	ld.const.u32 	%r3212, [%rd377];
	// inline asm
	{
	mad.lo.cc.u32   %r3210, %r3212, %r1142, %r4288;
	madc.hi.u32     %r4288, %r3212, %r1142,  0;
	}
	// inline asm
	st.local.u32 	[%rd457], %r3210;
	add.s64 	%rd457, %rd457, 4;
	add.s64 	%rd377, %rd377, 4;
	add.s32 	%r4287, %r4287, 1;
	setp.ne.s32	%p453, %r4287, 0;
	@%p453 bra 	BB48_719;

	and.b32  	%r1148, %r1140, -2147483648;
	st.local.u32 	[%rd3], %r4288;
	mov.u32 	%r3215, 6;
	sub.s32 	%r3216, %r3215, %r1143;
	mul.wide.s32 	%rd310, %r3216, 4;
	add.s64 	%rd163, %rd1, %rd310;
	ld.local.u32 	%r4289, [%rd163];
	ld.local.u32 	%r4290, [%rd163+-4];
	and.b32  	%r1151, %r1141, 31;
	setp.eq.s32	%p454, %r1151, 0;
	@%p454 bra 	BB48_722;

	mov.u32 	%r3217, 32;
	sub.s32 	%r3218, %r3217, %r1151;
	shr.u32 	%r3219, %r4290, %r3218;
	shl.b32 	%r3220, %r4289, %r1151;
	add.s32 	%r4289, %r3219, %r3220;
	ld.local.u32 	%r3221, [%rd163+-8];
	shr.u32 	%r3222, %r3221, %r3218;
	shl.b32 	%r3223, %r4290, %r1151;
	add.s32 	%r4290, %r3222, %r3223;

BB48_722:
	shr.u32 	%r3224, %r4290, 30;
	shl.b32 	%r3225, %r4289, 2;
	add.s32 	%r4291, %r3224, %r3225;
	shl.b32 	%r1157, %r4290, 2;
	shr.u32 	%r3226, %r4291, 31;
	shr.u32 	%r3227, %r4289, 30;
	add.s32 	%r1158, %r3226, %r3227;
	setp.eq.s32	%p455, %r3226, 0;
	mov.u32 	%r4292, %r1148;
	mov.u32 	%r4293, %r1157;
	@%p455 bra 	BB48_724;

	not.b32 	%r3228, %r4291;
	neg.s32 	%r1159, %r1157;
	setp.eq.s32	%p456, %r1157, 0;
	selp.u32	%r3229, 1, 0, %p456;
	add.s32 	%r4291, %r3229, %r3228;
	xor.b32  	%r1161, %r1148, -2147483648;
	mov.u32 	%r4292, %r1161;
	mov.u32 	%r4293, %r1159;

BB48_724:
	mov.u32 	%r1163, %r4292;
	neg.s32 	%r3230, %r1158;
	setp.eq.s32	%p457, %r1148, 0;
	selp.b32	%r4296, %r1158, %r3230, %p457;
	clz.b32 	%r4295, %r4291;
	setp.eq.s32	%p458, %r4295, 0;
	shl.b32 	%r3231, %r4291, %r4295;
	mov.u32 	%r3232, 32;
	sub.s32 	%r3233, %r3232, %r4295;
	shr.u32 	%r3234, %r4293, %r3233;
	add.s32 	%r3235, %r3234, %r3231;
	selp.b32	%r1167, %r4291, %r3235, %p458;
	mov.u32 	%r3236, -921707870;
	mul.hi.u32 	%r4294, %r1167, %r3236;
	setp.lt.s32	%p459, %r4294, 1;
	@%p459 bra 	BB48_726;

	mul.lo.s32 	%r3237, %r1167, -921707870;
	shr.u32 	%r3238, %r3237, 31;
	shl.b32 	%r3239, %r4294, 1;
	add.s32 	%r4294, %r3238, %r3239;
	add.s32 	%r4295, %r4295, 1;

BB48_726:
	mov.u32 	%r3240, 126;
	sub.s32 	%r3241, %r3240, %r4295;
	shl.b32 	%r3242, %r3241, 23;
	add.s32 	%r3243, %r4294, 1;
	shr.u32 	%r3244, %r3243, 7;
	add.s32 	%r3245, %r3244, 1;
	shr.u32 	%r3246, %r3245, 1;
	add.s32 	%r3247, %r3246, %r3242;
	or.b32  	%r3248, %r3247, %r1163;
	mov.b32 	 %f2421, %r3248;

BB48_727:
	mul.rn.f32 	%f626, %f2421, %f2421;
	add.s32 	%r1174, %r4296, 1;
	and.b32  	%r1175, %r1174, 1;
	setp.eq.s32	%p460, %r1175, 0;
	@%p460 bra 	BB48_729;

	mov.f32 	%f1835, 0fBAB6061A;
	mov.f32 	%f1836, 0f37CCF5CE;
	fma.rn.f32 	%f2422, %f1836, %f626, %f1835;
	bra.uni 	BB48_730;

BB48_729:
	mov.f32 	%f1837, 0f3C08839E;
	mov.f32 	%f1838, 0fB94CA1F9;
	fma.rn.f32 	%f2422, %f1838, %f626, %f1837;

BB48_730:
	@%p460 bra 	BB48_732;

	mov.f32 	%f1839, 0f3D2AAAA5;
	fma.rn.f32 	%f1840, %f2422, %f626, %f1839;
	mov.f32 	%f1841, 0fBF000000;
	fma.rn.f32 	%f2423, %f1840, %f626, %f1841;
	bra.uni 	BB48_733;

BB48_732:
	mov.f32 	%f1842, 0fBE2AAAA3;
	fma.rn.f32 	%f1843, %f2422, %f626, %f1842;
	mov.f32 	%f1844, 0f00000000;
	fma.rn.f32 	%f2423, %f1843, %f626, %f1844;

BB48_733:
	fma.rn.f32 	%f2424, %f2423, %f2421, %f2421;
	@%p460 bra 	BB48_735;

	mov.f32 	%f1845, 0f3F800000;
	fma.rn.f32 	%f2424, %f2423, %f626, %f1845;

BB48_735:
	and.b32  	%r3249, %r1174, 2;
	setp.eq.s32	%p463, %r3249, 0;
	@%p463 bra 	BB48_737;

	mov.f32 	%f1846, 0f00000000;
	mov.f32 	%f1847, 0fBF800000;
	fma.rn.f32 	%f2424, %f2424, %f1847, %f1846;

BB48_737:
	mul.f32 	%f638, %f2420, %f2424;
	mov.f32 	%f2527, %f494;
	@%p386 bra 	BB48_739;

	mov.f32 	%f1848, 0f00000000;
	mul.rn.f32 	%f2527, %f494, %f1848;

BB48_739:
	mul.f32 	%f1849, %f2527, 0f3F22F983;
	cvt.rni.s32.f32	%r4306, %f1849;
	cvt.rn.f32.s32	%f1850, %r4306;
	neg.f32 	%f1851, %f1850;
	fma.rn.f32 	%f1853, %f1851, %f1668, %f2527;
	fma.rn.f32 	%f1855, %f1851, %f1670, %f1853;
	fma.rn.f32 	%f2425, %f1851, %f1672, %f1855;
	abs.f32 	%f1857, %f2527;
	setp.leu.f32	%p465, %f1857, 0f47CE4780;
	@%p465 bra 	BB48_749;

	mov.b32 	 %r1177, %f2527;
	shr.u32 	%r1178, %r1177, 23;
	bfe.u32 	%r3252, %r1177, 23, 8;
	add.s32 	%r3253, %r3252, -128;
	shl.b32 	%r3254, %r1177, 8;
	or.b32  	%r1179, %r3254, -2147483648;
	shr.u32 	%r1180, %r3253, 5;
	mov.u32 	%r4298, 0;
	mov.u64 	%rd378, __cudart_i2opi_f;
	mov.u32 	%r4297, -6;
	mov.u64 	%rd456, %rd1;

BB48_741:
	.pragma "nounroll";
	ld.const.u32 	%r3257, [%rd378];
	// inline asm
	{
	mad.lo.cc.u32   %r3255, %r3257, %r1179, %r4298;
	madc.hi.u32     %r4298, %r3257, %r1179,  0;
	}
	// inline asm
	st.local.u32 	[%rd456], %r3255;
	add.s64 	%rd456, %rd456, 4;
	add.s64 	%rd378, %rd378, 4;
	add.s32 	%r4297, %r4297, 1;
	setp.ne.s32	%p466, %r4297, 0;
	@%p466 bra 	BB48_741;

	and.b32  	%r1185, %r1177, -2147483648;
	st.local.u32 	[%rd3], %r4298;
	mov.u32 	%r3260, 6;
	sub.s32 	%r3261, %r3260, %r1180;
	mul.wide.s32 	%rd312, %r3261, 4;
	add.s64 	%rd168, %rd1, %rd312;
	ld.local.u32 	%r4299, [%rd168];
	ld.local.u32 	%r4300, [%rd168+-4];
	and.b32  	%r1188, %r1178, 31;
	setp.eq.s32	%p467, %r1188, 0;
	@%p467 bra 	BB48_744;

	mov.u32 	%r3262, 32;
	sub.s32 	%r3263, %r3262, %r1188;
	shr.u32 	%r3264, %r4300, %r3263;
	shl.b32 	%r3265, %r4299, %r1188;
	add.s32 	%r4299, %r3264, %r3265;
	ld.local.u32 	%r3266, [%rd168+-8];
	shr.u32 	%r3267, %r3266, %r3263;
	shl.b32 	%r3268, %r4300, %r1188;
	add.s32 	%r4300, %r3267, %r3268;

BB48_744:
	shr.u32 	%r3269, %r4300, 30;
	shl.b32 	%r3270, %r4299, 2;
	add.s32 	%r4301, %r3269, %r3270;
	shl.b32 	%r1194, %r4300, 2;
	shr.u32 	%r3271, %r4301, 31;
	shr.u32 	%r3272, %r4299, 30;
	add.s32 	%r1195, %r3271, %r3272;
	setp.eq.s32	%p468, %r3271, 0;
	mov.u32 	%r4302, %r1185;
	mov.u32 	%r4303, %r1194;
	@%p468 bra 	BB48_746;

	not.b32 	%r3273, %r4301;
	neg.s32 	%r1196, %r1194;
	setp.eq.s32	%p469, %r1194, 0;
	selp.u32	%r3274, 1, 0, %p469;
	add.s32 	%r4301, %r3274, %r3273;
	xor.b32  	%r1198, %r1185, -2147483648;
	mov.u32 	%r4302, %r1198;
	mov.u32 	%r4303, %r1196;

BB48_746:
	mov.u32 	%r1200, %r4302;
	neg.s32 	%r3275, %r1195;
	setp.eq.s32	%p470, %r1185, 0;
	selp.b32	%r4306, %r1195, %r3275, %p470;
	clz.b32 	%r4305, %r4301;
	setp.eq.s32	%p471, %r4305, 0;
	shl.b32 	%r3276, %r4301, %r4305;
	mov.u32 	%r3277, 32;
	sub.s32 	%r3278, %r3277, %r4305;
	shr.u32 	%r3279, %r4303, %r3278;
	add.s32 	%r3280, %r3279, %r3276;
	selp.b32	%r1204, %r4301, %r3280, %p471;
	mov.u32 	%r3281, -921707870;
	mul.hi.u32 	%r4304, %r1204, %r3281;
	setp.lt.s32	%p472, %r4304, 1;
	@%p472 bra 	BB48_748;

	mul.lo.s32 	%r3282, %r1204, -921707870;
	shr.u32 	%r3283, %r3282, 31;
	shl.b32 	%r3284, %r4304, 1;
	add.s32 	%r4304, %r3283, %r3284;
	add.s32 	%r4305, %r4305, 1;

BB48_748:
	mov.u32 	%r3285, 126;
	sub.s32 	%r3286, %r3285, %r4305;
	shl.b32 	%r3287, %r3286, 23;
	add.s32 	%r3288, %r4304, 1;
	shr.u32 	%r3289, %r3288, 7;
	add.s32 	%r3290, %r3289, 1;
	shr.u32 	%r3291, %r3290, 1;
	add.s32 	%r3292, %r3291, %r3287;
	or.b32  	%r3293, %r3292, %r1200;
	mov.b32 	 %f2425, %r3293;

BB48_749:
	mul.rn.f32 	%f644, %f2425, %f2425;
	add.s32 	%r1211, %r4306, 1;
	and.b32  	%r1212, %r1211, 1;
	setp.eq.s32	%p473, %r1212, 0;
	@%p473 bra 	BB48_751;

	mov.f32 	%f1858, 0fBAB6061A;
	mov.f32 	%f1859, 0f37CCF5CE;
	fma.rn.f32 	%f2426, %f1859, %f644, %f1858;
	bra.uni 	BB48_752;

BB48_751:
	mov.f32 	%f1860, 0f3C08839E;
	mov.f32 	%f1861, 0fB94CA1F9;
	fma.rn.f32 	%f2426, %f1861, %f644, %f1860;

BB48_752:
	@%p473 bra 	BB48_754;

	mov.f32 	%f1862, 0f3D2AAAA5;
	fma.rn.f32 	%f1863, %f2426, %f644, %f1862;
	mov.f32 	%f1864, 0fBF000000;
	fma.rn.f32 	%f2427, %f1863, %f644, %f1864;
	bra.uni 	BB48_755;

BB48_754:
	mov.f32 	%f1865, 0fBE2AAAA3;
	fma.rn.f32 	%f1866, %f2426, %f644, %f1865;
	mov.f32 	%f1867, 0f00000000;
	fma.rn.f32 	%f2427, %f1866, %f644, %f1867;

BB48_755:
	fma.rn.f32 	%f2428, %f2427, %f2425, %f2425;
	@%p473 bra 	BB48_757;

	mov.f32 	%f1868, 0f3F800000;
	fma.rn.f32 	%f2428, %f2427, %f644, %f1868;

BB48_757:
	and.b32  	%r3294, %r1211, 2;
	setp.eq.s32	%p476, %r3294, 0;
	@%p476 bra 	BB48_759;

	mov.f32 	%f1869, 0f00000000;
	mov.f32 	%f1870, 0fBF800000;
	fma.rn.f32 	%f2428, %f2428, %f1870, %f1869;

BB48_759:
	mul.f32 	%f656, %f638, %f2428;
	mov.f32 	%f2488, %f480;
	@%p360 bra 	BB48_761;

	mov.f32 	%f1871, 0f00000000;
	mul.rn.f32 	%f2488, %f480, %f1871;

BB48_761:
	mul.f32 	%f1872, %f2488, 0f3F22F983;
	cvt.rni.s32.f32	%r4316, %f1872;
	cvt.rn.f32.s32	%f1873, %r4316;
	neg.f32 	%f1874, %f1873;
	fma.rn.f32 	%f1876, %f1874, %f1668, %f2488;
	fma.rn.f32 	%f1878, %f1874, %f1670, %f1876;
	fma.rn.f32 	%f2429, %f1874, %f1672, %f1878;
	abs.f32 	%f1880, %f2488;
	setp.leu.f32	%p478, %f1880, 0f47CE4780;
	@%p478 bra 	BB48_771;

	mov.b32 	 %r1214, %f2488;
	shr.u32 	%r1215, %r1214, 23;
	bfe.u32 	%r3297, %r1214, 23, 8;
	add.s32 	%r3298, %r3297, -128;
	shl.b32 	%r3299, %r1214, 8;
	or.b32  	%r1216, %r3299, -2147483648;
	shr.u32 	%r1217, %r3298, 5;
	mov.u32 	%r4308, 0;
	mov.u64 	%rd379, __cudart_i2opi_f;
	mov.u32 	%r4307, -6;
	mov.u64 	%rd455, %rd1;

BB48_763:
	.pragma "nounroll";
	ld.const.u32 	%r3302, [%rd379];
	// inline asm
	{
	mad.lo.cc.u32   %r3300, %r3302, %r1216, %r4308;
	madc.hi.u32     %r4308, %r3302, %r1216,  0;
	}
	// inline asm
	st.local.u32 	[%rd455], %r3300;
	add.s64 	%rd455, %rd455, 4;
	add.s64 	%rd379, %rd379, 4;
	add.s32 	%r4307, %r4307, 1;
	setp.ne.s32	%p479, %r4307, 0;
	@%p479 bra 	BB48_763;

	and.b32  	%r1222, %r1214, -2147483648;
	st.local.u32 	[%rd3], %r4308;
	mov.u32 	%r3305, 6;
	sub.s32 	%r3306, %r3305, %r1217;
	mul.wide.s32 	%rd314, %r3306, 4;
	add.s64 	%rd173, %rd1, %rd314;
	ld.local.u32 	%r4309, [%rd173];
	ld.local.u32 	%r4310, [%rd173+-4];
	and.b32  	%r1225, %r1215, 31;
	setp.eq.s32	%p480, %r1225, 0;
	@%p480 bra 	BB48_766;

	mov.u32 	%r3307, 32;
	sub.s32 	%r3308, %r3307, %r1225;
	shr.u32 	%r3309, %r4310, %r3308;
	shl.b32 	%r3310, %r4309, %r1225;
	add.s32 	%r4309, %r3309, %r3310;
	ld.local.u32 	%r3311, [%rd173+-8];
	shr.u32 	%r3312, %r3311, %r3308;
	shl.b32 	%r3313, %r4310, %r1225;
	add.s32 	%r4310, %r3312, %r3313;

BB48_766:
	shr.u32 	%r3314, %r4310, 30;
	shl.b32 	%r3315, %r4309, 2;
	add.s32 	%r4311, %r3314, %r3315;
	shl.b32 	%r1231, %r4310, 2;
	shr.u32 	%r3316, %r4311, 31;
	shr.u32 	%r3317, %r4309, 30;
	add.s32 	%r1232, %r3316, %r3317;
	setp.eq.s32	%p481, %r3316, 0;
	mov.u32 	%r4312, %r1222;
	mov.u32 	%r4313, %r1231;
	@%p481 bra 	BB48_768;

	not.b32 	%r3318, %r4311;
	neg.s32 	%r1233, %r1231;
	setp.eq.s32	%p482, %r1231, 0;
	selp.u32	%r3319, 1, 0, %p482;
	add.s32 	%r4311, %r3319, %r3318;
	xor.b32  	%r1235, %r1222, -2147483648;
	mov.u32 	%r4312, %r1235;
	mov.u32 	%r4313, %r1233;

BB48_768:
	mov.u32 	%r1237, %r4312;
	neg.s32 	%r3320, %r1232;
	setp.eq.s32	%p483, %r1222, 0;
	selp.b32	%r4316, %r1232, %r3320, %p483;
	clz.b32 	%r4315, %r4311;
	setp.eq.s32	%p484, %r4315, 0;
	shl.b32 	%r3321, %r4311, %r4315;
	mov.u32 	%r3322, 32;
	sub.s32 	%r3323, %r3322, %r4315;
	shr.u32 	%r3324, %r4313, %r3323;
	add.s32 	%r3325, %r3324, %r3321;
	selp.b32	%r1241, %r4311, %r3325, %p484;
	mov.u32 	%r3326, -921707870;
	mul.hi.u32 	%r4314, %r1241, %r3326;
	setp.lt.s32	%p485, %r4314, 1;
	@%p485 bra 	BB48_770;

	mul.lo.s32 	%r3327, %r1241, -921707870;
	shr.u32 	%r3328, %r3327, 31;
	shl.b32 	%r3329, %r4314, 1;
	add.s32 	%r4314, %r3328, %r3329;
	add.s32 	%r4315, %r4315, 1;

BB48_770:
	mov.u32 	%r3330, 126;
	sub.s32 	%r3331, %r3330, %r4315;
	shl.b32 	%r3332, %r3331, 23;
	add.s32 	%r3333, %r4314, 1;
	shr.u32 	%r3334, %r3333, 7;
	add.s32 	%r3335, %r3334, 1;
	shr.u32 	%r3336, %r3335, 1;
	add.s32 	%r3337, %r3336, %r3332;
	or.b32  	%r3338, %r3337, %r1237;
	mov.b32 	 %f2429, %r3338;

BB48_771:
	mul.rn.f32 	%f662, %f2429, %f2429;
	add.s32 	%r1248, %r4316, 1;
	and.b32  	%r1249, %r1248, 1;
	setp.eq.s32	%p486, %r1249, 0;
	@%p486 bra 	BB48_773;

	mov.f32 	%f1881, 0fBAB6061A;
	mov.f32 	%f1882, 0f37CCF5CE;
	fma.rn.f32 	%f2430, %f1882, %f662, %f1881;
	bra.uni 	BB48_774;

BB48_773:
	mov.f32 	%f1883, 0f3C08839E;
	mov.f32 	%f1884, 0fB94CA1F9;
	fma.rn.f32 	%f2430, %f1884, %f662, %f1883;

BB48_774:
	@%p486 bra 	BB48_776;

	mov.f32 	%f1885, 0f3D2AAAA5;
	fma.rn.f32 	%f1886, %f2430, %f662, %f1885;
	mov.f32 	%f1887, 0fBF000000;
	fma.rn.f32 	%f2431, %f1886, %f662, %f1887;
	bra.uni 	BB48_777;

BB48_776:
	mov.f32 	%f1888, 0fBE2AAAA3;
	fma.rn.f32 	%f1889, %f2430, %f662, %f1888;
	mov.f32 	%f1890, 0f00000000;
	fma.rn.f32 	%f2431, %f1889, %f662, %f1890;

BB48_777:
	fma.rn.f32 	%f2432, %f2431, %f2429, %f2429;
	@%p486 bra 	BB48_779;

	mov.f32 	%f1891, 0f3F800000;
	fma.rn.f32 	%f2432, %f2431, %f662, %f1891;

BB48_779:
	and.b32  	%r3339, %r1248, 2;
	setp.eq.s32	%p489, %r3339, 0;
	@%p489 bra 	BB48_781;

	mov.f32 	%f1892, 0f00000000;
	mov.f32 	%f1893, 0fBF800000;
	fma.rn.f32 	%f2432, %f2432, %f1893, %f1892;

BB48_781:
	mov.f32 	%f2507, %f487;
	@%p373 bra 	BB48_783;

	mov.f32 	%f1894, 0f00000000;
	mul.rn.f32 	%f2507, %f487, %f1894;

BB48_783:
	mul.f32 	%f1895, %f2507, 0f3F22F983;
	cvt.rni.s32.f32	%r4326, %f1895;
	cvt.rn.f32.s32	%f1896, %r4326;
	neg.f32 	%f1897, %f1896;
	fma.rn.f32 	%f1899, %f1897, %f1668, %f2507;
	fma.rn.f32 	%f1901, %f1897, %f1670, %f1899;
	fma.rn.f32 	%f2433, %f1897, %f1672, %f1901;
	abs.f32 	%f1903, %f2507;
	setp.leu.f32	%p491, %f1903, 0f47CE4780;
	@%p491 bra 	BB48_793;

	mov.b32 	 %r1251, %f2507;
	shr.u32 	%r1252, %r1251, 23;
	bfe.u32 	%r3342, %r1251, 23, 8;
	add.s32 	%r3343, %r3342, -128;
	shl.b32 	%r3344, %r1251, 8;
	or.b32  	%r1253, %r3344, -2147483648;
	shr.u32 	%r1254, %r3343, 5;
	mov.u32 	%r4318, 0;
	mov.u64 	%rd380, __cudart_i2opi_f;
	mov.u32 	%r4317, -6;
	mov.u64 	%rd454, %rd1;

BB48_785:
	.pragma "nounroll";
	ld.const.u32 	%r3347, [%rd380];
	// inline asm
	{
	mad.lo.cc.u32   %r3345, %r3347, %r1253, %r4318;
	madc.hi.u32     %r4318, %r3347, %r1253,  0;
	}
	// inline asm
	st.local.u32 	[%rd454], %r3345;
	add.s64 	%rd454, %rd454, 4;
	add.s64 	%rd380, %rd380, 4;
	add.s32 	%r4317, %r4317, 1;
	setp.ne.s32	%p492, %r4317, 0;
	@%p492 bra 	BB48_785;

	and.b32  	%r1259, %r1251, -2147483648;
	st.local.u32 	[%rd3], %r4318;
	mov.u32 	%r3350, 6;
	sub.s32 	%r3351, %r3350, %r1254;
	mul.wide.s32 	%rd316, %r3351, 4;
	add.s64 	%rd178, %rd1, %rd316;
	ld.local.u32 	%r4319, [%rd178];
	ld.local.u32 	%r4320, [%rd178+-4];
	and.b32  	%r1262, %r1252, 31;
	setp.eq.s32	%p493, %r1262, 0;
	@%p493 bra 	BB48_788;

	mov.u32 	%r3352, 32;
	sub.s32 	%r3353, %r3352, %r1262;
	shr.u32 	%r3354, %r4320, %r3353;
	shl.b32 	%r3355, %r4319, %r1262;
	add.s32 	%r4319, %r3354, %r3355;
	ld.local.u32 	%r3356, [%rd178+-8];
	shr.u32 	%r3357, %r3356, %r3353;
	shl.b32 	%r3358, %r4320, %r1262;
	add.s32 	%r4320, %r3357, %r3358;

BB48_788:
	shr.u32 	%r3359, %r4320, 30;
	shl.b32 	%r3360, %r4319, 2;
	add.s32 	%r4321, %r3359, %r3360;
	shl.b32 	%r1268, %r4320, 2;
	shr.u32 	%r3361, %r4321, 31;
	shr.u32 	%r3362, %r4319, 30;
	add.s32 	%r1269, %r3361, %r3362;
	setp.eq.s32	%p494, %r3361, 0;
	mov.u32 	%r4322, %r1259;
	mov.u32 	%r4323, %r1268;
	@%p494 bra 	BB48_790;

	not.b32 	%r3363, %r4321;
	neg.s32 	%r1270, %r1268;
	setp.eq.s32	%p495, %r1268, 0;
	selp.u32	%r3364, 1, 0, %p495;
	add.s32 	%r4321, %r3364, %r3363;
	xor.b32  	%r1272, %r1259, -2147483648;
	mov.u32 	%r4322, %r1272;
	mov.u32 	%r4323, %r1270;

BB48_790:
	mov.u32 	%r1274, %r4322;
	neg.s32 	%r3365, %r1269;
	setp.eq.s32	%p496, %r1259, 0;
	selp.b32	%r4326, %r1269, %r3365, %p496;
	clz.b32 	%r4325, %r4321;
	setp.eq.s32	%p497, %r4325, 0;
	shl.b32 	%r3366, %r4321, %r4325;
	mov.u32 	%r3367, 32;
	sub.s32 	%r3368, %r3367, %r4325;
	shr.u32 	%r3369, %r4323, %r3368;
	add.s32 	%r3370, %r3369, %r3366;
	selp.b32	%r1278, %r4321, %r3370, %p497;
	mov.u32 	%r3371, -921707870;
	mul.hi.u32 	%r4324, %r1278, %r3371;
	setp.lt.s32	%p498, %r4324, 1;
	@%p498 bra 	BB48_792;

	mul.lo.s32 	%r3372, %r1278, -921707870;
	shr.u32 	%r3373, %r3372, 31;
	shl.b32 	%r3374, %r4324, 1;
	add.s32 	%r4324, %r3373, %r3374;
	add.s32 	%r4325, %r4325, 1;

BB48_792:
	mov.u32 	%r3375, 126;
	sub.s32 	%r3376, %r3375, %r4325;
	shl.b32 	%r3377, %r3376, 23;
	add.s32 	%r3378, %r4324, 1;
	shr.u32 	%r3379, %r3378, 7;
	add.s32 	%r3380, %r3379, 1;
	shr.u32 	%r3381, %r3380, 1;
	add.s32 	%r3382, %r3381, %r3377;
	or.b32  	%r3383, %r3382, %r1274;
	mov.b32 	 %f2433, %r3383;

BB48_793:
	mul.rn.f32 	%f679, %f2433, %f2433;
	and.b32  	%r1285, %r4326, 1;
	setp.eq.s32	%p499, %r1285, 0;
	@%p499 bra 	BB48_795;

	mov.f32 	%f1904, 0fBAB6061A;
	mov.f32 	%f1905, 0f37CCF5CE;
	fma.rn.f32 	%f2434, %f1905, %f679, %f1904;
	bra.uni 	BB48_796;

BB48_795:
	mov.f32 	%f1906, 0f3C08839E;
	mov.f32 	%f1907, 0fB94CA1F9;
	fma.rn.f32 	%f2434, %f1907, %f679, %f1906;

BB48_796:
	@%p499 bra 	BB48_798;

	mov.f32 	%f1908, 0f3D2AAAA5;
	fma.rn.f32 	%f1909, %f2434, %f679, %f1908;
	mov.f32 	%f1910, 0fBF000000;
	fma.rn.f32 	%f2435, %f1909, %f679, %f1910;
	bra.uni 	BB48_799;

BB48_798:
	mov.f32 	%f1911, 0fBE2AAAA3;
	fma.rn.f32 	%f1912, %f2434, %f679, %f1911;
	mov.f32 	%f1913, 0f00000000;
	fma.rn.f32 	%f2435, %f1912, %f679, %f1913;

BB48_799:
	fma.rn.f32 	%f2436, %f2435, %f2433, %f2433;
	@%p499 bra 	BB48_801;

	mov.f32 	%f1914, 0f3F800000;
	fma.rn.f32 	%f2436, %f2435, %f679, %f1914;

BB48_801:
	and.b32  	%r3384, %r4326, 2;
	setp.eq.s32	%p502, %r3384, 0;
	@%p502 bra 	BB48_803;

	mov.f32 	%f1915, 0f00000000;
	mov.f32 	%f1916, 0fBF800000;
	fma.rn.f32 	%f2436, %f2436, %f1916, %f1915;

BB48_803:
	mul.f32 	%f691, %f2432, %f2436;
	mov.f32 	%f2526, %f494;
	@%p386 bra 	BB48_805;

	mov.f32 	%f1917, 0f00000000;
	mul.rn.f32 	%f2526, %f494, %f1917;

BB48_805:
	mul.f32 	%f1918, %f2526, 0f3F22F983;
	cvt.rni.s32.f32	%r4336, %f1918;
	cvt.rn.f32.s32	%f1919, %r4336;
	neg.f32 	%f1920, %f1919;
	fma.rn.f32 	%f1922, %f1920, %f1668, %f2526;
	fma.rn.f32 	%f1924, %f1920, %f1670, %f1922;
	fma.rn.f32 	%f2437, %f1920, %f1672, %f1924;
	abs.f32 	%f1926, %f2526;
	setp.leu.f32	%p504, %f1926, 0f47CE4780;
	@%p504 bra 	BB48_815;

	mov.b32 	 %r1287, %f2526;
	shr.u32 	%r1288, %r1287, 23;
	bfe.u32 	%r3387, %r1287, 23, 8;
	add.s32 	%r3388, %r3387, -128;
	shl.b32 	%r3389, %r1287, 8;
	or.b32  	%r1289, %r3389, -2147483648;
	shr.u32 	%r1290, %r3388, 5;
	mov.u32 	%r4328, 0;
	mov.u64 	%rd381, __cudart_i2opi_f;
	mov.u32 	%r4327, -6;
	mov.u64 	%rd453, %rd1;

BB48_807:
	.pragma "nounroll";
	ld.const.u32 	%r3392, [%rd381];
	// inline asm
	{
	mad.lo.cc.u32   %r3390, %r3392, %r1289, %r4328;
	madc.hi.u32     %r4328, %r3392, %r1289,  0;
	}
	// inline asm
	st.local.u32 	[%rd453], %r3390;
	add.s64 	%rd453, %rd453, 4;
	add.s64 	%rd381, %rd381, 4;
	add.s32 	%r4327, %r4327, 1;
	setp.ne.s32	%p505, %r4327, 0;
	@%p505 bra 	BB48_807;

	and.b32  	%r1295, %r1287, -2147483648;
	st.local.u32 	[%rd3], %r4328;
	mov.u32 	%r3395, 6;
	sub.s32 	%r3396, %r3395, %r1290;
	mul.wide.s32 	%rd318, %r3396, 4;
	add.s64 	%rd183, %rd1, %rd318;
	ld.local.u32 	%r4329, [%rd183];
	ld.local.u32 	%r4330, [%rd183+-4];
	and.b32  	%r1298, %r1288, 31;
	setp.eq.s32	%p506, %r1298, 0;
	@%p506 bra 	BB48_810;

	mov.u32 	%r3397, 32;
	sub.s32 	%r3398, %r3397, %r1298;
	shr.u32 	%r3399, %r4330, %r3398;
	shl.b32 	%r3400, %r4329, %r1298;
	add.s32 	%r4329, %r3399, %r3400;
	ld.local.u32 	%r3401, [%rd183+-8];
	shr.u32 	%r3402, %r3401, %r3398;
	shl.b32 	%r3403, %r4330, %r1298;
	add.s32 	%r4330, %r3402, %r3403;

BB48_810:
	shr.u32 	%r3404, %r4330, 30;
	shl.b32 	%r3405, %r4329, 2;
	add.s32 	%r4331, %r3404, %r3405;
	shl.b32 	%r1304, %r4330, 2;
	shr.u32 	%r3406, %r4331, 31;
	shr.u32 	%r3407, %r4329, 30;
	add.s32 	%r1305, %r3406, %r3407;
	setp.eq.s32	%p507, %r3406, 0;
	mov.u32 	%r4332, %r1295;
	mov.u32 	%r4333, %r1304;
	@%p507 bra 	BB48_812;

	not.b32 	%r3408, %r4331;
	neg.s32 	%r1306, %r1304;
	setp.eq.s32	%p508, %r1304, 0;
	selp.u32	%r3409, 1, 0, %p508;
	add.s32 	%r4331, %r3409, %r3408;
	xor.b32  	%r1308, %r1295, -2147483648;
	mov.u32 	%r4332, %r1308;
	mov.u32 	%r4333, %r1306;

BB48_812:
	mov.u32 	%r1310, %r4332;
	neg.s32 	%r3410, %r1305;
	setp.eq.s32	%p509, %r1295, 0;
	selp.b32	%r4336, %r1305, %r3410, %p509;
	clz.b32 	%r4335, %r4331;
	setp.eq.s32	%p510, %r4335, 0;
	shl.b32 	%r3411, %r4331, %r4335;
	mov.u32 	%r3412, 32;
	sub.s32 	%r3413, %r3412, %r4335;
	shr.u32 	%r3414, %r4333, %r3413;
	add.s32 	%r3415, %r3414, %r3411;
	selp.b32	%r1314, %r4331, %r3415, %p510;
	mov.u32 	%r3416, -921707870;
	mul.hi.u32 	%r4334, %r1314, %r3416;
	setp.lt.s32	%p511, %r4334, 1;
	@%p511 bra 	BB48_814;

	mul.lo.s32 	%r3417, %r1314, -921707870;
	shr.u32 	%r3418, %r3417, 31;
	shl.b32 	%r3419, %r4334, 1;
	add.s32 	%r4334, %r3418, %r3419;
	add.s32 	%r4335, %r4335, 1;

BB48_814:
	mov.u32 	%r3420, 126;
	sub.s32 	%r3421, %r3420, %r4335;
	shl.b32 	%r3422, %r3421, 23;
	add.s32 	%r3423, %r4334, 1;
	shr.u32 	%r3424, %r3423, 7;
	add.s32 	%r3425, %r3424, 1;
	shr.u32 	%r3426, %r3425, 1;
	add.s32 	%r3427, %r3426, %r3422;
	or.b32  	%r3428, %r3427, %r1310;
	mov.b32 	 %f2437, %r3428;

BB48_815:
	mul.rn.f32 	%f697, %f2437, %f2437;
	and.b32  	%r1321, %r4336, 1;
	setp.eq.s32	%p512, %r1321, 0;
	@%p512 bra 	BB48_817;

	mov.f32 	%f1927, 0fBAB6061A;
	mov.f32 	%f1928, 0f37CCF5CE;
	fma.rn.f32 	%f2438, %f1928, %f697, %f1927;
	bra.uni 	BB48_818;

BB48_817:
	mov.f32 	%f1929, 0f3C08839E;
	mov.f32 	%f1930, 0fB94CA1F9;
	fma.rn.f32 	%f2438, %f1930, %f697, %f1929;

BB48_818:
	@%p512 bra 	BB48_820;

	mov.f32 	%f1931, 0f3D2AAAA5;
	fma.rn.f32 	%f1932, %f2438, %f697, %f1931;
	mov.f32 	%f1933, 0fBF000000;
	fma.rn.f32 	%f2439, %f1932, %f697, %f1933;
	bra.uni 	BB48_821;

BB48_820:
	mov.f32 	%f1934, 0fBE2AAAA3;
	fma.rn.f32 	%f1935, %f2438, %f697, %f1934;
	mov.f32 	%f1936, 0f00000000;
	fma.rn.f32 	%f2439, %f1935, %f697, %f1936;

BB48_821:
	fma.rn.f32 	%f2440, %f2439, %f2437, %f2437;
	@%p512 bra 	BB48_823;

	mov.f32 	%f1937, 0f3F800000;
	fma.rn.f32 	%f2440, %f2439, %f697, %f1937;

BB48_823:
	and.b32  	%r3429, %r4336, 2;
	setp.eq.s32	%p515, %r3429, 0;
	@%p515 bra 	BB48_825;

	mov.f32 	%f1938, 0f00000000;
	mov.f32 	%f1939, 0fBF800000;
	fma.rn.f32 	%f2440, %f2440, %f1939, %f1938;

BB48_825:
	mul.f32 	%f1940, %f691, %f2440;
	sub.f32 	%f709, %f656, %f1940;
	mov.f32 	%f2487, %f480;
	@%p360 bra 	BB48_827;

	mov.f32 	%f1941, 0f00000000;
	mul.rn.f32 	%f2487, %f480, %f1941;

BB48_827:
	mul.f32 	%f1942, %f2487, 0f3F22F983;
	cvt.rni.s32.f32	%r4346, %f1942;
	cvt.rn.f32.s32	%f1943, %r4346;
	neg.f32 	%f1944, %f1943;
	fma.rn.f32 	%f1946, %f1944, %f1668, %f2487;
	fma.rn.f32 	%f1948, %f1944, %f1670, %f1946;
	fma.rn.f32 	%f2441, %f1944, %f1672, %f1948;
	abs.f32 	%f1950, %f2487;
	setp.leu.f32	%p517, %f1950, 0f47CE4780;
	@%p517 bra 	BB48_837;

	mov.b32 	 %r1323, %f2487;
	shr.u32 	%r1324, %r1323, 23;
	bfe.u32 	%r3432, %r1323, 23, 8;
	add.s32 	%r3433, %r3432, -128;
	shl.b32 	%r3434, %r1323, 8;
	or.b32  	%r1325, %r3434, -2147483648;
	shr.u32 	%r1326, %r3433, 5;
	mov.u32 	%r4338, 0;
	mov.u64 	%rd382, __cudart_i2opi_f;
	mov.u32 	%r4337, -6;
	mov.u64 	%rd452, %rd1;

BB48_829:
	.pragma "nounroll";
	ld.const.u32 	%r3437, [%rd382];
	// inline asm
	{
	mad.lo.cc.u32   %r3435, %r3437, %r1325, %r4338;
	madc.hi.u32     %r4338, %r3437, %r1325,  0;
	}
	// inline asm
	st.local.u32 	[%rd452], %r3435;
	add.s64 	%rd452, %rd452, 4;
	add.s64 	%rd382, %rd382, 4;
	add.s32 	%r4337, %r4337, 1;
	setp.ne.s32	%p518, %r4337, 0;
	@%p518 bra 	BB48_829;

	and.b32  	%r1331, %r1323, -2147483648;
	st.local.u32 	[%rd3], %r4338;
	mov.u32 	%r3440, 6;
	sub.s32 	%r3441, %r3440, %r1326;
	mul.wide.s32 	%rd320, %r3441, 4;
	add.s64 	%rd188, %rd1, %rd320;
	ld.local.u32 	%r4339, [%rd188];
	ld.local.u32 	%r4340, [%rd188+-4];
	and.b32  	%r1334, %r1324, 31;
	setp.eq.s32	%p519, %r1334, 0;
	@%p519 bra 	BB48_832;

	mov.u32 	%r3442, 32;
	sub.s32 	%r3443, %r3442, %r1334;
	shr.u32 	%r3444, %r4340, %r3443;
	shl.b32 	%r3445, %r4339, %r1334;
	add.s32 	%r4339, %r3444, %r3445;
	ld.local.u32 	%r3446, [%rd188+-8];
	shr.u32 	%r3447, %r3446, %r3443;
	shl.b32 	%r3448, %r4340, %r1334;
	add.s32 	%r4340, %r3447, %r3448;

BB48_832:
	shr.u32 	%r3449, %r4340, 30;
	shl.b32 	%r3450, %r4339, 2;
	add.s32 	%r4341, %r3449, %r3450;
	shl.b32 	%r1340, %r4340, 2;
	shr.u32 	%r3451, %r4341, 31;
	shr.u32 	%r3452, %r4339, 30;
	add.s32 	%r1341, %r3451, %r3452;
	setp.eq.s32	%p520, %r3451, 0;
	mov.u32 	%r4342, %r1331;
	mov.u32 	%r4343, %r1340;
	@%p520 bra 	BB48_834;

	not.b32 	%r3453, %r4341;
	neg.s32 	%r1342, %r1340;
	setp.eq.s32	%p521, %r1340, 0;
	selp.u32	%r3454, 1, 0, %p521;
	add.s32 	%r4341, %r3454, %r3453;
	xor.b32  	%r1344, %r1331, -2147483648;
	mov.u32 	%r4342, %r1344;
	mov.u32 	%r4343, %r1342;

BB48_834:
	mov.u32 	%r1346, %r4342;
	neg.s32 	%r3455, %r1341;
	setp.eq.s32	%p522, %r1331, 0;
	selp.b32	%r4346, %r1341, %r3455, %p522;
	clz.b32 	%r4345, %r4341;
	setp.eq.s32	%p523, %r4345, 0;
	shl.b32 	%r3456, %r4341, %r4345;
	mov.u32 	%r3457, 32;
	sub.s32 	%r3458, %r3457, %r4345;
	shr.u32 	%r3459, %r4343, %r3458;
	add.s32 	%r3460, %r3459, %r3456;
	selp.b32	%r1350, %r4341, %r3460, %p523;
	mov.u32 	%r3461, -921707870;
	mul.hi.u32 	%r4344, %r1350, %r3461;
	setp.lt.s32	%p524, %r4344, 1;
	@%p524 bra 	BB48_836;

	mul.lo.s32 	%r3462, %r1350, -921707870;
	shr.u32 	%r3463, %r3462, 31;
	shl.b32 	%r3464, %r4344, 1;
	add.s32 	%r4344, %r3463, %r3464;
	add.s32 	%r4345, %r4345, 1;

BB48_836:
	mov.u32 	%r3465, 126;
	sub.s32 	%r3466, %r3465, %r4345;
	shl.b32 	%r3467, %r3466, 23;
	add.s32 	%r3468, %r4344, 1;
	shr.u32 	%r3469, %r3468, 7;
	add.s32 	%r3470, %r3469, 1;
	shr.u32 	%r3471, %r3470, 1;
	add.s32 	%r3472, %r3471, %r3467;
	or.b32  	%r3473, %r3472, %r1346;
	mov.b32 	 %f2441, %r3473;

BB48_837:
	mul.rn.f32 	%f715, %f2441, %f2441;
	add.s32 	%r1357, %r4346, 1;
	and.b32  	%r1358, %r1357, 1;
	setp.eq.s32	%p525, %r1358, 0;
	@%p525 bra 	BB48_839;

	mov.f32 	%f1951, 0fBAB6061A;
	mov.f32 	%f1952, 0f37CCF5CE;
	fma.rn.f32 	%f2442, %f1952, %f715, %f1951;
	bra.uni 	BB48_840;

BB48_839:
	mov.f32 	%f1953, 0f3C08839E;
	mov.f32 	%f1954, 0fB94CA1F9;
	fma.rn.f32 	%f2442, %f1954, %f715, %f1953;

BB48_840:
	@%p525 bra 	BB48_842;

	mov.f32 	%f1955, 0f3D2AAAA5;
	fma.rn.f32 	%f1956, %f2442, %f715, %f1955;
	mov.f32 	%f1957, 0fBF000000;
	fma.rn.f32 	%f2443, %f1956, %f715, %f1957;
	bra.uni 	BB48_843;

BB48_842:
	mov.f32 	%f1958, 0fBE2AAAA3;
	fma.rn.f32 	%f1959, %f2442, %f715, %f1958;
	mov.f32 	%f1960, 0f00000000;
	fma.rn.f32 	%f2443, %f1959, %f715, %f1960;

BB48_843:
	fma.rn.f32 	%f2444, %f2443, %f2441, %f2441;
	@%p525 bra 	BB48_845;

	mov.f32 	%f1961, 0f3F800000;
	fma.rn.f32 	%f2444, %f2443, %f715, %f1961;

BB48_845:
	and.b32  	%r3474, %r1357, 2;
	setp.eq.s32	%p528, %r3474, 0;
	@%p528 bra 	BB48_847;

	mov.f32 	%f1962, 0f00000000;
	mov.f32 	%f1963, 0fBF800000;
	fma.rn.f32 	%f2444, %f2444, %f1963, %f1962;

BB48_847:
	mov.f32 	%f2506, %f487;
	@%p373 bra 	BB48_849;

	mov.f32 	%f1964, 0f00000000;
	mul.rn.f32 	%f2506, %f487, %f1964;

BB48_849:
	mul.f32 	%f1965, %f2506, 0f3F22F983;
	cvt.rni.s32.f32	%r4356, %f1965;
	cvt.rn.f32.s32	%f1966, %r4356;
	neg.f32 	%f1967, %f1966;
	fma.rn.f32 	%f1969, %f1967, %f1668, %f2506;
	fma.rn.f32 	%f1971, %f1967, %f1670, %f1969;
	fma.rn.f32 	%f2445, %f1967, %f1672, %f1971;
	abs.f32 	%f1973, %f2506;
	setp.leu.f32	%p530, %f1973, 0f47CE4780;
	@%p530 bra 	BB48_859;

	mov.b32 	 %r1360, %f2506;
	shr.u32 	%r1361, %r1360, 23;
	bfe.u32 	%r3477, %r1360, 23, 8;
	add.s32 	%r3478, %r3477, -128;
	shl.b32 	%r3479, %r1360, 8;
	or.b32  	%r1362, %r3479, -2147483648;
	shr.u32 	%r1363, %r3478, 5;
	mov.u32 	%r4348, 0;
	mov.u64 	%rd383, __cudart_i2opi_f;
	mov.u32 	%r4347, -6;
	mov.u64 	%rd451, %rd1;

BB48_851:
	.pragma "nounroll";
	ld.const.u32 	%r3482, [%rd383];
	// inline asm
	{
	mad.lo.cc.u32   %r3480, %r3482, %r1362, %r4348;
	madc.hi.u32     %r4348, %r3482, %r1362,  0;
	}
	// inline asm
	st.local.u32 	[%rd451], %r3480;
	add.s64 	%rd451, %rd451, 4;
	add.s64 	%rd383, %rd383, 4;
	add.s32 	%r4347, %r4347, 1;
	setp.ne.s32	%p531, %r4347, 0;
	@%p531 bra 	BB48_851;

	and.b32  	%r1368, %r1360, -2147483648;
	st.local.u32 	[%rd3], %r4348;
	mov.u32 	%r3485, 6;
	sub.s32 	%r3486, %r3485, %r1363;
	mul.wide.s32 	%rd322, %r3486, 4;
	add.s64 	%rd193, %rd1, %rd322;
	ld.local.u32 	%r4349, [%rd193];
	ld.local.u32 	%r4350, [%rd193+-4];
	and.b32  	%r1371, %r1361, 31;
	setp.eq.s32	%p532, %r1371, 0;
	@%p532 bra 	BB48_854;

	mov.u32 	%r3487, 32;
	sub.s32 	%r3488, %r3487, %r1371;
	shr.u32 	%r3489, %r4350, %r3488;
	shl.b32 	%r3490, %r4349, %r1371;
	add.s32 	%r4349, %r3489, %r3490;
	ld.local.u32 	%r3491, [%rd193+-8];
	shr.u32 	%r3492, %r3491, %r3488;
	shl.b32 	%r3493, %r4350, %r1371;
	add.s32 	%r4350, %r3492, %r3493;

BB48_854:
	shr.u32 	%r3494, %r4350, 30;
	shl.b32 	%r3495, %r4349, 2;
	add.s32 	%r4351, %r3494, %r3495;
	shl.b32 	%r1377, %r4350, 2;
	shr.u32 	%r3496, %r4351, 31;
	shr.u32 	%r3497, %r4349, 30;
	add.s32 	%r1378, %r3496, %r3497;
	setp.eq.s32	%p533, %r3496, 0;
	mov.u32 	%r4352, %r1368;
	mov.u32 	%r4353, %r1377;
	@%p533 bra 	BB48_856;

	not.b32 	%r3498, %r4351;
	neg.s32 	%r1379, %r1377;
	setp.eq.s32	%p534, %r1377, 0;
	selp.u32	%r3499, 1, 0, %p534;
	add.s32 	%r4351, %r3499, %r3498;
	xor.b32  	%r1381, %r1368, -2147483648;
	mov.u32 	%r4352, %r1381;
	mov.u32 	%r4353, %r1379;

BB48_856:
	mov.u32 	%r1383, %r4352;
	neg.s32 	%r3500, %r1378;
	setp.eq.s32	%p535, %r1368, 0;
	selp.b32	%r4356, %r1378, %r3500, %p535;
	clz.b32 	%r4355, %r4351;
	setp.eq.s32	%p536, %r4355, 0;
	shl.b32 	%r3501, %r4351, %r4355;
	mov.u32 	%r3502, 32;
	sub.s32 	%r3503, %r3502, %r4355;
	shr.u32 	%r3504, %r4353, %r3503;
	add.s32 	%r3505, %r3504, %r3501;
	selp.b32	%r1387, %r4351, %r3505, %p536;
	mov.u32 	%r3506, -921707870;
	mul.hi.u32 	%r4354, %r1387, %r3506;
	setp.lt.s32	%p537, %r4354, 1;
	@%p537 bra 	BB48_858;

	mul.lo.s32 	%r3507, %r1387, -921707870;
	shr.u32 	%r3508, %r3507, 31;
	shl.b32 	%r3509, %r4354, 1;
	add.s32 	%r4354, %r3508, %r3509;
	add.s32 	%r4355, %r4355, 1;

BB48_858:
	mov.u32 	%r3510, 126;
	sub.s32 	%r3511, %r3510, %r4355;
	shl.b32 	%r3512, %r3511, 23;
	add.s32 	%r3513, %r4354, 1;
	shr.u32 	%r3514, %r3513, 7;
	add.s32 	%r3515, %r3514, 1;
	shr.u32 	%r3516, %r3515, 1;
	add.s32 	%r3517, %r3516, %r3512;
	or.b32  	%r3518, %r3517, %r1383;
	mov.b32 	 %f2445, %r3518;

BB48_859:
	mul.rn.f32 	%f732, %f2445, %f2445;
	and.b32  	%r1394, %r4356, 1;
	setp.eq.s32	%p538, %r1394, 0;
	@%p538 bra 	BB48_861;

	mov.f32 	%f1974, 0fBAB6061A;
	mov.f32 	%f1975, 0f37CCF5CE;
	fma.rn.f32 	%f2446, %f1975, %f732, %f1974;
	bra.uni 	BB48_862;

BB48_861:
	mov.f32 	%f1976, 0f3C08839E;
	mov.f32 	%f1977, 0fB94CA1F9;
	fma.rn.f32 	%f2446, %f1977, %f732, %f1976;

BB48_862:
	@%p538 bra 	BB48_864;

	mov.f32 	%f1978, 0f3D2AAAA5;
	fma.rn.f32 	%f1979, %f2446, %f732, %f1978;
	mov.f32 	%f1980, 0fBF000000;
	fma.rn.f32 	%f2447, %f1979, %f732, %f1980;
	bra.uni 	BB48_865;

BB48_864:
	mov.f32 	%f1981, 0fBE2AAAA3;
	fma.rn.f32 	%f1982, %f2446, %f732, %f1981;
	mov.f32 	%f1983, 0f00000000;
	fma.rn.f32 	%f2447, %f1982, %f732, %f1983;

BB48_865:
	fma.rn.f32 	%f2448, %f2447, %f2445, %f2445;
	@%p538 bra 	BB48_867;

	mov.f32 	%f1984, 0f3F800000;
	fma.rn.f32 	%f2448, %f2447, %f732, %f1984;

BB48_867:
	and.b32  	%r3519, %r4356, 2;
	setp.eq.s32	%p541, %r3519, 0;
	@%p541 bra 	BB48_869;

	mov.f32 	%f1985, 0f00000000;
	mov.f32 	%f1986, 0fBF800000;
	fma.rn.f32 	%f2448, %f2448, %f1986, %f1985;

BB48_869:
	mul.f32 	%f744, %f2444, %f2448;
	mov.f32 	%f2525, %f494;
	@%p386 bra 	BB48_871;

	mov.f32 	%f1987, 0f00000000;
	mul.rn.f32 	%f2525, %f494, %f1987;

BB48_871:
	mul.f32 	%f1988, %f2525, 0f3F22F983;
	cvt.rni.s32.f32	%r4366, %f1988;
	cvt.rn.f32.s32	%f1989, %r4366;
	neg.f32 	%f1990, %f1989;
	fma.rn.f32 	%f1992, %f1990, %f1668, %f2525;
	fma.rn.f32 	%f1994, %f1990, %f1670, %f1992;
	fma.rn.f32 	%f2449, %f1990, %f1672, %f1994;
	abs.f32 	%f1996, %f2525;
	setp.leu.f32	%p543, %f1996, 0f47CE4780;
	@%p543 bra 	BB48_881;

	mov.b32 	 %r1396, %f2525;
	shr.u32 	%r1397, %r1396, 23;
	bfe.u32 	%r3522, %r1396, 23, 8;
	add.s32 	%r3523, %r3522, -128;
	shl.b32 	%r3524, %r1396, 8;
	or.b32  	%r1398, %r3524, -2147483648;
	shr.u32 	%r1399, %r3523, 5;
	mov.u32 	%r4358, 0;
	mov.u64 	%rd384, __cudart_i2opi_f;
	mov.u32 	%r4357, -6;
	mov.u64 	%rd450, %rd1;

BB48_873:
	.pragma "nounroll";
	ld.const.u32 	%r3527, [%rd384];
	// inline asm
	{
	mad.lo.cc.u32   %r3525, %r3527, %r1398, %r4358;
	madc.hi.u32     %r4358, %r3527, %r1398,  0;
	}
	// inline asm
	st.local.u32 	[%rd450], %r3525;
	add.s64 	%rd450, %rd450, 4;
	add.s64 	%rd384, %rd384, 4;
	add.s32 	%r4357, %r4357, 1;
	setp.ne.s32	%p544, %r4357, 0;
	@%p544 bra 	BB48_873;

	and.b32  	%r1404, %r1396, -2147483648;
	st.local.u32 	[%rd3], %r4358;
	mov.u32 	%r3530, 6;
	sub.s32 	%r3531, %r3530, %r1399;
	mul.wide.s32 	%rd324, %r3531, 4;
	add.s64 	%rd198, %rd1, %rd324;
	ld.local.u32 	%r4359, [%rd198];
	ld.local.u32 	%r4360, [%rd198+-4];
	and.b32  	%r1407, %r1397, 31;
	setp.eq.s32	%p545, %r1407, 0;
	@%p545 bra 	BB48_876;

	mov.u32 	%r3532, 32;
	sub.s32 	%r3533, %r3532, %r1407;
	shr.u32 	%r3534, %r4360, %r3533;
	shl.b32 	%r3535, %r4359, %r1407;
	add.s32 	%r4359, %r3534, %r3535;
	ld.local.u32 	%r3536, [%rd198+-8];
	shr.u32 	%r3537, %r3536, %r3533;
	shl.b32 	%r3538, %r4360, %r1407;
	add.s32 	%r4360, %r3537, %r3538;

BB48_876:
	shr.u32 	%r3539, %r4360, 30;
	shl.b32 	%r3540, %r4359, 2;
	add.s32 	%r4361, %r3539, %r3540;
	shl.b32 	%r1413, %r4360, 2;
	shr.u32 	%r3541, %r4361, 31;
	shr.u32 	%r3542, %r4359, 30;
	add.s32 	%r1414, %r3541, %r3542;
	setp.eq.s32	%p546, %r3541, 0;
	mov.u32 	%r4362, %r1404;
	mov.u32 	%r4363, %r1413;
	@%p546 bra 	BB48_878;

	not.b32 	%r3543, %r4361;
	neg.s32 	%r1415, %r1413;
	setp.eq.s32	%p547, %r1413, 0;
	selp.u32	%r3544, 1, 0, %p547;
	add.s32 	%r4361, %r3544, %r3543;
	xor.b32  	%r1417, %r1404, -2147483648;
	mov.u32 	%r4362, %r1417;
	mov.u32 	%r4363, %r1415;

BB48_878:
	mov.u32 	%r1419, %r4362;
	neg.s32 	%r3545, %r1414;
	setp.eq.s32	%p548, %r1404, 0;
	selp.b32	%r4366, %r1414, %r3545, %p548;
	clz.b32 	%r4365, %r4361;
	setp.eq.s32	%p549, %r4365, 0;
	shl.b32 	%r3546, %r4361, %r4365;
	mov.u32 	%r3547, 32;
	sub.s32 	%r3548, %r3547, %r4365;
	shr.u32 	%r3549, %r4363, %r3548;
	add.s32 	%r3550, %r3549, %r3546;
	selp.b32	%r1423, %r4361, %r3550, %p549;
	mov.u32 	%r3551, -921707870;
	mul.hi.u32 	%r4364, %r1423, %r3551;
	setp.lt.s32	%p550, %r4364, 1;
	@%p550 bra 	BB48_880;

	mul.lo.s32 	%r3552, %r1423, -921707870;
	shr.u32 	%r3553, %r3552, 31;
	shl.b32 	%r3554, %r4364, 1;
	add.s32 	%r4364, %r3553, %r3554;
	add.s32 	%r4365, %r4365, 1;

BB48_880:
	mov.u32 	%r3555, 126;
	sub.s32 	%r3556, %r3555, %r4365;
	shl.b32 	%r3557, %r3556, 23;
	add.s32 	%r3558, %r4364, 1;
	shr.u32 	%r3559, %r3558, 7;
	add.s32 	%r3560, %r3559, 1;
	shr.u32 	%r3561, %r3560, 1;
	add.s32 	%r3562, %r3561, %r3557;
	or.b32  	%r3563, %r3562, %r1419;
	mov.b32 	 %f2449, %r3563;

BB48_881:
	mul.rn.f32 	%f750, %f2449, %f2449;
	add.s32 	%r1430, %r4366, 1;
	and.b32  	%r1431, %r1430, 1;
	setp.eq.s32	%p551, %r1431, 0;
	@%p551 bra 	BB48_883;

	mov.f32 	%f1997, 0fBAB6061A;
	mov.f32 	%f1998, 0f37CCF5CE;
	fma.rn.f32 	%f2450, %f1998, %f750, %f1997;
	bra.uni 	BB48_884;

BB48_883:
	mov.f32 	%f1999, 0f3C08839E;
	mov.f32 	%f2000, 0fB94CA1F9;
	fma.rn.f32 	%f2450, %f2000, %f750, %f1999;

BB48_884:
	@%p551 bra 	BB48_886;

	mov.f32 	%f2001, 0f3D2AAAA5;
	fma.rn.f32 	%f2002, %f2450, %f750, %f2001;
	mov.f32 	%f2003, 0fBF000000;
	fma.rn.f32 	%f2451, %f2002, %f750, %f2003;
	bra.uni 	BB48_887;

BB48_886:
	mov.f32 	%f2004, 0fBE2AAAA3;
	fma.rn.f32 	%f2005, %f2450, %f750, %f2004;
	mov.f32 	%f2006, 0f00000000;
	fma.rn.f32 	%f2451, %f2005, %f750, %f2006;

BB48_887:
	fma.rn.f32 	%f2452, %f2451, %f2449, %f2449;
	@%p551 bra 	BB48_889;

	mov.f32 	%f2007, 0f3F800000;
	fma.rn.f32 	%f2452, %f2451, %f750, %f2007;

BB48_889:
	and.b32  	%r3564, %r1430, 2;
	setp.eq.s32	%p554, %r3564, 0;
	@%p554 bra 	BB48_891;

	mov.f32 	%f2008, 0f00000000;
	mov.f32 	%f2009, 0fBF800000;
	fma.rn.f32 	%f2452, %f2452, %f2009, %f2008;

BB48_891:
	mul.f32 	%f762, %f744, %f2452;
	mov.f32 	%f2486, %f480;
	@%p360 bra 	BB48_893;

	mov.f32 	%f2010, 0f00000000;
	mul.rn.f32 	%f2486, %f480, %f2010;

BB48_893:
	mul.f32 	%f2011, %f2486, 0f3F22F983;
	cvt.rni.s32.f32	%r4376, %f2011;
	cvt.rn.f32.s32	%f2012, %r4376;
	neg.f32 	%f2013, %f2012;
	fma.rn.f32 	%f2015, %f2013, %f1668, %f2486;
	fma.rn.f32 	%f2017, %f2013, %f1670, %f2015;
	fma.rn.f32 	%f2453, %f2013, %f1672, %f2017;
	abs.f32 	%f2019, %f2486;
	setp.leu.f32	%p556, %f2019, 0f47CE4780;
	@%p556 bra 	BB48_903;

	mov.b32 	 %r1433, %f2486;
	shr.u32 	%r1434, %r1433, 23;
	bfe.u32 	%r3567, %r1433, 23, 8;
	add.s32 	%r3568, %r3567, -128;
	shl.b32 	%r3569, %r1433, 8;
	or.b32  	%r1435, %r3569, -2147483648;
	shr.u32 	%r1436, %r3568, 5;
	mov.u32 	%r4368, 0;
	mov.u64 	%rd385, __cudart_i2opi_f;
	mov.u32 	%r4367, -6;
	mov.u64 	%rd449, %rd1;

BB48_895:
	.pragma "nounroll";
	ld.const.u32 	%r3572, [%rd385];
	// inline asm
	{
	mad.lo.cc.u32   %r3570, %r3572, %r1435, %r4368;
	madc.hi.u32     %r4368, %r3572, %r1435,  0;
	}
	// inline asm
	st.local.u32 	[%rd449], %r3570;
	add.s64 	%rd449, %rd449, 4;
	add.s64 	%rd385, %rd385, 4;
	add.s32 	%r4367, %r4367, 1;
	setp.ne.s32	%p557, %r4367, 0;
	@%p557 bra 	BB48_895;

	and.b32  	%r1441, %r1433, -2147483648;
	st.local.u32 	[%rd3], %r4368;
	mov.u32 	%r3575, 6;
	sub.s32 	%r3576, %r3575, %r1436;
	mul.wide.s32 	%rd326, %r3576, 4;
	add.s64 	%rd203, %rd1, %rd326;
	ld.local.u32 	%r4369, [%rd203];
	ld.local.u32 	%r4370, [%rd203+-4];
	and.b32  	%r1444, %r1434, 31;
	setp.eq.s32	%p558, %r1444, 0;
	@%p558 bra 	BB48_898;

	mov.u32 	%r3577, 32;
	sub.s32 	%r3578, %r3577, %r1444;
	shr.u32 	%r3579, %r4370, %r3578;
	shl.b32 	%r3580, %r4369, %r1444;
	add.s32 	%r4369, %r3579, %r3580;
	ld.local.u32 	%r3581, [%rd203+-8];
	shr.u32 	%r3582, %r3581, %r3578;
	shl.b32 	%r3583, %r4370, %r1444;
	add.s32 	%r4370, %r3582, %r3583;

BB48_898:
	shr.u32 	%r3584, %r4370, 30;
	shl.b32 	%r3585, %r4369, 2;
	add.s32 	%r4371, %r3584, %r3585;
	shl.b32 	%r1450, %r4370, 2;
	shr.u32 	%r3586, %r4371, 31;
	shr.u32 	%r3587, %r4369, 30;
	add.s32 	%r1451, %r3586, %r3587;
	setp.eq.s32	%p559, %r3586, 0;
	mov.u32 	%r4372, %r1441;
	mov.u32 	%r4373, %r1450;
	@%p559 bra 	BB48_900;

	not.b32 	%r3588, %r4371;
	neg.s32 	%r1452, %r1450;
	setp.eq.s32	%p560, %r1450, 0;
	selp.u32	%r3589, 1, 0, %p560;
	add.s32 	%r4371, %r3589, %r3588;
	xor.b32  	%r1454, %r1441, -2147483648;
	mov.u32 	%r4372, %r1454;
	mov.u32 	%r4373, %r1452;

BB48_900:
	mov.u32 	%r1456, %r4372;
	neg.s32 	%r3590, %r1451;
	setp.eq.s32	%p561, %r1441, 0;
	selp.b32	%r4376, %r1451, %r3590, %p561;
	clz.b32 	%r4375, %r4371;
	setp.eq.s32	%p562, %r4375, 0;
	shl.b32 	%r3591, %r4371, %r4375;
	mov.u32 	%r3592, 32;
	sub.s32 	%r3593, %r3592, %r4375;
	shr.u32 	%r3594, %r4373, %r3593;
	add.s32 	%r3595, %r3594, %r3591;
	selp.b32	%r1460, %r4371, %r3595, %p562;
	mov.u32 	%r3596, -921707870;
	mul.hi.u32 	%r4374, %r1460, %r3596;
	setp.lt.s32	%p563, %r4374, 1;
	@%p563 bra 	BB48_902;

	mul.lo.s32 	%r3597, %r1460, -921707870;
	shr.u32 	%r3598, %r3597, 31;
	shl.b32 	%r3599, %r4374, 1;
	add.s32 	%r4374, %r3598, %r3599;
	add.s32 	%r4375, %r4375, 1;

BB48_902:
	mov.u32 	%r3600, 126;
	sub.s32 	%r3601, %r3600, %r4375;
	shl.b32 	%r3602, %r3601, 23;
	add.s32 	%r3603, %r4374, 1;
	shr.u32 	%r3604, %r3603, 7;
	add.s32 	%r3605, %r3604, 1;
	shr.u32 	%r3606, %r3605, 1;
	add.s32 	%r3607, %r3606, %r3602;
	or.b32  	%r3608, %r3607, %r1456;
	mov.b32 	 %f2453, %r3608;

BB48_903:
	mul.rn.f32 	%f768, %f2453, %f2453;
	and.b32  	%r1467, %r4376, 1;
	setp.eq.s32	%p564, %r1467, 0;
	@%p564 bra 	BB48_905;

	mov.f32 	%f2020, 0fBAB6061A;
	mov.f32 	%f2021, 0f37CCF5CE;
	fma.rn.f32 	%f2454, %f2021, %f768, %f2020;
	bra.uni 	BB48_906;

BB48_905:
	mov.f32 	%f2022, 0f3C08839E;
	mov.f32 	%f2023, 0fB94CA1F9;
	fma.rn.f32 	%f2454, %f2023, %f768, %f2022;

BB48_906:
	@%p564 bra 	BB48_908;

	mov.f32 	%f2024, 0f3D2AAAA5;
	fma.rn.f32 	%f2025, %f2454, %f768, %f2024;
	mov.f32 	%f2026, 0fBF000000;
	fma.rn.f32 	%f2455, %f2025, %f768, %f2026;
	bra.uni 	BB48_909;

BB48_908:
	mov.f32 	%f2027, 0fBE2AAAA3;
	fma.rn.f32 	%f2028, %f2454, %f768, %f2027;
	mov.f32 	%f2029, 0f00000000;
	fma.rn.f32 	%f2455, %f2028, %f768, %f2029;

BB48_909:
	fma.rn.f32 	%f2456, %f2455, %f2453, %f2453;
	@%p564 bra 	BB48_911;

	mov.f32 	%f2030, 0f3F800000;
	fma.rn.f32 	%f2456, %f2455, %f768, %f2030;

BB48_911:
	and.b32  	%r3609, %r4376, 2;
	setp.eq.s32	%p567, %r3609, 0;
	@%p567 bra 	BB48_913;

	mov.f32 	%f2031, 0f00000000;
	mov.f32 	%f2032, 0fBF800000;
	fma.rn.f32 	%f2456, %f2456, %f2032, %f2031;

BB48_913:
	mov.f32 	%f2505, %f487;
	@%p373 bra 	BB48_915;

	mov.f32 	%f2033, 0f00000000;
	mul.rn.f32 	%f2505, %f487, %f2033;

BB48_915:
	mul.f32 	%f2034, %f2505, 0f3F22F983;
	cvt.rni.s32.f32	%r4386, %f2034;
	cvt.rn.f32.s32	%f2035, %r4386;
	neg.f32 	%f2036, %f2035;
	fma.rn.f32 	%f2038, %f2036, %f1668, %f2505;
	fma.rn.f32 	%f2040, %f2036, %f1670, %f2038;
	fma.rn.f32 	%f2457, %f2036, %f1672, %f2040;
	abs.f32 	%f2042, %f2505;
	setp.leu.f32	%p569, %f2042, 0f47CE4780;
	@%p569 bra 	BB48_925;

	mov.b32 	 %r1469, %f2505;
	shr.u32 	%r1470, %r1469, 23;
	bfe.u32 	%r3612, %r1469, 23, 8;
	add.s32 	%r3613, %r3612, -128;
	shl.b32 	%r3614, %r1469, 8;
	or.b32  	%r1471, %r3614, -2147483648;
	shr.u32 	%r1472, %r3613, 5;
	mov.u32 	%r4378, 0;
	mov.u64 	%rd386, __cudart_i2opi_f;
	mov.u32 	%r4377, -6;
	mov.u64 	%rd448, %rd1;

BB48_917:
	.pragma "nounroll";
	ld.const.u32 	%r3617, [%rd386];
	// inline asm
	{
	mad.lo.cc.u32   %r3615, %r3617, %r1471, %r4378;
	madc.hi.u32     %r4378, %r3617, %r1471,  0;
	}
	// inline asm
	st.local.u32 	[%rd448], %r3615;
	add.s64 	%rd448, %rd448, 4;
	add.s64 	%rd386, %rd386, 4;
	add.s32 	%r4377, %r4377, 1;
	setp.ne.s32	%p570, %r4377, 0;
	@%p570 bra 	BB48_917;

	and.b32  	%r1477, %r1469, -2147483648;
	st.local.u32 	[%rd3], %r4378;
	mov.u32 	%r3620, 6;
	sub.s32 	%r3621, %r3620, %r1472;
	mul.wide.s32 	%rd328, %r3621, 4;
	add.s64 	%rd208, %rd1, %rd328;
	ld.local.u32 	%r4379, [%rd208];
	ld.local.u32 	%r4380, [%rd208+-4];
	and.b32  	%r1480, %r1470, 31;
	setp.eq.s32	%p571, %r1480, 0;
	@%p571 bra 	BB48_920;

	mov.u32 	%r3622, 32;
	sub.s32 	%r3623, %r3622, %r1480;
	shr.u32 	%r3624, %r4380, %r3623;
	shl.b32 	%r3625, %r4379, %r1480;
	add.s32 	%r4379, %r3624, %r3625;
	ld.local.u32 	%r3626, [%rd208+-8];
	shr.u32 	%r3627, %r3626, %r3623;
	shl.b32 	%r3628, %r4380, %r1480;
	add.s32 	%r4380, %r3627, %r3628;

BB48_920:
	shr.u32 	%r3629, %r4380, 30;
	shl.b32 	%r3630, %r4379, 2;
	add.s32 	%r4381, %r3629, %r3630;
	shl.b32 	%r1486, %r4380, 2;
	shr.u32 	%r3631, %r4381, 31;
	shr.u32 	%r3632, %r4379, 30;
	add.s32 	%r1487, %r3631, %r3632;
	setp.eq.s32	%p572, %r3631, 0;
	mov.u32 	%r4382, %r1477;
	mov.u32 	%r4383, %r1486;
	@%p572 bra 	BB48_922;

	not.b32 	%r3633, %r4381;
	neg.s32 	%r1488, %r1486;
	setp.eq.s32	%p573, %r1486, 0;
	selp.u32	%r3634, 1, 0, %p573;
	add.s32 	%r4381, %r3634, %r3633;
	xor.b32  	%r1490, %r1477, -2147483648;
	mov.u32 	%r4382, %r1490;
	mov.u32 	%r4383, %r1488;

BB48_922:
	mov.u32 	%r1492, %r4382;
	neg.s32 	%r3635, %r1487;
	setp.eq.s32	%p574, %r1477, 0;
	selp.b32	%r4386, %r1487, %r3635, %p574;
	clz.b32 	%r4385, %r4381;
	setp.eq.s32	%p575, %r4385, 0;
	shl.b32 	%r3636, %r4381, %r4385;
	mov.u32 	%r3637, 32;
	sub.s32 	%r3638, %r3637, %r4385;
	shr.u32 	%r3639, %r4383, %r3638;
	add.s32 	%r3640, %r3639, %r3636;
	selp.b32	%r1496, %r4381, %r3640, %p575;
	mov.u32 	%r3641, -921707870;
	mul.hi.u32 	%r4384, %r1496, %r3641;
	setp.lt.s32	%p576, %r4384, 1;
	@%p576 bra 	BB48_924;

	mul.lo.s32 	%r3642, %r1496, -921707870;
	shr.u32 	%r3643, %r3642, 31;
	shl.b32 	%r3644, %r4384, 1;
	add.s32 	%r4384, %r3643, %r3644;
	add.s32 	%r4385, %r4385, 1;

BB48_924:
	mov.u32 	%r3645, 126;
	sub.s32 	%r3646, %r3645, %r4385;
	shl.b32 	%r3647, %r3646, 23;
	add.s32 	%r3648, %r4384, 1;
	shr.u32 	%r3649, %r3648, 7;
	add.s32 	%r3650, %r3649, 1;
	shr.u32 	%r3651, %r3650, 1;
	add.s32 	%r3652, %r3651, %r3647;
	or.b32  	%r3653, %r3652, %r1492;
	mov.b32 	 %f2457, %r3653;

BB48_925:
	mul.rn.f32 	%f785, %f2457, %f2457;
	add.s32 	%r1503, %r4386, 1;
	and.b32  	%r1504, %r1503, 1;
	setp.eq.s32	%p577, %r1504, 0;
	@%p577 bra 	BB48_927;

	mov.f32 	%f2043, 0fBAB6061A;
	mov.f32 	%f2044, 0f37CCF5CE;
	fma.rn.f32 	%f2458, %f2044, %f785, %f2043;
	bra.uni 	BB48_928;

BB48_927:
	mov.f32 	%f2045, 0f3C08839E;
	mov.f32 	%f2046, 0fB94CA1F9;
	fma.rn.f32 	%f2458, %f2046, %f785, %f2045;

BB48_928:
	@%p577 bra 	BB48_930;

	mov.f32 	%f2047, 0f3D2AAAA5;
	fma.rn.f32 	%f2048, %f2458, %f785, %f2047;
	mov.f32 	%f2049, 0fBF000000;
	fma.rn.f32 	%f2459, %f2048, %f785, %f2049;
	bra.uni 	BB48_931;

BB48_930:
	mov.f32 	%f2050, 0fBE2AAAA3;
	fma.rn.f32 	%f2051, %f2458, %f785, %f2050;
	mov.f32 	%f2052, 0f00000000;
	fma.rn.f32 	%f2459, %f2051, %f785, %f2052;

BB48_931:
	fma.rn.f32 	%f2460, %f2459, %f2457, %f2457;
	@%p577 bra 	BB48_933;

	mov.f32 	%f2053, 0f3F800000;
	fma.rn.f32 	%f2460, %f2459, %f785, %f2053;

BB48_933:
	and.b32  	%r3654, %r1503, 2;
	setp.eq.s32	%p580, %r3654, 0;
	@%p580 bra 	BB48_935;

	mov.f32 	%f2054, 0f00000000;
	mov.f32 	%f2055, 0fBF800000;
	fma.rn.f32 	%f2460, %f2460, %f2055, %f2054;

BB48_935:
	mul.f32 	%f797, %f2456, %f2460;
	mov.f32 	%f2524, %f494;
	@%p386 bra 	BB48_937;

	mov.f32 	%f2056, 0f00000000;
	mul.rn.f32 	%f2524, %f494, %f2056;

BB48_937:
	mul.f32 	%f2057, %f2524, 0f3F22F983;
	cvt.rni.s32.f32	%r4396, %f2057;
	cvt.rn.f32.s32	%f2058, %r4396;
	neg.f32 	%f2059, %f2058;
	fma.rn.f32 	%f2061, %f2059, %f1668, %f2524;
	fma.rn.f32 	%f2063, %f2059, %f1670, %f2061;
	fma.rn.f32 	%f2461, %f2059, %f1672, %f2063;
	abs.f32 	%f2065, %f2524;
	setp.leu.f32	%p582, %f2065, 0f47CE4780;
	@%p582 bra 	BB48_947;

	mov.b32 	 %r1506, %f2524;
	shr.u32 	%r1507, %r1506, 23;
	bfe.u32 	%r3657, %r1506, 23, 8;
	add.s32 	%r3658, %r3657, -128;
	shl.b32 	%r3659, %r1506, 8;
	or.b32  	%r1508, %r3659, -2147483648;
	shr.u32 	%r1509, %r3658, 5;
	mov.u32 	%r4388, 0;
	mov.u64 	%rd387, __cudart_i2opi_f;
	mov.u32 	%r4387, -6;
	mov.u64 	%rd447, %rd1;

BB48_939:
	.pragma "nounroll";
	ld.const.u32 	%r3662, [%rd387];
	// inline asm
	{
	mad.lo.cc.u32   %r3660, %r3662, %r1508, %r4388;
	madc.hi.u32     %r4388, %r3662, %r1508,  0;
	}
	// inline asm
	st.local.u32 	[%rd447], %r3660;
	add.s64 	%rd447, %rd447, 4;
	add.s64 	%rd387, %rd387, 4;
	add.s32 	%r4387, %r4387, 1;
	setp.ne.s32	%p583, %r4387, 0;
	@%p583 bra 	BB48_939;

	and.b32  	%r1514, %r1506, -2147483648;
	st.local.u32 	[%rd3], %r4388;
	mov.u32 	%r3665, 6;
	sub.s32 	%r3666, %r3665, %r1509;
	mul.wide.s32 	%rd330, %r3666, 4;
	add.s64 	%rd213, %rd1, %rd330;
	ld.local.u32 	%r4389, [%rd213];
	ld.local.u32 	%r4390, [%rd213+-4];
	and.b32  	%r1517, %r1507, 31;
	setp.eq.s32	%p584, %r1517, 0;
	@%p584 bra 	BB48_942;

	mov.u32 	%r3667, 32;
	sub.s32 	%r3668, %r3667, %r1517;
	shr.u32 	%r3669, %r4390, %r3668;
	shl.b32 	%r3670, %r4389, %r1517;
	add.s32 	%r4389, %r3669, %r3670;
	ld.local.u32 	%r3671, [%rd213+-8];
	shr.u32 	%r3672, %r3671, %r3668;
	shl.b32 	%r3673, %r4390, %r1517;
	add.s32 	%r4390, %r3672, %r3673;

BB48_942:
	shr.u32 	%r3674, %r4390, 30;
	shl.b32 	%r3675, %r4389, 2;
	add.s32 	%r4391, %r3674, %r3675;
	shl.b32 	%r1523, %r4390, 2;
	shr.u32 	%r3676, %r4391, 31;
	shr.u32 	%r3677, %r4389, 30;
	add.s32 	%r1524, %r3676, %r3677;
	setp.eq.s32	%p585, %r3676, 0;
	mov.u32 	%r4392, %r1514;
	mov.u32 	%r4393, %r1523;
	@%p585 bra 	BB48_944;

	not.b32 	%r3678, %r4391;
	neg.s32 	%r1525, %r1523;
	setp.eq.s32	%p586, %r1523, 0;
	selp.u32	%r3679, 1, 0, %p586;
	add.s32 	%r4391, %r3679, %r3678;
	xor.b32  	%r1527, %r1514, -2147483648;
	mov.u32 	%r4392, %r1527;
	mov.u32 	%r4393, %r1525;

BB48_944:
	mov.u32 	%r1529, %r4392;
	neg.s32 	%r3680, %r1524;
	setp.eq.s32	%p587, %r1514, 0;
	selp.b32	%r4396, %r1524, %r3680, %p587;
	clz.b32 	%r4395, %r4391;
	setp.eq.s32	%p588, %r4395, 0;
	shl.b32 	%r3681, %r4391, %r4395;
	mov.u32 	%r3682, 32;
	sub.s32 	%r3683, %r3682, %r4395;
	shr.u32 	%r3684, %r4393, %r3683;
	add.s32 	%r3685, %r3684, %r3681;
	selp.b32	%r1533, %r4391, %r3685, %p588;
	mov.u32 	%r3686, -921707870;
	mul.hi.u32 	%r4394, %r1533, %r3686;
	setp.lt.s32	%p589, %r4394, 1;
	@%p589 bra 	BB48_946;

	mul.lo.s32 	%r3687, %r1533, -921707870;
	shr.u32 	%r3688, %r3687, 31;
	shl.b32 	%r3689, %r4394, 1;
	add.s32 	%r4394, %r3688, %r3689;
	add.s32 	%r4395, %r4395, 1;

BB48_946:
	mov.u32 	%r3690, 126;
	sub.s32 	%r3691, %r3690, %r4395;
	shl.b32 	%r3692, %r3691, 23;
	add.s32 	%r3693, %r4394, 1;
	shr.u32 	%r3694, %r3693, 7;
	add.s32 	%r3695, %r3694, 1;
	shr.u32 	%r3696, %r3695, 1;
	add.s32 	%r3697, %r3696, %r3692;
	or.b32  	%r3698, %r3697, %r1529;
	mov.b32 	 %f2461, %r3698;

BB48_947:
	mul.rn.f32 	%f803, %f2461, %f2461;
	and.b32  	%r1540, %r4396, 1;
	setp.eq.s32	%p590, %r1540, 0;
	@%p590 bra 	BB48_949;

	mov.f32 	%f2066, 0fBAB6061A;
	mov.f32 	%f2067, 0f37CCF5CE;
	fma.rn.f32 	%f2462, %f2067, %f803, %f2066;
	bra.uni 	BB48_950;

BB48_949:
	mov.f32 	%f2068, 0f3C08839E;
	mov.f32 	%f2069, 0fB94CA1F9;
	fma.rn.f32 	%f2462, %f2069, %f803, %f2068;

BB48_950:
	@%p590 bra 	BB48_952;

	mov.f32 	%f2070, 0f3D2AAAA5;
	fma.rn.f32 	%f2071, %f2462, %f803, %f2070;
	mov.f32 	%f2072, 0fBF000000;
	fma.rn.f32 	%f2463, %f2071, %f803, %f2072;
	bra.uni 	BB48_953;

BB48_952:
	mov.f32 	%f2073, 0fBE2AAAA3;
	fma.rn.f32 	%f2074, %f2462, %f803, %f2073;
	mov.f32 	%f2075, 0f00000000;
	fma.rn.f32 	%f2463, %f2074, %f803, %f2075;

BB48_953:
	fma.rn.f32 	%f2464, %f2463, %f2461, %f2461;
	@%p590 bra 	BB48_955;

	mov.f32 	%f2076, 0f3F800000;
	fma.rn.f32 	%f2464, %f2463, %f803, %f2076;

BB48_955:
	and.b32  	%r3699, %r4396, 2;
	setp.eq.s32	%p593, %r3699, 0;
	@%p593 bra 	BB48_957;

	mov.f32 	%f2077, 0f00000000;
	mov.f32 	%f2078, 0fBF800000;
	fma.rn.f32 	%f2464, %f2464, %f2078, %f2077;

BB48_957:
	mul.f32 	%f2079, %f797, %f2464;
	sub.f32 	%f815, %f762, %f2079;
	mov.f32 	%f2485, %f480;
	@%p360 bra 	BB48_959;

	mov.f32 	%f2080, 0f00000000;
	mul.rn.f32 	%f2485, %f480, %f2080;

BB48_959:
	mul.f32 	%f2081, %f2485, 0f3F22F983;
	cvt.rni.s32.f32	%r4406, %f2081;
	cvt.rn.f32.s32	%f2082, %r4406;
	neg.f32 	%f2083, %f2082;
	fma.rn.f32 	%f2085, %f2083, %f1668, %f2485;
	fma.rn.f32 	%f2087, %f2083, %f1670, %f2085;
	fma.rn.f32 	%f2465, %f2083, %f1672, %f2087;
	abs.f32 	%f2089, %f2485;
	setp.leu.f32	%p595, %f2089, 0f47CE4780;
	@%p595 bra 	BB48_969;

	mov.b32 	 %r1542, %f2485;
	shr.u32 	%r1543, %r1542, 23;
	bfe.u32 	%r3702, %r1542, 23, 8;
	add.s32 	%r3703, %r3702, -128;
	shl.b32 	%r3704, %r1542, 8;
	or.b32  	%r1544, %r3704, -2147483648;
	shr.u32 	%r1545, %r3703, 5;
	mov.u32 	%r4398, 0;
	mov.u64 	%rd388, __cudart_i2opi_f;
	mov.u32 	%r4397, -6;
	mov.u64 	%rd446, %rd1;

BB48_961:
	.pragma "nounroll";
	ld.const.u32 	%r3707, [%rd388];
	// inline asm
	{
	mad.lo.cc.u32   %r3705, %r3707, %r1544, %r4398;
	madc.hi.u32     %r4398, %r3707, %r1544,  0;
	}
	// inline asm
	st.local.u32 	[%rd446], %r3705;
	add.s64 	%rd446, %rd446, 4;
	add.s64 	%rd388, %rd388, 4;
	add.s32 	%r4397, %r4397, 1;
	setp.ne.s32	%p596, %r4397, 0;
	@%p596 bra 	BB48_961;

	and.b32  	%r1550, %r1542, -2147483648;
	st.local.u32 	[%rd3], %r4398;
	mov.u32 	%r3710, 6;
	sub.s32 	%r3711, %r3710, %r1545;
	mul.wide.s32 	%rd332, %r3711, 4;
	add.s64 	%rd218, %rd1, %rd332;
	ld.local.u32 	%r4399, [%rd218];
	ld.local.u32 	%r4400, [%rd218+-4];
	and.b32  	%r1553, %r1543, 31;
	setp.eq.s32	%p597, %r1553, 0;
	@%p597 bra 	BB48_964;

	mov.u32 	%r3712, 32;
	sub.s32 	%r3713, %r3712, %r1553;
	shr.u32 	%r3714, %r4400, %r3713;
	shl.b32 	%r3715, %r4399, %r1553;
	add.s32 	%r4399, %r3714, %r3715;
	ld.local.u32 	%r3716, [%rd218+-8];
	shr.u32 	%r3717, %r3716, %r3713;
	shl.b32 	%r3718, %r4400, %r1553;
	add.s32 	%r4400, %r3717, %r3718;

BB48_964:
	shr.u32 	%r3719, %r4400, 30;
	shl.b32 	%r3720, %r4399, 2;
	add.s32 	%r4401, %r3719, %r3720;
	shl.b32 	%r1559, %r4400, 2;
	shr.u32 	%r3721, %r4401, 31;
	shr.u32 	%r3722, %r4399, 30;
	add.s32 	%r1560, %r3721, %r3722;
	setp.eq.s32	%p598, %r3721, 0;
	mov.u32 	%r4402, %r1550;
	mov.u32 	%r4403, %r1559;
	@%p598 bra 	BB48_966;

	not.b32 	%r3723, %r4401;
	neg.s32 	%r1561, %r1559;
	setp.eq.s32	%p599, %r1559, 0;
	selp.u32	%r3724, 1, 0, %p599;
	add.s32 	%r4401, %r3724, %r3723;
	xor.b32  	%r1563, %r1550, -2147483648;
	mov.u32 	%r4402, %r1563;
	mov.u32 	%r4403, %r1561;

BB48_966:
	mov.u32 	%r1565, %r4402;
	neg.s32 	%r3725, %r1560;
	setp.eq.s32	%p600, %r1550, 0;
	selp.b32	%r4406, %r1560, %r3725, %p600;
	clz.b32 	%r4405, %r4401;
	setp.eq.s32	%p601, %r4405, 0;
	shl.b32 	%r3726, %r4401, %r4405;
	mov.u32 	%r3727, 32;
	sub.s32 	%r3728, %r3727, %r4405;
	shr.u32 	%r3729, %r4403, %r3728;
	add.s32 	%r3730, %r3729, %r3726;
	selp.b32	%r1569, %r4401, %r3730, %p601;
	mov.u32 	%r3731, -921707870;
	mul.hi.u32 	%r4404, %r1569, %r3731;
	setp.lt.s32	%p602, %r4404, 1;
	@%p602 bra 	BB48_968;

	mul.lo.s32 	%r3732, %r1569, -921707870;
	shr.u32 	%r3733, %r3732, 31;
	shl.b32 	%r3734, %r4404, 1;
	add.s32 	%r4404, %r3733, %r3734;
	add.s32 	%r4405, %r4405, 1;

BB48_968:
	mov.u32 	%r3735, 126;
	sub.s32 	%r3736, %r3735, %r4405;
	shl.b32 	%r3737, %r3736, 23;
	add.s32 	%r3738, %r4404, 1;
	shr.u32 	%r3739, %r3738, 7;
	add.s32 	%r3740, %r3739, 1;
	shr.u32 	%r3741, %r3740, 1;
	add.s32 	%r3742, %r3741, %r3737;
	or.b32  	%r3743, %r3742, %r1565;
	mov.b32 	 %f2465, %r3743;

BB48_969:
	mul.rn.f32 	%f821, %f2465, %f2465;
	add.s32 	%r1576, %r4406, 1;
	and.b32  	%r1577, %r1576, 1;
	setp.eq.s32	%p603, %r1577, 0;
	@%p603 bra 	BB48_971;

	mov.f32 	%f2090, 0fBAB6061A;
	mov.f32 	%f2091, 0f37CCF5CE;
	fma.rn.f32 	%f2466, %f2091, %f821, %f2090;
	bra.uni 	BB48_972;

BB48_971:
	mov.f32 	%f2092, 0f3C08839E;
	mov.f32 	%f2093, 0fB94CA1F9;
	fma.rn.f32 	%f2466, %f2093, %f821, %f2092;

BB48_972:
	@%p603 bra 	BB48_974;

	mov.f32 	%f2094, 0f3D2AAAA5;
	fma.rn.f32 	%f2095, %f2466, %f821, %f2094;
	mov.f32 	%f2096, 0fBF000000;
	fma.rn.f32 	%f2467, %f2095, %f821, %f2096;
	bra.uni 	BB48_975;

BB48_974:
	mov.f32 	%f2097, 0fBE2AAAA3;
	fma.rn.f32 	%f2098, %f2466, %f821, %f2097;
	mov.f32 	%f2099, 0f00000000;
	fma.rn.f32 	%f2467, %f2098, %f821, %f2099;

BB48_975:
	fma.rn.f32 	%f2468, %f2467, %f2465, %f2465;
	@%p603 bra 	BB48_977;

	mov.f32 	%f2100, 0f3F800000;
	fma.rn.f32 	%f2468, %f2467, %f821, %f2100;

BB48_977:
	and.b32  	%r3744, %r1576, 2;
	setp.eq.s32	%p606, %r3744, 0;
	@%p606 bra 	BB48_979;

	mov.f32 	%f2101, 0f00000000;
	mov.f32 	%f2102, 0fBF800000;
	fma.rn.f32 	%f2468, %f2468, %f2102, %f2101;

BB48_979:
	mov.f32 	%f2504, %f487;
	@%p373 bra 	BB48_981;

	mov.f32 	%f2103, 0f00000000;
	mul.rn.f32 	%f2504, %f487, %f2103;

BB48_981:
	mul.f32 	%f2104, %f2504, 0f3F22F983;
	cvt.rni.s32.f32	%r4416, %f2104;
	cvt.rn.f32.s32	%f2105, %r4416;
	neg.f32 	%f2106, %f2105;
	fma.rn.f32 	%f2108, %f2106, %f1668, %f2504;
	fma.rn.f32 	%f2110, %f2106, %f1670, %f2108;
	fma.rn.f32 	%f2469, %f2106, %f1672, %f2110;
	abs.f32 	%f2112, %f2504;
	setp.leu.f32	%p608, %f2112, 0f47CE4780;
	@%p608 bra 	BB48_991;

	mov.b32 	 %r1579, %f2504;
	shr.u32 	%r1580, %r1579, 23;
	bfe.u32 	%r3747, %r1579, 23, 8;
	add.s32 	%r3748, %r3747, -128;
	shl.b32 	%r3749, %r1579, 8;
	or.b32  	%r1581, %r3749, -2147483648;
	shr.u32 	%r1582, %r3748, 5;
	mov.u32 	%r4408, 0;
	mov.u64 	%rd389, __cudart_i2opi_f;
	mov.u32 	%r4407, -6;
	mov.u64 	%rd445, %rd1;

BB48_983:
	.pragma "nounroll";
	ld.const.u32 	%r3752, [%rd389];
	// inline asm
	{
	mad.lo.cc.u32   %r3750, %r3752, %r1581, %r4408;
	madc.hi.u32     %r4408, %r3752, %r1581,  0;
	}
	// inline asm
	st.local.u32 	[%rd445], %r3750;
	add.s64 	%rd445, %rd445, 4;
	add.s64 	%rd389, %rd389, 4;
	add.s32 	%r4407, %r4407, 1;
	setp.ne.s32	%p609, %r4407, 0;
	@%p609 bra 	BB48_983;

	and.b32  	%r1587, %r1579, -2147483648;
	st.local.u32 	[%rd3], %r4408;
	mov.u32 	%r3755, 6;
	sub.s32 	%r3756, %r3755, %r1582;
	mul.wide.s32 	%rd334, %r3756, 4;
	add.s64 	%rd223, %rd1, %rd334;
	ld.local.u32 	%r4409, [%rd223];
	ld.local.u32 	%r4410, [%rd223+-4];
	and.b32  	%r1590, %r1580, 31;
	setp.eq.s32	%p610, %r1590, 0;
	@%p610 bra 	BB48_986;

	mov.u32 	%r3757, 32;
	sub.s32 	%r3758, %r3757, %r1590;
	shr.u32 	%r3759, %r4410, %r3758;
	shl.b32 	%r3760, %r4409, %r1590;
	add.s32 	%r4409, %r3759, %r3760;
	ld.local.u32 	%r3761, [%rd223+-8];
	shr.u32 	%r3762, %r3761, %r3758;
	shl.b32 	%r3763, %r4410, %r1590;
	add.s32 	%r4410, %r3762, %r3763;

BB48_986:
	shr.u32 	%r3764, %r4410, 30;
	shl.b32 	%r3765, %r4409, 2;
	add.s32 	%r4411, %r3764, %r3765;
	shl.b32 	%r1596, %r4410, 2;
	shr.u32 	%r3766, %r4411, 31;
	shr.u32 	%r3767, %r4409, 30;
	add.s32 	%r1597, %r3766, %r3767;
	setp.eq.s32	%p611, %r3766, 0;
	mov.u32 	%r4412, %r1587;
	mov.u32 	%r4413, %r1596;
	@%p611 bra 	BB48_988;

	not.b32 	%r3768, %r4411;
	neg.s32 	%r1598, %r1596;
	setp.eq.s32	%p612, %r1596, 0;
	selp.u32	%r3769, 1, 0, %p612;
	add.s32 	%r4411, %r3769, %r3768;
	xor.b32  	%r1600, %r1587, -2147483648;
	mov.u32 	%r4412, %r1600;
	mov.u32 	%r4413, %r1598;

BB48_988:
	mov.u32 	%r1602, %r4412;
	neg.s32 	%r3770, %r1597;
	setp.eq.s32	%p613, %r1587, 0;
	selp.b32	%r4416, %r1597, %r3770, %p613;
	clz.b32 	%r4415, %r4411;
	setp.eq.s32	%p614, %r4415, 0;
	shl.b32 	%r3771, %r4411, %r4415;
	mov.u32 	%r3772, 32;
	sub.s32 	%r3773, %r3772, %r4415;
	shr.u32 	%r3774, %r4413, %r3773;
	add.s32 	%r3775, %r3774, %r3771;
	selp.b32	%r1606, %r4411, %r3775, %p614;
	mov.u32 	%r3776, -921707870;
	mul.hi.u32 	%r4414, %r1606, %r3776;
	setp.lt.s32	%p615, %r4414, 1;
	@%p615 bra 	BB48_990;

	mul.lo.s32 	%r3777, %r1606, -921707870;
	shr.u32 	%r3778, %r3777, 31;
	shl.b32 	%r3779, %r4414, 1;
	add.s32 	%r4414, %r3778, %r3779;
	add.s32 	%r4415, %r4415, 1;

BB48_990:
	mov.u32 	%r3780, 126;
	sub.s32 	%r3781, %r3780, %r4415;
	shl.b32 	%r3782, %r3781, 23;
	add.s32 	%r3783, %r4414, 1;
	shr.u32 	%r3784, %r3783, 7;
	add.s32 	%r3785, %r3784, 1;
	shr.u32 	%r3786, %r3785, 1;
	add.s32 	%r3787, %r3786, %r3782;
	or.b32  	%r3788, %r3787, %r1602;
	mov.b32 	 %f2469, %r3788;

BB48_991:
	mul.rn.f32 	%f838, %f2469, %f2469;
	add.s32 	%r1613, %r4416, 1;
	and.b32  	%r1614, %r1613, 1;
	setp.eq.s32	%p616, %r1614, 0;
	@%p616 bra 	BB48_993;

	mov.f32 	%f2113, 0fBAB6061A;
	mov.f32 	%f2114, 0f37CCF5CE;
	fma.rn.f32 	%f2470, %f2114, %f838, %f2113;
	bra.uni 	BB48_994;

BB48_993:
	mov.f32 	%f2115, 0f3C08839E;
	mov.f32 	%f2116, 0fB94CA1F9;
	fma.rn.f32 	%f2470, %f2116, %f838, %f2115;

BB48_994:
	@%p616 bra 	BB48_996;

	mov.f32 	%f2117, 0f3D2AAAA5;
	fma.rn.f32 	%f2118, %f2470, %f838, %f2117;
	mov.f32 	%f2119, 0fBF000000;
	fma.rn.f32 	%f2471, %f2118, %f838, %f2119;
	bra.uni 	BB48_997;

BB48_996:
	mov.f32 	%f2120, 0fBE2AAAA3;
	fma.rn.f32 	%f2121, %f2470, %f838, %f2120;
	mov.f32 	%f2122, 0f00000000;
	fma.rn.f32 	%f2471, %f2121, %f838, %f2122;

BB48_997:
	fma.rn.f32 	%f2472, %f2471, %f2469, %f2469;
	@%p616 bra 	BB48_999;

	mov.f32 	%f2123, 0f3F800000;
	fma.rn.f32 	%f2472, %f2471, %f838, %f2123;

BB48_999:
	and.b32  	%r3789, %r1613, 2;
	setp.eq.s32	%p619, %r3789, 0;
	@%p619 bra 	BB48_1001;

	mov.f32 	%f2124, 0f00000000;
	mov.f32 	%f2125, 0fBF800000;
	fma.rn.f32 	%f2472, %f2472, %f2125, %f2124;

BB48_1001:
	mul.f32 	%f850, %f2468, %f2472;
	mov.f32 	%f2523, %f494;
	@%p386 bra 	BB48_1003;

	mov.f32 	%f2126, 0f00000000;
	mul.rn.f32 	%f2523, %f494, %f2126;

BB48_1003:
	mul.f32 	%f2127, %f2523, 0f3F22F983;
	cvt.rni.s32.f32	%r4426, %f2127;
	cvt.rn.f32.s32	%f2128, %r4426;
	neg.f32 	%f2129, %f2128;
	fma.rn.f32 	%f2131, %f2129, %f1668, %f2523;
	fma.rn.f32 	%f2133, %f2129, %f1670, %f2131;
	fma.rn.f32 	%f2473, %f2129, %f1672, %f2133;
	abs.f32 	%f2135, %f2523;
	setp.leu.f32	%p621, %f2135, 0f47CE4780;
	@%p621 bra 	BB48_1013;

	mov.b32 	 %r1616, %f2523;
	shr.u32 	%r1617, %r1616, 23;
	bfe.u32 	%r3792, %r1616, 23, 8;
	add.s32 	%r3793, %r3792, -128;
	shl.b32 	%r3794, %r1616, 8;
	or.b32  	%r1618, %r3794, -2147483648;
	shr.u32 	%r1619, %r3793, 5;
	mov.u32 	%r4418, 0;
	mov.u64 	%rd390, __cudart_i2opi_f;
	mov.u32 	%r4417, -6;
	mov.u64 	%rd444, %rd1;

BB48_1005:
	.pragma "nounroll";
	ld.const.u32 	%r3797, [%rd390];
	// inline asm
	{
	mad.lo.cc.u32   %r3795, %r3797, %r1618, %r4418;
	madc.hi.u32     %r4418, %r3797, %r1618,  0;
	}
	// inline asm
	st.local.u32 	[%rd444], %r3795;
	add.s64 	%rd444, %rd444, 4;
	add.s64 	%rd390, %rd390, 4;
	add.s32 	%r4417, %r4417, 1;
	setp.ne.s32	%p622, %r4417, 0;
	@%p622 bra 	BB48_1005;

	and.b32  	%r1624, %r1616, -2147483648;
	st.local.u32 	[%rd3], %r4418;
	mov.u32 	%r3800, 6;
	sub.s32 	%r3801, %r3800, %r1619;
	mul.wide.s32 	%rd336, %r3801, 4;
	add.s64 	%rd228, %rd1, %rd336;
	ld.local.u32 	%r4419, [%rd228];
	ld.local.u32 	%r4420, [%rd228+-4];
	and.b32  	%r1627, %r1617, 31;
	setp.eq.s32	%p623, %r1627, 0;
	@%p623 bra 	BB48_1008;

	mov.u32 	%r3802, 32;
	sub.s32 	%r3803, %r3802, %r1627;
	shr.u32 	%r3804, %r4420, %r3803;
	shl.b32 	%r3805, %r4419, %r1627;
	add.s32 	%r4419, %r3804, %r3805;
	ld.local.u32 	%r3806, [%rd228+-8];
	shr.u32 	%r3807, %r3806, %r3803;
	shl.b32 	%r3808, %r4420, %r1627;
	add.s32 	%r4420, %r3807, %r3808;

BB48_1008:
	shr.u32 	%r3809, %r4420, 30;
	shl.b32 	%r3810, %r4419, 2;
	add.s32 	%r4421, %r3809, %r3810;
	shl.b32 	%r1633, %r4420, 2;
	shr.u32 	%r3811, %r4421, 31;
	shr.u32 	%r3812, %r4419, 30;
	add.s32 	%r1634, %r3811, %r3812;
	setp.eq.s32	%p624, %r3811, 0;
	mov.u32 	%r4422, %r1624;
	mov.u32 	%r4423, %r1633;
	@%p624 bra 	BB48_1010;

	not.b32 	%r3813, %r4421;
	neg.s32 	%r1635, %r1633;
	setp.eq.s32	%p625, %r1633, 0;
	selp.u32	%r3814, 1, 0, %p625;
	add.s32 	%r4421, %r3814, %r3813;
	xor.b32  	%r1637, %r1624, -2147483648;
	mov.u32 	%r4422, %r1637;
	mov.u32 	%r4423, %r1635;

BB48_1010:
	mov.u32 	%r1639, %r4422;
	neg.s32 	%r3815, %r1634;
	setp.eq.s32	%p626, %r1624, 0;
	selp.b32	%r4426, %r1634, %r3815, %p626;
	clz.b32 	%r4425, %r4421;
	setp.eq.s32	%p627, %r4425, 0;
	shl.b32 	%r3816, %r4421, %r4425;
	mov.u32 	%r3817, 32;
	sub.s32 	%r3818, %r3817, %r4425;
	shr.u32 	%r3819, %r4423, %r3818;
	add.s32 	%r3820, %r3819, %r3816;
	selp.b32	%r1643, %r4421, %r3820, %p627;
	mov.u32 	%r3821, -921707870;
	mul.hi.u32 	%r4424, %r1643, %r3821;
	setp.lt.s32	%p628, %r4424, 1;
	@%p628 bra 	BB48_1012;

	mul.lo.s32 	%r3822, %r1643, -921707870;
	shr.u32 	%r3823, %r3822, 31;
	shl.b32 	%r3824, %r4424, 1;
	add.s32 	%r4424, %r3823, %r3824;
	add.s32 	%r4425, %r4425, 1;

BB48_1012:
	mov.u32 	%r3825, 126;
	sub.s32 	%r3826, %r3825, %r4425;
	shl.b32 	%r3827, %r3826, 23;
	add.s32 	%r3828, %r4424, 1;
	shr.u32 	%r3829, %r3828, 7;
	add.s32 	%r3830, %r3829, 1;
	shr.u32 	%r3831, %r3830, 1;
	add.s32 	%r3832, %r3831, %r3827;
	or.b32  	%r3833, %r3832, %r1639;
	mov.b32 	 %f2473, %r3833;

BB48_1013:
	mul.rn.f32 	%f856, %f2473, %f2473;
	and.b32  	%r1650, %r4426, 1;
	setp.eq.s32	%p629, %r1650, 0;
	@%p629 bra 	BB48_1015;

	mov.f32 	%f2136, 0fBAB6061A;
	mov.f32 	%f2137, 0f37CCF5CE;
	fma.rn.f32 	%f2474, %f2137, %f856, %f2136;
	bra.uni 	BB48_1016;

BB48_1015:
	mov.f32 	%f2138, 0f3C08839E;
	mov.f32 	%f2139, 0fB94CA1F9;
	fma.rn.f32 	%f2474, %f2139, %f856, %f2138;

BB48_1016:
	@%p629 bra 	BB48_1018;

	mov.f32 	%f2140, 0f3D2AAAA5;
	fma.rn.f32 	%f2141, %f2474, %f856, %f2140;
	mov.f32 	%f2142, 0fBF000000;
	fma.rn.f32 	%f2475, %f2141, %f856, %f2142;
	bra.uni 	BB48_1019;

BB48_1018:
	mov.f32 	%f2143, 0fBE2AAAA3;
	fma.rn.f32 	%f2144, %f2474, %f856, %f2143;
	mov.f32 	%f2145, 0f00000000;
	fma.rn.f32 	%f2475, %f2144, %f856, %f2145;

BB48_1019:
	fma.rn.f32 	%f2476, %f2475, %f2473, %f2473;
	@%p629 bra 	BB48_1021;

	mov.f32 	%f2146, 0f3F800000;
	fma.rn.f32 	%f2476, %f2475, %f856, %f2146;

BB48_1021:
	and.b32  	%r3834, %r4426, 2;
	setp.eq.s32	%p632, %r3834, 0;
	@%p632 bra 	BB48_1023;

	mov.f32 	%f2147, 0f00000000;
	mov.f32 	%f2148, 0fBF800000;
	fma.rn.f32 	%f2476, %f2476, %f2148, %f2147;

BB48_1023:
	mul.f32 	%f868, %f850, %f2476;
	mov.f32 	%f2484, %f480;
	@%p360 bra 	BB48_1025;

	mov.f32 	%f2149, 0f00000000;
	mul.rn.f32 	%f2484, %f480, %f2149;

BB48_1025:
	mul.f32 	%f2150, %f2484, 0f3F22F983;
	cvt.rni.s32.f32	%r4436, %f2150;
	cvt.rn.f32.s32	%f2151, %r4436;
	neg.f32 	%f2152, %f2151;
	fma.rn.f32 	%f2154, %f2152, %f1668, %f2484;
	fma.rn.f32 	%f2156, %f2152, %f1670, %f2154;
	fma.rn.f32 	%f2492, %f2152, %f1672, %f2156;
	abs.f32 	%f2158, %f2484;
	setp.leu.f32	%p634, %f2158, 0f47CE4780;
	@%p634 bra 	BB48_1035;

	mov.b32 	 %r1652, %f2484;
	shr.u32 	%r1653, %r1652, 23;
	bfe.u32 	%r3837, %r1652, 23, 8;
	add.s32 	%r3838, %r3837, -128;
	shl.b32 	%r3839, %r1652, 8;
	or.b32  	%r1654, %r3839, -2147483648;
	shr.u32 	%r1655, %r3838, 5;
	mov.u32 	%r4428, 0;
	mov.u64 	%rd391, __cudart_i2opi_f;
	mov.u32 	%r4427, -6;
	mov.u64 	%rd443, %rd1;

BB48_1027:
	.pragma "nounroll";
	ld.const.u32 	%r3842, [%rd391];
	// inline asm
	{
	mad.lo.cc.u32   %r3840, %r3842, %r1654, %r4428;
	madc.hi.u32     %r4428, %r3842, %r1654,  0;
	}
	// inline asm
	st.local.u32 	[%rd443], %r3840;
	add.s64 	%rd443, %rd443, 4;
	add.s64 	%rd391, %rd391, 4;
	add.s32 	%r4427, %r4427, 1;
	setp.ne.s32	%p635, %r4427, 0;
	@%p635 bra 	BB48_1027;

	and.b32  	%r1660, %r1652, -2147483648;
	st.local.u32 	[%rd3], %r4428;
	mov.u32 	%r3845, 6;
	sub.s32 	%r3846, %r3845, %r1655;
	mul.wide.s32 	%rd338, %r3846, 4;
	add.s64 	%rd233, %rd1, %rd338;
	ld.local.u32 	%r4429, [%rd233];
	ld.local.u32 	%r4430, [%rd233+-4];
	and.b32  	%r1663, %r1653, 31;
	setp.eq.s32	%p636, %r1663, 0;
	@%p636 bra 	BB48_1030;

	mov.u32 	%r3847, 32;
	sub.s32 	%r3848, %r3847, %r1663;
	shr.u32 	%r3849, %r4430, %r3848;
	shl.b32 	%r3850, %r4429, %r1663;
	add.s32 	%r4429, %r3849, %r3850;
	ld.local.u32 	%r3851, [%rd233+-8];
	shr.u32 	%r3852, %r3851, %r3848;
	shl.b32 	%r3853, %r4430, %r1663;
	add.s32 	%r4430, %r3852, %r3853;

BB48_1030:
	shr.u32 	%r3854, %r4430, 30;
	shl.b32 	%r3855, %r4429, 2;
	add.s32 	%r4431, %r3854, %r3855;
	shl.b32 	%r1669, %r4430, 2;
	shr.u32 	%r3856, %r4431, 31;
	shr.u32 	%r3857, %r4429, 30;
	add.s32 	%r1670, %r3856, %r3857;
	setp.eq.s32	%p637, %r3856, 0;
	mov.u32 	%r4432, %r1660;
	mov.u32 	%r4433, %r1669;
	@%p637 bra 	BB48_1032;

	not.b32 	%r3858, %r4431;
	neg.s32 	%r1671, %r1669;
	setp.eq.s32	%p638, %r1669, 0;
	selp.u32	%r3859, 1, 0, %p638;
	add.s32 	%r4431, %r3859, %r3858;
	xor.b32  	%r1673, %r1660, -2147483648;
	mov.u32 	%r4432, %r1673;
	mov.u32 	%r4433, %r1671;

BB48_1032:
	mov.u32 	%r1675, %r4432;
	neg.s32 	%r3860, %r1670;
	setp.eq.s32	%p639, %r1660, 0;
	selp.b32	%r4436, %r1670, %r3860, %p639;
	clz.b32 	%r4435, %r4431;
	setp.eq.s32	%p640, %r4435, 0;
	shl.b32 	%r3861, %r4431, %r4435;
	mov.u32 	%r3862, 32;
	sub.s32 	%r3863, %r3862, %r4435;
	shr.u32 	%r3864, %r4433, %r3863;
	add.s32 	%r3865, %r3864, %r3861;
	selp.b32	%r1679, %r4431, %r3865, %p640;
	mov.u32 	%r3866, -921707870;
	mul.hi.u32 	%r4434, %r1679, %r3866;
	setp.lt.s32	%p641, %r4434, 1;
	@%p641 bra 	BB48_1034;

	mul.lo.s32 	%r3867, %r1679, -921707870;
	shr.u32 	%r3868, %r3867, 31;
	shl.b32 	%r3869, %r4434, 1;
	add.s32 	%r4434, %r3868, %r3869;
	add.s32 	%r4435, %r4435, 1;

BB48_1034:
	mov.u32 	%r3870, 126;
	sub.s32 	%r3871, %r3870, %r4435;
	shl.b32 	%r3872, %r3871, 23;
	add.s32 	%r3873, %r4434, 1;
	shr.u32 	%r3874, %r3873, 7;
	add.s32 	%r3875, %r3874, 1;
	shr.u32 	%r3876, %r3875, 1;
	add.s32 	%r3877, %r3876, %r3872;
	or.b32  	%r3878, %r3877, %r1675;
	mov.b32 	 %f2492, %r3878;

BB48_1035:
	mul.rn.f32 	%f874, %f2492, %f2492;
	and.b32  	%r1686, %r4436, 1;
	setp.eq.s32	%p642, %r1686, 0;
	@%p642 bra 	BB48_1037;

	mov.f32 	%f2159, 0fBAB6061A;
	mov.f32 	%f2160, 0f37CCF5CE;
	fma.rn.f32 	%f2493, %f2160, %f874, %f2159;
	bra.uni 	BB48_1038;

BB48_1037:
	mov.f32 	%f2161, 0f3C08839E;
	mov.f32 	%f2162, 0fB94CA1F9;
	fma.rn.f32 	%f2493, %f2162, %f874, %f2161;

BB48_1038:
	@%p642 bra 	BB48_1040;

	mov.f32 	%f2163, 0f3D2AAAA5;
	fma.rn.f32 	%f2164, %f2493, %f874, %f2163;
	mov.f32 	%f2165, 0fBF000000;
	fma.rn.f32 	%f2494, %f2164, %f874, %f2165;
	bra.uni 	BB48_1041;

BB48_1040:
	mov.f32 	%f2166, 0fBE2AAAA3;
	fma.rn.f32 	%f2167, %f2493, %f874, %f2166;
	mov.f32 	%f2168, 0f00000000;
	fma.rn.f32 	%f2494, %f2167, %f874, %f2168;

BB48_1041:
	fma.rn.f32 	%f2495, %f2494, %f2492, %f2492;
	@%p642 bra 	BB48_1043;

	mov.f32 	%f2169, 0f3F800000;
	fma.rn.f32 	%f2495, %f2494, %f874, %f2169;

BB48_1043:
	and.b32  	%r3879, %r4436, 2;
	setp.eq.s32	%p645, %r3879, 0;
	@%p645 bra 	BB48_1045;

	mov.f32 	%f2170, 0f00000000;
	mov.f32 	%f2171, 0fBF800000;
	fma.rn.f32 	%f2495, %f2495, %f2171, %f2170;

BB48_1045:
	mov.f32 	%f2503, %f487;
	@%p373 bra 	BB48_1047;

	mov.f32 	%f2172, 0f00000000;
	mul.rn.f32 	%f2503, %f487, %f2172;

BB48_1047:
	mul.f32 	%f2173, %f2503, 0f3F22F983;
	cvt.rni.s32.f32	%r4446, %f2173;
	cvt.rn.f32.s32	%f2174, %r4446;
	neg.f32 	%f2175, %f2174;
	fma.rn.f32 	%f2177, %f2175, %f1668, %f2503;
	fma.rn.f32 	%f2179, %f2175, %f1670, %f2177;
	fma.rn.f32 	%f2511, %f2175, %f1672, %f2179;
	abs.f32 	%f2181, %f2503;
	setp.leu.f32	%p647, %f2181, 0f47CE4780;
	@%p647 bra 	BB48_1057;

	mov.b32 	 %r1688, %f2503;
	shr.u32 	%r1689, %r1688, 23;
	bfe.u32 	%r3882, %r1688, 23, 8;
	add.s32 	%r3883, %r3882, -128;
	shl.b32 	%r3884, %r1688, 8;
	or.b32  	%r1690, %r3884, -2147483648;
	shr.u32 	%r1691, %r3883, 5;
	mov.u32 	%r4438, 0;
	mov.u64 	%rd392, __cudart_i2opi_f;
	mov.u32 	%r4437, -6;
	mov.u64 	%rd442, %rd1;

BB48_1049:
	.pragma "nounroll";
	ld.const.u32 	%r3887, [%rd392];
	// inline asm
	{
	mad.lo.cc.u32   %r3885, %r3887, %r1690, %r4438;
	madc.hi.u32     %r4438, %r3887, %r1690,  0;
	}
	// inline asm
	st.local.u32 	[%rd442], %r3885;
	add.s64 	%rd442, %rd442, 4;
	add.s64 	%rd392, %rd392, 4;
	add.s32 	%r4437, %r4437, 1;
	setp.ne.s32	%p648, %r4437, 0;
	@%p648 bra 	BB48_1049;

	and.b32  	%r1696, %r1688, -2147483648;
	st.local.u32 	[%rd3], %r4438;
	mov.u32 	%r3890, 6;
	sub.s32 	%r3891, %r3890, %r1691;
	mul.wide.s32 	%rd340, %r3891, 4;
	add.s64 	%rd238, %rd1, %rd340;
	ld.local.u32 	%r4439, [%rd238];
	ld.local.u32 	%r4440, [%rd238+-4];
	and.b32  	%r1699, %r1689, 31;
	setp.eq.s32	%p649, %r1699, 0;
	@%p649 bra 	BB48_1052;

	mov.u32 	%r3892, 32;
	sub.s32 	%r3893, %r3892, %r1699;
	shr.u32 	%r3894, %r4440, %r3893;
	shl.b32 	%r3895, %r4439, %r1699;
	add.s32 	%r4439, %r3894, %r3895;
	ld.local.u32 	%r3896, [%rd238+-8];
	shr.u32 	%r3897, %r3896, %r3893;
	shl.b32 	%r3898, %r4440, %r1699;
	add.s32 	%r4440, %r3897, %r3898;

BB48_1052:
	shr.u32 	%r3899, %r4440, 30;
	shl.b32 	%r3900, %r4439, 2;
	add.s32 	%r4441, %r3899, %r3900;
	shl.b32 	%r1705, %r4440, 2;
	shr.u32 	%r3901, %r4441, 31;
	shr.u32 	%r3902, %r4439, 30;
	add.s32 	%r1706, %r3901, %r3902;
	setp.eq.s32	%p650, %r3901, 0;
	mov.u32 	%r4442, %r1696;
	mov.u32 	%r4443, %r1705;
	@%p650 bra 	BB48_1054;

	not.b32 	%r3903, %r4441;
	neg.s32 	%r1707, %r1705;
	setp.eq.s32	%p651, %r1705, 0;
	selp.u32	%r3904, 1, 0, %p651;
	add.s32 	%r4441, %r3904, %r3903;
	xor.b32  	%r1709, %r1696, -2147483648;
	mov.u32 	%r4442, %r1709;
	mov.u32 	%r4443, %r1707;

BB48_1054:
	mov.u32 	%r1711, %r4442;
	neg.s32 	%r3905, %r1706;
	setp.eq.s32	%p652, %r1696, 0;
	selp.b32	%r4446, %r1706, %r3905, %p652;
	clz.b32 	%r4445, %r4441;
	setp.eq.s32	%p653, %r4445, 0;
	shl.b32 	%r3906, %r4441, %r4445;
	mov.u32 	%r3907, 32;
	sub.s32 	%r3908, %r3907, %r4445;
	shr.u32 	%r3909, %r4443, %r3908;
	add.s32 	%r3910, %r3909, %r3906;
	selp.b32	%r1715, %r4441, %r3910, %p653;
	mov.u32 	%r3911, -921707870;
	mul.hi.u32 	%r4444, %r1715, %r3911;
	setp.lt.s32	%p654, %r4444, 1;
	@%p654 bra 	BB48_1056;

	mul.lo.s32 	%r3912, %r1715, -921707870;
	shr.u32 	%r3913, %r3912, 31;
	shl.b32 	%r3914, %r4444, 1;
	add.s32 	%r4444, %r3913, %r3914;
	add.s32 	%r4445, %r4445, 1;

BB48_1056:
	mov.u32 	%r3915, 126;
	sub.s32 	%r3916, %r3915, %r4445;
	shl.b32 	%r3917, %r3916, 23;
	add.s32 	%r3918, %r4444, 1;
	shr.u32 	%r3919, %r3918, 7;
	add.s32 	%r3920, %r3919, 1;
	shr.u32 	%r3921, %r3920, 1;
	add.s32 	%r3922, %r3921, %r3917;
	or.b32  	%r3923, %r3922, %r1711;
	mov.b32 	 %f2511, %r3923;

BB48_1057:
	mul.rn.f32 	%f891, %f2511, %f2511;
	and.b32  	%r1722, %r4446, 1;
	setp.eq.s32	%p655, %r1722, 0;
	@%p655 bra 	BB48_1059;

	mov.f32 	%f2182, 0fBAB6061A;
	mov.f32 	%f2183, 0f37CCF5CE;
	fma.rn.f32 	%f2512, %f2183, %f891, %f2182;
	bra.uni 	BB48_1060;

BB48_1059:
	mov.f32 	%f2184, 0f3C08839E;
	mov.f32 	%f2185, 0fB94CA1F9;
	fma.rn.f32 	%f2512, %f2185, %f891, %f2184;

BB48_1060:
	@%p655 bra 	BB48_1062;

	mov.f32 	%f2186, 0f3D2AAAA5;
	fma.rn.f32 	%f2187, %f2512, %f891, %f2186;
	mov.f32 	%f2188, 0fBF000000;
	fma.rn.f32 	%f2513, %f2187, %f891, %f2188;
	bra.uni 	BB48_1063;

BB48_1062:
	mov.f32 	%f2189, 0fBE2AAAA3;
	fma.rn.f32 	%f2190, %f2512, %f891, %f2189;
	mov.f32 	%f2191, 0f00000000;
	fma.rn.f32 	%f2513, %f2190, %f891, %f2191;

BB48_1063:
	fma.rn.f32 	%f2514, %f2513, %f2511, %f2511;
	@%p655 bra 	BB48_1065;

	mov.f32 	%f2192, 0f3F800000;
	fma.rn.f32 	%f2514, %f2513, %f891, %f2192;

BB48_1065:
	and.b32  	%r3924, %r4446, 2;
	setp.eq.s32	%p658, %r3924, 0;
	@%p658 bra 	BB48_1067;

	mov.f32 	%f2193, 0f00000000;
	mov.f32 	%f2194, 0fBF800000;
	fma.rn.f32 	%f2514, %f2514, %f2194, %f2193;

BB48_1067:
	mul.f32 	%f903, %f2495, %f2514;
	mov.f32 	%f2522, %f494;
	@%p386 bra 	BB48_1069;

	mov.f32 	%f2195, 0f00000000;
	mul.rn.f32 	%f2522, %f494, %f2195;

BB48_1069:
	mul.f32 	%f2196, %f2522, 0f3F22F983;
	cvt.rni.s32.f32	%r4456, %f2196;
	cvt.rn.f32.s32	%f2197, %r4456;
	neg.f32 	%f2198, %f2197;
	fma.rn.f32 	%f2200, %f2198, %f1668, %f2522;
	fma.rn.f32 	%f2202, %f2198, %f1670, %f2200;
	fma.rn.f32 	%f2530, %f2198, %f1672, %f2202;
	abs.f32 	%f2204, %f2522;
	setp.leu.f32	%p660, %f2204, 0f47CE4780;
	@%p660 bra 	BB48_1079;

	mov.b32 	 %r1724, %f2522;
	shr.u32 	%r1725, %r1724, 23;
	bfe.u32 	%r3927, %r1724, 23, 8;
	add.s32 	%r3928, %r3927, -128;
	shl.b32 	%r3929, %r1724, 8;
	or.b32  	%r1726, %r3929, -2147483648;
	shr.u32 	%r1727, %r3928, 5;
	mov.u32 	%r4448, 0;
	mov.u64 	%rd393, __cudart_i2opi_f;
	mov.u32 	%r4447, -6;
	mov.u64 	%rd441, %rd1;

BB48_1071:
	.pragma "nounroll";
	ld.const.u32 	%r3932, [%rd393];
	// inline asm
	{
	mad.lo.cc.u32   %r3930, %r3932, %r1726, %r4448;
	madc.hi.u32     %r4448, %r3932, %r1726,  0;
	}
	// inline asm
	st.local.u32 	[%rd441], %r3930;
	add.s64 	%rd441, %rd441, 4;
	add.s64 	%rd393, %rd393, 4;
	add.s32 	%r4447, %r4447, 1;
	setp.ne.s32	%p661, %r4447, 0;
	@%p661 bra 	BB48_1071;

	and.b32  	%r1732, %r1724, -2147483648;
	st.local.u32 	[%rd3], %r4448;
	mov.u32 	%r3935, 6;
	sub.s32 	%r3936, %r3935, %r1727;
	mul.wide.s32 	%rd342, %r3936, 4;
	add.s64 	%rd243, %rd1, %rd342;
	ld.local.u32 	%r4449, [%rd243];
	ld.local.u32 	%r4450, [%rd243+-4];
	and.b32  	%r1735, %r1725, 31;
	setp.eq.s32	%p662, %r1735, 0;
	@%p662 bra 	BB48_1074;

	mov.u32 	%r3937, 32;
	sub.s32 	%r3938, %r3937, %r1735;
	shr.u32 	%r3939, %r4450, %r3938;
	shl.b32 	%r3940, %r4449, %r1735;
	add.s32 	%r4449, %r3939, %r3940;
	ld.local.u32 	%r3941, [%rd243+-8];
	shr.u32 	%r3942, %r3941, %r3938;
	shl.b32 	%r3943, %r4450, %r1735;
	add.s32 	%r4450, %r3942, %r3943;

BB48_1074:
	shr.u32 	%r3944, %r4450, 30;
	shl.b32 	%r3945, %r4449, 2;
	add.s32 	%r4451, %r3944, %r3945;
	shl.b32 	%r1741, %r4450, 2;
	shr.u32 	%r3946, %r4451, 31;
	shr.u32 	%r3947, %r4449, 30;
	add.s32 	%r1742, %r3946, %r3947;
	setp.eq.s32	%p663, %r3946, 0;
	mov.u32 	%r4452, %r1732;
	mov.u32 	%r4453, %r1741;
	@%p663 bra 	BB48_1076;

	not.b32 	%r3948, %r4451;
	neg.s32 	%r1743, %r1741;
	setp.eq.s32	%p664, %r1741, 0;
	selp.u32	%r3949, 1, 0, %p664;
	add.s32 	%r4451, %r3949, %r3948;
	xor.b32  	%r1745, %r1732, -2147483648;
	mov.u32 	%r4452, %r1745;
	mov.u32 	%r4453, %r1743;

BB48_1076:
	mov.u32 	%r1747, %r4452;
	neg.s32 	%r3950, %r1742;
	setp.eq.s32	%p665, %r1732, 0;
	selp.b32	%r4456, %r1742, %r3950, %p665;
	clz.b32 	%r4455, %r4451;
	setp.eq.s32	%p666, %r4455, 0;
	shl.b32 	%r3951, %r4451, %r4455;
	mov.u32 	%r3952, 32;
	sub.s32 	%r3953, %r3952, %r4455;
	shr.u32 	%r3954, %r4453, %r3953;
	add.s32 	%r3955, %r3954, %r3951;
	selp.b32	%r1751, %r4451, %r3955, %p666;
	mov.u32 	%r3956, -921707870;
	mul.hi.u32 	%r4454, %r1751, %r3956;
	setp.lt.s32	%p667, %r4454, 1;
	@%p667 bra 	BB48_1078;

	mul.lo.s32 	%r3957, %r1751, -921707870;
	shr.u32 	%r3958, %r3957, 31;
	shl.b32 	%r3959, %r4454, 1;
	add.s32 	%r4454, %r3958, %r3959;
	add.s32 	%r4455, %r4455, 1;

BB48_1078:
	mov.u32 	%r3960, 126;
	sub.s32 	%r3961, %r3960, %r4455;
	shl.b32 	%r3962, %r3961, 23;
	add.s32 	%r3963, %r4454, 1;
	shr.u32 	%r3964, %r3963, 7;
	add.s32 	%r3965, %r3964, 1;
	shr.u32 	%r3966, %r3965, 1;
	add.s32 	%r3967, %r3966, %r3962;
	or.b32  	%r3968, %r3967, %r1747;
	mov.b32 	 %f2530, %r3968;

BB48_1079:
	mul.rn.f32 	%f909, %f2530, %f2530;
	add.s32 	%r1758, %r4456, 1;
	and.b32  	%r1759, %r1758, 1;
	setp.eq.s32	%p668, %r1759, 0;
	@%p668 bra 	BB48_1081;

	mov.f32 	%f2205, 0fBAB6061A;
	mov.f32 	%f2206, 0f37CCF5CE;
	fma.rn.f32 	%f2531, %f2206, %f909, %f2205;
	bra.uni 	BB48_1082;

BB48_1081:
	mov.f32 	%f2207, 0f3C08839E;
	mov.f32 	%f2208, 0fB94CA1F9;
	fma.rn.f32 	%f2531, %f2208, %f909, %f2207;

BB48_1082:
	@%p668 bra 	BB48_1084;

	mov.f32 	%f2209, 0f3D2AAAA5;
	fma.rn.f32 	%f2210, %f2531, %f909, %f2209;
	mov.f32 	%f2211, 0fBF000000;
	fma.rn.f32 	%f2532, %f2210, %f909, %f2211;
	bra.uni 	BB48_1085;

BB48_1084:
	mov.f32 	%f2212, 0fBE2AAAA3;
	fma.rn.f32 	%f2213, %f2531, %f909, %f2212;
	mov.f32 	%f2214, 0f00000000;
	fma.rn.f32 	%f2532, %f2213, %f909, %f2214;

BB48_1085:
	fma.rn.f32 	%f2533, %f2532, %f2530, %f2530;
	@%p668 bra 	BB48_1087;

	mov.f32 	%f2215, 0f3F800000;
	fma.rn.f32 	%f2533, %f2532, %f909, %f2215;

BB48_1087:
	and.b32  	%r3969, %r1758, 2;
	setp.eq.s32	%p671, %r3969, 0;
	@%p671 bra 	BB48_1089;

	mov.f32 	%f2216, 0f00000000;
	mov.f32 	%f2217, 0fBF800000;
	fma.rn.f32 	%f2533, %f2533, %f2217, %f2216;

BB48_1089:
	mov.u32 	%r3976, %tid.x;
	mov.u32 	%r3975, %ctaid.x;
	mov.u32 	%r3974, %ntid.x;
	mad.lo.s32 	%r3973, %r3974, %r3975, %r3976;
	mul.wide.s32 	%rd345, %r3973, 16;
	ld.param.u64 	%rd344, [_Z18actfunc_quat_multiP6float4iiii_param_0];
	add.s64 	%rd343, %rd344, %rd345;
	fma.rn.f32 	%f2218, %f903, %f2533, %f868;
	st.v4.f32 	[%rd343], {%f603, %f709, %f815, %f2218};

BB48_1090:
	ret;
}

	// .globl	_Z5qsignddi
.visible .func  (.param .b64 func_retval0) _Z5qsignddi(
	.param .b64 _Z5qsignddi_param_0,
	.param .b64 _Z5qsignddi_param_1,
	.param .b32 _Z5qsignddi_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<22>;


	ld.param.f64 	%fd8, [_Z5qsignddi_param_0];
	ld.param.f64 	%fd9, [_Z5qsignddi_param_1];
	ld.param.u32 	%r1, [_Z5qsignddi_param_2];
	cvt.rn.f64.s32	%fd10, %r1;
	mov.f64 	%fd11, 0d401921FB54442D18;
	div.rn.f64 	%fd12, %fd11, %fd10;
	mul.f64 	%fd1, %fd12, %fd9;
	mul.f64 	%fd2, %fd1, 0d3FE0000000000000;
	sub.f64 	%fd13, %fd8, %fd2;
	mul.f64 	%fd3, %fd9, 0d400921FB54442D18;
	add.f64 	%fd14, %fd3, %fd13;
	div.rn.f64 	%fd21, %fd14, %fd1;
	abs.f64 	%fd5, %fd21;
	setp.ge.f64	%p1, %fd5, 0d4330000000000000;
	@%p1 bra 	BB49_2;

	add.f64 	%fd15, %fd5, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd16, %fd15;
	setp.lt.f64	%p2, %fd5, 0d3FE0000000000000;
	selp.f64	%fd17, 0d0000000000000000, %fd16, %p2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd17;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd17;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd21;
	}
	and.b32  	%r5, %r4, -2147483648;
	or.b32  	%r6, %r3, %r5;
	mov.b64 	%fd21, {%r2, %r6};

BB49_2:
	mul.f64 	%fd18, %fd1, %fd21;
	sub.f64 	%fd19, %fd18, %fd3;
	add.f64 	%fd20, %fd2, %fd19;
	st.param.f64	[func_retval0+0], %fd20;
	ret;
}

	// .globl	_Z18actfunc_quat_multiP7double4iiii
.visible .func _Z18actfunc_quat_multiP7double4iiii(
	.param .b64 _Z18actfunc_quat_multiP7double4iiii_param_0,
	.param .b32 _Z18actfunc_quat_multiP7double4iiii_param_1,
	.param .b32 _Z18actfunc_quat_multiP7double4iiii_param_2,
	.param .b32 _Z18actfunc_quat_multiP7double4iiii_param_3,
	.param .b32 _Z18actfunc_quat_multiP7double4iiii_param_4
)
{
	.local .align 4 .b8 	__local_depot50[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<295>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<676>;
	.reg .f64 	%fd<3493>;
	.reg .b64 	%rd<175>;


	mov.u64 	%rd174, __local_depot50;
	cvta.local.u64 	%SP, %rd174;
	ld.param.u64 	%rd50, [_Z18actfunc_quat_multiP7double4iiii_param_0];
	ld.param.u32 	%r156, [_Z18actfunc_quat_multiP7double4iiii_param_4];
	add.u64 	%rd51, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd51;
	mov.u32 	%r157, %ntid.x;
	mov.u32 	%r158, %ctaid.x;
	mov.u32 	%r159, %tid.x;
	mad.lo.s32 	%r1, %r157, %r158, %r159;
	setp.ge.s32	%p1, %r1, %r156;
	@%p1 bra 	BB50_325;

	mul.wide.s32 	%rd99, %r1, 32;
	add.s64 	%rd49, %rd50, %rd99;
	ld.v2.f64 	{%fd496, %fd497}, [%rd49+16];
	ld.v2.f64 	{%fd498, %fd499}, [%rd49];
	mul.f64 	%fd500, %fd499, %fd499;
	fma.rn.f64 	%fd501, %fd498, %fd498, %fd500;
	fma.rn.f64 	%fd502, %fd496, %fd496, %fd501;
	fma.rn.f64 	%fd503, %fd497, %fd497, %fd502;
	sqrt.rn.f64 	%fd5, %fd503;
	setp.eq.f64	%p2, %fd5, 0d0000000000000000;
	mov.f64 	%fd3396, 0d0000000000000000;
	mov.f64 	%fd3395, %fd3396;
	mov.f64 	%fd3394, %fd3396;
	@%p2 bra 	BB50_174;

	div.rn.f64 	%fd6, %fd498, %fd5;
	div.rn.f64 	%fd7, %fd496, %fd5;
	div.rn.f64 	%fd8, %fd499, %fd5;
	mul.f64 	%fd505, %fd8, %fd7;
	div.rn.f64 	%fd9, %fd497, %fd5;
	mul.f64 	%fd506, %fd6, %fd9;
	sub.f64 	%fd507, %fd505, %fd506;
	add.f64 	%fd10, %fd507, %fd507;
	setp.gt.f64	%p3, %fd10, 0d3FF0000000000000;
	mov.f64 	%fd3288, 0d3FF0000000000000;
	@%p3 bra 	BB50_4;

	setp.lt.f64	%p4, %fd10, 0dBFF0000000000000;
	selp.f64	%fd3288, 0dBFF0000000000000, %fd10, %p4;

BB50_4:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd3288;
	}
	mov.b32 	 %f1, %r2;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p5, %f2, 0f3FE26666;
	@%p5 bra 	BB50_6;
	bra.uni 	BB50_5;

BB50_6:
	mul.f64 	%fd558, %fd3288, %fd3288;
	mov.f64 	%fd559, 0dBFB3823B180754AF;
	mov.f64 	%fd560, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd561, %fd560, %fd558, %fd559;
	mov.f64 	%fd562, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd563, %fd561, %fd558, %fd562;
	mov.f64 	%fd564, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd565, %fd563, %fd558, %fd564;
	mov.f64 	%fd566, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd567, %fd565, %fd558, %fd566;
	mov.f64 	%fd568, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd569, %fd567, %fd558, %fd568;
	mov.f64 	%fd570, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd571, %fd569, %fd558, %fd570;
	mov.f64 	%fd572, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd573, %fd571, %fd558, %fd572;
	mov.f64 	%fd574, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd575, %fd573, %fd558, %fd574;
	mov.f64 	%fd576, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd577, %fd575, %fd558, %fd576;
	mov.f64 	%fd578, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd579, %fd577, %fd558, %fd578;
	mov.f64 	%fd580, 0d3FB333333320F91B;
	fma.rn.f64 	%fd581, %fd579, %fd558, %fd580;
	mov.f64 	%fd582, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd583, %fd581, %fd558, %fd582;
	mul.f64 	%fd584, %fd558, %fd583;
	fma.rn.f64 	%fd3289, %fd584, %fd3288, %fd3288;
	bra.uni 	BB50_7;

BB50_5:
	abs.f64 	%fd510, %fd3288;
	mov.f64 	%fd511, 0d3FE0000000000000;
	mov.f64 	%fd512, 0dBFE0000000000000;
	fma.rn.f64 	%fd509, %fd512, %fd510, %fd511;
	// inline asm
	rsqrt.approx.ftz.f64 %fd508, %fd509;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r160, %temp}, %fd508;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r161}, %fd508;
	}
	add.s32 	%r162, %r161, -1048576;
	mov.b64 	%fd513, {%r160, %r162};
	mul.f64 	%fd514, %fd509, %fd508;
	neg.f64 	%fd515, %fd514;
	fma.rn.f64 	%fd516, %fd514, %fd515, %fd509;
	fma.rn.f64 	%fd517, %fd516, %fd513, %fd514;
	neg.f64 	%fd518, %fd517;
	mov.f64 	%fd519, 0d3FF0000000000000;
	fma.rn.f64 	%fd520, %fd508, %fd518, %fd519;
	fma.rn.f64 	%fd521, %fd520, %fd513, %fd513;
	fma.rn.f64 	%fd522, %fd517, %fd518, %fd509;
	fma.rn.f64 	%fd523, %fd522, %fd521, %fd517;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r163}, %fd509;
	}
	setp.lt.s32	%p6, %r163, 0;
	selp.f64	%fd524, 0dFFF8000000000000, %fd523, %p6;
	setp.equ.f64	%p7, %fd509, 0d0000000000000000;
	selp.f64	%fd525, %fd509, %fd524, %p7;
	mov.f64 	%fd526, 0dBFB3823B180754AF;
	mov.f64 	%fd527, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd528, %fd527, %fd509, %fd526;
	mov.f64 	%fd529, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd530, %fd528, %fd509, %fd529;
	mov.f64 	%fd531, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd532, %fd530, %fd509, %fd531;
	mov.f64 	%fd533, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd534, %fd532, %fd509, %fd533;
	mov.f64 	%fd535, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd536, %fd534, %fd509, %fd535;
	mov.f64 	%fd537, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd538, %fd536, %fd509, %fd537;
	mov.f64 	%fd539, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd540, %fd538, %fd509, %fd539;
	mov.f64 	%fd541, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd542, %fd540, %fd509, %fd541;
	mov.f64 	%fd543, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd544, %fd542, %fd509, %fd543;
	mov.f64 	%fd545, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd546, %fd544, %fd509, %fd545;
	mov.f64 	%fd547, 0d3FB333333320F91B;
	fma.rn.f64 	%fd548, %fd546, %fd509, %fd547;
	mov.f64 	%fd549, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd550, %fd548, %fd509, %fd549;
	mul.f64 	%fd551, %fd509, %fd550;
	mul.f64 	%fd552, %fd525, 0dC000000000000000;
	mov.f64 	%fd553, 0d3C91A62633145C07;
	fma.rn.f64 	%fd554, %fd552, %fd551, %fd553;
	add.f64 	%fd555, %fd552, 0d3FE921FB54442D18;
	add.f64 	%fd556, %fd555, %fd554;
	add.f64 	%fd557, %fd556, 0d3FE921FB54442D18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r164, %temp}, %fd557;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r165}, %fd557;
	}
	and.b32  	%r166, %r2, -2147483648;
	or.b32  	%r167, %r165, %r166;
	mov.b64 	%fd3289, {%r164, %r167};

BB50_7:
	mul.f64 	%fd16, %fd3289, 0dBFE0000000000000;
	setp.neu.f64	%p8, %fd16, 0d3FE921FB54442D18;
	setp.neu.f64	%p9, %fd16, 0dBFE921FB54442D18;
	and.pred  	%p10, %p8, %p9;
	mul.f64 	%fd585, %fd8, %fd8;
	mul.f64 	%fd17, %fd6, %fd6;
	sub.f64 	%fd18, %fd17, %fd585;
	@%p10 bra 	BB50_13;
	bra.uni 	BB50_8;

BB50_13:
	fma.rn.f64 	%fd650, %fd7, %fd7, %fd18;
	mul.f64 	%fd24, %fd9, %fd9;
	sub.f64 	%fd651, %fd650, %fd24;
	mul.f64 	%fd652, %fd6, %fd8;
	fma.rn.f64 	%fd653, %fd6, %fd8, %fd652;
	fma.rn.f64 	%fd654, %fd9, %fd7, %fd653;
	fma.rn.f64 	%fd655, %fd7, %fd9, %fd654;
	abs.f64 	%fd25, %fd651;
	abs.f64 	%fd26, %fd655;
	setp.eq.f64	%p22, %fd25, 0d0000000000000000;
	setp.eq.f64	%p23, %fd26, 0d0000000000000000;
	and.pred  	%p24, %p22, %p23;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd651;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r178}, %fd655;
	}
	and.b32  	%r6, %r178, -2147483648;
	@%p24 bra 	BB50_17;
	bra.uni 	BB50_14;

BB50_17:
	setp.lt.s32	%p32, %r5, 0;
	selp.f64	%fd708, 0d400921FB54442D18, 0d0000000000000000, %p32;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r185, %temp}, %fd708;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r186}, %fd708;
	}
	or.b32  	%r187, %r186, %r6;
	mov.b64 	%fd3290, {%r185, %r187};
	bra.uni 	BB50_18;

BB50_8:
	mul.f64 	%fd586, %fd7, %fd7;
	sub.f64 	%fd587, %fd18, %fd586;
	fma.rn.f64 	%fd588, %fd9, %fd9, %fd587;
	mul.f64 	%fd589, %fd6, %fd7;
	mul.f64 	%fd590, %fd8, %fd9;
	sub.f64 	%fd591, %fd589, %fd590;
	fma.rn.f64 	%fd592, %fd6, %fd7, %fd591;
	sub.f64 	%fd593, %fd592, %fd590;
	abs.f64 	%fd19, %fd588;
	abs.f64 	%fd20, %fd593;
	setp.eq.f64	%p11, %fd19, 0d0000000000000000;
	setp.eq.f64	%p12, %fd20, 0d0000000000000000;
	and.pred  	%p13, %p11, %p12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd588;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r168}, %fd593;
	}
	and.b32  	%r4, %r168, -2147483648;
	@%p13 bra 	BB50_12;
	bra.uni 	BB50_9;

BB50_12:
	setp.lt.s32	%p21, %r3, 0;
	selp.f64	%fd649, 0d400921FB54442D18, 0d0000000000000000, %p21;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r175, %temp}, %fd649;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd649;
	}
	or.b32  	%r177, %r176, %r4;
	mov.b64 	%fd3291, {%r175, %r177};
	mov.f64 	%fd3393, 0d0000000000000000;
	bra.uni 	BB50_24;

BB50_14:
	setp.eq.f64	%p25, %fd25, 0d7FF0000000000000;
	setp.eq.f64	%p26, %fd26, 0d7FF0000000000000;
	and.pred  	%p27, %p25, %p26;
	@%p27 bra 	BB50_16;
	bra.uni 	BB50_15;

BB50_16:
	setp.lt.s32	%p31, %r5, 0;
	selp.f64	%fd707, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p31;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r182, %temp}, %fd707;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r183}, %fd707;
	}
	or.b32  	%r184, %r183, %r6;
	mov.b64 	%fd3290, {%r182, %r184};
	bra.uni 	BB50_18;

BB50_9:
	setp.eq.f64	%p14, %fd19, 0d7FF0000000000000;
	setp.eq.f64	%p15, %fd20, 0d7FF0000000000000;
	and.pred  	%p16, %p14, %p15;
	@%p16 bra 	BB50_11;
	bra.uni 	BB50_10;

BB50_11:
	setp.lt.s32	%p20, %r3, 0;
	selp.f64	%fd647, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r172, %temp}, %fd647;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r173}, %fd647;
	}
	or.b32  	%r174, %r173, %r4;
	mov.b64 	%fd3291, {%r172, %r174};
	mov.f64 	%fd3393, 0d0000000000000000;
	bra.uni 	BB50_24;

BB50_15:
	setp.lt.s32	%p28, %r5, 0;
	min.f64 	%fd656, %fd26, %fd25;
	max.f64 	%fd657, %fd26, %fd25;
	div.rn.f64 	%fd658, %fd656, %fd657;
	mul.f64 	%fd659, %fd658, %fd658;
	mov.f64 	%fd660, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd661, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd662, %fd661, %fd659, %fd660;
	mov.f64 	%fd663, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd664, %fd662, %fd659, %fd663;
	mov.f64 	%fd665, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd666, %fd664, %fd659, %fd665;
	mov.f64 	%fd667, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd668, %fd666, %fd659, %fd667;
	mov.f64 	%fd669, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd670, %fd668, %fd659, %fd669;
	mov.f64 	%fd671, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd672, %fd670, %fd659, %fd671;
	mov.f64 	%fd673, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd674, %fd672, %fd659, %fd673;
	mov.f64 	%fd675, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd676, %fd674, %fd659, %fd675;
	mov.f64 	%fd677, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd678, %fd676, %fd659, %fd677;
	mov.f64 	%fd679, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd680, %fd678, %fd659, %fd679;
	mov.f64 	%fd681, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd682, %fd680, %fd659, %fd681;
	mov.f64 	%fd683, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd684, %fd682, %fd659, %fd683;
	mov.f64 	%fd685, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd686, %fd684, %fd659, %fd685;
	mov.f64 	%fd687, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd688, %fd686, %fd659, %fd687;
	mov.f64 	%fd689, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd690, %fd688, %fd659, %fd689;
	mov.f64 	%fd691, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd692, %fd690, %fd659, %fd691;
	mov.f64 	%fd693, 0d3FC99999999840D2;
	fma.rn.f64 	%fd694, %fd692, %fd659, %fd693;
	mov.f64 	%fd695, 0dBFD555555555544C;
	fma.rn.f64 	%fd696, %fd694, %fd659, %fd695;
	mul.f64 	%fd697, %fd659, %fd696;
	fma.rn.f64 	%fd698, %fd697, %fd658, %fd658;
	mov.f64 	%fd699, 0d3FF921FB54442D18;
	sub.f64 	%fd700, %fd699, %fd698;
	setp.gt.f64	%p29, %fd26, %fd25;
	selp.f64	%fd701, %fd700, %fd698, %p29;
	mov.f64 	%fd702, 0d400921FB54442D18;
	sub.f64 	%fd703, %fd702, %fd701;
	selp.f64	%fd704, %fd703, %fd701, %p28;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r179, %temp}, %fd704;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r180}, %fd704;
	}
	or.b32  	%r181, %r180, %r6;
	mov.b64 	%fd705, {%r179, %r181};
	add.f64 	%fd706, %fd25, %fd26;
	setp.gtu.f64	%p30, %fd706, 0d7FF0000000000000;
	selp.f64	%fd3290, %fd706, %fd705, %p30;

BB50_18:
	mul.f64 	%fd2858, %fd9, %fd9;
	mul.f64 	%fd2857, %fd6, %fd6;
	mul.f64 	%fd3393, %fd3290, 0d3FE0000000000000;
	fma.rn.f64 	%fd709, %fd8, %fd8, %fd2857;
	mul.f64 	%fd710, %fd7, %fd7;
	sub.f64 	%fd711, %fd709, %fd710;
	sub.f64 	%fd712, %fd711, %fd2858;
	mul.f64 	%fd713, %fd9, %fd8;
	fma.rn.f64 	%fd714, %fd6, %fd7, %fd713;
	fma.rn.f64 	%fd715, %fd6, %fd7, %fd714;
	add.f64 	%fd716, %fd715, %fd713;
	abs.f64 	%fd32, %fd712;
	abs.f64 	%fd33, %fd716;
	setp.eq.f64	%p33, %fd32, 0d0000000000000000;
	setp.eq.f64	%p34, %fd33, 0d0000000000000000;
	and.pred  	%p35, %p33, %p34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd712;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r188}, %fd716;
	}
	and.b32  	%r8, %r188, -2147483648;
	@%p35 bra 	BB50_22;
	bra.uni 	BB50_19;

BB50_22:
	setp.lt.s32	%p43, %r7, 0;
	selp.f64	%fd769, 0d400921FB54442D18, 0d0000000000000000, %p43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r195, %temp}, %fd769;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r196}, %fd769;
	}
	or.b32  	%r197, %r196, %r8;
	mov.b64 	%fd3291, {%r195, %r197};
	bra.uni 	BB50_23;

BB50_19:
	setp.eq.f64	%p36, %fd32, 0d7FF0000000000000;
	setp.eq.f64	%p37, %fd33, 0d7FF0000000000000;
	and.pred  	%p38, %p36, %p37;
	@%p38 bra 	BB50_21;
	bra.uni 	BB50_20;

BB50_21:
	setp.lt.s32	%p42, %r7, 0;
	selp.f64	%fd768, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p42;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r192, %temp}, %fd768;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r193}, %fd768;
	}
	or.b32  	%r194, %r193, %r8;
	mov.b64 	%fd3291, {%r192, %r194};

BB50_23:
	bra.uni 	BB50_24;

BB50_10:
	setp.lt.s32	%p17, %r3, 0;
	min.f64 	%fd595, %fd20, %fd19;
	max.f64 	%fd596, %fd20, %fd19;
	div.rn.f64 	%fd597, %fd595, %fd596;
	mul.f64 	%fd598, %fd597, %fd597;
	mov.f64 	%fd599, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd600, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd601, %fd600, %fd598, %fd599;
	mov.f64 	%fd602, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd603, %fd601, %fd598, %fd602;
	mov.f64 	%fd604, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd605, %fd603, %fd598, %fd604;
	mov.f64 	%fd606, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd607, %fd605, %fd598, %fd606;
	mov.f64 	%fd608, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd609, %fd607, %fd598, %fd608;
	mov.f64 	%fd610, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd611, %fd609, %fd598, %fd610;
	mov.f64 	%fd612, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd613, %fd611, %fd598, %fd612;
	mov.f64 	%fd614, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd615, %fd613, %fd598, %fd614;
	mov.f64 	%fd616, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd617, %fd615, %fd598, %fd616;
	mov.f64 	%fd618, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd619, %fd617, %fd598, %fd618;
	mov.f64 	%fd620, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd621, %fd619, %fd598, %fd620;
	mov.f64 	%fd622, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd623, %fd621, %fd598, %fd622;
	mov.f64 	%fd624, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd625, %fd623, %fd598, %fd624;
	mov.f64 	%fd626, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd627, %fd625, %fd598, %fd626;
	mov.f64 	%fd628, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd629, %fd627, %fd598, %fd628;
	mov.f64 	%fd630, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd631, %fd629, %fd598, %fd630;
	mov.f64 	%fd632, 0d3FC99999999840D2;
	fma.rn.f64 	%fd633, %fd631, %fd598, %fd632;
	mov.f64 	%fd634, 0dBFD555555555544C;
	fma.rn.f64 	%fd635, %fd633, %fd598, %fd634;
	mul.f64 	%fd636, %fd598, %fd635;
	fma.rn.f64 	%fd637, %fd636, %fd597, %fd597;
	mov.f64 	%fd638, 0d3FF921FB54442D18;
	sub.f64 	%fd639, %fd638, %fd637;
	setp.gt.f64	%p18, %fd20, %fd19;
	selp.f64	%fd640, %fd639, %fd637, %p18;
	mov.f64 	%fd641, 0d400921FB54442D18;
	sub.f64 	%fd642, %fd641, %fd640;
	selp.f64	%fd643, %fd642, %fd640, %p17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r169, %temp}, %fd643;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r170}, %fd643;
	}
	or.b32  	%r171, %r170, %r4;
	mov.b64 	%fd644, {%r169, %r171};
	add.f64 	%fd645, %fd19, %fd20;
	setp.gtu.f64	%p19, %fd645, 0d7FF0000000000000;
	selp.f64	%fd3291, %fd645, %fd644, %p19;
	mov.f64 	%fd3393, 0d0000000000000000;
	bra.uni 	BB50_24;

BB50_20:
	setp.lt.s32	%p39, %r7, 0;
	min.f64 	%fd717, %fd33, %fd32;
	max.f64 	%fd718, %fd33, %fd32;
	div.rn.f64 	%fd719, %fd717, %fd718;
	mul.f64 	%fd720, %fd719, %fd719;
	mov.f64 	%fd721, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd722, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd723, %fd722, %fd720, %fd721;
	mov.f64 	%fd724, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd725, %fd723, %fd720, %fd724;
	mov.f64 	%fd726, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd727, %fd725, %fd720, %fd726;
	mov.f64 	%fd728, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd729, %fd727, %fd720, %fd728;
	mov.f64 	%fd730, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd731, %fd729, %fd720, %fd730;
	mov.f64 	%fd732, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd733, %fd731, %fd720, %fd732;
	mov.f64 	%fd734, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd735, %fd733, %fd720, %fd734;
	mov.f64 	%fd736, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd737, %fd735, %fd720, %fd736;
	mov.f64 	%fd738, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd739, %fd737, %fd720, %fd738;
	mov.f64 	%fd740, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd741, %fd739, %fd720, %fd740;
	mov.f64 	%fd742, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd743, %fd741, %fd720, %fd742;
	mov.f64 	%fd744, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd745, %fd743, %fd720, %fd744;
	mov.f64 	%fd746, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd747, %fd745, %fd720, %fd746;
	mov.f64 	%fd748, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd749, %fd747, %fd720, %fd748;
	mov.f64 	%fd750, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd751, %fd749, %fd720, %fd750;
	mov.f64 	%fd752, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd753, %fd751, %fd720, %fd752;
	mov.f64 	%fd754, 0d3FC99999999840D2;
	fma.rn.f64 	%fd755, %fd753, %fd720, %fd754;
	mov.f64 	%fd756, 0dBFD555555555544C;
	fma.rn.f64 	%fd757, %fd755, %fd720, %fd756;
	mul.f64 	%fd758, %fd720, %fd757;
	fma.rn.f64 	%fd759, %fd758, %fd719, %fd719;
	mov.f64 	%fd760, 0d3FF921FB54442D18;
	sub.f64 	%fd761, %fd760, %fd759;
	setp.gt.f64	%p40, %fd33, %fd32;
	selp.f64	%fd762, %fd761, %fd759, %p40;
	mov.f64 	%fd763, 0d400921FB54442D18;
	sub.f64 	%fd764, %fd763, %fd762;
	selp.f64	%fd765, %fd764, %fd762, %p39;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r189, %temp}, %fd765;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r190}, %fd765;
	}
	or.b32  	%r191, %r190, %r8;
	mov.b64 	%fd766, {%r189, %r191};
	add.f64 	%fd767, %fd32, %fd33;
	setp.gtu.f64	%p41, %fd767, 0d7FF0000000000000;
	selp.f64	%fd3291, %fd767, %fd766, %p41;

BB50_24:
	mov.f64 	%fd37, %fd3393;
	mul.f64 	%fd39, %fd3291, 0d3FE0000000000000;
	abs.f64 	%fd40, %fd37;
	setp.neu.f64	%p44, %fd40, 0d7FF0000000000000;
	mov.f64 	%fd3392, %fd37;
	@%p44 bra 	BB50_26;

	mov.f64 	%fd770, 0d0000000000000000;
	mul.rn.f64 	%fd41, %fd37, %fd770;
	mov.f64 	%fd3392, %fd41;

BB50_26:
	mov.f64 	%fd42, %fd3392;
	mul.f64 	%fd771, %fd42, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r628, %fd771;
	st.local.u32 	[%rd1], %r628;
	cvt.rn.f64.s32	%fd772, %r628;
	neg.f64 	%fd773, %fd772;
	mov.f64 	%fd774, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd775, %fd773, %fd774, %fd42;
	mov.f64 	%fd776, 0d3C91A62633145C00;
	fma.rn.f64 	%fd777, %fd773, %fd776, %fd775;
	mov.f64 	%fd778, 0d397B839A252049C0;
	fma.rn.f64 	%fd3292, %fd773, %fd778, %fd777;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r198}, %fd42;
	}
	and.b32  	%r199, %r198, 2145386496;
	setp.lt.u32	%p45, %r199, 1105199104;
	@%p45 bra 	BB50_28;

	// Callseq Start 26
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd42;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3292, [retval0+0];
	
	//{
	}// Callseq End 26
	ld.local.u32 	%r628, [%rd1];

BB50_28:
	mul.rn.f64 	%fd779, %fd3292, %fd3292;
	mov.f64 	%fd780, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd781, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd782, %fd781, %fd779, %fd780;
	mov.f64 	%fd783, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd784, %fd782, %fd779, %fd783;
	mov.f64 	%fd785, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd786, %fd784, %fd779, %fd785;
	mov.f64 	%fd787, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd788, %fd786, %fd779, %fd787;
	mov.f64 	%fd789, 0d3FA5555555555551;
	fma.rn.f64 	%fd790, %fd788, %fd779, %fd789;
	mov.f64 	%fd791, 0dBFE0000000000000;
	fma.rn.f64 	%fd792, %fd790, %fd779, %fd791;
	mov.f64 	%fd793, 0d3FF0000000000000;
	fma.rn.f64 	%fd794, %fd792, %fd779, %fd793;
	mov.f64 	%fd795, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd796, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd797, %fd796, %fd779, %fd795;
	mov.f64 	%fd798, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd799, %fd797, %fd779, %fd798;
	mov.f64 	%fd800, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd801, %fd799, %fd779, %fd800;
	mov.f64 	%fd802, 0d3F81111111110818;
	fma.rn.f64 	%fd803, %fd801, %fd779, %fd802;
	mov.f64 	%fd804, 0dBFC5555555555554;
	fma.rn.f64 	%fd805, %fd803, %fd779, %fd804;
	mov.f64 	%fd806, 0d0000000000000000;
	fma.rn.f64 	%fd807, %fd805, %fd779, %fd806;
	fma.rn.f64 	%fd808, %fd807, %fd3292, %fd3292;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r200}, %fd808;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r201, %temp}, %fd808;
	}
	xor.b32  	%r202, %r200, -2147483648;
	mov.b64 	%fd809, {%r201, %r202};
	and.b32  	%r203, %r628, 1;
	setp.eq.b32	%p46, %r203, 1;
	not.pred 	%p47, %p46;
	selp.f64	%fd3293, %fd794, %fd809, %p47;
	and.b32  	%r204, %r628, 2;
	setp.eq.s32	%p48, %r204, 0;
	@%p48 bra 	BB50_30;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r205}, %fd3293;
	}
	xor.b32  	%r206, %r205, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r207, %temp}, %fd3293;
	}
	mov.b64 	%fd3293, {%r207, %r206};

BB50_30:
	abs.f64 	%fd49, %fd39;
	setp.neu.f64	%p49, %fd49, 0d7FF0000000000000;
	mov.f64 	%fd3350, %fd39;
	@%p49 bra 	BB50_32;

	mul.rn.f64 	%fd50, %fd39, %fd806;
	mov.f64 	%fd3350, %fd50;

BB50_32:
	mov.f64 	%fd51, %fd3350;
	mov.f64 	%fd2929, 0d397B839A252049C0;
	mov.f64 	%fd2894, 0d3C91A62633145C00;
	mov.f64 	%fd2893, 0d3FF921FB54442D18;
	mul.f64 	%fd811, %fd51, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r629, %fd811;
	st.local.u32 	[%rd1], %r629;
	cvt.rn.f64.s32	%fd812, %r629;
	neg.f64 	%fd813, %fd812;
	fma.rn.f64 	%fd815, %fd813, %fd2893, %fd51;
	fma.rn.f64 	%fd817, %fd813, %fd2894, %fd815;
	fma.rn.f64 	%fd3294, %fd813, %fd2929, %fd817;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r208}, %fd51;
	}
	and.b32  	%r209, %r208, 2145386496;
	setp.lt.u32	%p50, %r209, 1105199104;
	@%p50 bra 	BB50_34;

	// Callseq Start 27
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd51;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3294, [retval0+0];
	
	//{
	}// Callseq End 27
	ld.local.u32 	%r629, [%rd1];

BB50_34:
	mov.f64 	%fd3232, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3231, 0d3FF0000000000000;
	mov.f64 	%fd3209, 0dBFE0000000000000;
	mov.f64 	%fd3208, 0d3FA5555555555551;
	mov.f64 	%fd3154, 0dBF56C16C16C15D47;
	mov.f64 	%fd3115, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3114, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3072, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3071, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd819, %fd3294, %fd3294;
	fma.rn.f64 	%fd822, %fd3071, %fd819, %fd3072;
	fma.rn.f64 	%fd824, %fd822, %fd819, %fd3114;
	fma.rn.f64 	%fd826, %fd824, %fd819, %fd3115;
	fma.rn.f64 	%fd828, %fd826, %fd819, %fd3154;
	fma.rn.f64 	%fd830, %fd828, %fd819, %fd3208;
	fma.rn.f64 	%fd832, %fd830, %fd819, %fd3209;
	fma.rn.f64 	%fd834, %fd832, %fd819, %fd3231;
	fma.rn.f64 	%fd837, %fd3232, %fd819, %fd795;
	fma.rn.f64 	%fd839, %fd837, %fd819, %fd798;
	fma.rn.f64 	%fd841, %fd839, %fd819, %fd800;
	fma.rn.f64 	%fd843, %fd841, %fd819, %fd802;
	fma.rn.f64 	%fd845, %fd843, %fd819, %fd804;
	fma.rn.f64 	%fd847, %fd845, %fd819, %fd806;
	fma.rn.f64 	%fd848, %fd847, %fd3294, %fd3294;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r210}, %fd848;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r211, %temp}, %fd848;
	}
	xor.b32  	%r212, %r210, -2147483648;
	mov.b64 	%fd849, {%r211, %r212};
	and.b32  	%r213, %r629, 1;
	setp.eq.b32	%p51, %r213, 1;
	not.pred 	%p52, %p51;
	selp.f64	%fd3295, %fd834, %fd849, %p52;
	and.b32  	%r214, %r629, 2;
	setp.eq.s32	%p53, %r214, 0;
	@%p53 bra 	BB50_36;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r215}, %fd3295;
	}
	xor.b32  	%r216, %r215, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r217, %temp}, %fd3295;
	}
	mov.b64 	%fd3295, {%r217, %r216};

BB50_36:
	mul.f64 	%fd58, %fd3293, %fd3295;
	abs.f64 	%fd59, %fd16;
	setp.neu.f64	%p54, %fd59, 0d7FF0000000000000;
	mov.f64 	%fd3367, %fd16;
	@%p54 bra 	BB50_38;

	mul.rn.f64 	%fd60, %fd16, %fd806;
	mov.f64 	%fd3367, %fd60;

BB50_38:
	mov.f64 	%fd61, %fd3367;
	mov.f64 	%fd2897, 0d397B839A252049C0;
	mov.f64 	%fd2896, 0d3C91A62633145C00;
	mov.f64 	%fd2895, 0d3FF921FB54442D18;
	mul.f64 	%fd851, %fd61, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r630, %fd851;
	st.local.u32 	[%rd1], %r630;
	cvt.rn.f64.s32	%fd852, %r630;
	neg.f64 	%fd853, %fd852;
	fma.rn.f64 	%fd855, %fd853, %fd2895, %fd61;
	fma.rn.f64 	%fd857, %fd853, %fd2896, %fd855;
	fma.rn.f64 	%fd3296, %fd853, %fd2897, %fd857;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r218}, %fd61;
	}
	and.b32  	%r219, %r218, 2145386496;
	setp.lt.u32	%p55, %r219, 1105199104;
	@%p55 bra 	BB50_40;

	// Callseq Start 28
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd61;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3296, [retval0+0];
	
	//{
	}// Callseq End 28
	ld.local.u32 	%r630, [%rd1];

BB50_40:
	mov.f64 	%fd3259, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3234, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3233, 0d3FF0000000000000;
	mov.f64 	%fd3211, 0dBFE0000000000000;
	mov.f64 	%fd3210, 0d3FA5555555555551;
	mov.f64 	%fd3155, 0dBF56C16C16C15D47;
	mov.f64 	%fd3117, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3116, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3074, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3073, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd859, %fd3296, %fd3296;
	fma.rn.f64 	%fd862, %fd3073, %fd859, %fd3074;
	fma.rn.f64 	%fd864, %fd862, %fd859, %fd3116;
	fma.rn.f64 	%fd866, %fd864, %fd859, %fd3117;
	fma.rn.f64 	%fd868, %fd866, %fd859, %fd3155;
	fma.rn.f64 	%fd870, %fd868, %fd859, %fd3210;
	fma.rn.f64 	%fd872, %fd870, %fd859, %fd3211;
	fma.rn.f64 	%fd874, %fd872, %fd859, %fd3233;
	fma.rn.f64 	%fd877, %fd3234, %fd859, %fd3259;
	fma.rn.f64 	%fd879, %fd877, %fd859, %fd798;
	fma.rn.f64 	%fd881, %fd879, %fd859, %fd800;
	fma.rn.f64 	%fd883, %fd881, %fd859, %fd802;
	fma.rn.f64 	%fd885, %fd883, %fd859, %fd804;
	fma.rn.f64 	%fd887, %fd885, %fd859, %fd806;
	fma.rn.f64 	%fd888, %fd887, %fd3296, %fd3296;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r220}, %fd888;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r221, %temp}, %fd888;
	}
	xor.b32  	%r222, %r220, -2147483648;
	mov.b64 	%fd889, {%r221, %r222};
	and.b32  	%r223, %r630, 1;
	setp.eq.b32	%p56, %r223, 1;
	not.pred 	%p57, %p56;
	selp.f64	%fd3297, %fd874, %fd889, %p57;
	and.b32  	%r224, %r630, 2;
	setp.eq.s32	%p58, %r224, 0;
	@%p58 bra 	BB50_42;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r225}, %fd3297;
	}
	xor.b32  	%r226, %r225, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r227, %temp}, %fd3297;
	}
	mov.b64 	%fd3297, {%r227, %r226};

BB50_42:
	mul.f64 	%fd68, %fd58, %fd3297;
	mov.f64 	%fd3391, %fd37;
	@%p44 bra 	BB50_44;

	mul.rn.f64 	%fd3391, %fd37, %fd806;

BB50_44:
	mov.f64 	%fd2898, 0d397B839A252049C0;
	mov.f64 	%fd2886, 0d3C91A62633145C00;
	mov.f64 	%fd2885, 0d3FF921FB54442D18;
	mul.f64 	%fd891, %fd3391, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r631, %fd891;
	st.local.u32 	[%rd1], %r631;
	cvt.rn.f64.s32	%fd892, %r631;
	neg.f64 	%fd893, %fd892;
	fma.rn.f64 	%fd895, %fd893, %fd2885, %fd3391;
	fma.rn.f64 	%fd897, %fd893, %fd2886, %fd895;
	fma.rn.f64 	%fd3298, %fd893, %fd2898, %fd897;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r228}, %fd3391;
	}
	and.b32  	%r229, %r228, 2145386496;
	setp.lt.u32	%p60, %r229, 1105199104;
	@%p60 bra 	BB50_46;

	// Callseq Start 29
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3391;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3298, [retval0+0];
	
	//{
	}// Callseq End 29
	ld.local.u32 	%r631, [%rd1];

BB50_46:
	mov.f64 	%fd3246, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3215, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3214, 0d3FF0000000000000;
	mov.f64 	%fd3213, 0dBFE0000000000000;
	mov.f64 	%fd3212, 0d3FA5555555555551;
	mov.f64 	%fd3145, 0dBF56C16C16C15D47;
	mov.f64 	%fd3113, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3112, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3070, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3069, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd899, %fd3298, %fd3298;
	fma.rn.f64 	%fd902, %fd3069, %fd899, %fd3070;
	fma.rn.f64 	%fd904, %fd902, %fd899, %fd3112;
	fma.rn.f64 	%fd906, %fd904, %fd899, %fd3113;
	fma.rn.f64 	%fd908, %fd906, %fd899, %fd3145;
	fma.rn.f64 	%fd910, %fd908, %fd899, %fd3212;
	fma.rn.f64 	%fd912, %fd910, %fd899, %fd3213;
	fma.rn.f64 	%fd914, %fd912, %fd899, %fd3214;
	fma.rn.f64 	%fd917, %fd3215, %fd899, %fd3246;
	fma.rn.f64 	%fd919, %fd917, %fd899, %fd798;
	fma.rn.f64 	%fd921, %fd919, %fd899, %fd800;
	fma.rn.f64 	%fd923, %fd921, %fd899, %fd802;
	fma.rn.f64 	%fd925, %fd923, %fd899, %fd804;
	fma.rn.f64 	%fd927, %fd925, %fd899, %fd806;
	fma.rn.f64 	%fd928, %fd927, %fd3298, %fd3298;
	and.b32  	%r230, %r631, 1;
	setp.eq.b32	%p61, %r230, 1;
	not.pred 	%p62, %p61;
	selp.f64	%fd3299, %fd928, %fd914, %p62;
	and.b32  	%r231, %r631, 2;
	setp.eq.s32	%p63, %r231, 0;
	@%p63 bra 	BB50_48;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r232}, %fd3299;
	}
	xor.b32  	%r233, %r232, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r234, %temp}, %fd3299;
	}
	mov.b64 	%fd3299, {%r234, %r233};

BB50_48:
	mov.f64 	%fd3349, %fd39;
	@%p49 bra 	BB50_50;

	mul.rn.f64 	%fd3349, %fd39, %fd806;

BB50_50:
	mov.f64 	%fd2899, 0d397B839A252049C0;
	mov.f64 	%fd2888, 0d3C91A62633145C00;
	mov.f64 	%fd2887, 0d3FF921FB54442D18;
	mul.f64 	%fd930, %fd3349, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r632, %fd930;
	st.local.u32 	[%rd1], %r632;
	cvt.rn.f64.s32	%fd931, %r632;
	neg.f64 	%fd932, %fd931;
	fma.rn.f64 	%fd934, %fd932, %fd2887, %fd3349;
	fma.rn.f64 	%fd936, %fd932, %fd2888, %fd934;
	fma.rn.f64 	%fd3300, %fd932, %fd2899, %fd936;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r235}, %fd3349;
	}
	and.b32  	%r236, %r235, 2145386496;
	setp.lt.u32	%p65, %r236, 1105199104;
	@%p65 bra 	BB50_52;

	// Callseq Start 30
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3349;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3300, [retval0+0];
	
	//{
	}// Callseq End 30
	ld.local.u32 	%r632, [%rd1];

BB50_52:
	mov.f64 	%fd3247, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3217, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3216, 0d3FF0000000000000;
	mov.f64 	%fd3187, 0dBFE0000000000000;
	mov.f64 	%fd3186, 0d3FA5555555555551;
	mov.f64 	%fd3118, 0dBF56C16C16C15D47;
	mov.f64 	%fd3076, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3075, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3031, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3030, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd938, %fd3300, %fd3300;
	fma.rn.f64 	%fd941, %fd3030, %fd938, %fd3031;
	fma.rn.f64 	%fd943, %fd941, %fd938, %fd3075;
	fma.rn.f64 	%fd945, %fd943, %fd938, %fd3076;
	fma.rn.f64 	%fd947, %fd945, %fd938, %fd3118;
	fma.rn.f64 	%fd949, %fd947, %fd938, %fd3186;
	fma.rn.f64 	%fd951, %fd949, %fd938, %fd3187;
	fma.rn.f64 	%fd953, %fd951, %fd938, %fd3216;
	fma.rn.f64 	%fd956, %fd3217, %fd938, %fd3247;
	fma.rn.f64 	%fd958, %fd956, %fd938, %fd798;
	fma.rn.f64 	%fd960, %fd958, %fd938, %fd800;
	fma.rn.f64 	%fd962, %fd960, %fd938, %fd802;
	fma.rn.f64 	%fd964, %fd962, %fd938, %fd804;
	fma.rn.f64 	%fd966, %fd964, %fd938, %fd806;
	fma.rn.f64 	%fd967, %fd966, %fd3300, %fd3300;
	and.b32  	%r237, %r632, 1;
	setp.eq.b32	%p66, %r237, 1;
	not.pred 	%p67, %p66;
	selp.f64	%fd3301, %fd967, %fd953, %p67;
	and.b32  	%r238, %r632, 2;
	setp.eq.s32	%p68, %r238, 0;
	@%p68 bra 	BB50_54;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r239}, %fd3301;
	}
	xor.b32  	%r240, %r239, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r241, %temp}, %fd3301;
	}
	mov.b64 	%fd3301, {%r241, %r240};

BB50_54:
	mul.f64 	%fd85, %fd3299, %fd3301;
	mov.f64 	%fd3366, %fd16;
	@%p54 bra 	BB50_56;

	mul.rn.f64 	%fd3366, %fd16, %fd806;

BB50_56:
	mov.f64 	%fd2891, 0d397B839A252049C0;
	mov.f64 	%fd2890, 0d3C91A62633145C00;
	mov.f64 	%fd2889, 0d3FF921FB54442D18;
	mul.f64 	%fd969, %fd3366, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r633, %fd969;
	st.local.u32 	[%rd1], %r633;
	cvt.rn.f64.s32	%fd970, %r633;
	neg.f64 	%fd971, %fd970;
	fma.rn.f64 	%fd973, %fd971, %fd2889, %fd3366;
	fma.rn.f64 	%fd975, %fd971, %fd2890, %fd973;
	fma.rn.f64 	%fd3302, %fd971, %fd2891, %fd975;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r242}, %fd3366;
	}
	and.b32  	%r243, %r242, 2145386496;
	setp.lt.u32	%p70, %r243, 1105199104;
	@%p70 bra 	BB50_58;

	// Callseq Start 31
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3366;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3302, [retval0+0];
	
	//{
	}// Callseq End 31
	ld.local.u32 	%r633, [%rd1];

BB50_58:
	mov.f64 	%fd3235, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3191, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3190, 0d3FF0000000000000;
	mov.f64 	%fd3189, 0dBFE0000000000000;
	mov.f64 	%fd3188, 0d3FA5555555555551;
	mov.f64 	%fd3119, 0dBF56C16C16C15D47;
	mov.f64 	%fd3077, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3028, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3027, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3026, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd977, %fd3302, %fd3302;
	fma.rn.f64 	%fd980, %fd3026, %fd977, %fd3027;
	fma.rn.f64 	%fd982, %fd980, %fd977, %fd3028;
	fma.rn.f64 	%fd984, %fd982, %fd977, %fd3077;
	fma.rn.f64 	%fd986, %fd984, %fd977, %fd3119;
	fma.rn.f64 	%fd988, %fd986, %fd977, %fd3188;
	fma.rn.f64 	%fd990, %fd988, %fd977, %fd3189;
	fma.rn.f64 	%fd992, %fd990, %fd977, %fd3190;
	fma.rn.f64 	%fd995, %fd3191, %fd977, %fd3235;
	fma.rn.f64 	%fd997, %fd995, %fd977, %fd798;
	fma.rn.f64 	%fd999, %fd997, %fd977, %fd800;
	fma.rn.f64 	%fd1001, %fd999, %fd977, %fd802;
	fma.rn.f64 	%fd1003, %fd1001, %fd977, %fd804;
	fma.rn.f64 	%fd1005, %fd1003, %fd977, %fd806;
	fma.rn.f64 	%fd1006, %fd1005, %fd3302, %fd3302;
	and.b32  	%r244, %r633, 1;
	setp.eq.b32	%p71, %r244, 1;
	not.pred 	%p72, %p71;
	selp.f64	%fd3303, %fd1006, %fd992, %p72;
	and.b32  	%r245, %r633, 2;
	setp.eq.s32	%p73, %r245, 0;
	@%p73 bra 	BB50_60;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r246}, %fd3303;
	}
	xor.b32  	%r247, %r246, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r248, %temp}, %fd3303;
	}
	mov.b64 	%fd3303, {%r248, %r247};

BB50_60:
	fma.rn.f64 	%fd94, %fd85, %fd3303, %fd68;
	mov.f64 	%fd3390, %fd37;
	@%p44 bra 	BB50_62;

	mul.rn.f64 	%fd3390, %fd37, %fd806;

BB50_62:
	mov.f64 	%fd2892, 0d397B839A252049C0;
	mov.f64 	%fd2860, 0d3C91A62633145C00;
	mov.f64 	%fd2859, 0d3FF921FB54442D18;
	mul.f64 	%fd1008, %fd3390, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r634, %fd1008;
	st.local.u32 	[%rd1], %r634;
	cvt.rn.f64.s32	%fd1009, %r634;
	neg.f64 	%fd1010, %fd1009;
	fma.rn.f64 	%fd1012, %fd1010, %fd2859, %fd3390;
	fma.rn.f64 	%fd1014, %fd1010, %fd2860, %fd1012;
	fma.rn.f64 	%fd3304, %fd1010, %fd2892, %fd1014;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r249}, %fd3390;
	}
	and.b32  	%r250, %r249, 2145386496;
	setp.lt.u32	%p75, %r250, 1105199104;
	@%p75 bra 	BB50_64;

	// Callseq Start 32
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3390;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3304, [retval0+0];
	
	//{
	}// Callseq End 32
	ld.local.u32 	%r634, [%rd1];

BB50_64:
	mov.f64 	%fd3218, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3193, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3192, 0d3FF0000000000000;
	mov.f64 	%fd3157, 0dBFE0000000000000;
	mov.f64 	%fd3156, 0d3FA5555555555551;
	mov.f64 	%fd3078, 0dBF56C16C16C15D47;
	mov.f64 	%fd3032, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3029, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2978, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2977, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1016, %fd3304, %fd3304;
	fma.rn.f64 	%fd1019, %fd2977, %fd1016, %fd2978;
	fma.rn.f64 	%fd1021, %fd1019, %fd1016, %fd3029;
	fma.rn.f64 	%fd1023, %fd1021, %fd1016, %fd3032;
	fma.rn.f64 	%fd1025, %fd1023, %fd1016, %fd3078;
	fma.rn.f64 	%fd1027, %fd1025, %fd1016, %fd3156;
	fma.rn.f64 	%fd1029, %fd1027, %fd1016, %fd3157;
	fma.rn.f64 	%fd1031, %fd1029, %fd1016, %fd3192;
	fma.rn.f64 	%fd1034, %fd3193, %fd1016, %fd3218;
	fma.rn.f64 	%fd1036, %fd1034, %fd1016, %fd798;
	fma.rn.f64 	%fd1038, %fd1036, %fd1016, %fd800;
	fma.rn.f64 	%fd1040, %fd1038, %fd1016, %fd802;
	fma.rn.f64 	%fd1042, %fd1040, %fd1016, %fd804;
	fma.rn.f64 	%fd1044, %fd1042, %fd1016, %fd806;
	fma.rn.f64 	%fd1045, %fd1044, %fd3304, %fd3304;
	and.b32  	%r251, %r634, 1;
	setp.eq.b32	%p76, %r251, 1;
	not.pred 	%p77, %p76;
	selp.f64	%fd3305, %fd1045, %fd1031, %p77;
	and.b32  	%r252, %r634, 2;
	setp.eq.s32	%p78, %r252, 0;
	@%p78 bra 	BB50_66;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r253}, %fd3305;
	}
	xor.b32  	%r254, %r253, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r255, %temp}, %fd3305;
	}
	mov.b64 	%fd3305, {%r255, %r254};

BB50_66:
	mov.f64 	%fd3348, %fd39;
	@%p49 bra 	BB50_68;

	mul.rn.f64 	%fd3348, %fd39, %fd806;

BB50_68:
	mov.f64 	%fd2865, 0d397B839A252049C0;
	mov.f64 	%fd2862, 0d3C91A62633145C00;
	mov.f64 	%fd2861, 0d3FF921FB54442D18;
	mul.f64 	%fd1047, %fd3348, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r635, %fd1047;
	st.local.u32 	[%rd1], %r635;
	cvt.rn.f64.s32	%fd1048, %r635;
	neg.f64 	%fd1049, %fd1048;
	fma.rn.f64 	%fd1051, %fd1049, %fd2861, %fd3348;
	fma.rn.f64 	%fd1053, %fd1049, %fd2862, %fd1051;
	fma.rn.f64 	%fd3306, %fd1049, %fd2865, %fd1053;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r256}, %fd3348;
	}
	and.b32  	%r257, %r256, 2145386496;
	setp.lt.u32	%p80, %r257, 1105199104;
	@%p80 bra 	BB50_70;

	// Callseq Start 33
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3348;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3306, [retval0+0];
	
	//{
	}// Callseq End 33
	ld.local.u32 	%r635, [%rd1];

BB50_70:
	mov.f64 	%fd3219, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3161, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3160, 0d3FF0000000000000;
	mov.f64 	%fd3159, 0dBFE0000000000000;
	mov.f64 	%fd3158, 0d3FA5555555555551;
	mov.f64 	%fd3079, 0dBF56C16C16C15D47;
	mov.f64 	%fd2982, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2981, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2980, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2979, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1055, %fd3306, %fd3306;
	fma.rn.f64 	%fd1058, %fd2979, %fd1055, %fd2980;
	fma.rn.f64 	%fd1060, %fd1058, %fd1055, %fd2981;
	fma.rn.f64 	%fd1062, %fd1060, %fd1055, %fd2982;
	fma.rn.f64 	%fd1064, %fd1062, %fd1055, %fd3079;
	fma.rn.f64 	%fd1066, %fd1064, %fd1055, %fd3158;
	fma.rn.f64 	%fd1068, %fd1066, %fd1055, %fd3159;
	fma.rn.f64 	%fd1070, %fd1068, %fd1055, %fd3160;
	fma.rn.f64 	%fd1073, %fd3161, %fd1055, %fd3219;
	fma.rn.f64 	%fd1075, %fd1073, %fd1055, %fd798;
	fma.rn.f64 	%fd1077, %fd1075, %fd1055, %fd800;
	fma.rn.f64 	%fd1079, %fd1077, %fd1055, %fd802;
	fma.rn.f64 	%fd1081, %fd1079, %fd1055, %fd804;
	fma.rn.f64 	%fd1083, %fd1081, %fd1055, %fd806;
	fma.rn.f64 	%fd1084, %fd1083, %fd3306, %fd3306;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r258}, %fd1084;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r259, %temp}, %fd1084;
	}
	xor.b32  	%r260, %r258, -2147483648;
	mov.b64 	%fd1085, {%r259, %r260};
	and.b32  	%r261, %r635, 1;
	setp.eq.b32	%p81, %r261, 1;
	not.pred 	%p82, %p81;
	selp.f64	%fd3307, %fd1070, %fd1085, %p82;
	and.b32  	%r262, %r635, 2;
	setp.eq.s32	%p83, %r262, 0;
	@%p83 bra 	BB50_72;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r263}, %fd3307;
	}
	xor.b32  	%r264, %r263, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r265, %temp}, %fd3307;
	}
	mov.b64 	%fd3307, {%r265, %r264};

BB50_72:
	mul.f64 	%fd111, %fd3305, %fd3307;
	mov.f64 	%fd3365, %fd16;
	@%p54 bra 	BB50_74;

	mul.rn.f64 	%fd3365, %fd16, %fd806;

BB50_74:
	mov.f64 	%fd2866, 0d397B839A252049C0;
	mov.f64 	%fd2864, 0d3C91A62633145C00;
	mov.f64 	%fd2863, 0d3FF921FB54442D18;
	mul.f64 	%fd1087, %fd3365, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r636, %fd1087;
	st.local.u32 	[%rd1], %r636;
	cvt.rn.f64.s32	%fd1088, %r636;
	neg.f64 	%fd1089, %fd1088;
	fma.rn.f64 	%fd1091, %fd1089, %fd2863, %fd3365;
	fma.rn.f64 	%fd1093, %fd1089, %fd2864, %fd1091;
	fma.rn.f64 	%fd3308, %fd1089, %fd2866, %fd1093;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r266}, %fd3365;
	}
	and.b32  	%r267, %r266, 2145386496;
	setp.lt.u32	%p85, %r267, 1105199104;
	@%p85 bra 	BB50_76;

	// Callseq Start 34
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3365;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3308, [retval0+0];
	
	//{
	}// Callseq End 34
	ld.local.u32 	%r636, [%rd1];

BB50_76:
	mov.f64 	%fd3195, 0d3EC71DE369ACE392;
	mov.f64 	%fd3194, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3163, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3162, 0d3FF0000000000000;
	mov.f64 	%fd3147, 0dBFE0000000000000;
	mov.f64 	%fd3146, 0d3FA5555555555551;
	mov.f64 	%fd3033, 0dBF56C16C16C15D47;
	mov.f64 	%fd2984, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2983, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2970, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2969, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1095, %fd3308, %fd3308;
	fma.rn.f64 	%fd1098, %fd2969, %fd1095, %fd2970;
	fma.rn.f64 	%fd1100, %fd1098, %fd1095, %fd2983;
	fma.rn.f64 	%fd1102, %fd1100, %fd1095, %fd2984;
	fma.rn.f64 	%fd1104, %fd1102, %fd1095, %fd3033;
	fma.rn.f64 	%fd1106, %fd1104, %fd1095, %fd3146;
	fma.rn.f64 	%fd1108, %fd1106, %fd1095, %fd3147;
	fma.rn.f64 	%fd1110, %fd1108, %fd1095, %fd3162;
	fma.rn.f64 	%fd1113, %fd3163, %fd1095, %fd3194;
	fma.rn.f64 	%fd1115, %fd1113, %fd1095, %fd3195;
	fma.rn.f64 	%fd1117, %fd1115, %fd1095, %fd800;
	fma.rn.f64 	%fd1119, %fd1117, %fd1095, %fd802;
	fma.rn.f64 	%fd1121, %fd1119, %fd1095, %fd804;
	fma.rn.f64 	%fd1123, %fd1121, %fd1095, %fd806;
	fma.rn.f64 	%fd1124, %fd1123, %fd3308, %fd3308;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r268}, %fd1124;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r269, %temp}, %fd1124;
	}
	xor.b32  	%r270, %r268, -2147483648;
	mov.b64 	%fd1125, {%r269, %r270};
	and.b32  	%r271, %r636, 1;
	setp.eq.b32	%p86, %r271, 1;
	not.pred 	%p87, %p86;
	selp.f64	%fd3309, %fd1110, %fd1125, %p87;
	and.b32  	%r272, %r636, 2;
	setp.eq.s32	%p88, %r272, 0;
	@%p88 bra 	BB50_78;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r273}, %fd3309;
	}
	xor.b32  	%r274, %r273, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r275, %temp}, %fd3309;
	}
	mov.b64 	%fd3309, {%r275, %r274};

BB50_78:
	mul.f64 	%fd120, %fd111, %fd3309;
	mov.f64 	%fd3389, %fd37;
	@%p44 bra 	BB50_80;

	mul.rn.f64 	%fd3389, %fd37, %fd806;

BB50_80:
	mov.f64 	%fd2867, 0d397B839A252049C0;
	mov.f64 	%fd2849, 0d3C91A62633145C00;
	mov.f64 	%fd2848, 0d3FF921FB54442D18;
	mul.f64 	%fd1127, %fd3389, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r637, %fd1127;
	st.local.u32 	[%rd1], %r637;
	cvt.rn.f64.s32	%fd1128, %r637;
	neg.f64 	%fd1129, %fd1128;
	fma.rn.f64 	%fd1131, %fd1129, %fd2848, %fd3389;
	fma.rn.f64 	%fd1133, %fd1129, %fd2849, %fd1131;
	fma.rn.f64 	%fd3310, %fd1129, %fd2867, %fd1133;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r276}, %fd3389;
	}
	and.b32  	%r277, %r276, 2145386496;
	setp.lt.u32	%p90, %r277, 1105199104;
	@%p90 bra 	BB50_82;

	// Callseq Start 35
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3389;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3310, [retval0+0];
	
	//{
	}// Callseq End 35
	ld.local.u32 	%r637, [%rd1];

BB50_82:
	mov.f64 	%fd3197, 0d3EC71DE369ACE392;
	mov.f64 	%fd3196, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3151, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3150, 0d3FF0000000000000;
	mov.f64 	%fd3149, 0dBFE0000000000000;
	mov.f64 	%fd3148, 0d3FA5555555555551;
	mov.f64 	%fd3034, 0dBF56C16C16C15D47;
	mov.f64 	%fd2974, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2973, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2972, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2971, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1135, %fd3310, %fd3310;
	fma.rn.f64 	%fd1138, %fd2971, %fd1135, %fd2972;
	fma.rn.f64 	%fd1140, %fd1138, %fd1135, %fd2973;
	fma.rn.f64 	%fd1142, %fd1140, %fd1135, %fd2974;
	fma.rn.f64 	%fd1144, %fd1142, %fd1135, %fd3034;
	fma.rn.f64 	%fd1146, %fd1144, %fd1135, %fd3148;
	fma.rn.f64 	%fd1148, %fd1146, %fd1135, %fd3149;
	fma.rn.f64 	%fd1150, %fd1148, %fd1135, %fd3150;
	fma.rn.f64 	%fd1153, %fd3151, %fd1135, %fd3196;
	fma.rn.f64 	%fd1155, %fd1153, %fd1135, %fd3197;
	fma.rn.f64 	%fd1157, %fd1155, %fd1135, %fd800;
	fma.rn.f64 	%fd1159, %fd1157, %fd1135, %fd802;
	fma.rn.f64 	%fd1161, %fd1159, %fd1135, %fd804;
	fma.rn.f64 	%fd1163, %fd1161, %fd1135, %fd806;
	fma.rn.f64 	%fd1164, %fd1163, %fd3310, %fd3310;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r278}, %fd1164;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r279, %temp}, %fd1164;
	}
	xor.b32  	%r280, %r278, -2147483648;
	mov.b64 	%fd1165, {%r279, %r280};
	and.b32  	%r281, %r637, 1;
	setp.eq.b32	%p91, %r281, 1;
	not.pred 	%p92, %p91;
	selp.f64	%fd3311, %fd1150, %fd1165, %p92;
	and.b32  	%r282, %r637, 2;
	setp.eq.s32	%p93, %r282, 0;
	@%p93 bra 	BB50_84;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r283}, %fd3311;
	}
	xor.b32  	%r284, %r283, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r285, %temp}, %fd3311;
	}
	mov.b64 	%fd3311, {%r285, %r284};

BB50_84:
	mov.f64 	%fd3347, %fd39;
	@%p49 bra 	BB50_86;

	mul.rn.f64 	%fd3347, %fd39, %fd806;

BB50_86:
	mov.f64 	%fd2854, 0d397B839A252049C0;
	mov.f64 	%fd2851, 0d3C91A62633145C00;
	mov.f64 	%fd2850, 0d3FF921FB54442D18;
	mul.f64 	%fd1167, %fd3347, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r638, %fd1167;
	st.local.u32 	[%rd1], %r638;
	cvt.rn.f64.s32	%fd1168, %r638;
	neg.f64 	%fd1169, %fd1168;
	fma.rn.f64 	%fd1171, %fd1169, %fd2850, %fd3347;
	fma.rn.f64 	%fd1173, %fd1169, %fd2851, %fd1171;
	fma.rn.f64 	%fd3312, %fd1169, %fd2854, %fd1173;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r286}, %fd3347;
	}
	and.b32  	%r287, %r286, 2145386496;
	setp.lt.u32	%p95, %r287, 1105199104;
	@%p95 bra 	BB50_88;

	// Callseq Start 36
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3347;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3312, [retval0+0];
	
	//{
	}// Callseq End 36
	ld.local.u32 	%r638, [%rd1];

BB50_88:
	mov.f64 	%fd3165, 0d3EC71DE369ACE392;
	mov.f64 	%fd3164, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3153, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3152, 0d3FF0000000000000;
	mov.f64 	%fd3121, 0dBFE0000000000000;
	mov.f64 	%fd3120, 0d3FA5555555555551;
	mov.f64 	%fd2985, 0dBF56C16C16C15D47;
	mov.f64 	%fd2976, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2975, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2923, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2922, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1175, %fd3312, %fd3312;
	fma.rn.f64 	%fd1178, %fd2922, %fd1175, %fd2923;
	fma.rn.f64 	%fd1180, %fd1178, %fd1175, %fd2975;
	fma.rn.f64 	%fd1182, %fd1180, %fd1175, %fd2976;
	fma.rn.f64 	%fd1184, %fd1182, %fd1175, %fd2985;
	fma.rn.f64 	%fd1186, %fd1184, %fd1175, %fd3120;
	fma.rn.f64 	%fd1188, %fd1186, %fd1175, %fd3121;
	fma.rn.f64 	%fd1190, %fd1188, %fd1175, %fd3152;
	fma.rn.f64 	%fd1193, %fd3153, %fd1175, %fd3164;
	fma.rn.f64 	%fd1195, %fd1193, %fd1175, %fd3165;
	fma.rn.f64 	%fd1197, %fd1195, %fd1175, %fd800;
	fma.rn.f64 	%fd1199, %fd1197, %fd1175, %fd802;
	fma.rn.f64 	%fd1201, %fd1199, %fd1175, %fd804;
	fma.rn.f64 	%fd1203, %fd1201, %fd1175, %fd806;
	fma.rn.f64 	%fd1204, %fd1203, %fd3312, %fd3312;
	and.b32  	%r288, %r638, 1;
	setp.eq.b32	%p96, %r288, 1;
	not.pred 	%p97, %p96;
	selp.f64	%fd3313, %fd1204, %fd1190, %p97;
	and.b32  	%r289, %r638, 2;
	setp.eq.s32	%p98, %r289, 0;
	@%p98 bra 	BB50_90;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r290}, %fd3313;
	}
	xor.b32  	%r291, %r290, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r292, %temp}, %fd3313;
	}
	mov.b64 	%fd3313, {%r292, %r291};

BB50_90:
	mul.f64 	%fd137, %fd3311, %fd3313;
	mov.f64 	%fd3364, %fd16;
	@%p54 bra 	BB50_92;

	mul.rn.f64 	%fd3364, %fd16, %fd806;

BB50_92:
	mov.f64 	%fd2855, 0d397B839A252049C0;
	mov.f64 	%fd2853, 0d3C91A62633145C00;
	mov.f64 	%fd2852, 0d3FF921FB54442D18;
	mul.f64 	%fd1206, %fd3364, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r639, %fd1206;
	st.local.u32 	[%rd1], %r639;
	cvt.rn.f64.s32	%fd1207, %r639;
	neg.f64 	%fd1208, %fd1207;
	fma.rn.f64 	%fd1210, %fd1208, %fd2852, %fd3364;
	fma.rn.f64 	%fd1212, %fd1208, %fd2853, %fd1210;
	fma.rn.f64 	%fd3314, %fd1208, %fd2855, %fd1212;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r293}, %fd3364;
	}
	and.b32  	%r294, %r293, 2145386496;
	setp.lt.u32	%p100, %r294, 1105199104;
	@%p100 bra 	BB50_94;

	// Callseq Start 37
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3364;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3314, [retval0+0];
	
	//{
	}// Callseq End 37
	ld.local.u32 	%r639, [%rd1];

BB50_94:
	mov.f64 	%fd3166, 0d3EC71DE369ACE392;
	mov.f64 	%fd3126, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3125, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3124, 0d3FF0000000000000;
	mov.f64 	%fd3123, 0dBFE0000000000000;
	mov.f64 	%fd3122, 0d3FA5555555555551;
	mov.f64 	%fd2986, 0dBF56C16C16C15D47;
	mov.f64 	%fd2927, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2926, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2925, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2924, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1214, %fd3314, %fd3314;
	fma.rn.f64 	%fd1217, %fd2924, %fd1214, %fd2925;
	fma.rn.f64 	%fd1219, %fd1217, %fd1214, %fd2926;
	fma.rn.f64 	%fd1221, %fd1219, %fd1214, %fd2927;
	fma.rn.f64 	%fd1223, %fd1221, %fd1214, %fd2986;
	fma.rn.f64 	%fd1225, %fd1223, %fd1214, %fd3122;
	fma.rn.f64 	%fd1227, %fd1225, %fd1214, %fd3123;
	fma.rn.f64 	%fd1229, %fd1227, %fd1214, %fd3124;
	fma.rn.f64 	%fd1232, %fd3125, %fd1214, %fd3126;
	fma.rn.f64 	%fd1234, %fd1232, %fd1214, %fd3166;
	fma.rn.f64 	%fd1236, %fd1234, %fd1214, %fd800;
	fma.rn.f64 	%fd1238, %fd1236, %fd1214, %fd802;
	fma.rn.f64 	%fd1240, %fd1238, %fd1214, %fd804;
	fma.rn.f64 	%fd1242, %fd1240, %fd1214, %fd806;
	fma.rn.f64 	%fd1243, %fd1242, %fd3314, %fd3314;
	and.b32  	%r295, %r639, 1;
	setp.eq.b32	%p101, %r295, 1;
	not.pred 	%p102, %p101;
	selp.f64	%fd3315, %fd1243, %fd1229, %p102;
	and.b32  	%r296, %r639, 2;
	setp.eq.s32	%p103, %r296, 0;
	@%p103 bra 	BB50_96;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r297}, %fd3315;
	}
	xor.b32  	%r298, %r297, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r299, %temp}, %fd3315;
	}
	mov.b64 	%fd3315, {%r299, %r298};

BB50_96:
	mul.f64 	%fd1244, %fd137, %fd3315;
	sub.f64 	%fd146, %fd120, %fd1244;
	mov.f64 	%fd3388, %fd37;
	@%p44 bra 	BB50_98;

	mul.rn.f64 	%fd3388, %fd37, %fd806;

BB50_98:
	mov.f64 	%fd2856, 0d397B839A252049C0;
	mov.f64 	%fd2827, 0d3C91A62633145C00;
	mov.f64 	%fd2826, 0d3FF921FB54442D18;
	mul.f64 	%fd1246, %fd3388, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r640, %fd1246;
	st.local.u32 	[%rd1], %r640;
	cvt.rn.f64.s32	%fd1247, %r640;
	neg.f64 	%fd1248, %fd1247;
	fma.rn.f64 	%fd1250, %fd1248, %fd2826, %fd3388;
	fma.rn.f64 	%fd1252, %fd1248, %fd2827, %fd1250;
	fma.rn.f64 	%fd3316, %fd1248, %fd2856, %fd1252;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r300}, %fd3388;
	}
	and.b32  	%r301, %r300, 2145386496;
	setp.lt.u32	%p105, %r301, 1105199104;
	@%p105 bra 	BB50_100;

	// Callseq Start 38
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3388;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3316, [retval0+0];
	
	//{
	}// Callseq End 38
	ld.local.u32 	%r640, [%rd1];

BB50_100:
	mov.f64 	%fd3129, 0d3EC71DE369ACE392;
	mov.f64 	%fd3128, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3127, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3082, 0d3FF0000000000000;
	mov.f64 	%fd3081, 0dBFE0000000000000;
	mov.f64 	%fd3080, 0d3FA5555555555551;
	mov.f64 	%fd2930, 0dBF56C16C16C15D47;
	mov.f64 	%fd2928, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2902, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2901, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2900, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1254, %fd3316, %fd3316;
	fma.rn.f64 	%fd1257, %fd2900, %fd1254, %fd2901;
	fma.rn.f64 	%fd1259, %fd1257, %fd1254, %fd2902;
	fma.rn.f64 	%fd1261, %fd1259, %fd1254, %fd2928;
	fma.rn.f64 	%fd1263, %fd1261, %fd1254, %fd2930;
	fma.rn.f64 	%fd1265, %fd1263, %fd1254, %fd3080;
	fma.rn.f64 	%fd1267, %fd1265, %fd1254, %fd3081;
	fma.rn.f64 	%fd1269, %fd1267, %fd1254, %fd3082;
	fma.rn.f64 	%fd1272, %fd3127, %fd1254, %fd3128;
	fma.rn.f64 	%fd1274, %fd1272, %fd1254, %fd3129;
	fma.rn.f64 	%fd1276, %fd1274, %fd1254, %fd800;
	fma.rn.f64 	%fd1278, %fd1276, %fd1254, %fd802;
	fma.rn.f64 	%fd1280, %fd1278, %fd1254, %fd804;
	fma.rn.f64 	%fd1282, %fd1280, %fd1254, %fd806;
	fma.rn.f64 	%fd1283, %fd1282, %fd3316, %fd3316;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r302}, %fd1283;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r303, %temp}, %fd1283;
	}
	xor.b32  	%r304, %r302, -2147483648;
	mov.b64 	%fd1284, {%r303, %r304};
	and.b32  	%r305, %r640, 1;
	setp.eq.b32	%p106, %r305, 1;
	not.pred 	%p107, %p106;
	selp.f64	%fd3317, %fd1269, %fd1284, %p107;
	and.b32  	%r306, %r640, 2;
	setp.eq.s32	%p108, %r306, 0;
	@%p108 bra 	BB50_102;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r307}, %fd3317;
	}
	xor.b32  	%r308, %r307, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r309, %temp}, %fd3317;
	}
	mov.b64 	%fd3317, {%r309, %r308};

BB50_102:
	mov.f64 	%fd3346, %fd39;
	@%p49 bra 	BB50_104;

	mul.rn.f64 	%fd3346, %fd39, %fd806;

BB50_104:
	mov.f64 	%fd2832, 0d397B839A252049C0;
	mov.f64 	%fd2829, 0d3C91A62633145C00;
	mov.f64 	%fd2828, 0d3FF921FB54442D18;
	mul.f64 	%fd1286, %fd3346, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r641, %fd1286;
	st.local.u32 	[%rd1], %r641;
	cvt.rn.f64.s32	%fd1287, %r641;
	neg.f64 	%fd1288, %fd1287;
	fma.rn.f64 	%fd1290, %fd1288, %fd2828, %fd3346;
	fma.rn.f64 	%fd1292, %fd1288, %fd2829, %fd1290;
	fma.rn.f64 	%fd3318, %fd1288, %fd2832, %fd1292;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r310}, %fd3346;
	}
	and.b32  	%r311, %r310, 2145386496;
	setp.lt.u32	%p110, %r311, 1105199104;
	@%p110 bra 	BB50_106;

	// Callseq Start 39
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3346;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3318, [retval0+0];
	
	//{
	}// Callseq End 39
	ld.local.u32 	%r641, [%rd1];

BB50_106:
	mov.f64 	%fd3088, 0d3EC71DE369ACE392;
	mov.f64 	%fd3087, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3086, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3085, 0d3FF0000000000000;
	mov.f64 	%fd3084, 0dBFE0000000000000;
	mov.f64 	%fd3083, 0d3FA5555555555551;
	mov.f64 	%fd2931, 0dBF56C16C16C15D47;
	mov.f64 	%fd2906, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2905, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2904, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2903, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1294, %fd3318, %fd3318;
	fma.rn.f64 	%fd1297, %fd2903, %fd1294, %fd2904;
	fma.rn.f64 	%fd1299, %fd1297, %fd1294, %fd2905;
	fma.rn.f64 	%fd1301, %fd1299, %fd1294, %fd2906;
	fma.rn.f64 	%fd1303, %fd1301, %fd1294, %fd2931;
	fma.rn.f64 	%fd1305, %fd1303, %fd1294, %fd3083;
	fma.rn.f64 	%fd1307, %fd1305, %fd1294, %fd3084;
	fma.rn.f64 	%fd1309, %fd1307, %fd1294, %fd3085;
	fma.rn.f64 	%fd1312, %fd3086, %fd1294, %fd3087;
	fma.rn.f64 	%fd1314, %fd1312, %fd1294, %fd3088;
	fma.rn.f64 	%fd1316, %fd1314, %fd1294, %fd800;
	fma.rn.f64 	%fd1318, %fd1316, %fd1294, %fd802;
	fma.rn.f64 	%fd1320, %fd1318, %fd1294, %fd804;
	fma.rn.f64 	%fd1322, %fd1320, %fd1294, %fd806;
	fma.rn.f64 	%fd1323, %fd1322, %fd3318, %fd3318;
	and.b32  	%r312, %r641, 1;
	setp.eq.b32	%p111, %r312, 1;
	not.pred 	%p112, %p111;
	selp.f64	%fd3319, %fd1323, %fd1309, %p112;
	and.b32  	%r313, %r641, 2;
	setp.eq.s32	%p113, %r313, 0;
	@%p113 bra 	BB50_108;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r314}, %fd3319;
	}
	xor.b32  	%r315, %r314, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r316, %temp}, %fd3319;
	}
	mov.b64 	%fd3319, {%r316, %r315};

BB50_108:
	mul.f64 	%fd163, %fd3317, %fd3319;
	mov.f64 	%fd3363, %fd16;
	@%p54 bra 	BB50_110;

	mul.rn.f64 	%fd3363, %fd16, %fd806;

BB50_110:
	mov.f64 	%fd2833, 0d397B839A252049C0;
	mov.f64 	%fd2831, 0d3C91A62633145C00;
	mov.f64 	%fd2830, 0d3FF921FB54442D18;
	mul.f64 	%fd1325, %fd3363, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r642, %fd1325;
	st.local.u32 	[%rd1], %r642;
	cvt.rn.f64.s32	%fd1326, %r642;
	neg.f64 	%fd1327, %fd1326;
	fma.rn.f64 	%fd1329, %fd1327, %fd2830, %fd3363;
	fma.rn.f64 	%fd1331, %fd1327, %fd2831, %fd1329;
	fma.rn.f64 	%fd3320, %fd1327, %fd2833, %fd1331;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r317}, %fd3363;
	}
	and.b32  	%r318, %r317, 2145386496;
	setp.lt.u32	%p115, %r318, 1105199104;
	@%p115 bra 	BB50_112;

	// Callseq Start 40
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3363;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3320, [retval0+0];
	
	//{
	}// Callseq End 40
	ld.local.u32 	%r642, [%rd1];

BB50_112:
	mov.f64 	%fd3090, 0d3EC71DE369ACE392;
	mov.f64 	%fd3089, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3038, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3037, 0d3FF0000000000000;
	mov.f64 	%fd3036, 0dBFE0000000000000;
	mov.f64 	%fd3035, 0d3FA5555555555551;
	mov.f64 	%fd2907, 0dBF56C16C16C15D47;
	mov.f64 	%fd2871, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2870, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2869, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2868, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1333, %fd3320, %fd3320;
	fma.rn.f64 	%fd1336, %fd2868, %fd1333, %fd2869;
	fma.rn.f64 	%fd1338, %fd1336, %fd1333, %fd2870;
	fma.rn.f64 	%fd1340, %fd1338, %fd1333, %fd2871;
	fma.rn.f64 	%fd1342, %fd1340, %fd1333, %fd2907;
	fma.rn.f64 	%fd1344, %fd1342, %fd1333, %fd3035;
	fma.rn.f64 	%fd1346, %fd1344, %fd1333, %fd3036;
	fma.rn.f64 	%fd1348, %fd1346, %fd1333, %fd3037;
	fma.rn.f64 	%fd1351, %fd3038, %fd1333, %fd3089;
	fma.rn.f64 	%fd1353, %fd1351, %fd1333, %fd3090;
	fma.rn.f64 	%fd1355, %fd1353, %fd1333, %fd800;
	fma.rn.f64 	%fd1357, %fd1355, %fd1333, %fd802;
	fma.rn.f64 	%fd1359, %fd1357, %fd1333, %fd804;
	fma.rn.f64 	%fd1361, %fd1359, %fd1333, %fd806;
	fma.rn.f64 	%fd1362, %fd1361, %fd3320, %fd3320;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r319}, %fd1362;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r320, %temp}, %fd1362;
	}
	xor.b32  	%r321, %r319, -2147483648;
	mov.b64 	%fd1363, {%r320, %r321};
	and.b32  	%r322, %r642, 1;
	setp.eq.b32	%p116, %r322, 1;
	not.pred 	%p117, %p116;
	selp.f64	%fd3321, %fd1348, %fd1363, %p117;
	and.b32  	%r323, %r642, 2;
	setp.eq.s32	%p118, %r323, 0;
	@%p118 bra 	BB50_114;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r324}, %fd3321;
	}
	xor.b32  	%r325, %r324, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r326, %temp}, %fd3321;
	}
	mov.b64 	%fd3321, {%r326, %r325};

BB50_114:
	mul.f64 	%fd172, %fd163, %fd3321;
	mov.f64 	%fd3387, %fd37;
	@%p44 bra 	BB50_116;

	mul.rn.f64 	%fd3387, %fd37, %fd806;

BB50_116:
	mov.f64 	%fd2814, 0d397B839A252049C0;
	mov.f64 	%fd2813, 0d3C91A62633145C00;
	mov.f64 	%fd2812, 0d3FF921FB54442D18;
	mul.f64 	%fd1365, %fd3387, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r643, %fd1365;
	st.local.u32 	[%rd1], %r643;
	cvt.rn.f64.s32	%fd1366, %r643;
	neg.f64 	%fd1367, %fd1366;
	fma.rn.f64 	%fd1369, %fd1367, %fd2812, %fd3387;
	fma.rn.f64 	%fd1371, %fd1367, %fd2813, %fd1369;
	fma.rn.f64 	%fd3322, %fd1367, %fd2814, %fd1371;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r327}, %fd3387;
	}
	and.b32  	%r328, %r327, 2145386496;
	setp.lt.u32	%p120, %r328, 1105199104;
	@%p120 bra 	BB50_118;

	// Callseq Start 41
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3387;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3322, [retval0+0];
	
	//{
	}// Callseq End 41
	ld.local.u32 	%r643, [%rd1];

BB50_118:
	mov.f64 	%fd3045, 0dBF2A01A019DB62A1;
	mov.f64 	%fd3044, 0d3EC71DE369ACE392;
	mov.f64 	%fd3043, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3042, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3041, 0d3FF0000000000000;
	mov.f64 	%fd3040, 0dBFE0000000000000;
	mov.f64 	%fd3039, 0d3FA5555555555551;
	mov.f64 	%fd2908, 0dBF56C16C16C15D47;
	mov.f64 	%fd2875, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2874, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2873, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2872, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1373, %fd3322, %fd3322;
	fma.rn.f64 	%fd1376, %fd2872, %fd1373, %fd2873;
	fma.rn.f64 	%fd1378, %fd1376, %fd1373, %fd2874;
	fma.rn.f64 	%fd1380, %fd1378, %fd1373, %fd2875;
	fma.rn.f64 	%fd1382, %fd1380, %fd1373, %fd2908;
	fma.rn.f64 	%fd1384, %fd1382, %fd1373, %fd3039;
	fma.rn.f64 	%fd1386, %fd1384, %fd1373, %fd3040;
	fma.rn.f64 	%fd1388, %fd1386, %fd1373, %fd3041;
	fma.rn.f64 	%fd1391, %fd3042, %fd1373, %fd3043;
	fma.rn.f64 	%fd1393, %fd1391, %fd1373, %fd3044;
	fma.rn.f64 	%fd1395, %fd1393, %fd1373, %fd3045;
	fma.rn.f64 	%fd1397, %fd1395, %fd1373, %fd802;
	fma.rn.f64 	%fd1399, %fd1397, %fd1373, %fd804;
	fma.rn.f64 	%fd1401, %fd1399, %fd1373, %fd806;
	fma.rn.f64 	%fd1402, %fd1401, %fd3322, %fd3322;
	and.b32  	%r329, %r643, 1;
	setp.eq.b32	%p121, %r329, 1;
	not.pred 	%p122, %p121;
	selp.f64	%fd3323, %fd1402, %fd1388, %p122;
	and.b32  	%r330, %r643, 2;
	setp.eq.s32	%p123, %r330, 0;
	@%p123 bra 	BB50_120;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r331}, %fd3323;
	}
	xor.b32  	%r332, %r331, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r333, %temp}, %fd3323;
	}
	mov.b64 	%fd3323, {%r333, %r332};

BB50_120:
	mov.f64 	%fd3345, %fd39;
	@%p49 bra 	BB50_122;

	mul.rn.f64 	%fd3345, %fd39, %fd806;

BB50_122:
	mov.f64 	%fd2817, 0d397B839A252049C0;
	mov.f64 	%fd2816, 0d3C91A62633145C00;
	mov.f64 	%fd2815, 0d3FF921FB54442D18;
	mul.f64 	%fd1404, %fd3345, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r644, %fd1404;
	st.local.u32 	[%rd1], %r644;
	cvt.rn.f64.s32	%fd1405, %r644;
	neg.f64 	%fd1406, %fd1405;
	fma.rn.f64 	%fd1408, %fd1406, %fd2815, %fd3345;
	fma.rn.f64 	%fd1410, %fd1406, %fd2816, %fd1408;
	fma.rn.f64 	%fd3324, %fd1406, %fd2817, %fd1410;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r334}, %fd3345;
	}
	and.b32  	%r335, %r334, 2145386496;
	setp.lt.u32	%p125, %r335, 1105199104;
	@%p125 bra 	BB50_124;

	// Callseq Start 42
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3345;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3324, [retval0+0];
	
	//{
	}// Callseq End 42
	ld.local.u32 	%r644, [%rd1];

BB50_124:
	mov.f64 	%fd3047, 0dBF2A01A019DB62A1;
	mov.f64 	%fd3046, 0d3EC71DE369ACE392;
	mov.f64 	%fd2991, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2990, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2989, 0d3FF0000000000000;
	mov.f64 	%fd2988, 0dBFE0000000000000;
	mov.f64 	%fd2987, 0d3FA5555555555551;
	mov.f64 	%fd2876, 0dBF56C16C16C15D47;
	mov.f64 	%fd2837, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2836, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2835, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2834, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1412, %fd3324, %fd3324;
	fma.rn.f64 	%fd1415, %fd2834, %fd1412, %fd2835;
	fma.rn.f64 	%fd1417, %fd1415, %fd1412, %fd2836;
	fma.rn.f64 	%fd1419, %fd1417, %fd1412, %fd2837;
	fma.rn.f64 	%fd1421, %fd1419, %fd1412, %fd2876;
	fma.rn.f64 	%fd1423, %fd1421, %fd1412, %fd2987;
	fma.rn.f64 	%fd1425, %fd1423, %fd1412, %fd2988;
	fma.rn.f64 	%fd1427, %fd1425, %fd1412, %fd2989;
	fma.rn.f64 	%fd1430, %fd2990, %fd1412, %fd2991;
	fma.rn.f64 	%fd1432, %fd1430, %fd1412, %fd3046;
	fma.rn.f64 	%fd1434, %fd1432, %fd1412, %fd3047;
	fma.rn.f64 	%fd1436, %fd1434, %fd1412, %fd802;
	fma.rn.f64 	%fd1438, %fd1436, %fd1412, %fd804;
	fma.rn.f64 	%fd1440, %fd1438, %fd1412, %fd806;
	fma.rn.f64 	%fd1441, %fd1440, %fd3324, %fd3324;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r336}, %fd1441;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r337, %temp}, %fd1441;
	}
	xor.b32  	%r338, %r336, -2147483648;
	mov.b64 	%fd1442, {%r337, %r338};
	and.b32  	%r339, %r644, 1;
	setp.eq.b32	%p126, %r339, 1;
	not.pred 	%p127, %p126;
	selp.f64	%fd3325, %fd1427, %fd1442, %p127;
	and.b32  	%r340, %r644, 2;
	setp.eq.s32	%p128, %r340, 0;
	@%p128 bra 	BB50_126;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r341}, %fd3325;
	}
	xor.b32  	%r342, %r341, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r343, %temp}, %fd3325;
	}
	mov.b64 	%fd3325, {%r343, %r342};

BB50_126:
	mul.f64 	%fd189, %fd3323, %fd3325;
	mov.f64 	%fd3362, %fd16;
	@%p54 bra 	BB50_128;

	mul.rn.f64 	%fd3362, %fd16, %fd806;

BB50_128:
	mov.f64 	%fd2820, 0d397B839A252049C0;
	mov.f64 	%fd2819, 0d3C91A62633145C00;
	mov.f64 	%fd2818, 0d3FF921FB54442D18;
	mul.f64 	%fd1444, %fd3362, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r645, %fd1444;
	st.local.u32 	[%rd1], %r645;
	cvt.rn.f64.s32	%fd1445, %r645;
	neg.f64 	%fd1446, %fd1445;
	fma.rn.f64 	%fd1448, %fd1446, %fd2818, %fd3362;
	fma.rn.f64 	%fd1450, %fd1446, %fd2819, %fd1448;
	fma.rn.f64 	%fd3326, %fd1446, %fd2820, %fd1450;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r344}, %fd3362;
	}
	and.b32  	%r345, %r344, 2145386496;
	setp.lt.u32	%p130, %r345, 1105199104;
	@%p130 bra 	BB50_130;

	// Callseq Start 43
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3362;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3326, [retval0+0];
	
	//{
	}// Callseq End 43
	ld.local.u32 	%r645, [%rd1];

BB50_130:
	mov.f64 	%fd2998, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2997, 0d3EC71DE369ACE392;
	mov.f64 	%fd2996, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2995, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2994, 0d3FF0000000000000;
	mov.f64 	%fd2993, 0dBFE0000000000000;
	mov.f64 	%fd2992, 0d3FA5555555555551;
	mov.f64 	%fd2877, 0dBF56C16C16C15D47;
	mov.f64 	%fd2824, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2823, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2822, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2821, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1452, %fd3326, %fd3326;
	fma.rn.f64 	%fd1455, %fd2821, %fd1452, %fd2822;
	fma.rn.f64 	%fd1457, %fd1455, %fd1452, %fd2823;
	fma.rn.f64 	%fd1459, %fd1457, %fd1452, %fd2824;
	fma.rn.f64 	%fd1461, %fd1459, %fd1452, %fd2877;
	fma.rn.f64 	%fd1463, %fd1461, %fd1452, %fd2992;
	fma.rn.f64 	%fd1465, %fd1463, %fd1452, %fd2993;
	fma.rn.f64 	%fd1467, %fd1465, %fd1452, %fd2994;
	fma.rn.f64 	%fd1470, %fd2995, %fd1452, %fd2996;
	fma.rn.f64 	%fd1472, %fd1470, %fd1452, %fd2997;
	fma.rn.f64 	%fd1474, %fd1472, %fd1452, %fd2998;
	fma.rn.f64 	%fd1476, %fd1474, %fd1452, %fd802;
	fma.rn.f64 	%fd1478, %fd1476, %fd1452, %fd804;
	fma.rn.f64 	%fd1480, %fd1478, %fd1452, %fd806;
	fma.rn.f64 	%fd1481, %fd1480, %fd3326, %fd3326;
	and.b32  	%r346, %r645, 1;
	setp.eq.b32	%p131, %r346, 1;
	not.pred 	%p132, %p131;
	selp.f64	%fd3327, %fd1481, %fd1467, %p132;
	and.b32  	%r347, %r645, 2;
	setp.eq.s32	%p133, %r347, 0;
	@%p133 bra 	BB50_132;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r348}, %fd3327;
	}
	xor.b32  	%r349, %r348, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r350, %temp}, %fd3327;
	}
	mov.b64 	%fd3327, {%r350, %r349};

BB50_132:
	mul.f64 	%fd1482, %fd189, %fd3327;
	sub.f64 	%fd198, %fd172, %fd1482;
	mov.f64 	%fd3386, %fd37;
	@%p44 bra 	BB50_134;

	mul.rn.f64 	%fd3386, %fd37, %fd806;

BB50_134:
	mov.f64 	%fd2781, 0d397B839A252049C0;
	mov.f64 	%fd2780, 0d3C91A62633145C00;
	mov.f64 	%fd2779, 0d3FF921FB54442D18;
	mul.f64 	%fd1484, %fd3386, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r646, %fd1484;
	st.local.u32 	[%rd1], %r646;
	cvt.rn.f64.s32	%fd1485, %r646;
	neg.f64 	%fd1486, %fd1485;
	fma.rn.f64 	%fd1488, %fd1486, %fd2779, %fd3386;
	fma.rn.f64 	%fd1490, %fd1486, %fd2780, %fd1488;
	fma.rn.f64 	%fd3328, %fd1486, %fd2781, %fd1490;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r351}, %fd3386;
	}
	and.b32  	%r352, %r351, 2145386496;
	setp.lt.u32	%p135, %r352, 1105199104;
	@%p135 bra 	BB50_136;

	// Callseq Start 44
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3386;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3328, [retval0+0];
	
	//{
	}// Callseq End 44
	ld.local.u32 	%r646, [%rd1];

BB50_136:
	mov.f64 	%fd3000, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2999, 0d3EC71DE369ACE392;
	mov.f64 	%fd2936, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2935, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2934, 0d3FF0000000000000;
	mov.f64 	%fd2933, 0dBFE0000000000000;
	mov.f64 	%fd2932, 0d3FA5555555555551;
	mov.f64 	%fd2838, 0dBF56C16C16C15D47;
	mov.f64 	%fd2825, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2784, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2783, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2782, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1492, %fd3328, %fd3328;
	fma.rn.f64 	%fd1495, %fd2782, %fd1492, %fd2783;
	fma.rn.f64 	%fd1497, %fd1495, %fd1492, %fd2784;
	fma.rn.f64 	%fd1499, %fd1497, %fd1492, %fd2825;
	fma.rn.f64 	%fd1501, %fd1499, %fd1492, %fd2838;
	fma.rn.f64 	%fd1503, %fd1501, %fd1492, %fd2932;
	fma.rn.f64 	%fd1505, %fd1503, %fd1492, %fd2933;
	fma.rn.f64 	%fd1507, %fd1505, %fd1492, %fd2934;
	fma.rn.f64 	%fd1510, %fd2935, %fd1492, %fd2936;
	fma.rn.f64 	%fd1512, %fd1510, %fd1492, %fd2999;
	fma.rn.f64 	%fd1514, %fd1512, %fd1492, %fd3000;
	fma.rn.f64 	%fd1516, %fd1514, %fd1492, %fd802;
	fma.rn.f64 	%fd1518, %fd1516, %fd1492, %fd804;
	fma.rn.f64 	%fd1520, %fd1518, %fd1492, %fd806;
	fma.rn.f64 	%fd1521, %fd1520, %fd3328, %fd3328;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r353}, %fd1521;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r354, %temp}, %fd1521;
	}
	xor.b32  	%r355, %r353, -2147483648;
	mov.b64 	%fd1522, {%r354, %r355};
	and.b32  	%r356, %r646, 1;
	setp.eq.b32	%p136, %r356, 1;
	not.pred 	%p137, %p136;
	selp.f64	%fd3329, %fd1507, %fd1522, %p137;
	and.b32  	%r357, %r646, 2;
	setp.eq.s32	%p138, %r357, 0;
	@%p138 bra 	BB50_138;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r358}, %fd3329;
	}
	xor.b32  	%r359, %r358, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r360, %temp}, %fd3329;
	}
	mov.b64 	%fd3329, {%r360, %r359};

BB50_138:
	mov.f64 	%fd3344, %fd39;
	@%p49 bra 	BB50_140;

	mul.rn.f64 	%fd3344, %fd39, %fd806;

BB50_140:
	mov.f64 	%fd2787, 0d397B839A252049C0;
	mov.f64 	%fd2786, 0d3C91A62633145C00;
	mov.f64 	%fd2785, 0d3FF921FB54442D18;
	mul.f64 	%fd1524, %fd3344, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r647, %fd1524;
	st.local.u32 	[%rd1], %r647;
	cvt.rn.f64.s32	%fd1525, %r647;
	neg.f64 	%fd1526, %fd1525;
	fma.rn.f64 	%fd1528, %fd1526, %fd2785, %fd3344;
	fma.rn.f64 	%fd1530, %fd1526, %fd2786, %fd1528;
	fma.rn.f64 	%fd3330, %fd1526, %fd2787, %fd1530;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r361}, %fd3344;
	}
	and.b32  	%r362, %r361, 2145386496;
	setp.lt.u32	%p140, %r362, 1105199104;
	@%p140 bra 	BB50_142;

	// Callseq Start 45
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3344;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3330, [retval0+0];
	
	//{
	}// Callseq End 45
	ld.local.u32 	%r647, [%rd1];

BB50_142:
	mov.f64 	%fd2943, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2942, 0d3EC71DE369ACE392;
	mov.f64 	%fd2941, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2940, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2939, 0d3FF0000000000000;
	mov.f64 	%fd2938, 0dBFE0000000000000;
	mov.f64 	%fd2937, 0d3FA5555555555551;
	mov.f64 	%fd2792, 0dBF56C16C16C15D47;
	mov.f64 	%fd2791, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2790, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2789, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2788, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1532, %fd3330, %fd3330;
	fma.rn.f64 	%fd1535, %fd2788, %fd1532, %fd2789;
	fma.rn.f64 	%fd1537, %fd1535, %fd1532, %fd2790;
	fma.rn.f64 	%fd1539, %fd1537, %fd1532, %fd2791;
	fma.rn.f64 	%fd1541, %fd1539, %fd1532, %fd2792;
	fma.rn.f64 	%fd1543, %fd1541, %fd1532, %fd2937;
	fma.rn.f64 	%fd1545, %fd1543, %fd1532, %fd2938;
	fma.rn.f64 	%fd1547, %fd1545, %fd1532, %fd2939;
	fma.rn.f64 	%fd1550, %fd2940, %fd1532, %fd2941;
	fma.rn.f64 	%fd1552, %fd1550, %fd1532, %fd2942;
	fma.rn.f64 	%fd1554, %fd1552, %fd1532, %fd2943;
	fma.rn.f64 	%fd1556, %fd1554, %fd1532, %fd802;
	fma.rn.f64 	%fd1558, %fd1556, %fd1532, %fd804;
	fma.rn.f64 	%fd1560, %fd1558, %fd1532, %fd806;
	fma.rn.f64 	%fd1561, %fd1560, %fd3330, %fd3330;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r363}, %fd1561;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r364, %temp}, %fd1561;
	}
	xor.b32  	%r365, %r363, -2147483648;
	mov.b64 	%fd1562, {%r364, %r365};
	and.b32  	%r366, %r647, 1;
	setp.eq.b32	%p141, %r366, 1;
	not.pred 	%p142, %p141;
	selp.f64	%fd3331, %fd1547, %fd1562, %p142;
	and.b32  	%r367, %r647, 2;
	setp.eq.s32	%p143, %r367, 0;
	@%p143 bra 	BB50_144;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r368}, %fd3331;
	}
	xor.b32  	%r369, %r368, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r370, %temp}, %fd3331;
	}
	mov.b64 	%fd3331, {%r370, %r369};

BB50_144:
	mul.f64 	%fd215, %fd3329, %fd3331;
	mov.f64 	%fd3361, %fd16;
	@%p54 bra 	BB50_146;

	mul.rn.f64 	%fd3361, %fd16, %fd806;

BB50_146:
	mov.f64 	%fd2768, 0d397B839A252049C0;
	mov.f64 	%fd2767, 0d3C91A62633145C00;
	mov.f64 	%fd2766, 0d3FF921FB54442D18;
	mul.f64 	%fd1564, %fd3361, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r648, %fd1564;
	st.local.u32 	[%rd1], %r648;
	cvt.rn.f64.s32	%fd1565, %r648;
	neg.f64 	%fd1566, %fd1565;
	fma.rn.f64 	%fd1568, %fd1566, %fd2766, %fd3361;
	fma.rn.f64 	%fd1570, %fd1566, %fd2767, %fd1568;
	fma.rn.f64 	%fd3332, %fd1566, %fd2768, %fd1570;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r371}, %fd3361;
	}
	and.b32  	%r372, %r371, 2145386496;
	setp.lt.u32	%p145, %r372, 1105199104;
	@%p145 bra 	BB50_148;

	// Callseq Start 46
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3361;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3332, [retval0+0];
	
	//{
	}// Callseq End 46
	ld.local.u32 	%r648, [%rd1];

BB50_148:
	mov.f64 	%fd2944, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2914, 0d3EC71DE369ACE392;
	mov.f64 	%fd2913, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2912, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2911, 0d3FF0000000000000;
	mov.f64 	%fd2910, 0dBFE0000000000000;
	mov.f64 	%fd2909, 0d3FA5555555555551;
	mov.f64 	%fd2773, 0dBF56C16C16C15D47;
	mov.f64 	%fd2772, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2771, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2770, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2769, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1572, %fd3332, %fd3332;
	fma.rn.f64 	%fd1575, %fd2769, %fd1572, %fd2770;
	fma.rn.f64 	%fd1577, %fd1575, %fd1572, %fd2771;
	fma.rn.f64 	%fd1579, %fd1577, %fd1572, %fd2772;
	fma.rn.f64 	%fd1581, %fd1579, %fd1572, %fd2773;
	fma.rn.f64 	%fd1583, %fd1581, %fd1572, %fd2909;
	fma.rn.f64 	%fd1585, %fd1583, %fd1572, %fd2910;
	fma.rn.f64 	%fd1587, %fd1585, %fd1572, %fd2911;
	fma.rn.f64 	%fd1590, %fd2912, %fd1572, %fd2913;
	fma.rn.f64 	%fd1592, %fd1590, %fd1572, %fd2914;
	fma.rn.f64 	%fd1594, %fd1592, %fd1572, %fd2944;
	fma.rn.f64 	%fd1596, %fd1594, %fd1572, %fd802;
	fma.rn.f64 	%fd1598, %fd1596, %fd1572, %fd804;
	fma.rn.f64 	%fd1600, %fd1598, %fd1572, %fd806;
	fma.rn.f64 	%fd1601, %fd1600, %fd3332, %fd3332;
	and.b32  	%r373, %r648, 1;
	setp.eq.b32	%p146, %r373, 1;
	not.pred 	%p147, %p146;
	selp.f64	%fd3333, %fd1601, %fd1587, %p147;
	and.b32  	%r374, %r648, 2;
	setp.eq.s32	%p148, %r374, 0;
	@%p148 bra 	BB50_150;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r375}, %fd3333;
	}
	xor.b32  	%r376, %r375, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r377, %temp}, %fd3333;
	}
	mov.b64 	%fd3333, {%r377, %r376};

BB50_150:
	mul.f64 	%fd224, %fd215, %fd3333;
	mov.f64 	%fd3385, %fd37;
	@%p44 bra 	BB50_152;

	mul.rn.f64 	%fd3385, %fd37, %fd806;

BB50_152:
	mov.f64 	%fd2728, 0d397B839A252049C0;
	mov.f64 	%fd2727, 0d3C91A62633145C00;
	mov.f64 	%fd2726, 0d3FF921FB54442D18;
	mul.f64 	%fd1603, %fd3385, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r649, %fd1603;
	st.local.u32 	[%rd1], %r649;
	cvt.rn.f64.s32	%fd1604, %r649;
	neg.f64 	%fd1605, %fd1604;
	fma.rn.f64 	%fd1607, %fd1605, %fd2726, %fd3385;
	fma.rn.f64 	%fd1609, %fd1605, %fd2727, %fd1607;
	fma.rn.f64 	%fd3334, %fd1605, %fd2728, %fd1609;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r378}, %fd3385;
	}
	and.b32  	%r379, %r378, 2145386496;
	setp.lt.u32	%p150, %r379, 1105199104;
	@%p150 bra 	BB50_154;

	// Callseq Start 47
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3385;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3334, [retval0+0];
	
	//{
	}// Callseq End 47
	ld.local.u32 	%r649, [%rd1];

BB50_154:
	mov.f64 	%fd2799, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2798, 0d3EC71DE369ACE392;
	mov.f64 	%fd2797, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2796, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2795, 0d3FF0000000000000;
	mov.f64 	%fd2794, 0dBFE0000000000000;
	mov.f64 	%fd2793, 0d3FA5555555555551;
	mov.f64 	%fd2778, 0dBF56C16C16C15D47;
	mov.f64 	%fd2777, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2776, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2775, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2774, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1611, %fd3334, %fd3334;
	fma.rn.f64 	%fd1614, %fd2774, %fd1611, %fd2775;
	fma.rn.f64 	%fd1616, %fd1614, %fd1611, %fd2776;
	fma.rn.f64 	%fd1618, %fd1616, %fd1611, %fd2777;
	fma.rn.f64 	%fd1620, %fd1618, %fd1611, %fd2778;
	fma.rn.f64 	%fd1622, %fd1620, %fd1611, %fd2793;
	fma.rn.f64 	%fd1624, %fd1622, %fd1611, %fd2794;
	fma.rn.f64 	%fd1626, %fd1624, %fd1611, %fd2795;
	fma.rn.f64 	%fd1629, %fd2796, %fd1611, %fd2797;
	fma.rn.f64 	%fd1631, %fd1629, %fd1611, %fd2798;
	fma.rn.f64 	%fd1633, %fd1631, %fd1611, %fd2799;
	fma.rn.f64 	%fd1635, %fd1633, %fd1611, %fd802;
	fma.rn.f64 	%fd1637, %fd1635, %fd1611, %fd804;
	fma.rn.f64 	%fd1639, %fd1637, %fd1611, %fd806;
	fma.rn.f64 	%fd1640, %fd1639, %fd3334, %fd3334;
	and.b32  	%r380, %r649, 1;
	setp.eq.b32	%p151, %r380, 1;
	not.pred 	%p152, %p151;
	selp.f64	%fd3335, %fd1640, %fd1626, %p152;
	and.b32  	%r381, %r649, 2;
	setp.eq.s32	%p153, %r381, 0;
	@%p153 bra 	BB50_156;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r382}, %fd3335;
	}
	xor.b32  	%r383, %r382, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r384, %temp}, %fd3335;
	}
	mov.b64 	%fd3335, {%r384, %r383};

BB50_156:
	mov.f64 	%fd3343, %fd39;
	@%p49 bra 	BB50_158;

	mul.rn.f64 	%fd3343, %fd39, %fd806;

BB50_158:
	mov.f64 	%fd2731, 0d397B839A252049C0;
	mov.f64 	%fd2730, 0d3C91A62633145C00;
	mov.f64 	%fd2729, 0d3FF921FB54442D18;
	mul.f64 	%fd1642, %fd3343, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r650, %fd1642;
	st.local.u32 	[%rd1], %r650;
	cvt.rn.f64.s32	%fd1643, %r650;
	neg.f64 	%fd1644, %fd1643;
	fma.rn.f64 	%fd1646, %fd1644, %fd2729, %fd3343;
	fma.rn.f64 	%fd1648, %fd1644, %fd2730, %fd1646;
	fma.rn.f64 	%fd3351, %fd1644, %fd2731, %fd1648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r385}, %fd3343;
	}
	and.b32  	%r386, %r385, 2145386496;
	setp.lt.u32	%p155, %r386, 1105199104;
	@%p155 bra 	BB50_160;

	// Callseq Start 48
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3343;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3351, [retval0+0];
	
	//{
	}// Callseq End 48
	ld.local.u32 	%r650, [%rd1];

BB50_160:
	mov.f64 	%fd2802, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2801, 0d3EC71DE369ACE392;
	mov.f64 	%fd2800, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2743, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2742, 0d3FF0000000000000;
	mov.f64 	%fd2741, 0dBFE0000000000000;
	mov.f64 	%fd2740, 0d3FA5555555555551;
	mov.f64 	%fd2739, 0dBF56C16C16C15D47;
	mov.f64 	%fd2738, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2737, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2736, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2735, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1650, %fd3351, %fd3351;
	fma.rn.f64 	%fd1653, %fd2735, %fd1650, %fd2736;
	fma.rn.f64 	%fd1655, %fd1653, %fd1650, %fd2737;
	fma.rn.f64 	%fd1657, %fd1655, %fd1650, %fd2738;
	fma.rn.f64 	%fd1659, %fd1657, %fd1650, %fd2739;
	fma.rn.f64 	%fd1661, %fd1659, %fd1650, %fd2740;
	fma.rn.f64 	%fd1663, %fd1661, %fd1650, %fd2741;
	fma.rn.f64 	%fd1665, %fd1663, %fd1650, %fd2742;
	fma.rn.f64 	%fd1668, %fd2743, %fd1650, %fd2800;
	fma.rn.f64 	%fd1670, %fd1668, %fd1650, %fd2801;
	fma.rn.f64 	%fd1672, %fd1670, %fd1650, %fd2802;
	fma.rn.f64 	%fd1674, %fd1672, %fd1650, %fd802;
	fma.rn.f64 	%fd1676, %fd1674, %fd1650, %fd804;
	fma.rn.f64 	%fd1678, %fd1676, %fd1650, %fd806;
	fma.rn.f64 	%fd1679, %fd1678, %fd3351, %fd3351;
	and.b32  	%r387, %r650, 1;
	setp.eq.b32	%p156, %r387, 1;
	not.pred 	%p157, %p156;
	selp.f64	%fd3352, %fd1679, %fd1665, %p157;
	and.b32  	%r388, %r650, 2;
	setp.eq.s32	%p158, %r388, 0;
	@%p158 bra 	BB50_162;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r389}, %fd3352;
	}
	xor.b32  	%r390, %r389, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r391, %temp}, %fd3352;
	}
	mov.b64 	%fd3352, {%r391, %r390};

BB50_162:
	mul.f64 	%fd241, %fd3335, %fd3352;
	mov.f64 	%fd3360, %fd16;
	@%p54 bra 	BB50_164;

	mul.rn.f64 	%fd3360, %fd16, %fd806;

BB50_164:
	mov.f64 	%fd2734, 0d397B839A252049C0;
	mov.f64 	%fd2733, 0d3C91A62633145C00;
	mov.f64 	%fd2732, 0d3FF921FB54442D18;
	mul.f64 	%fd1681, %fd3360, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r651, %fd1681;
	st.local.u32 	[%rd1], %r651;
	cvt.rn.f64.s32	%fd1682, %r651;
	neg.f64 	%fd1683, %fd1682;
	fma.rn.f64 	%fd1685, %fd1683, %fd2732, %fd3360;
	fma.rn.f64 	%fd1687, %fd1683, %fd2733, %fd1685;
	fma.rn.f64 	%fd3368, %fd1683, %fd2734, %fd1687;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r392}, %fd3360;
	}
	and.b32  	%r393, %r392, 2145386496;
	setp.lt.u32	%p160, %r393, 1105199104;
	@%p160 bra 	BB50_166;

	// Callseq Start 49
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3360;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3368, [retval0+0];
	
	//{
	}// Callseq End 49
	ld.local.u32 	%r651, [%rd1];

BB50_166:
	mov.f64 	%fd2756, 0d3F81111111110818;
	mov.f64 	%fd2755, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2754, 0d3EC71DE369ACE392;
	mov.f64 	%fd2753, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2752, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2751, 0d3FF0000000000000;
	mov.f64 	%fd2750, 0dBFE0000000000000;
	mov.f64 	%fd2749, 0d3FA5555555555551;
	mov.f64 	%fd2748, 0dBF56C16C16C15D47;
	mov.f64 	%fd2747, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2746, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2745, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2744, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1689, %fd3368, %fd3368;
	fma.rn.f64 	%fd1692, %fd2744, %fd1689, %fd2745;
	fma.rn.f64 	%fd1694, %fd1692, %fd1689, %fd2746;
	fma.rn.f64 	%fd1696, %fd1694, %fd1689, %fd2747;
	fma.rn.f64 	%fd1698, %fd1696, %fd1689, %fd2748;
	fma.rn.f64 	%fd1700, %fd1698, %fd1689, %fd2749;
	fma.rn.f64 	%fd1702, %fd1700, %fd1689, %fd2750;
	fma.rn.f64 	%fd1704, %fd1702, %fd1689, %fd2751;
	fma.rn.f64 	%fd1707, %fd2752, %fd1689, %fd2753;
	fma.rn.f64 	%fd1709, %fd1707, %fd1689, %fd2754;
	fma.rn.f64 	%fd1711, %fd1709, %fd1689, %fd2755;
	fma.rn.f64 	%fd1713, %fd1711, %fd1689, %fd2756;
	fma.rn.f64 	%fd1715, %fd1713, %fd1689, %fd804;
	fma.rn.f64 	%fd1717, %fd1715, %fd1689, %fd806;
	fma.rn.f64 	%fd1718, %fd1717, %fd3368, %fd3368;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r394}, %fd1718;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r395, %temp}, %fd1718;
	}
	xor.b32  	%r396, %r394, -2147483648;
	mov.b64 	%fd1719, {%r395, %r396};
	and.b32  	%r397, %r651, 1;
	setp.eq.b32	%p161, %r397, 1;
	not.pred 	%p162, %p161;
	selp.f64	%fd3369, %fd1704, %fd1719, %p162;
	and.b32  	%r398, %r651, 2;
	setp.eq.s32	%p163, %r398, 0;
	@%p163 bra 	BB50_168;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r399}, %fd3369;
	}
	xor.b32  	%r400, %r399, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r401, %temp}, %fd3369;
	}
	mov.b64 	%fd3369, {%r401, %r400};

BB50_168:
	fma.rn.f64 	%fd250, %fd241, %fd3369, %fd224;
	neg.f64 	%fd1720, %fd6;
	add.f64 	%fd1721, %fd94, %fd6;
	abs.f64 	%fd1722, %fd1721;
	abs.f64 	%fd1723, %fd1720;
	abs.f64 	%fd1724, %fd94;
	max.f64 	%fd1725, %fd1724, %fd1723;
	mul.f64 	%fd1726, %fd1725, 0d3D83880000000000;
	setp.geu.f64	%p164, %fd1722, %fd1726;
	mov.f64 	%fd3384, %fd37;
	@%p164 bra 	BB50_173;

	neg.f64 	%fd1727, %fd8;
	add.f64 	%fd1728, %fd146, %fd8;
	abs.f64 	%fd1729, %fd1728;
	abs.f64 	%fd1730, %fd1727;
	abs.f64 	%fd1731, %fd146;
	max.f64 	%fd1732, %fd1731, %fd1730;
	mul.f64 	%fd1733, %fd1732, 0d3D83880000000000;
	setp.geu.f64	%p165, %fd1729, %fd1733;
	mov.f64 	%fd3384, %fd37;
	@%p165 bra 	BB50_173;

	neg.f64 	%fd1734, %fd7;
	add.f64 	%fd1735, %fd198, %fd7;
	abs.f64 	%fd1736, %fd1735;
	abs.f64 	%fd1737, %fd1734;
	abs.f64 	%fd1738, %fd198;
	max.f64 	%fd1739, %fd1738, %fd1737;
	mul.f64 	%fd1740, %fd1739, 0d3D83880000000000;
	setp.geu.f64	%p166, %fd1736, %fd1740;
	mov.f64 	%fd3384, %fd37;
	@%p166 bra 	BB50_173;

	neg.f64 	%fd1741, %fd9;
	add.f64 	%fd1742, %fd250, %fd9;
	abs.f64 	%fd1743, %fd1742;
	abs.f64 	%fd1744, %fd1741;
	abs.f64 	%fd1745, %fd250;
	max.f64 	%fd1746, %fd1745, %fd1744;
	mul.f64 	%fd1747, %fd1746, 0d3D83880000000000;
	setp.geu.f64	%p167, %fd1743, %fd1747;
	mov.f64 	%fd3384, %fd37;
	@%p167 bra 	BB50_173;

	setp.ltu.f64	%p168, %fd37, 0d0000000000000000;
	selp.f64	%fd1748, 0dC00921FB54442D18, 0d400921FB54442D18, %p168;
	sub.f64 	%fd3384, %fd37, %fd1748;

BB50_173:
	mov.f64 	%fd3396, %fd16;
	mov.f64 	%fd3395, %fd39;
	mov.f64 	%fd3394, %fd3384;

BB50_174:
	ld.param.u32 	%r621, [_Z18actfunc_quat_multiP7double4iiii_param_1];
	cvt.rn.f64.s32	%fd1749, %r621;
	mov.f64 	%fd1750, 0d401921FB54442D18;
	div.rn.f64 	%fd261, %fd1750, %fd1749;
	mul.f64 	%fd262, %fd261, 0d3FE0000000000000;
	sub.f64 	%fd1751, %fd3394, %fd262;
	add.f64 	%fd1752, %fd1751, 0d400921FB54442D18;
	div.rn.f64 	%fd3397, %fd1752, %fd261;
	abs.f64 	%fd264, %fd3397;
	setp.ge.f64	%p169, %fd264, 0d4330000000000000;
	@%p169 bra 	BB50_176;

	add.f64 	%fd1753, %fd264, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd1754, %fd1753;
	setp.lt.f64	%p170, %fd264, 0d3FE0000000000000;
	selp.f64	%fd1755, 0d0000000000000000, %fd1754, %p170;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r402, %temp}, %fd1755;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r403}, %fd1755;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r404}, %fd3397;
	}
	and.b32  	%r405, %r404, -2147483648;
	or.b32  	%r406, %r403, %r405;
	mov.b64 	%fd3397, {%r402, %r406};

BB50_176:
	ld.param.u32 	%r622, [_Z18actfunc_quat_multiP7double4iiii_param_2];
	fma.rn.f64 	%fd1756, %fd261, %fd3397, 0dC00921FB54442D18;
	add.f64 	%fd267, %fd262, %fd1756;
	cvt.rn.f64.s32	%fd1757, %r622;
	div.rn.f64 	%fd1759, %fd1750, %fd1757;
	mul.f64 	%fd268, %fd1759, 0d3FE0000000000000;
	mul.f64 	%fd269, %fd268, 0d3FE0000000000000;
	sub.f64 	%fd1760, %fd3395, %fd269;
	add.f64 	%fd1761, %fd1760, 0d3FF921FB54442D18;
	div.rn.f64 	%fd3398, %fd1761, %fd268;
	abs.f64 	%fd271, %fd3398;
	setp.ge.f64	%p171, %fd271, 0d4330000000000000;
	@%p171 bra 	BB50_178;

	add.f64 	%fd1762, %fd271, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd1763, %fd1762;
	setp.lt.f64	%p172, %fd271, 0d3FE0000000000000;
	selp.f64	%fd1764, 0d0000000000000000, %fd1763, %p172;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r407, %temp}, %fd1764;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r408}, %fd1764;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r409}, %fd3398;
	}
	and.b32  	%r410, %r409, -2147483648;
	or.b32  	%r411, %r408, %r410;
	mov.b64 	%fd3398, {%r407, %r411};

BB50_178:
	ld.param.u32 	%r623, [_Z18actfunc_quat_multiP7double4iiii_param_3];
	fma.rn.f64 	%fd1765, %fd268, %fd3398, 0dBFF921FB54442D18;
	add.f64 	%fd274, %fd269, %fd1765;
	cvt.rn.f64.s32	%fd1766, %r623;
	div.rn.f64 	%fd1768, %fd1750, %fd1766;
	mul.f64 	%fd275, %fd1768, 0d3FD0000000000000;
	mul.f64 	%fd276, %fd275, 0d3FE0000000000000;
	sub.f64 	%fd1769, %fd3396, %fd276;
	add.f64 	%fd1770, %fd1769, 0d3FE921FB54442D18;
	div.rn.f64 	%fd3399, %fd1770, %fd275;
	abs.f64 	%fd278, %fd3399;
	setp.ge.f64	%p173, %fd278, 0d4330000000000000;
	@%p173 bra 	BB50_180;

	add.f64 	%fd1771, %fd278, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd1772, %fd1771;
	setp.lt.f64	%p174, %fd278, 0d3FE0000000000000;
	selp.f64	%fd1773, 0d0000000000000000, %fd1772, %p174;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r412, %temp}, %fd1773;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r413}, %fd1773;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r414}, %fd3399;
	}
	and.b32  	%r415, %r414, -2147483648;
	or.b32  	%r416, %r413, %r415;
	mov.b64 	%fd3399, {%r412, %r416};

BB50_180:
	fma.rn.f64 	%fd1774, %fd275, %fd3399, 0dBFE921FB54442D18;
	add.f64 	%fd281, %fd276, %fd1774;
	abs.f64 	%fd282, %fd267;
	setp.neu.f64	%p175, %fd282, 0d7FF0000000000000;
	mov.f64 	%fd3456, %fd267;
	@%p175 bra 	BB50_182;

	mov.f64 	%fd1775, 0d0000000000000000;
	mul.rn.f64 	%fd283, %fd267, %fd1775;
	mov.f64 	%fd3456, %fd283;

BB50_182:
	mov.f64 	%fd284, %fd3456;
	mul.f64 	%fd1776, %fd284, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r652, %fd1776;
	st.local.u32 	[%rd1], %r652;
	cvt.rn.f64.s32	%fd1777, %r652;
	neg.f64 	%fd1778, %fd1777;
	mov.f64 	%fd1779, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd1780, %fd1778, %fd1779, %fd284;
	mov.f64 	%fd1781, 0d3C91A62633145C00;
	fma.rn.f64 	%fd1782, %fd1778, %fd1781, %fd1780;
	mov.f64 	%fd1783, 0d397B839A252049C0;
	fma.rn.f64 	%fd3400, %fd1778, %fd1783, %fd1782;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r417}, %fd284;
	}
	and.b32  	%r418, %r417, 2145386496;
	setp.lt.u32	%p176, %r418, 1105199104;
	@%p176 bra 	BB50_184;

	// Callseq Start 50
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd284;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3400, [retval0+0];
	
	//{
	}// Callseq End 50
	ld.local.u32 	%r652, [%rd1];

BB50_184:
	mul.rn.f64 	%fd1784, %fd3400, %fd3400;
	mov.f64 	%fd1785, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1786, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd1787, %fd1786, %fd1784, %fd1785;
	mov.f64 	%fd1788, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd1789, %fd1787, %fd1784, %fd1788;
	mov.f64 	%fd1790, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd1791, %fd1789, %fd1784, %fd1790;
	mov.f64 	%fd1792, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd1793, %fd1791, %fd1784, %fd1792;
	mov.f64 	%fd1794, 0d3FA5555555555551;
	fma.rn.f64 	%fd1795, %fd1793, %fd1784, %fd1794;
	mov.f64 	%fd1796, 0dBFE0000000000000;
	fma.rn.f64 	%fd1797, %fd1795, %fd1784, %fd1796;
	mov.f64 	%fd1798, 0d3FF0000000000000;
	fma.rn.f64 	%fd1799, %fd1797, %fd1784, %fd1798;
	mov.f64 	%fd1800, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1801, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd1802, %fd1801, %fd1784, %fd1800;
	mov.f64 	%fd1803, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd1804, %fd1802, %fd1784, %fd1803;
	mov.f64 	%fd1805, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd1806, %fd1804, %fd1784, %fd1805;
	mov.f64 	%fd1807, 0d3F81111111110818;
	fma.rn.f64 	%fd1808, %fd1806, %fd1784, %fd1807;
	mov.f64 	%fd1809, 0dBFC5555555555554;
	fma.rn.f64 	%fd1810, %fd1808, %fd1784, %fd1809;
	mov.f64 	%fd1811, 0d0000000000000000;
	fma.rn.f64 	%fd1812, %fd1810, %fd1784, %fd1811;
	fma.rn.f64 	%fd1813, %fd1812, %fd3400, %fd3400;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r419}, %fd1813;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r420, %temp}, %fd1813;
	}
	xor.b32  	%r421, %r419, -2147483648;
	mov.b64 	%fd1814, {%r420, %r421};
	and.b32  	%r422, %r652, 1;
	setp.eq.b32	%p177, %r422, 1;
	not.pred 	%p178, %p177;
	selp.f64	%fd3401, %fd1799, %fd1814, %p178;
	and.b32  	%r423, %r652, 2;
	setp.eq.s32	%p179, %r423, 0;
	@%p179 bra 	BB50_186;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r424}, %fd3401;
	}
	xor.b32  	%r425, %r424, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r426, %temp}, %fd3401;
	}
	mov.b64 	%fd3401, {%r426, %r425};

BB50_186:
	abs.f64 	%fd291, %fd274;
	setp.neu.f64	%p180, %fd291, 0d7FF0000000000000;
	mov.f64 	%fd3473, %fd274;
	@%p180 bra 	BB50_188;

	mul.rn.f64 	%fd292, %fd274, %fd1811;
	mov.f64 	%fd3473, %fd292;

BB50_188:
	mov.f64 	%fd293, %fd3473;
	mov.f64 	%fd3130, 0d397B839A252049C0;
	mov.f64 	%fd3092, 0d3C91A62633145C00;
	mov.f64 	%fd3091, 0d3FF921FB54442D18;
	mul.f64 	%fd1816, %fd293, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r653, %fd1816;
	st.local.u32 	[%rd1], %r653;
	cvt.rn.f64.s32	%fd1817, %r653;
	neg.f64 	%fd1818, %fd1817;
	fma.rn.f64 	%fd1820, %fd1818, %fd3091, %fd293;
	fma.rn.f64 	%fd1822, %fd1818, %fd3092, %fd1820;
	fma.rn.f64 	%fd3402, %fd1818, %fd3130, %fd1822;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r427}, %fd293;
	}
	and.b32  	%r428, %r427, 2145386496;
	setp.lt.u32	%p181, %r428, 1105199104;
	@%p181 bra 	BB50_190;

	add.u64 	%rd173, %SP, 0;
	// Callseq Start 51
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd293;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd173;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3402, [retval0+0];
	
	//{
	}// Callseq End 51
	ld.local.u32 	%r653, [%rd1];

BB50_190:
	mov.f64 	%fd3271, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3260, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3248, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1824, %fd3402, %fd3402;
	fma.rn.f64 	%fd1827, %fd3248, %fd1824, %fd3260;
	fma.rn.f64 	%fd1829, %fd1827, %fd1824, %fd3271;
	fma.rn.f64 	%fd1831, %fd1829, %fd1824, %fd1790;
	fma.rn.f64 	%fd1833, %fd1831, %fd1824, %fd1792;
	fma.rn.f64 	%fd1835, %fd1833, %fd1824, %fd1794;
	fma.rn.f64 	%fd1837, %fd1835, %fd1824, %fd1796;
	fma.rn.f64 	%fd1839, %fd1837, %fd1824, %fd1798;
	fma.rn.f64 	%fd1842, %fd1801, %fd1824, %fd1800;
	fma.rn.f64 	%fd1844, %fd1842, %fd1824, %fd1803;
	fma.rn.f64 	%fd1846, %fd1844, %fd1824, %fd1805;
	fma.rn.f64 	%fd1848, %fd1846, %fd1824, %fd1807;
	fma.rn.f64 	%fd1850, %fd1848, %fd1824, %fd1809;
	fma.rn.f64 	%fd1852, %fd1850, %fd1824, %fd1811;
	fma.rn.f64 	%fd1853, %fd1852, %fd3402, %fd3402;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r429}, %fd1853;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r430, %temp}, %fd1853;
	}
	xor.b32  	%r431, %r429, -2147483648;
	mov.b64 	%fd1854, {%r430, %r431};
	and.b32  	%r432, %r653, 1;
	setp.eq.b32	%p182, %r432, 1;
	not.pred 	%p183, %p182;
	selp.f64	%fd3403, %fd1839, %fd1854, %p183;
	and.b32  	%r433, %r653, 2;
	setp.eq.s32	%p184, %r433, 0;
	@%p184 bra 	BB50_192;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r434}, %fd3403;
	}
	xor.b32  	%r435, %r434, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r436, %temp}, %fd3403;
	}
	mov.b64 	%fd3403, {%r436, %r435};

BB50_192:
	mul.f64 	%fd300, %fd3401, %fd3403;
	abs.f64 	%fd301, %fd281;
	setp.neu.f64	%p185, %fd301, 0d7FF0000000000000;
	mov.f64 	%fd3490, %fd281;
	@%p185 bra 	BB50_194;

	mul.rn.f64 	%fd302, %fd281, %fd1811;
	mov.f64 	%fd3490, %fd302;

BB50_194:
	mov.f64 	%fd303, %fd3490;
	mov.f64 	%fd3094, 0d397B839A252049C0;
	mov.f64 	%fd3093, 0d3C91A62633145C00;
	mov.f64 	%fd3048, 0d3FF921FB54442D18;
	mul.f64 	%fd1856, %fd303, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r654, %fd1856;
	st.local.u32 	[%rd1], %r654;
	cvt.rn.f64.s32	%fd1857, %r654;
	neg.f64 	%fd1858, %fd1857;
	fma.rn.f64 	%fd1860, %fd1858, %fd3048, %fd303;
	fma.rn.f64 	%fd1862, %fd1858, %fd3093, %fd1860;
	fma.rn.f64 	%fd3404, %fd1858, %fd3094, %fd1862;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r437}, %fd303;
	}
	and.b32  	%r438, %r437, 2145386496;
	setp.lt.u32	%p186, %r438, 1105199104;
	@%p186 bra 	BB50_196;

	add.u64 	%rd172, %SP, 0;
	// Callseq Start 52
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd303;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd172;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3404, [retval0+0];
	
	//{
	}// Callseq End 52
	ld.local.u32 	%r654, [%rd1];

BB50_196:
	mov.f64 	%fd3287, 0d3FA5555555555551;
	mov.f64 	%fd3284, 0dBF56C16C16C15D47;
	mov.f64 	%fd3281, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3272, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3261, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3249, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1864, %fd3404, %fd3404;
	fma.rn.f64 	%fd1867, %fd3249, %fd1864, %fd3261;
	fma.rn.f64 	%fd1869, %fd1867, %fd1864, %fd3272;
	fma.rn.f64 	%fd1871, %fd1869, %fd1864, %fd3281;
	fma.rn.f64 	%fd1873, %fd1871, %fd1864, %fd3284;
	fma.rn.f64 	%fd1875, %fd1873, %fd1864, %fd3287;
	fma.rn.f64 	%fd1877, %fd1875, %fd1864, %fd1796;
	fma.rn.f64 	%fd1879, %fd1877, %fd1864, %fd1798;
	fma.rn.f64 	%fd1882, %fd1801, %fd1864, %fd1800;
	fma.rn.f64 	%fd1884, %fd1882, %fd1864, %fd1803;
	fma.rn.f64 	%fd1886, %fd1884, %fd1864, %fd1805;
	fma.rn.f64 	%fd1888, %fd1886, %fd1864, %fd1807;
	fma.rn.f64 	%fd1890, %fd1888, %fd1864, %fd1809;
	fma.rn.f64 	%fd1892, %fd1890, %fd1864, %fd1811;
	fma.rn.f64 	%fd1893, %fd1892, %fd3404, %fd3404;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r439}, %fd1893;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r440, %temp}, %fd1893;
	}
	xor.b32  	%r441, %r439, -2147483648;
	mov.b64 	%fd1894, {%r440, %r441};
	and.b32  	%r442, %r654, 1;
	setp.eq.b32	%p187, %r442, 1;
	not.pred 	%p188, %p187;
	selp.f64	%fd3405, %fd1879, %fd1894, %p188;
	and.b32  	%r443, %r654, 2;
	setp.eq.s32	%p189, %r443, 0;
	@%p189 bra 	BB50_198;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r444}, %fd3405;
	}
	xor.b32  	%r445, %r444, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r446, %temp}, %fd3405;
	}
	mov.b64 	%fd3405, {%r446, %r445};

BB50_198:
	mul.f64 	%fd310, %fd300, %fd3405;
	mov.f64 	%fd3455, %fd267;
	@%p175 bra 	BB50_200;

	mul.rn.f64 	%fd3455, %fd267, %fd1811;

BB50_200:
	mov.f64 	%fd3095, 0d397B839A252049C0;
	mov.f64 	%fd3050, 0d3C91A62633145C00;
	mov.f64 	%fd3049, 0d3FF921FB54442D18;
	mul.f64 	%fd1896, %fd3455, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r655, %fd1896;
	st.local.u32 	[%rd1], %r655;
	cvt.rn.f64.s32	%fd1897, %r655;
	neg.f64 	%fd1898, %fd1897;
	fma.rn.f64 	%fd1900, %fd1898, %fd3049, %fd3455;
	fma.rn.f64 	%fd1902, %fd1898, %fd3050, %fd1900;
	fma.rn.f64 	%fd3406, %fd1898, %fd3095, %fd1902;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r447}, %fd3455;
	}
	and.b32  	%r448, %r447, 2145386496;
	setp.lt.u32	%p191, %r448, 1105199104;
	@%p191 bra 	BB50_202;

	add.u64 	%rd171, %SP, 0;
	// Callseq Start 53
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3455;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd171;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3406, [retval0+0];
	
	//{
	}// Callseq End 53
	ld.local.u32 	%r655, [%rd1];

BB50_202:
	mov.f64 	%fd3285, 0d3FA5555555555551;
	mov.f64 	%fd3282, 0dBF56C16C16C15D47;
	mov.f64 	%fd3274, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3273, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3262, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3245, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1904, %fd3406, %fd3406;
	fma.rn.f64 	%fd1907, %fd3245, %fd1904, %fd3262;
	fma.rn.f64 	%fd1909, %fd1907, %fd1904, %fd3273;
	fma.rn.f64 	%fd1911, %fd1909, %fd1904, %fd3274;
	fma.rn.f64 	%fd1913, %fd1911, %fd1904, %fd3282;
	fma.rn.f64 	%fd1915, %fd1913, %fd1904, %fd3285;
	fma.rn.f64 	%fd1917, %fd1915, %fd1904, %fd1796;
	fma.rn.f64 	%fd1919, %fd1917, %fd1904, %fd1798;
	fma.rn.f64 	%fd1922, %fd1801, %fd1904, %fd1800;
	fma.rn.f64 	%fd1924, %fd1922, %fd1904, %fd1803;
	fma.rn.f64 	%fd1926, %fd1924, %fd1904, %fd1805;
	fma.rn.f64 	%fd1928, %fd1926, %fd1904, %fd1807;
	fma.rn.f64 	%fd1930, %fd1928, %fd1904, %fd1809;
	fma.rn.f64 	%fd1932, %fd1930, %fd1904, %fd1811;
	fma.rn.f64 	%fd1933, %fd1932, %fd3406, %fd3406;
	and.b32  	%r449, %r655, 1;
	setp.eq.b32	%p192, %r449, 1;
	not.pred 	%p193, %p192;
	selp.f64	%fd3407, %fd1933, %fd1919, %p193;
	and.b32  	%r450, %r655, 2;
	setp.eq.s32	%p194, %r450, 0;
	@%p194 bra 	BB50_204;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r451}, %fd3407;
	}
	xor.b32  	%r452, %r451, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r453, %temp}, %fd3407;
	}
	mov.b64 	%fd3407, {%r453, %r452};

BB50_204:
	mov.f64 	%fd3472, %fd274;
	@%p180 bra 	BB50_206;

	mul.rn.f64 	%fd3472, %fd274, %fd1811;

BB50_206:
	mov.f64 	%fd3052, 0d397B839A252049C0;
	mov.f64 	%fd3051, 0d3C91A62633145C00;
	mov.f64 	%fd3001, 0d3FF921FB54442D18;
	mul.f64 	%fd1935, %fd3472, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r656, %fd1935;
	st.local.u32 	[%rd1], %r656;
	cvt.rn.f64.s32	%fd1936, %r656;
	neg.f64 	%fd1937, %fd1936;
	fma.rn.f64 	%fd1939, %fd1937, %fd3001, %fd3472;
	fma.rn.f64 	%fd1941, %fd1937, %fd3051, %fd1939;
	fma.rn.f64 	%fd3408, %fd1937, %fd3052, %fd1941;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r454}, %fd3472;
	}
	and.b32  	%r455, %r454, 2145386496;
	setp.lt.u32	%p196, %r455, 1105199104;
	@%p196 bra 	BB50_208;

	add.u64 	%rd170, %SP, 0;
	// Callseq Start 54
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3472;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd170;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3408, [retval0+0];
	
	//{
	}// Callseq End 54
	ld.local.u32 	%r656, [%rd1];

BB50_208:
	mov.f64 	%fd3286, 0d3FA5555555555551;
	mov.f64 	%fd3276, 0dBF56C16C16C15D47;
	mov.f64 	%fd3275, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3263, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3250, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3236, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1943, %fd3408, %fd3408;
	fma.rn.f64 	%fd1946, %fd3236, %fd1943, %fd3250;
	fma.rn.f64 	%fd1948, %fd1946, %fd1943, %fd3263;
	fma.rn.f64 	%fd1950, %fd1948, %fd1943, %fd3275;
	fma.rn.f64 	%fd1952, %fd1950, %fd1943, %fd3276;
	fma.rn.f64 	%fd1954, %fd1952, %fd1943, %fd3286;
	fma.rn.f64 	%fd1956, %fd1954, %fd1943, %fd1796;
	fma.rn.f64 	%fd1958, %fd1956, %fd1943, %fd1798;
	fma.rn.f64 	%fd1961, %fd1801, %fd1943, %fd1800;
	fma.rn.f64 	%fd1963, %fd1961, %fd1943, %fd1803;
	fma.rn.f64 	%fd1965, %fd1963, %fd1943, %fd1805;
	fma.rn.f64 	%fd1967, %fd1965, %fd1943, %fd1807;
	fma.rn.f64 	%fd1969, %fd1967, %fd1943, %fd1809;
	fma.rn.f64 	%fd1971, %fd1969, %fd1943, %fd1811;
	fma.rn.f64 	%fd1972, %fd1971, %fd3408, %fd3408;
	and.b32  	%r456, %r656, 1;
	setp.eq.b32	%p197, %r456, 1;
	not.pred 	%p198, %p197;
	selp.f64	%fd3409, %fd1972, %fd1958, %p198;
	and.b32  	%r457, %r656, 2;
	setp.eq.s32	%p199, %r457, 0;
	@%p199 bra 	BB50_210;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r458}, %fd3409;
	}
	xor.b32  	%r459, %r458, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r460, %temp}, %fd3409;
	}
	mov.b64 	%fd3409, {%r460, %r459};

BB50_210:
	mul.f64 	%fd327, %fd3407, %fd3409;
	mov.f64 	%fd3489, %fd281;
	@%p185 bra 	BB50_212;

	mul.rn.f64 	%fd3489, %fd281, %fd1811;

BB50_212:
	mov.f64 	%fd3053, 0d397B839A252049C0;
	mov.f64 	%fd3004, 0d3C91A62633145C00;
	mov.f64 	%fd3002, 0d3FF921FB54442D18;
	mul.f64 	%fd1974, %fd3489, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r657, %fd1974;
	st.local.u32 	[%rd1], %r657;
	cvt.rn.f64.s32	%fd1975, %r657;
	neg.f64 	%fd1976, %fd1975;
	fma.rn.f64 	%fd1978, %fd1976, %fd3002, %fd3489;
	fma.rn.f64 	%fd1980, %fd1976, %fd3004, %fd1978;
	fma.rn.f64 	%fd3410, %fd1976, %fd3053, %fd1980;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r461}, %fd3489;
	}
	and.b32  	%r462, %r461, 2145386496;
	setp.lt.u32	%p201, %r462, 1105199104;
	@%p201 bra 	BB50_214;

	add.u64 	%rd169, %SP, 0;
	// Callseq Start 55
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3489;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd169;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3410, [retval0+0];
	
	//{
	}// Callseq End 55
	ld.local.u32 	%r657, [%rd1];

BB50_214:
	mov.f64 	%fd3283, 0d3FA5555555555551;
	mov.f64 	%fd3277, 0dBF56C16C16C15D47;
	mov.f64 	%fd3265, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3264, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3251, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3230, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1982, %fd3410, %fd3410;
	fma.rn.f64 	%fd1985, %fd3230, %fd1982, %fd3251;
	fma.rn.f64 	%fd1987, %fd1985, %fd1982, %fd3264;
	fma.rn.f64 	%fd1989, %fd1987, %fd1982, %fd3265;
	fma.rn.f64 	%fd1991, %fd1989, %fd1982, %fd3277;
	fma.rn.f64 	%fd1993, %fd1991, %fd1982, %fd3283;
	fma.rn.f64 	%fd1995, %fd1993, %fd1982, %fd1796;
	fma.rn.f64 	%fd1997, %fd1995, %fd1982, %fd1798;
	fma.rn.f64 	%fd2000, %fd1801, %fd1982, %fd1800;
	fma.rn.f64 	%fd2002, %fd2000, %fd1982, %fd1803;
	fma.rn.f64 	%fd2004, %fd2002, %fd1982, %fd1805;
	fma.rn.f64 	%fd2006, %fd2004, %fd1982, %fd1807;
	fma.rn.f64 	%fd2008, %fd2006, %fd1982, %fd1809;
	fma.rn.f64 	%fd2010, %fd2008, %fd1982, %fd1811;
	fma.rn.f64 	%fd2011, %fd2010, %fd3410, %fd3410;
	and.b32  	%r463, %r657, 1;
	setp.eq.b32	%p202, %r463, 1;
	not.pred 	%p203, %p202;
	selp.f64	%fd3411, %fd2011, %fd1997, %p203;
	and.b32  	%r464, %r657, 2;
	setp.eq.s32	%p204, %r464, 0;
	@%p204 bra 	BB50_216;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r465}, %fd3411;
	}
	xor.b32  	%r466, %r465, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r467, %temp}, %fd3411;
	}
	mov.b64 	%fd3411, {%r467, %r466};

BB50_216:
	fma.rn.f64 	%fd336, %fd327, %fd3411, %fd310;
	mov.f64 	%fd3454, %fd267;
	@%p175 bra 	BB50_218;

	mul.rn.f64 	%fd3454, %fd267, %fd1811;

BB50_218:
	mov.f64 	%fd3007, 0d397B839A252049C0;
	mov.f64 	%fd3005, 0d3C91A62633145C00;
	mov.f64 	%fd3003, 0d3FF921FB54442D18;
	mul.f64 	%fd2013, %fd3454, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r658, %fd2013;
	st.local.u32 	[%rd1], %r658;
	cvt.rn.f64.s32	%fd2014, %r658;
	neg.f64 	%fd2015, %fd2014;
	fma.rn.f64 	%fd2017, %fd2015, %fd3003, %fd3454;
	fma.rn.f64 	%fd2019, %fd2015, %fd3005, %fd2017;
	fma.rn.f64 	%fd3412, %fd2015, %fd3007, %fd2019;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r468}, %fd3454;
	}
	and.b32  	%r469, %r468, 2145386496;
	setp.lt.u32	%p206, %r469, 1105199104;
	@%p206 bra 	BB50_220;

	add.u64 	%rd168, %SP, 0;
	// Callseq Start 56
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3454;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd168;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3412, [retval0+0];
	
	//{
	}// Callseq End 56
	ld.local.u32 	%r658, [%rd1];

BB50_220:
	mov.f64 	%fd3278, 0d3FA5555555555551;
	mov.f64 	%fd3267, 0dBF56C16C16C15D47;
	mov.f64 	%fd3266, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3252, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3237, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3220, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2021, %fd3412, %fd3412;
	fma.rn.f64 	%fd2024, %fd3220, %fd2021, %fd3237;
	fma.rn.f64 	%fd2026, %fd2024, %fd2021, %fd3252;
	fma.rn.f64 	%fd2028, %fd2026, %fd2021, %fd3266;
	fma.rn.f64 	%fd2030, %fd2028, %fd2021, %fd3267;
	fma.rn.f64 	%fd2032, %fd2030, %fd2021, %fd3278;
	fma.rn.f64 	%fd2034, %fd2032, %fd2021, %fd1796;
	fma.rn.f64 	%fd2036, %fd2034, %fd2021, %fd1798;
	fma.rn.f64 	%fd2039, %fd1801, %fd2021, %fd1800;
	fma.rn.f64 	%fd2041, %fd2039, %fd2021, %fd1803;
	fma.rn.f64 	%fd2043, %fd2041, %fd2021, %fd1805;
	fma.rn.f64 	%fd2045, %fd2043, %fd2021, %fd1807;
	fma.rn.f64 	%fd2047, %fd2045, %fd2021, %fd1809;
	fma.rn.f64 	%fd2049, %fd2047, %fd2021, %fd1811;
	fma.rn.f64 	%fd2050, %fd2049, %fd3412, %fd3412;
	and.b32  	%r470, %r658, 1;
	setp.eq.b32	%p207, %r470, 1;
	not.pred 	%p208, %p207;
	selp.f64	%fd3413, %fd2050, %fd2036, %p208;
	and.b32  	%r471, %r658, 2;
	setp.eq.s32	%p209, %r471, 0;
	@%p209 bra 	BB50_222;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r472}, %fd3413;
	}
	xor.b32  	%r473, %r472, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r474, %temp}, %fd3413;
	}
	mov.b64 	%fd3413, {%r474, %r473};

BB50_222:
	mov.f64 	%fd3471, %fd274;
	@%p180 bra 	BB50_224;

	mul.rn.f64 	%fd3471, %fd274, %fd1811;

BB50_224:
	mov.f64 	%fd3008, 0d397B839A252049C0;
	mov.f64 	%fd3006, 0d3C91A62633145C00;
	mov.f64 	%fd2945, 0d3FF921FB54442D18;
	mul.f64 	%fd2052, %fd3471, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r659, %fd2052;
	st.local.u32 	[%rd1], %r659;
	cvt.rn.f64.s32	%fd2053, %r659;
	neg.f64 	%fd2054, %fd2053;
	fma.rn.f64 	%fd2056, %fd2054, %fd2945, %fd3471;
	fma.rn.f64 	%fd2058, %fd2054, %fd3006, %fd2056;
	fma.rn.f64 	%fd3414, %fd2054, %fd3008, %fd2058;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r475}, %fd3471;
	}
	and.b32  	%r476, %r475, 2145386496;
	setp.lt.u32	%p211, %r476, 1105199104;
	@%p211 bra 	BB50_226;

	add.u64 	%rd167, %SP, 0;
	// Callseq Start 57
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3471;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd167;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3414, [retval0+0];
	
	//{
	}// Callseq End 57
	ld.local.u32 	%r659, [%rd1];

BB50_226:
	mov.f64 	%fd3279, 0d3FA5555555555551;
	mov.f64 	%fd3268, 0dBF56C16C16C15D47;
	mov.f64 	%fd3254, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3253, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3222, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3221, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2060, %fd3414, %fd3414;
	fma.rn.f64 	%fd2063, %fd3221, %fd2060, %fd3222;
	fma.rn.f64 	%fd2065, %fd2063, %fd2060, %fd3253;
	fma.rn.f64 	%fd2067, %fd2065, %fd2060, %fd3254;
	fma.rn.f64 	%fd2069, %fd2067, %fd2060, %fd3268;
	fma.rn.f64 	%fd2071, %fd2069, %fd2060, %fd3279;
	fma.rn.f64 	%fd2073, %fd2071, %fd2060, %fd1796;
	fma.rn.f64 	%fd2075, %fd2073, %fd2060, %fd1798;
	fma.rn.f64 	%fd2078, %fd1801, %fd2060, %fd1800;
	fma.rn.f64 	%fd2080, %fd2078, %fd2060, %fd1803;
	fma.rn.f64 	%fd2082, %fd2080, %fd2060, %fd1805;
	fma.rn.f64 	%fd2084, %fd2082, %fd2060, %fd1807;
	fma.rn.f64 	%fd2086, %fd2084, %fd2060, %fd1809;
	fma.rn.f64 	%fd2088, %fd2086, %fd2060, %fd1811;
	fma.rn.f64 	%fd2089, %fd2088, %fd3414, %fd3414;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r477}, %fd2089;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r478, %temp}, %fd2089;
	}
	xor.b32  	%r479, %r477, -2147483648;
	mov.b64 	%fd2090, {%r478, %r479};
	and.b32  	%r480, %r659, 1;
	setp.eq.b32	%p212, %r480, 1;
	not.pred 	%p213, %p212;
	selp.f64	%fd3415, %fd2075, %fd2090, %p213;
	and.b32  	%r481, %r659, 2;
	setp.eq.s32	%p214, %r481, 0;
	@%p214 bra 	BB50_228;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r482}, %fd3415;
	}
	xor.b32  	%r483, %r482, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r484, %temp}, %fd3415;
	}
	mov.b64 	%fd3415, {%r484, %r483};

BB50_228:
	mul.f64 	%fd353, %fd3413, %fd3415;
	mov.f64 	%fd3488, %fd281;
	@%p185 bra 	BB50_230;

	mul.rn.f64 	%fd3488, %fd281, %fd1811;

BB50_230:
	mov.f64 	%fd3009, 0d397B839A252049C0;
	mov.f64 	%fd2948, 0d3C91A62633145C00;
	mov.f64 	%fd2946, 0d3FF921FB54442D18;
	mul.f64 	%fd2092, %fd3488, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r660, %fd2092;
	st.local.u32 	[%rd1], %r660;
	cvt.rn.f64.s32	%fd2093, %r660;
	neg.f64 	%fd2094, %fd2093;
	fma.rn.f64 	%fd2096, %fd2094, %fd2946, %fd3488;
	fma.rn.f64 	%fd2098, %fd2094, %fd2948, %fd2096;
	fma.rn.f64 	%fd3416, %fd2094, %fd3009, %fd2098;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r485}, %fd3488;
	}
	and.b32  	%r486, %r485, 2145386496;
	setp.lt.u32	%p216, %r486, 1105199104;
	@%p216 bra 	BB50_232;

	add.u64 	%rd166, %SP, 0;
	// Callseq Start 58
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3488;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd166;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3416, [retval0+0];
	
	//{
	}// Callseq End 58
	ld.local.u32 	%r660, [%rd1];

BB50_232:
	mov.f64 	%fd3280, 0dBFE0000000000000;
	mov.f64 	%fd3269, 0d3FA5555555555551;
	mov.f64 	%fd3256, 0dBF56C16C16C15D47;
	mov.f64 	%fd3255, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3238, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3223, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3204, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2100, %fd3416, %fd3416;
	fma.rn.f64 	%fd2103, %fd3204, %fd2100, %fd3223;
	fma.rn.f64 	%fd2105, %fd2103, %fd2100, %fd3238;
	fma.rn.f64 	%fd2107, %fd2105, %fd2100, %fd3255;
	fma.rn.f64 	%fd2109, %fd2107, %fd2100, %fd3256;
	fma.rn.f64 	%fd2111, %fd2109, %fd2100, %fd3269;
	fma.rn.f64 	%fd2113, %fd2111, %fd2100, %fd3280;
	fma.rn.f64 	%fd2115, %fd2113, %fd2100, %fd1798;
	fma.rn.f64 	%fd2118, %fd1801, %fd2100, %fd1800;
	fma.rn.f64 	%fd2120, %fd2118, %fd2100, %fd1803;
	fma.rn.f64 	%fd2122, %fd2120, %fd2100, %fd1805;
	fma.rn.f64 	%fd2124, %fd2122, %fd2100, %fd1807;
	fma.rn.f64 	%fd2126, %fd2124, %fd2100, %fd1809;
	fma.rn.f64 	%fd2128, %fd2126, %fd2100, %fd1811;
	fma.rn.f64 	%fd2129, %fd2128, %fd3416, %fd3416;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r487}, %fd2129;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r488, %temp}, %fd2129;
	}
	xor.b32  	%r489, %r487, -2147483648;
	mov.b64 	%fd2130, {%r488, %r489};
	and.b32  	%r490, %r660, 1;
	setp.eq.b32	%p217, %r490, 1;
	not.pred 	%p218, %p217;
	selp.f64	%fd3417, %fd2115, %fd2130, %p218;
	and.b32  	%r491, %r660, 2;
	setp.eq.s32	%p219, %r491, 0;
	@%p219 bra 	BB50_234;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r492}, %fd3417;
	}
	xor.b32  	%r493, %r492, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r494, %temp}, %fd3417;
	}
	mov.b64 	%fd3417, {%r494, %r493};

BB50_234:
	mul.f64 	%fd362, %fd353, %fd3417;
	mov.f64 	%fd3453, %fd267;
	@%p175 bra 	BB50_236;

	mul.rn.f64 	%fd3453, %fd267, %fd1811;

BB50_236:
	mov.f64 	%fd2950, 0d397B839A252049C0;
	mov.f64 	%fd2949, 0d3C91A62633145C00;
	mov.f64 	%fd2947, 0d3FF921FB54442D18;
	mul.f64 	%fd2132, %fd3453, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r661, %fd2132;
	st.local.u32 	[%rd1], %r661;
	cvt.rn.f64.s32	%fd2133, %r661;
	neg.f64 	%fd2134, %fd2133;
	fma.rn.f64 	%fd2136, %fd2134, %fd2947, %fd3453;
	fma.rn.f64 	%fd2138, %fd2134, %fd2949, %fd2136;
	fma.rn.f64 	%fd3418, %fd2134, %fd2950, %fd2138;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r495}, %fd3453;
	}
	and.b32  	%r496, %r495, 2145386496;
	setp.lt.u32	%p221, %r496, 1105199104;
	@%p221 bra 	BB50_238;

	add.u64 	%rd165, %SP, 0;
	// Callseq Start 59
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3453;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd165;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3418, [retval0+0];
	
	//{
	}// Callseq End 59
	ld.local.u32 	%r661, [%rd1];

BB50_238:
	mov.f64 	%fd3270, 0dBFE0000000000000;
	mov.f64 	%fd3257, 0d3FA5555555555551;
	mov.f64 	%fd3241, 0dBF56C16C16C15D47;
	mov.f64 	%fd3240, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3239, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3206, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3205, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2140, %fd3418, %fd3418;
	fma.rn.f64 	%fd2143, %fd3205, %fd2140, %fd3206;
	fma.rn.f64 	%fd2145, %fd2143, %fd2140, %fd3239;
	fma.rn.f64 	%fd2147, %fd2145, %fd2140, %fd3240;
	fma.rn.f64 	%fd2149, %fd2147, %fd2140, %fd3241;
	fma.rn.f64 	%fd2151, %fd2149, %fd2140, %fd3257;
	fma.rn.f64 	%fd2153, %fd2151, %fd2140, %fd3270;
	fma.rn.f64 	%fd2155, %fd2153, %fd2140, %fd1798;
	fma.rn.f64 	%fd2158, %fd1801, %fd2140, %fd1800;
	fma.rn.f64 	%fd2160, %fd2158, %fd2140, %fd1803;
	fma.rn.f64 	%fd2162, %fd2160, %fd2140, %fd1805;
	fma.rn.f64 	%fd2164, %fd2162, %fd2140, %fd1807;
	fma.rn.f64 	%fd2166, %fd2164, %fd2140, %fd1809;
	fma.rn.f64 	%fd2168, %fd2166, %fd2140, %fd1811;
	fma.rn.f64 	%fd2169, %fd2168, %fd3418, %fd3418;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r497}, %fd2169;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r498, %temp}, %fd2169;
	}
	xor.b32  	%r499, %r497, -2147483648;
	mov.b64 	%fd2170, {%r498, %r499};
	and.b32  	%r500, %r661, 1;
	setp.eq.b32	%p222, %r500, 1;
	not.pred 	%p223, %p222;
	selp.f64	%fd3419, %fd2155, %fd2170, %p223;
	and.b32  	%r501, %r661, 2;
	setp.eq.s32	%p224, %r501, 0;
	@%p224 bra 	BB50_240;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r502}, %fd3419;
	}
	xor.b32  	%r503, %r502, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r504, %temp}, %fd3419;
	}
	mov.b64 	%fd3419, {%r504, %r503};

BB50_240:
	mov.f64 	%fd3470, %fd274;
	@%p180 bra 	BB50_242;

	mul.rn.f64 	%fd3470, %fd274, %fd1811;

BB50_242:
	mov.f64 	%fd2951, 0d397B839A252049C0;
	mov.f64 	%fd2916, 0d3C91A62633145C00;
	mov.f64 	%fd2915, 0d3FF921FB54442D18;
	mul.f64 	%fd2172, %fd3470, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r662, %fd2172;
	st.local.u32 	[%rd1], %r662;
	cvt.rn.f64.s32	%fd2173, %r662;
	neg.f64 	%fd2174, %fd2173;
	fma.rn.f64 	%fd2176, %fd2174, %fd2915, %fd3470;
	fma.rn.f64 	%fd2178, %fd2174, %fd2916, %fd2176;
	fma.rn.f64 	%fd3420, %fd2174, %fd2951, %fd2178;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r505}, %fd3470;
	}
	and.b32  	%r506, %r505, 2145386496;
	setp.lt.u32	%p226, %r506, 1105199104;
	@%p226 bra 	BB50_244;

	add.u64 	%rd164, %SP, 0;
	// Callseq Start 60
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3470;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd164;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3420, [retval0+0];
	
	//{
	}// Callseq End 60
	ld.local.u32 	%r662, [%rd1];

BB50_244:
	mov.f64 	%fd3258, 0dBFE0000000000000;
	mov.f64 	%fd3243, 0d3FA5555555555551;
	mov.f64 	%fd3242, 0dBF56C16C16C15D47;
	mov.f64 	%fd3225, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3224, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3207, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3181, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2180, %fd3420, %fd3420;
	fma.rn.f64 	%fd2183, %fd3181, %fd2180, %fd3207;
	fma.rn.f64 	%fd2185, %fd2183, %fd2180, %fd3224;
	fma.rn.f64 	%fd2187, %fd2185, %fd2180, %fd3225;
	fma.rn.f64 	%fd2189, %fd2187, %fd2180, %fd3242;
	fma.rn.f64 	%fd2191, %fd2189, %fd2180, %fd3243;
	fma.rn.f64 	%fd2193, %fd2191, %fd2180, %fd3258;
	fma.rn.f64 	%fd2195, %fd2193, %fd2180, %fd1798;
	fma.rn.f64 	%fd2198, %fd1801, %fd2180, %fd1800;
	fma.rn.f64 	%fd2200, %fd2198, %fd2180, %fd1803;
	fma.rn.f64 	%fd2202, %fd2200, %fd2180, %fd1805;
	fma.rn.f64 	%fd2204, %fd2202, %fd2180, %fd1807;
	fma.rn.f64 	%fd2206, %fd2204, %fd2180, %fd1809;
	fma.rn.f64 	%fd2208, %fd2206, %fd2180, %fd1811;
	fma.rn.f64 	%fd2209, %fd2208, %fd3420, %fd3420;
	and.b32  	%r507, %r662, 1;
	setp.eq.b32	%p227, %r507, 1;
	not.pred 	%p228, %p227;
	selp.f64	%fd3421, %fd2209, %fd2195, %p228;
	and.b32  	%r508, %r662, 2;
	setp.eq.s32	%p229, %r508, 0;
	@%p229 bra 	BB50_246;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r509}, %fd3421;
	}
	xor.b32  	%r510, %r509, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r511, %temp}, %fd3421;
	}
	mov.b64 	%fd3421, {%r511, %r510};

BB50_246:
	mul.f64 	%fd379, %fd3419, %fd3421;
	mov.f64 	%fd3487, %fd281;
	@%p185 bra 	BB50_248;

	mul.rn.f64 	%fd3487, %fd281, %fd1811;

BB50_248:
	mov.f64 	%fd2920, 0d397B839A252049C0;
	mov.f64 	%fd2918, 0d3C91A62633145C00;
	mov.f64 	%fd2917, 0d3FF921FB54442D18;
	mul.f64 	%fd2211, %fd3487, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r663, %fd2211;
	st.local.u32 	[%rd1], %r663;
	cvt.rn.f64.s32	%fd2212, %r663;
	neg.f64 	%fd2213, %fd2212;
	fma.rn.f64 	%fd2215, %fd2213, %fd2917, %fd3487;
	fma.rn.f64 	%fd2217, %fd2213, %fd2918, %fd2215;
	fma.rn.f64 	%fd3422, %fd2213, %fd2920, %fd2217;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r512}, %fd3487;
	}
	and.b32  	%r513, %r512, 2145386496;
	setp.lt.u32	%p231, %r513, 1105199104;
	@%p231 bra 	BB50_250;

	add.u64 	%rd163, %SP, 0;
	// Callseq Start 61
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3487;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd163;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3422, [retval0+0];
	
	//{
	}// Callseq End 61
	ld.local.u32 	%r663, [%rd1];

BB50_250:
	mov.f64 	%fd3244, 0dBFE0000000000000;
	mov.f64 	%fd3228, 0d3FA5555555555551;
	mov.f64 	%fd3227, 0dBF56C16C16C15D47;
	mov.f64 	%fd3226, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3184, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3183, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3182, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2219, %fd3422, %fd3422;
	fma.rn.f64 	%fd2222, %fd3182, %fd2219, %fd3183;
	fma.rn.f64 	%fd2224, %fd2222, %fd2219, %fd3184;
	fma.rn.f64 	%fd2226, %fd2224, %fd2219, %fd3226;
	fma.rn.f64 	%fd2228, %fd2226, %fd2219, %fd3227;
	fma.rn.f64 	%fd2230, %fd2228, %fd2219, %fd3228;
	fma.rn.f64 	%fd2232, %fd2230, %fd2219, %fd3244;
	fma.rn.f64 	%fd2234, %fd2232, %fd2219, %fd1798;
	fma.rn.f64 	%fd2237, %fd1801, %fd2219, %fd1800;
	fma.rn.f64 	%fd2239, %fd2237, %fd2219, %fd1803;
	fma.rn.f64 	%fd2241, %fd2239, %fd2219, %fd1805;
	fma.rn.f64 	%fd2243, %fd2241, %fd2219, %fd1807;
	fma.rn.f64 	%fd2245, %fd2243, %fd2219, %fd1809;
	fma.rn.f64 	%fd2247, %fd2245, %fd2219, %fd1811;
	fma.rn.f64 	%fd2248, %fd2247, %fd3422, %fd3422;
	and.b32  	%r514, %r663, 1;
	setp.eq.b32	%p232, %r514, 1;
	not.pred 	%p233, %p232;
	selp.f64	%fd3423, %fd2248, %fd2234, %p233;
	and.b32  	%r515, %r663, 2;
	setp.eq.s32	%p234, %r515, 0;
	@%p234 bra 	BB50_252;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r516}, %fd3423;
	}
	xor.b32  	%r517, %r516, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r518, %temp}, %fd3423;
	}
	mov.b64 	%fd3423, {%r518, %r517};

BB50_252:
	mul.f64 	%fd2249, %fd379, %fd3423;
	sub.f64 	%fd388, %fd362, %fd2249;
	mov.f64 	%fd3452, %fd267;
	@%p175 bra 	BB50_254;

	mul.rn.f64 	%fd3452, %fd267, %fd1811;

BB50_254:
	mov.f64 	%fd2921, 0d397B839A252049C0;
	mov.f64 	%fd2919, 0d3C91A62633145C00;
	mov.f64 	%fd2878, 0d3FF921FB54442D18;
	mul.f64 	%fd2251, %fd3452, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r664, %fd2251;
	st.local.u32 	[%rd1], %r664;
	cvt.rn.f64.s32	%fd2252, %r664;
	neg.f64 	%fd2253, %fd2252;
	fma.rn.f64 	%fd2255, %fd2253, %fd2878, %fd3452;
	fma.rn.f64 	%fd2257, %fd2253, %fd2919, %fd2255;
	fma.rn.f64 	%fd3424, %fd2253, %fd2921, %fd2257;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r519}, %fd3452;
	}
	and.b32  	%r520, %r519, 2145386496;
	setp.lt.u32	%p236, %r520, 1105199104;
	@%p236 bra 	BB50_256;

	add.u64 	%rd162, %SP, 0;
	// Callseq Start 62
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3452;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd162;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3424, [retval0+0];
	
	//{
	}// Callseq End 62
	ld.local.u32 	%r664, [%rd1];

BB50_256:
	mov.f64 	%fd3229, 0dBFE0000000000000;
	mov.f64 	%fd3200, 0d3FA5555555555551;
	mov.f64 	%fd3199, 0dBF56C16C16C15D47;
	mov.f64 	%fd3198, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3185, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3168, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3167, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2259, %fd3424, %fd3424;
	fma.rn.f64 	%fd2262, %fd3167, %fd2259, %fd3168;
	fma.rn.f64 	%fd2264, %fd2262, %fd2259, %fd3185;
	fma.rn.f64 	%fd2266, %fd2264, %fd2259, %fd3198;
	fma.rn.f64 	%fd2268, %fd2266, %fd2259, %fd3199;
	fma.rn.f64 	%fd2270, %fd2268, %fd2259, %fd3200;
	fma.rn.f64 	%fd2272, %fd2270, %fd2259, %fd3229;
	fma.rn.f64 	%fd2274, %fd2272, %fd2259, %fd1798;
	fma.rn.f64 	%fd2277, %fd1801, %fd2259, %fd1800;
	fma.rn.f64 	%fd2279, %fd2277, %fd2259, %fd1803;
	fma.rn.f64 	%fd2281, %fd2279, %fd2259, %fd1805;
	fma.rn.f64 	%fd2283, %fd2281, %fd2259, %fd1807;
	fma.rn.f64 	%fd2285, %fd2283, %fd2259, %fd1809;
	fma.rn.f64 	%fd2287, %fd2285, %fd2259, %fd1811;
	fma.rn.f64 	%fd2288, %fd2287, %fd3424, %fd3424;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r521}, %fd2288;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r522, %temp}, %fd2288;
	}
	xor.b32  	%r523, %r521, -2147483648;
	mov.b64 	%fd2289, {%r522, %r523};
	and.b32  	%r524, %r664, 1;
	setp.eq.b32	%p237, %r524, 1;
	not.pred 	%p238, %p237;
	selp.f64	%fd3425, %fd2274, %fd2289, %p238;
	and.b32  	%r525, %r664, 2;
	setp.eq.s32	%p239, %r525, 0;
	@%p239 bra 	BB50_258;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r526}, %fd3425;
	}
	xor.b32  	%r527, %r526, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r528, %temp}, %fd3425;
	}
	mov.b64 	%fd3425, {%r528, %r527};

BB50_258:
	mov.f64 	%fd3469, %fd274;
	@%p180 bra 	BB50_260;

	mul.rn.f64 	%fd3469, %fd274, %fd1811;

BB50_260:
	mov.f64 	%fd2882, 0d397B839A252049C0;
	mov.f64 	%fd2881, 0d3C91A62633145C00;
	mov.f64 	%fd2879, 0d3FF921FB54442D18;
	mul.f64 	%fd2291, %fd3469, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r665, %fd2291;
	st.local.u32 	[%rd1], %r665;
	cvt.rn.f64.s32	%fd2292, %r665;
	neg.f64 	%fd2293, %fd2292;
	fma.rn.f64 	%fd2295, %fd2293, %fd2879, %fd3469;
	fma.rn.f64 	%fd2297, %fd2293, %fd2881, %fd2295;
	fma.rn.f64 	%fd3426, %fd2293, %fd2882, %fd2297;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r529}, %fd3469;
	}
	and.b32  	%r530, %r529, 2145386496;
	setp.lt.u32	%p241, %r530, 1105199104;
	@%p241 bra 	BB50_262;

	add.u64 	%rd161, %SP, 0;
	// Callseq Start 63
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3469;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd161;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3426, [retval0+0];
	
	//{
	}// Callseq End 63
	ld.local.u32 	%r665, [%rd1];

BB50_262:
	mov.f64 	%fd3203, 0d3FF0000000000000;
	mov.f64 	%fd3202, 0dBFE0000000000000;
	mov.f64 	%fd3201, 0d3FA5555555555551;
	mov.f64 	%fd3173, 0dBF56C16C16C15D47;
	mov.f64 	%fd3172, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3171, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3170, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3169, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2299, %fd3426, %fd3426;
	fma.rn.f64 	%fd2302, %fd3169, %fd2299, %fd3170;
	fma.rn.f64 	%fd2304, %fd2302, %fd2299, %fd3171;
	fma.rn.f64 	%fd2306, %fd2304, %fd2299, %fd3172;
	fma.rn.f64 	%fd2308, %fd2306, %fd2299, %fd3173;
	fma.rn.f64 	%fd2310, %fd2308, %fd2299, %fd3201;
	fma.rn.f64 	%fd2312, %fd2310, %fd2299, %fd3202;
	fma.rn.f64 	%fd2314, %fd2312, %fd2299, %fd3203;
	fma.rn.f64 	%fd2317, %fd1801, %fd2299, %fd1800;
	fma.rn.f64 	%fd2319, %fd2317, %fd2299, %fd1803;
	fma.rn.f64 	%fd2321, %fd2319, %fd2299, %fd1805;
	fma.rn.f64 	%fd2323, %fd2321, %fd2299, %fd1807;
	fma.rn.f64 	%fd2325, %fd2323, %fd2299, %fd1809;
	fma.rn.f64 	%fd2327, %fd2325, %fd2299, %fd1811;
	fma.rn.f64 	%fd2328, %fd2327, %fd3426, %fd3426;
	and.b32  	%r531, %r665, 1;
	setp.eq.b32	%p242, %r531, 1;
	not.pred 	%p243, %p242;
	selp.f64	%fd3427, %fd2328, %fd2314, %p243;
	and.b32  	%r532, %r665, 2;
	setp.eq.s32	%p244, %r532, 0;
	@%p244 bra 	BB50_264;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r533}, %fd3427;
	}
	xor.b32  	%r534, %r533, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r535, %temp}, %fd3427;
	}
	mov.b64 	%fd3427, {%r535, %r534};

BB50_264:
	mul.f64 	%fd405, %fd3425, %fd3427;
	mov.f64 	%fd3486, %fd281;
	@%p185 bra 	BB50_266;

	mul.rn.f64 	%fd3486, %fd281, %fd1811;

BB50_266:
	mov.f64 	%fd2884, 0d397B839A252049C0;
	mov.f64 	%fd2883, 0d3C91A62633145C00;
	mov.f64 	%fd2880, 0d3FF921FB54442D18;
	mul.f64 	%fd2330, %fd3486, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r666, %fd2330;
	st.local.u32 	[%rd1], %r666;
	cvt.rn.f64.s32	%fd2331, %r666;
	neg.f64 	%fd2332, %fd2331;
	fma.rn.f64 	%fd2334, %fd2332, %fd2880, %fd3486;
	fma.rn.f64 	%fd2336, %fd2332, %fd2883, %fd2334;
	fma.rn.f64 	%fd3428, %fd2332, %fd2884, %fd2336;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r536}, %fd3486;
	}
	and.b32  	%r537, %r536, 2145386496;
	setp.lt.u32	%p246, %r537, 1105199104;
	@%p246 bra 	BB50_268;

	add.u64 	%rd160, %SP, 0;
	// Callseq Start 64
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3486;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd160;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3428, [retval0+0];
	
	//{
	}// Callseq End 64
	ld.local.u32 	%r666, [%rd1];

BB50_268:
	mov.f64 	%fd3178, 0d3FF0000000000000;
	mov.f64 	%fd3177, 0dBFE0000000000000;
	mov.f64 	%fd3176, 0d3FA5555555555551;
	mov.f64 	%fd3175, 0dBF56C16C16C15D47;
	mov.f64 	%fd3174, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3133, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3132, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3131, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2338, %fd3428, %fd3428;
	fma.rn.f64 	%fd2341, %fd3131, %fd2338, %fd3132;
	fma.rn.f64 	%fd2343, %fd2341, %fd2338, %fd3133;
	fma.rn.f64 	%fd2345, %fd2343, %fd2338, %fd3174;
	fma.rn.f64 	%fd2347, %fd2345, %fd2338, %fd3175;
	fma.rn.f64 	%fd2349, %fd2347, %fd2338, %fd3176;
	fma.rn.f64 	%fd2351, %fd2349, %fd2338, %fd3177;
	fma.rn.f64 	%fd2353, %fd2351, %fd2338, %fd3178;
	fma.rn.f64 	%fd2356, %fd1801, %fd2338, %fd1800;
	fma.rn.f64 	%fd2358, %fd2356, %fd2338, %fd1803;
	fma.rn.f64 	%fd2360, %fd2358, %fd2338, %fd1805;
	fma.rn.f64 	%fd2362, %fd2360, %fd2338, %fd1807;
	fma.rn.f64 	%fd2364, %fd2362, %fd2338, %fd1809;
	fma.rn.f64 	%fd2366, %fd2364, %fd2338, %fd1811;
	fma.rn.f64 	%fd2367, %fd2366, %fd3428, %fd3428;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r538}, %fd2367;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r539, %temp}, %fd2367;
	}
	xor.b32  	%r540, %r538, -2147483648;
	mov.b64 	%fd2368, {%r539, %r540};
	and.b32  	%r541, %r666, 1;
	setp.eq.b32	%p247, %r541, 1;
	not.pred 	%p248, %p247;
	selp.f64	%fd3429, %fd2353, %fd2368, %p248;
	and.b32  	%r542, %r666, 2;
	setp.eq.s32	%p249, %r542, 0;
	@%p249 bra 	BB50_270;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r543}, %fd3429;
	}
	xor.b32  	%r544, %r543, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r545, %temp}, %fd3429;
	}
	mov.b64 	%fd3429, {%r545, %r544};

BB50_270:
	mul.f64 	%fd414, %fd405, %fd3429;
	mov.f64 	%fd3451, %fd267;
	@%p175 bra 	BB50_272;

	mul.rn.f64 	%fd3451, %fd267, %fd1811;

BB50_272:
	mov.f64 	%fd2841, 0d397B839A252049C0;
	mov.f64 	%fd2840, 0d3C91A62633145C00;
	mov.f64 	%fd2839, 0d3FF921FB54442D18;
	mul.f64 	%fd2370, %fd3451, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r667, %fd2370;
	st.local.u32 	[%rd1], %r667;
	cvt.rn.f64.s32	%fd2371, %r667;
	neg.f64 	%fd2372, %fd2371;
	fma.rn.f64 	%fd2374, %fd2372, %fd2839, %fd3451;
	fma.rn.f64 	%fd2376, %fd2372, %fd2840, %fd2374;
	fma.rn.f64 	%fd3430, %fd2372, %fd2841, %fd2376;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r546}, %fd3451;
	}
	and.b32  	%r547, %r546, 2145386496;
	setp.lt.u32	%p251, %r547, 1105199104;
	@%p251 bra 	BB50_274;

	add.u64 	%rd159, %SP, 0;
	// Callseq Start 65
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3451;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd159;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3430, [retval0+0];
	
	//{
	}// Callseq End 65
	ld.local.u32 	%r667, [%rd1];

BB50_274:
	mov.f64 	%fd3179, 0d3FF0000000000000;
	mov.f64 	%fd3140, 0dBFE0000000000000;
	mov.f64 	%fd3139, 0d3FA5555555555551;
	mov.f64 	%fd3138, 0dBF56C16C16C15D47;
	mov.f64 	%fd3137, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3136, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3135, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3134, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2378, %fd3430, %fd3430;
	fma.rn.f64 	%fd2381, %fd3134, %fd2378, %fd3135;
	fma.rn.f64 	%fd2383, %fd2381, %fd2378, %fd3136;
	fma.rn.f64 	%fd2385, %fd2383, %fd2378, %fd3137;
	fma.rn.f64 	%fd2387, %fd2385, %fd2378, %fd3138;
	fma.rn.f64 	%fd2389, %fd2387, %fd2378, %fd3139;
	fma.rn.f64 	%fd2391, %fd2389, %fd2378, %fd3140;
	fma.rn.f64 	%fd2393, %fd2391, %fd2378, %fd3179;
	fma.rn.f64 	%fd2396, %fd1801, %fd2378, %fd1800;
	fma.rn.f64 	%fd2398, %fd2396, %fd2378, %fd1803;
	fma.rn.f64 	%fd2400, %fd2398, %fd2378, %fd1805;
	fma.rn.f64 	%fd2402, %fd2400, %fd2378, %fd1807;
	fma.rn.f64 	%fd2404, %fd2402, %fd2378, %fd1809;
	fma.rn.f64 	%fd2406, %fd2404, %fd2378, %fd1811;
	fma.rn.f64 	%fd2407, %fd2406, %fd3430, %fd3430;
	and.b32  	%r548, %r667, 1;
	setp.eq.b32	%p252, %r548, 1;
	not.pred 	%p253, %p252;
	selp.f64	%fd3431, %fd2407, %fd2393, %p253;
	and.b32  	%r549, %r667, 2;
	setp.eq.s32	%p254, %r549, 0;
	@%p254 bra 	BB50_276;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r550}, %fd3431;
	}
	xor.b32  	%r551, %r550, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r552, %temp}, %fd3431;
	}
	mov.b64 	%fd3431, {%r552, %r551};

BB50_276:
	mov.f64 	%fd3468, %fd274;
	@%p180 bra 	BB50_278;

	mul.rn.f64 	%fd3468, %fd274, %fd1811;

BB50_278:
	mov.f64 	%fd2844, 0d397B839A252049C0;
	mov.f64 	%fd2843, 0d3C91A62633145C00;
	mov.f64 	%fd2842, 0d3FF921FB54442D18;
	mul.f64 	%fd2409, %fd3468, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r668, %fd2409;
	st.local.u32 	[%rd1], %r668;
	cvt.rn.f64.s32	%fd2410, %r668;
	neg.f64 	%fd2411, %fd2410;
	fma.rn.f64 	%fd2413, %fd2411, %fd2842, %fd3468;
	fma.rn.f64 	%fd2415, %fd2411, %fd2843, %fd2413;
	fma.rn.f64 	%fd3432, %fd2411, %fd2844, %fd2415;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r553}, %fd3468;
	}
	and.b32  	%r554, %r553, 2145386496;
	setp.lt.u32	%p256, %r554, 1105199104;
	@%p256 bra 	BB50_280;

	add.u64 	%rd158, %SP, 0;
	// Callseq Start 66
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3468;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd158;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3432, [retval0+0];
	
	//{
	}// Callseq End 66
	ld.local.u32 	%r668, [%rd1];

BB50_280:
	mov.f64 	%fd3144, 0d3FF0000000000000;
	mov.f64 	%fd3143, 0dBFE0000000000000;
	mov.f64 	%fd3142, 0d3FA5555555555551;
	mov.f64 	%fd3141, 0dBF56C16C16C15D47;
	mov.f64 	%fd3099, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3098, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3097, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3096, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2417, %fd3432, %fd3432;
	fma.rn.f64 	%fd2420, %fd3096, %fd2417, %fd3097;
	fma.rn.f64 	%fd2422, %fd2420, %fd2417, %fd3098;
	fma.rn.f64 	%fd2424, %fd2422, %fd2417, %fd3099;
	fma.rn.f64 	%fd2426, %fd2424, %fd2417, %fd3141;
	fma.rn.f64 	%fd2428, %fd2426, %fd2417, %fd3142;
	fma.rn.f64 	%fd2430, %fd2428, %fd2417, %fd3143;
	fma.rn.f64 	%fd2432, %fd2430, %fd2417, %fd3144;
	fma.rn.f64 	%fd2435, %fd1801, %fd2417, %fd1800;
	fma.rn.f64 	%fd2437, %fd2435, %fd2417, %fd1803;
	fma.rn.f64 	%fd2439, %fd2437, %fd2417, %fd1805;
	fma.rn.f64 	%fd2441, %fd2439, %fd2417, %fd1807;
	fma.rn.f64 	%fd2443, %fd2441, %fd2417, %fd1809;
	fma.rn.f64 	%fd2445, %fd2443, %fd2417, %fd1811;
	fma.rn.f64 	%fd2446, %fd2445, %fd3432, %fd3432;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r555}, %fd2446;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r556, %temp}, %fd2446;
	}
	xor.b32  	%r557, %r555, -2147483648;
	mov.b64 	%fd2447, {%r556, %r557};
	and.b32  	%r558, %r668, 1;
	setp.eq.b32	%p257, %r558, 1;
	not.pred 	%p258, %p257;
	selp.f64	%fd3433, %fd2432, %fd2447, %p258;
	and.b32  	%r559, %r668, 2;
	setp.eq.s32	%p259, %r559, 0;
	@%p259 bra 	BB50_282;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r560}, %fd3433;
	}
	xor.b32  	%r561, %r560, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r562, %temp}, %fd3433;
	}
	mov.b64 	%fd3433, {%r562, %r561};

BB50_282:
	mul.f64 	%fd431, %fd3431, %fd3433;
	mov.f64 	%fd3485, %fd281;
	@%p185 bra 	BB50_284;

	mul.rn.f64 	%fd3485, %fd281, %fd1811;

BB50_284:
	mov.f64 	%fd2847, 0d397B839A252049C0;
	mov.f64 	%fd2846, 0d3C91A62633145C00;
	mov.f64 	%fd2845, 0d3FF921FB54442D18;
	mul.f64 	%fd2449, %fd3485, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r669, %fd2449;
	st.local.u32 	[%rd1], %r669;
	cvt.rn.f64.s32	%fd2450, %r669;
	neg.f64 	%fd2451, %fd2450;
	fma.rn.f64 	%fd2453, %fd2451, %fd2845, %fd3485;
	fma.rn.f64 	%fd2455, %fd2451, %fd2846, %fd2453;
	fma.rn.f64 	%fd3434, %fd2451, %fd2847, %fd2455;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r563}, %fd3485;
	}
	and.b32  	%r564, %r563, 2145386496;
	setp.lt.u32	%p261, %r564, 1105199104;
	@%p261 bra 	BB50_286;

	add.u64 	%rd157, %SP, 0;
	// Callseq Start 67
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3485;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd157;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3434, [retval0+0];
	
	//{
	}// Callseq End 67
	ld.local.u32 	%r669, [%rd1];

BB50_286:
	mov.f64 	%fd3107, 0d3FF0000000000000;
	mov.f64 	%fd3106, 0dBFE0000000000000;
	mov.f64 	%fd3105, 0d3FA5555555555551;
	mov.f64 	%fd3104, 0dBF56C16C16C15D47;
	mov.f64 	%fd3103, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3102, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3101, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3100, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2457, %fd3434, %fd3434;
	fma.rn.f64 	%fd2460, %fd3100, %fd2457, %fd3101;
	fma.rn.f64 	%fd2462, %fd2460, %fd2457, %fd3102;
	fma.rn.f64 	%fd2464, %fd2462, %fd2457, %fd3103;
	fma.rn.f64 	%fd2466, %fd2464, %fd2457, %fd3104;
	fma.rn.f64 	%fd2468, %fd2466, %fd2457, %fd3105;
	fma.rn.f64 	%fd2470, %fd2468, %fd2457, %fd3106;
	fma.rn.f64 	%fd2472, %fd2470, %fd2457, %fd3107;
	fma.rn.f64 	%fd2475, %fd1801, %fd2457, %fd1800;
	fma.rn.f64 	%fd2477, %fd2475, %fd2457, %fd1803;
	fma.rn.f64 	%fd2479, %fd2477, %fd2457, %fd1805;
	fma.rn.f64 	%fd2481, %fd2479, %fd2457, %fd1807;
	fma.rn.f64 	%fd2483, %fd2481, %fd2457, %fd1809;
	fma.rn.f64 	%fd2485, %fd2483, %fd2457, %fd1811;
	fma.rn.f64 	%fd2486, %fd2485, %fd3434, %fd3434;
	and.b32  	%r565, %r669, 1;
	setp.eq.b32	%p262, %r565, 1;
	not.pred 	%p263, %p262;
	selp.f64	%fd3435, %fd2486, %fd2472, %p263;
	and.b32  	%r566, %r669, 2;
	setp.eq.s32	%p264, %r566, 0;
	@%p264 bra 	BB50_288;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r567}, %fd3435;
	}
	xor.b32  	%r568, %r567, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r569, %temp}, %fd3435;
	}
	mov.b64 	%fd3435, {%r569, %r568};

BB50_288:
	mul.f64 	%fd2487, %fd431, %fd3435;
	sub.f64 	%fd440, %fd414, %fd2487;
	mov.f64 	%fd3450, %fd267;
	@%p175 bra 	BB50_290;

	mul.rn.f64 	%fd3450, %fd267, %fd1811;

BB50_290:
	mov.f64 	%fd2805, 0d397B839A252049C0;
	mov.f64 	%fd2804, 0d3C91A62633145C00;
	mov.f64 	%fd2803, 0d3FF921FB54442D18;
	mul.f64 	%fd2489, %fd3450, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r670, %fd2489;
	st.local.u32 	[%rd1], %r670;
	cvt.rn.f64.s32	%fd2490, %r670;
	neg.f64 	%fd2491, %fd2490;
	fma.rn.f64 	%fd2493, %fd2491, %fd2803, %fd3450;
	fma.rn.f64 	%fd2495, %fd2491, %fd2804, %fd2493;
	fma.rn.f64 	%fd3436, %fd2491, %fd2805, %fd2495;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r570}, %fd3450;
	}
	and.b32  	%r571, %r570, 2145386496;
	setp.lt.u32	%p266, %r571, 1105199104;
	@%p266 bra 	BB50_292;

	add.u64 	%rd156, %SP, 0;
	// Callseq Start 68
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3450;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd156;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3436, [retval0+0];
	
	//{
	}// Callseq End 68
	ld.local.u32 	%r670, [%rd1];

BB50_292:
	mov.f64 	%fd3111, 0d3FF0000000000000;
	mov.f64 	%fd3110, 0dBFE0000000000000;
	mov.f64 	%fd3109, 0d3FA5555555555551;
	mov.f64 	%fd3108, 0dBF56C16C16C15D47;
	mov.f64 	%fd3057, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3056, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3055, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3054, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2497, %fd3436, %fd3436;
	fma.rn.f64 	%fd2500, %fd3054, %fd2497, %fd3055;
	fma.rn.f64 	%fd2502, %fd2500, %fd2497, %fd3056;
	fma.rn.f64 	%fd2504, %fd2502, %fd2497, %fd3057;
	fma.rn.f64 	%fd2506, %fd2504, %fd2497, %fd3108;
	fma.rn.f64 	%fd2508, %fd2506, %fd2497, %fd3109;
	fma.rn.f64 	%fd2510, %fd2508, %fd2497, %fd3110;
	fma.rn.f64 	%fd2512, %fd2510, %fd2497, %fd3111;
	fma.rn.f64 	%fd2515, %fd1801, %fd2497, %fd1800;
	fma.rn.f64 	%fd2517, %fd2515, %fd2497, %fd1803;
	fma.rn.f64 	%fd2519, %fd2517, %fd2497, %fd1805;
	fma.rn.f64 	%fd2521, %fd2519, %fd2497, %fd1807;
	fma.rn.f64 	%fd2523, %fd2521, %fd2497, %fd1809;
	fma.rn.f64 	%fd2525, %fd2523, %fd2497, %fd1811;
	fma.rn.f64 	%fd2526, %fd2525, %fd3436, %fd3436;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r572}, %fd2526;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r573, %temp}, %fd2526;
	}
	xor.b32  	%r574, %r572, -2147483648;
	mov.b64 	%fd2527, {%r573, %r574};
	and.b32  	%r575, %r670, 1;
	setp.eq.b32	%p267, %r575, 1;
	not.pred 	%p268, %p267;
	selp.f64	%fd3437, %fd2512, %fd2527, %p268;
	and.b32  	%r576, %r670, 2;
	setp.eq.s32	%p269, %r576, 0;
	@%p269 bra 	BB50_294;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r577}, %fd3437;
	}
	xor.b32  	%r578, %r577, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r579, %temp}, %fd3437;
	}
	mov.b64 	%fd3437, {%r579, %r578};

BB50_294:
	mov.f64 	%fd3467, %fd274;
	@%p180 bra 	BB50_296;

	mul.rn.f64 	%fd3467, %fd274, %fd1811;

BB50_296:
	mov.f64 	%fd2808, 0d397B839A252049C0;
	mov.f64 	%fd2807, 0d3C91A62633145C00;
	mov.f64 	%fd2806, 0d3FF921FB54442D18;
	mul.f64 	%fd2529, %fd3467, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r671, %fd2529;
	st.local.u32 	[%rd1], %r671;
	cvt.rn.f64.s32	%fd2530, %r671;
	neg.f64 	%fd2531, %fd2530;
	fma.rn.f64 	%fd2533, %fd2531, %fd2806, %fd3467;
	fma.rn.f64 	%fd2535, %fd2531, %fd2807, %fd2533;
	fma.rn.f64 	%fd3438, %fd2531, %fd2808, %fd2535;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r580}, %fd3467;
	}
	and.b32  	%r581, %r580, 2145386496;
	setp.lt.u32	%p271, %r581, 1105199104;
	@%p271 bra 	BB50_298;

	add.u64 	%rd155, %SP, 0;
	// Callseq Start 69
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3467;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd155;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3438, [retval0+0];
	
	//{
	}// Callseq End 69
	ld.local.u32 	%r671, [%rd1];

BB50_298:
	mov.f64 	%fd3065, 0d3FF0000000000000;
	mov.f64 	%fd3064, 0dBFE0000000000000;
	mov.f64 	%fd3063, 0d3FA5555555555551;
	mov.f64 	%fd3062, 0dBF56C16C16C15D47;
	mov.f64 	%fd3061, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3060, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3059, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3058, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2537, %fd3438, %fd3438;
	fma.rn.f64 	%fd2540, %fd3058, %fd2537, %fd3059;
	fma.rn.f64 	%fd2542, %fd2540, %fd2537, %fd3060;
	fma.rn.f64 	%fd2544, %fd2542, %fd2537, %fd3061;
	fma.rn.f64 	%fd2546, %fd2544, %fd2537, %fd3062;
	fma.rn.f64 	%fd2548, %fd2546, %fd2537, %fd3063;
	fma.rn.f64 	%fd2550, %fd2548, %fd2537, %fd3064;
	fma.rn.f64 	%fd2552, %fd2550, %fd2537, %fd3065;
	fma.rn.f64 	%fd2555, %fd1801, %fd2537, %fd1800;
	fma.rn.f64 	%fd2557, %fd2555, %fd2537, %fd1803;
	fma.rn.f64 	%fd2559, %fd2557, %fd2537, %fd1805;
	fma.rn.f64 	%fd2561, %fd2559, %fd2537, %fd1807;
	fma.rn.f64 	%fd2563, %fd2561, %fd2537, %fd1809;
	fma.rn.f64 	%fd2565, %fd2563, %fd2537, %fd1811;
	fma.rn.f64 	%fd2566, %fd2565, %fd3438, %fd3438;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r582}, %fd2566;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r583, %temp}, %fd2566;
	}
	xor.b32  	%r584, %r582, -2147483648;
	mov.b64 	%fd2567, {%r583, %r584};
	and.b32  	%r585, %r671, 1;
	setp.eq.b32	%p272, %r585, 1;
	not.pred 	%p273, %p272;
	selp.f64	%fd3439, %fd2552, %fd2567, %p273;
	and.b32  	%r586, %r671, 2;
	setp.eq.s32	%p274, %r586, 0;
	@%p274 bra 	BB50_300;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r587}, %fd3439;
	}
	xor.b32  	%r588, %r587, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r589, %temp}, %fd3439;
	}
	mov.b64 	%fd3439, {%r589, %r588};

BB50_300:
	mul.f64 	%fd457, %fd3437, %fd3439;
	mov.f64 	%fd3484, %fd281;
	@%p185 bra 	BB50_302;

	mul.rn.f64 	%fd3484, %fd281, %fd1811;

BB50_302:
	mov.f64 	%fd2811, 0d397B839A252049C0;
	mov.f64 	%fd2810, 0d3C91A62633145C00;
	mov.f64 	%fd2809, 0d3FF921FB54442D18;
	mul.f64 	%fd2569, %fd3484, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r672, %fd2569;
	st.local.u32 	[%rd1], %r672;
	cvt.rn.f64.s32	%fd2570, %r672;
	neg.f64 	%fd2571, %fd2570;
	fma.rn.f64 	%fd2573, %fd2571, %fd2809, %fd3484;
	fma.rn.f64 	%fd2575, %fd2571, %fd2810, %fd2573;
	fma.rn.f64 	%fd3440, %fd2571, %fd2811, %fd2575;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r590}, %fd3484;
	}
	and.b32  	%r591, %r590, 2145386496;
	setp.lt.u32	%p276, %r591, 1105199104;
	@%p276 bra 	BB50_304;

	add.u64 	%rd154, %SP, 0;
	// Callseq Start 70
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3484;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd154;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3440, [retval0+0];
	
	//{
	}// Callseq End 70
	ld.local.u32 	%r672, [%rd1];

BB50_304:
	mov.f64 	%fd3180, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3068, 0d3FF0000000000000;
	mov.f64 	%fd3067, 0dBFE0000000000000;
	mov.f64 	%fd3066, 0d3FA5555555555551;
	mov.f64 	%fd3014, 0dBF56C16C16C15D47;
	mov.f64 	%fd3013, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3012, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3011, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3010, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2577, %fd3440, %fd3440;
	fma.rn.f64 	%fd2580, %fd3010, %fd2577, %fd3011;
	fma.rn.f64 	%fd2582, %fd2580, %fd2577, %fd3012;
	fma.rn.f64 	%fd2584, %fd2582, %fd2577, %fd3013;
	fma.rn.f64 	%fd2586, %fd2584, %fd2577, %fd3014;
	fma.rn.f64 	%fd2588, %fd2586, %fd2577, %fd3066;
	fma.rn.f64 	%fd2590, %fd2588, %fd2577, %fd3067;
	fma.rn.f64 	%fd2592, %fd2590, %fd2577, %fd3068;
	fma.rn.f64 	%fd2595, %fd3180, %fd2577, %fd1800;
	fma.rn.f64 	%fd2597, %fd2595, %fd2577, %fd1803;
	fma.rn.f64 	%fd2599, %fd2597, %fd2577, %fd1805;
	fma.rn.f64 	%fd2601, %fd2599, %fd2577, %fd1807;
	fma.rn.f64 	%fd2603, %fd2601, %fd2577, %fd1809;
	fma.rn.f64 	%fd2605, %fd2603, %fd2577, %fd1811;
	fma.rn.f64 	%fd2606, %fd2605, %fd3440, %fd3440;
	and.b32  	%r592, %r672, 1;
	setp.eq.b32	%p277, %r592, 1;
	not.pred 	%p278, %p277;
	selp.f64	%fd3441, %fd2606, %fd2592, %p278;
	and.b32  	%r593, %r672, 2;
	setp.eq.s32	%p279, %r593, 0;
	@%p279 bra 	BB50_306;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r594}, %fd3441;
	}
	xor.b32  	%r595, %r594, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r596, %temp}, %fd3441;
	}
	mov.b64 	%fd3441, {%r596, %r595};

BB50_306:
	mul.f64 	%fd466, %fd457, %fd3441;
	mov.f64 	%fd3449, %fd267;
	@%p175 bra 	BB50_308;

	mul.rn.f64 	%fd3449, %fd267, %fd1811;

BB50_308:
	mov.f64 	%fd2759, 0d397B839A252049C0;
	mov.f64 	%fd2758, 0d3C91A62633145C00;
	mov.f64 	%fd2757, 0d3FF921FB54442D18;
	mul.f64 	%fd2608, %fd3449, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r673, %fd2608;
	st.local.u32 	[%rd1], %r673;
	cvt.rn.f64.s32	%fd2609, %r673;
	neg.f64 	%fd2610, %fd2609;
	fma.rn.f64 	%fd2612, %fd2610, %fd2757, %fd3449;
	fma.rn.f64 	%fd2614, %fd2610, %fd2758, %fd2612;
	fma.rn.f64 	%fd3457, %fd2610, %fd2759, %fd2614;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r597}, %fd3449;
	}
	and.b32  	%r598, %r597, 2145386496;
	setp.lt.u32	%p281, %r598, 1105199104;
	@%p281 bra 	BB50_310;

	add.u64 	%rd153, %SP, 0;
	// Callseq Start 71
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3449;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd153;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3457, [retval0+0];
	
	//{
	}// Callseq End 71
	ld.local.u32 	%r673, [%rd1];

BB50_310:
	mov.f64 	%fd3023, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3022, 0d3FF0000000000000;
	mov.f64 	%fd3021, 0dBFE0000000000000;
	mov.f64 	%fd3020, 0d3FA5555555555551;
	mov.f64 	%fd3019, 0dBF56C16C16C15D47;
	mov.f64 	%fd3018, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3017, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3016, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3015, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2616, %fd3457, %fd3457;
	fma.rn.f64 	%fd2619, %fd3015, %fd2616, %fd3016;
	fma.rn.f64 	%fd2621, %fd2619, %fd2616, %fd3017;
	fma.rn.f64 	%fd2623, %fd2621, %fd2616, %fd3018;
	fma.rn.f64 	%fd2625, %fd2623, %fd2616, %fd3019;
	fma.rn.f64 	%fd2627, %fd2625, %fd2616, %fd3020;
	fma.rn.f64 	%fd2629, %fd2627, %fd2616, %fd3021;
	fma.rn.f64 	%fd2631, %fd2629, %fd2616, %fd3022;
	fma.rn.f64 	%fd2634, %fd3023, %fd2616, %fd1800;
	fma.rn.f64 	%fd2636, %fd2634, %fd2616, %fd1803;
	fma.rn.f64 	%fd2638, %fd2636, %fd2616, %fd1805;
	fma.rn.f64 	%fd2640, %fd2638, %fd2616, %fd1807;
	fma.rn.f64 	%fd2642, %fd2640, %fd2616, %fd1809;
	fma.rn.f64 	%fd2644, %fd2642, %fd2616, %fd1811;
	fma.rn.f64 	%fd2645, %fd2644, %fd3457, %fd3457;
	and.b32  	%r599, %r673, 1;
	setp.eq.b32	%p282, %r599, 1;
	not.pred 	%p283, %p282;
	selp.f64	%fd3458, %fd2645, %fd2631, %p283;
	and.b32  	%r600, %r673, 2;
	setp.eq.s32	%p284, %r600, 0;
	@%p284 bra 	BB50_312;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r601}, %fd3458;
	}
	xor.b32  	%r602, %r601, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r603, %temp}, %fd3458;
	}
	mov.b64 	%fd3458, {%r603, %r602};

BB50_312:
	mov.f64 	%fd3466, %fd274;
	@%p180 bra 	BB50_314;

	mul.rn.f64 	%fd3466, %fd274, %fd1811;

BB50_314:
	mov.f64 	%fd2762, 0d397B839A252049C0;
	mov.f64 	%fd2761, 0d3C91A62633145C00;
	mov.f64 	%fd2760, 0d3FF921FB54442D18;
	mul.f64 	%fd2647, %fd3466, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r674, %fd2647;
	st.local.u32 	[%rd1], %r674;
	cvt.rn.f64.s32	%fd2648, %r674;
	neg.f64 	%fd2649, %fd2648;
	fma.rn.f64 	%fd2651, %fd2649, %fd2760, %fd3466;
	fma.rn.f64 	%fd2653, %fd2649, %fd2761, %fd2651;
	fma.rn.f64 	%fd3474, %fd2649, %fd2762, %fd2653;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r604}, %fd3466;
	}
	and.b32  	%r605, %r604, 2145386496;
	setp.lt.u32	%p286, %r605, 1105199104;
	@%p286 bra 	BB50_316;

	add.u64 	%rd152, %SP, 0;
	// Callseq Start 72
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3466;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd152;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3474, [retval0+0];
	
	//{
	}// Callseq End 72
	ld.local.u32 	%r674, [%rd1];

BB50_316:
	mov.f64 	%fd3025, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3024, 0d3FF0000000000000;
	mov.f64 	%fd2958, 0dBFE0000000000000;
	mov.f64 	%fd2957, 0d3FA5555555555551;
	mov.f64 	%fd2956, 0dBF56C16C16C15D47;
	mov.f64 	%fd2955, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2954, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2953, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2952, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2655, %fd3474, %fd3474;
	fma.rn.f64 	%fd2658, %fd2952, %fd2655, %fd2953;
	fma.rn.f64 	%fd2660, %fd2658, %fd2655, %fd2954;
	fma.rn.f64 	%fd2662, %fd2660, %fd2655, %fd2955;
	fma.rn.f64 	%fd2664, %fd2662, %fd2655, %fd2956;
	fma.rn.f64 	%fd2666, %fd2664, %fd2655, %fd2957;
	fma.rn.f64 	%fd2668, %fd2666, %fd2655, %fd2958;
	fma.rn.f64 	%fd2670, %fd2668, %fd2655, %fd3024;
	fma.rn.f64 	%fd2673, %fd3025, %fd2655, %fd1800;
	fma.rn.f64 	%fd2675, %fd2673, %fd2655, %fd1803;
	fma.rn.f64 	%fd2677, %fd2675, %fd2655, %fd1805;
	fma.rn.f64 	%fd2679, %fd2677, %fd2655, %fd1807;
	fma.rn.f64 	%fd2681, %fd2679, %fd2655, %fd1809;
	fma.rn.f64 	%fd2683, %fd2681, %fd2655, %fd1811;
	fma.rn.f64 	%fd2684, %fd2683, %fd3474, %fd3474;
	and.b32  	%r606, %r674, 1;
	setp.eq.b32	%p287, %r606, 1;
	not.pred 	%p288, %p287;
	selp.f64	%fd3475, %fd2684, %fd2670, %p288;
	and.b32  	%r607, %r674, 2;
	setp.eq.s32	%p289, %r607, 0;
	@%p289 bra 	BB50_318;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r608}, %fd3475;
	}
	xor.b32  	%r609, %r608, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r610, %temp}, %fd3475;
	}
	mov.b64 	%fd3475, {%r610, %r609};

BB50_318:
	mul.f64 	%fd483, %fd3458, %fd3475;
	mov.f64 	%fd3483, %fd281;
	@%p185 bra 	BB50_320;

	mul.rn.f64 	%fd3483, %fd281, %fd1811;

BB50_320:
	mov.f64 	%fd2765, 0d397B839A252049C0;
	mov.f64 	%fd2764, 0d3C91A62633145C00;
	mov.f64 	%fd2763, 0d3FF921FB54442D18;
	mul.f64 	%fd2686, %fd3483, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r675, %fd2686;
	st.local.u32 	[%rd1], %r675;
	cvt.rn.f64.s32	%fd2687, %r675;
	neg.f64 	%fd2688, %fd2687;
	fma.rn.f64 	%fd2690, %fd2688, %fd2763, %fd3483;
	fma.rn.f64 	%fd2692, %fd2688, %fd2764, %fd2690;
	fma.rn.f64 	%fd3491, %fd2688, %fd2765, %fd2692;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r611}, %fd3483;
	}
	and.b32  	%r612, %r611, 2145386496;
	setp.lt.u32	%p291, %r612, 1105199104;
	@%p291 bra 	BB50_322;

	add.u64 	%rd151, %SP, 0;
	// Callseq Start 73
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3483;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd151;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3491, [retval0+0];
	
	//{
	}// Callseq End 73
	ld.local.u32 	%r675, [%rd1];

BB50_322:
	mov.f64 	%fd2968, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2967, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2966, 0d3FF0000000000000;
	mov.f64 	%fd2965, 0dBFE0000000000000;
	mov.f64 	%fd2964, 0d3FA5555555555551;
	mov.f64 	%fd2963, 0dBF56C16C16C15D47;
	mov.f64 	%fd2962, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2961, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2960, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2959, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2694, %fd3491, %fd3491;
	fma.rn.f64 	%fd2697, %fd2959, %fd2694, %fd2960;
	fma.rn.f64 	%fd2699, %fd2697, %fd2694, %fd2961;
	fma.rn.f64 	%fd2701, %fd2699, %fd2694, %fd2962;
	fma.rn.f64 	%fd2703, %fd2701, %fd2694, %fd2963;
	fma.rn.f64 	%fd2705, %fd2703, %fd2694, %fd2964;
	fma.rn.f64 	%fd2707, %fd2705, %fd2694, %fd2965;
	fma.rn.f64 	%fd2709, %fd2707, %fd2694, %fd2966;
	fma.rn.f64 	%fd2712, %fd2967, %fd2694, %fd2968;
	fma.rn.f64 	%fd2714, %fd2712, %fd2694, %fd1803;
	fma.rn.f64 	%fd2716, %fd2714, %fd2694, %fd1805;
	fma.rn.f64 	%fd2718, %fd2716, %fd2694, %fd1807;
	fma.rn.f64 	%fd2720, %fd2718, %fd2694, %fd1809;
	fma.rn.f64 	%fd2722, %fd2720, %fd2694, %fd1811;
	fma.rn.f64 	%fd2723, %fd2722, %fd3491, %fd3491;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r613}, %fd2723;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r614, %temp}, %fd2723;
	}
	xor.b32  	%r615, %r613, -2147483648;
	mov.b64 	%fd2724, {%r614, %r615};
	and.b32  	%r616, %r675, 1;
	setp.eq.b32	%p292, %r616, 1;
	not.pred 	%p293, %p292;
	selp.f64	%fd3492, %fd2709, %fd2724, %p293;
	and.b32  	%r617, %r675, 2;
	setp.eq.s32	%p294, %r617, 0;
	@%p294 bra 	BB50_324;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r618}, %fd3492;
	}
	xor.b32  	%r619, %r618, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r620, %temp}, %fd3492;
	}
	mov.b64 	%fd3492, {%r620, %r619};

BB50_324:
	mov.u32 	%r627, %tid.x;
	mov.u32 	%r626, %ctaid.x;
	mov.u32 	%r625, %ntid.x;
	mad.lo.s32 	%r624, %r625, %r626, %r627;
	mul.wide.s32 	%rd150, %r624, 32;
	ld.param.u64 	%rd149, [_Z18actfunc_quat_multiP7double4iiii_param_0];
	add.s64 	%rd148, %rd149, %rd150;
	st.v2.f64 	[%rd148], {%fd336, %fd388};
	fma.rn.f64 	%fd2725, %fd483, %fd3492, %fd466;
	st.v2.f64 	[%rd148+16], {%fd440, %fd2725};

BB50_325:
	ret;
}

	// .globl	_Z12actfunc_realIfEvPT_i
.visible .func _Z12actfunc_realIfEvPT_i(
	.param .b64 _Z12actfunc_realIfEvPT_i_param_0,
	.param .b32 _Z12actfunc_realIfEvPT_i_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z12actfunc_realIfEvPT_i_param_0];
	ld.param.u32 	%r2, [_Z12actfunc_realIfEvPT_i_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB51_2;

	mul.wide.s32 	%rd2, %r1, 4;
	add.s64 	%rd3, %rd1, %rd2;
	ld.f32 	%f1, [%rd3];
	setp.ltu.f32	%p2, %f1, 0f00000000;
	selp.f32	%f2, 0fBF800000, 0f3F800000, %p2;
	st.f32 	[%rd3], %f2;

BB51_2:
	ret;
}

	// .globl	_Z12actfunc_realIdEvPT_i
.visible .func _Z12actfunc_realIdEvPT_i(
	.param .b64 _Z12actfunc_realIdEvPT_i_param_0,
	.param .b32 _Z12actfunc_realIdEvPT_i_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z12actfunc_realIdEvPT_i_param_0];
	ld.param.u32 	%r2, [_Z12actfunc_realIdEvPT_i_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB52_2;

	mul.wide.s32 	%rd2, %r1, 8;
	add.s64 	%rd3, %rd1, %rd2;
	ld.f64 	%fd1, [%rd3];
	setp.ltu.f64	%p2, %fd1, 0d0000000000000000;
	selp.f64	%fd2, 0dBFF0000000000000, 0d3FF0000000000000, %p2;
	st.f64 	[%rd3], %fd2;

BB52_2:
	ret;
}

	// .globl	fill_real_float
.visible .entry fill_real_float(
	.param .u64 fill_real_float_param_0,
	.param .f32 fill_real_float_param_1,
	.param .u32 fill_real_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_real_float_param_0];
	ld.param.f32 	%f1, [fill_real_float_param_1];
	ld.param.u32 	%r2, [fill_real_float_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB53_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f32 	[%rd4], %f1;

BB53_2:
	ret;
}

	// .globl	fill_real_double
.visible .entry fill_real_double(
	.param .u64 fill_real_double_param_0,
	.param .f64 fill_real_double_param_1,
	.param .u32 fill_real_double_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_real_double_param_0];
	ld.param.f64 	%fd1, [fill_real_double_param_1];
	ld.param.u32 	%r2, [fill_real_double_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB54_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 8;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f64 	[%rd4], %fd1;

BB54_2:
	ret;
}

	// .globl	fill_comp_float
.visible .entry fill_comp_float(
	.param .u64 fill_comp_float_param_0,
	.param .align 8 .b8 fill_comp_float_param_1[8],
	.param .u32 fill_comp_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_comp_float_param_0];
	ld.param.f32 	%f2, [fill_comp_float_param_1+4];
	ld.param.f32 	%f1, [fill_comp_float_param_1];
	ld.param.u32 	%r2, [fill_comp_float_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB55_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 8;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.v2.f32 	[%rd4], {%f1, %f2};

BB55_2:
	ret;
}

	// .globl	fill_comp_double
.visible .entry fill_comp_double(
	.param .u64 fill_comp_double_param_0,
	.param .align 16 .b8 fill_comp_double_param_1[16],
	.param .u32 fill_comp_double_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_comp_double_param_0];
	ld.param.f64 	%fd2, [fill_comp_double_param_1+8];
	ld.param.f64 	%fd1, [fill_comp_double_param_1];
	ld.param.u32 	%r2, [fill_comp_double_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB56_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 16;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.v2.f64 	[%rd4], {%fd1, %fd2};

BB56_2:
	ret;
}

	// .globl	fill_quat_float
.visible .entry fill_quat_float(
	.param .u64 fill_quat_float_param_0,
	.param .align 16 .b8 fill_quat_float_param_1[16],
	.param .u32 fill_quat_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_quat_float_param_0];
	ld.param.f32 	%f4, [fill_quat_float_param_1+12];
	ld.param.f32 	%f3, [fill_quat_float_param_1+8];
	ld.param.f32 	%f2, [fill_quat_float_param_1+4];
	ld.param.f32 	%f1, [fill_quat_float_param_1];
	ld.param.u32 	%r2, [fill_quat_float_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB57_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 16;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.v4.f32 	[%rd4], {%f1, %f2, %f3, %f4};

BB57_2:
	ret;
}

	// .globl	fill_quat_double
.visible .entry fill_quat_double(
	.param .u64 fill_quat_double_param_0,
	.param .align 16 .b8 fill_quat_double_param_1[32],
	.param .u32 fill_quat_double_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_quat_double_param_0];
	ld.param.f64 	%fd4, [fill_quat_double_param_1+24];
	ld.param.f64 	%fd3, [fill_quat_double_param_1+16];
	ld.param.f64 	%fd2, [fill_quat_double_param_1+8];
	ld.param.f64 	%fd1, [fill_quat_double_param_1];
	ld.param.u32 	%r2, [fill_quat_double_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB58_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 32;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.v2.f64 	[%rd4], {%fd1, %fd2};
	st.global.v2.f64 	[%rd4+16], {%fd3, %fd4};

BB58_2:
	ret;
}

	// .globl	compare_real_int
.visible .entry compare_real_int(
	.param .u64 compare_real_int_param_0,
	.param .u64 compare_real_int_param_1,
	.param .u64 compare_real_int_param_2,
	.param .u32 compare_real_int_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [compare_real_int_param_0];
	ld.param.u64 	%rd2, [compare_real_int_param_1];
	ld.param.u64 	%rd3, [compare_real_int_param_2];
	ld.param.u32 	%r2, [compare_real_int_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB59_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.u32 	%r6, [%rd8];
	ld.global.u32 	%r7, [%rd6];
	setp.ne.s32	%p2, %r7, %r6;
	selp.u32	%r8, 1, 0, %p2;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.u32 	[%rd10], %r8;

BB59_2:
	ret;
}

	// .globl	compare_real_float
.visible .entry compare_real_float(
	.param .u64 compare_real_float_param_0,
	.param .u64 compare_real_float_param_1,
	.param .u64 compare_real_float_param_2,
	.param .u32 compare_real_float_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [compare_real_float_param_0];
	ld.param.u64 	%rd2, [compare_real_float_param_1];
	ld.param.u64 	%rd3, [compare_real_float_param_2];
	ld.param.u32 	%r2, [compare_real_float_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB60_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.f32 	%f3, %f2, %f1;
	abs.f32 	%f4, %f3;
	cvt.f64.f32	%fd1, %f4;
	abs.f32 	%f5, %f2;
	abs.f32 	%f6, %f1;
	max.f32 	%f7, %f5, %f6;
	cvt.f64.f32	%fd2, %f7;
	mul.f64 	%fd3, %fd2, 0d3EE9000000000000;
	setp.geu.f64	%p2, %fd1, %fd3;
	selp.u32	%r6, 1, 0, %p2;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.u32 	[%rd10], %r6;

BB60_2:
	ret;
}

	// .globl	compare_real_double
.visible .entry compare_real_double(
	.param .u64 compare_real_double_param_0,
	.param .u64 compare_real_double_param_1,
	.param .u64 compare_real_double_param_2,
	.param .u32 compare_real_double_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [compare_real_double_param_0];
	ld.param.u64 	%rd2, [compare_real_double_param_1];
	ld.param.u64 	%rd3, [compare_real_double_param_2];
	ld.param.u32 	%r2, [compare_real_double_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB61_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	sub.f64 	%fd3, %fd2, %fd1;
	abs.f64 	%fd4, %fd3;
	abs.f64 	%fd5, %fd2;
	abs.f64 	%fd6, %fd1;
	max.f64 	%fd7, %fd5, %fd6;
	mul.f64 	%fd8, %fd7, 0d3D83880000000000;
	setp.geu.f64	%p2, %fd4, %fd8;
	selp.u32	%r6, 1, 0, %p2;
	cvta.to.global.u64 	%rd9, %rd1;
	mul.wide.s32 	%rd10, %r1, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r6;

BB61_2:
	ret;
}

	// .globl	compare_comp_float
.visible .entry compare_comp_float(
	.param .u64 compare_comp_float_param_0,
	.param .u64 compare_comp_float_param_1,
	.param .u64 compare_comp_float_param_2,
	.param .u32 compare_comp_float_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [compare_comp_float_param_0];
	ld.param.u64 	%rd3, [compare_comp_float_param_1];
	ld.param.u64 	%rd4, [compare_comp_float_param_2];
	ld.param.u32 	%r1, [compare_comp_float_param_3];
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r5, %r2, %r3, %r4;
	cvt.s64.s32	%rd1, %r5;
	setp.ge.s32	%p3, %r5, %r1;
	@%p3 bra 	BB62_4;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 3;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v2.f32 	{%f5, %f6}, [%rd7];
	cvta.to.global.u64 	%rd8, %rd4;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.v2.f32 	{%f7, %f8}, [%rd9];
	sub.f32 	%f9, %f5, %f7;
	abs.f32 	%f10, %f9;
	cvt.f64.f32	%fd1, %f10;
	abs.f32 	%f11, %f7;
	abs.f32 	%f12, %f5;
	max.f32 	%f13, %f12, %f11;
	cvt.f64.f32	%fd2, %f13;
	mul.f64 	%fd3, %fd2, 0d3EE9000000000000;
	mov.pred 	%p6, 0;
	setp.geu.f64	%p5, %fd1, %fd3;
	@%p5 bra 	BB62_3;

	sub.f32 	%f14, %f6, %f8;
	abs.f32 	%f15, %f14;
	cvt.f64.f32	%fd4, %f15;
	abs.f32 	%f16, %f8;
	abs.f32 	%f17, %f6;
	max.f32 	%f18, %f17, %f16;
	cvt.f64.f32	%fd5, %f18;
	mul.f64 	%fd6, %fd5, 0d3EE9000000000000;
	setp.lt.f64	%p6, %fd4, %fd6;

BB62_3:
	cvta.to.global.u64 	%rd10, %rd2;
	selp.u32	%r6, 1, 0, %p6;
	xor.b32  	%r7, %r6, 1;
	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.u32 	[%rd12], %r7;

BB62_4:
	ret;
}

	// .globl	compare_comp_double
.visible .entry compare_comp_double(
	.param .u64 compare_comp_double_param_0,
	.param .u64 compare_comp_double_param_1,
	.param .u64 compare_comp_double_param_2,
	.param .u32 compare_comp_double_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<21>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [compare_comp_double_param_0];
	ld.param.u64 	%rd3, [compare_comp_double_param_1];
	ld.param.u64 	%rd4, [compare_comp_double_param_2];
	ld.param.u32 	%r1, [compare_comp_double_param_3];
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r5, %r2, %r3, %r4;
	cvt.s64.s32	%rd1, %r5;
	setp.ge.s32	%p3, %r5, %r1;
	@%p3 bra 	BB63_4;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v2.f64 	{%fd5, %fd6}, [%rd7];
	cvta.to.global.u64 	%rd8, %rd4;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.v2.f64 	{%fd7, %fd8}, [%rd9];
	sub.f64 	%fd9, %fd5, %fd7;
	abs.f64 	%fd10, %fd9;
	abs.f64 	%fd11, %fd7;
	abs.f64 	%fd12, %fd5;
	max.f64 	%fd13, %fd12, %fd11;
	mul.f64 	%fd14, %fd13, 0d3D83880000000000;
	mov.pred 	%p6, 0;
	setp.geu.f64	%p5, %fd10, %fd14;
	@%p5 bra 	BB63_3;

	sub.f64 	%fd15, %fd6, %fd8;
	abs.f64 	%fd16, %fd15;
	abs.f64 	%fd17, %fd8;
	abs.f64 	%fd18, %fd6;
	max.f64 	%fd19, %fd18, %fd17;
	mul.f64 	%fd20, %fd19, 0d3D83880000000000;
	setp.lt.f64	%p6, %fd16, %fd20;

BB63_3:
	cvta.to.global.u64 	%rd10, %rd2;
	selp.u32	%r6, 1, 0, %p6;
	xor.b32  	%r7, %r6, 1;
	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.u32 	[%rd12], %r7;

BB63_4:
	ret;
}

	// .globl	compare_quat_float
.visible .entry compare_quat_float(
	.param .u64 compare_quat_float_param_0,
	.param .u64 compare_quat_float_param_1,
	.param .u64 compare_quat_float_param_2,
	.param .u32 compare_quat_float_param_3
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<13>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [compare_quat_float_param_0];
	ld.param.u64 	%rd3, [compare_quat_float_param_1];
	ld.param.u64 	%rd4, [compare_quat_float_param_2];
	ld.param.u32 	%r1, [compare_quat_float_param_3];
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r5, %r2, %r3, %r4;
	cvt.s64.s32	%rd1, %r5;
	setp.ge.s32	%p3, %r5, %r1;
	@%p3 bra 	BB64_6;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd7];
	cvta.to.global.u64 	%rd8, %rd4;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.v4.f32 	{%f13, %f14, %f15, %f16}, [%rd9];
	sub.f32 	%f17, %f9, %f13;
	abs.f32 	%f18, %f17;
	cvt.f64.f32	%fd1, %f18;
	abs.f32 	%f19, %f13;
	abs.f32 	%f20, %f9;
	max.f32 	%f21, %f20, %f19;
	cvt.f64.f32	%fd2, %f21;
	mul.f64 	%fd3, %fd2, 0d3EE9000000000000;
	mov.pred 	%p10, 0;
	setp.geu.f64	%p5, %fd1, %fd3;
	@%p5 bra 	BB64_5;

	sub.f32 	%f22, %f10, %f14;
	abs.f32 	%f23, %f22;
	cvt.f64.f32	%fd4, %f23;
	abs.f32 	%f24, %f14;
	abs.f32 	%f25, %f10;
	max.f32 	%f26, %f25, %f24;
	cvt.f64.f32	%fd5, %f26;
	mul.f64 	%fd6, %fd5, 0d3EE9000000000000;
	setp.geu.f64	%p7, %fd4, %fd6;
	@%p7 bra 	BB64_5;

	sub.f32 	%f27, %f11, %f15;
	abs.f32 	%f28, %f27;
	cvt.f64.f32	%fd7, %f28;
	abs.f32 	%f29, %f15;
	abs.f32 	%f30, %f11;
	max.f32 	%f31, %f30, %f29;
	cvt.f64.f32	%fd8, %f31;
	mul.f64 	%fd9, %fd8, 0d3EE9000000000000;
	setp.geu.f64	%p9, %fd7, %fd9;
	@%p9 bra 	BB64_5;

	sub.f32 	%f32, %f12, %f16;
	abs.f32 	%f33, %f32;
	cvt.f64.f32	%fd10, %f33;
	abs.f32 	%f34, %f16;
	abs.f32 	%f35, %f12;
	max.f32 	%f36, %f35, %f34;
	cvt.f64.f32	%fd11, %f36;
	mul.f64 	%fd12, %fd11, 0d3EE9000000000000;
	setp.lt.f64	%p10, %fd10, %fd12;

BB64_5:
	cvta.to.global.u64 	%rd10, %rd2;
	selp.u32	%r6, 1, 0, %p10;
	xor.b32  	%r7, %r6, 1;
	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.u32 	[%rd12], %r7;

BB64_6:
	ret;
}

	// .globl	compare_quat_double
.visible .entry compare_quat_double(
	.param .u64 compare_quat_double_param_0,
	.param .u64 compare_quat_double_param_1,
	.param .u64 compare_quat_double_param_2,
	.param .u32 compare_quat_double_param_3
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [compare_quat_double_param_0];
	ld.param.u64 	%rd3, [compare_quat_double_param_1];
	ld.param.u64 	%rd4, [compare_quat_double_param_2];
	ld.param.u32 	%r1, [compare_quat_double_param_3];
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r5, %r2, %r3, %r4;
	cvt.s64.s32	%rd1, %r5;
	setp.ge.s32	%p3, %r5, %r1;
	@%p3 bra 	BB65_6;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 5;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v2.f64 	{%fd9, %fd10}, [%rd7+16];
	ld.global.v2.f64 	{%fd11, %fd12}, [%rd7];
	cvta.to.global.u64 	%rd8, %rd4;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.v2.f64 	{%fd13, %fd14}, [%rd9+16];
	ld.global.v2.f64 	{%fd15, %fd16}, [%rd9];
	sub.f64 	%fd17, %fd11, %fd15;
	abs.f64 	%fd18, %fd17;
	abs.f64 	%fd19, %fd15;
	abs.f64 	%fd20, %fd11;
	max.f64 	%fd21, %fd20, %fd19;
	mul.f64 	%fd22, %fd21, 0d3D83880000000000;
	mov.pred 	%p10, 0;
	setp.geu.f64	%p5, %fd18, %fd22;
	@%p5 bra 	BB65_5;

	sub.f64 	%fd23, %fd12, %fd16;
	abs.f64 	%fd24, %fd23;
	abs.f64 	%fd25, %fd16;
	abs.f64 	%fd26, %fd12;
	max.f64 	%fd27, %fd26, %fd25;
	mul.f64 	%fd28, %fd27, 0d3D83880000000000;
	setp.geu.f64	%p7, %fd24, %fd28;
	@%p7 bra 	BB65_5;

	sub.f64 	%fd29, %fd9, %fd13;
	abs.f64 	%fd30, %fd29;
	abs.f64 	%fd31, %fd13;
	abs.f64 	%fd32, %fd9;
	max.f64 	%fd33, %fd32, %fd31;
	mul.f64 	%fd34, %fd33, 0d3D83880000000000;
	setp.geu.f64	%p9, %fd30, %fd34;
	@%p9 bra 	BB65_5;

	sub.f64 	%fd35, %fd10, %fd14;
	abs.f64 	%fd36, %fd35;
	abs.f64 	%fd37, %fd14;
	abs.f64 	%fd38, %fd10;
	max.f64 	%fd39, %fd38, %fd37;
	mul.f64 	%fd40, %fd39, 0d3D83880000000000;
	setp.lt.f64	%p10, %fd36, %fd40;

BB65_5:
	cvta.to.global.u64 	%rd10, %rd2;
	selp.u32	%r6, 1, 0, %p10;
	xor.b32  	%r7, %r6, 1;
	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.u32 	[%rd12], %r7;

BB65_6:
	ret;
}

	// .globl	q2c_float
.visible .entry q2c_float(
	.param .u64 q2c_float_param_0,
	.param .u64 q2c_float_param_1,
	.param .u32 q2c_float_param_2,
	.param .u32 q2c_float_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd1, [q2c_float_param_0];
	ld.param.u64 	%rd2, [q2c_float_param_1];
	ld.param.u32 	%r3, [q2c_float_param_2];
	ld.param.u32 	%r4, [q2c_float_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB66_2;
	bra.uni 	BB66_1;

BB66_1:
	cvta.to.global.u64 	%rd3, %rd1;
	shl.b32 	%r11, %r3, 1;
	mad.lo.s32 	%r12, %r11, %r2, %r1;
	add.s32 	%r13, %r12, %r3;
	add.s32 	%r14, %r2, %r4;
	mad.lo.s32 	%r15, %r11, %r14, %r1;
	add.s32 	%r16, %r15, %r3;
	mad.lo.s32 	%r17, %r2, %r3, %r1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r17, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd6];
	mul.wide.s32 	%rd7, %r12, 8;
	add.s64 	%rd8, %rd3, %rd7;
	st.global.v2.f32 	[%rd8], {%f1, %f2};
	mul.wide.s32 	%rd9, %r13, 8;
	add.s64 	%rd10, %rd3, %rd9;
	neg.f32 	%f8, %f3;
	st.global.v2.f32 	[%rd10], {%f8, %f4};
	mul.wide.s32 	%rd11, %r15, 8;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.v2.f32 	[%rd12], {%f3, %f4};
	mul.wide.s32 	%rd13, %r16, 8;
	add.s64 	%rd14, %rd3, %rd13;
	neg.f32 	%f10, %f2;
	st.global.v2.f32 	[%rd14], {%f1, %f10};

BB66_2:
	ret;
}

	// .globl	q2c_double
.visible .entry q2c_double(
	.param .u64 q2c_double_param_0,
	.param .u64 q2c_double_param_1,
	.param .u32 q2c_double_param_2,
	.param .u32 q2c_double_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<18>;
	.reg .f64 	%fd<11>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd1, [q2c_double_param_0];
	ld.param.u64 	%rd2, [q2c_double_param_1];
	ld.param.u32 	%r3, [q2c_double_param_2];
	ld.param.u32 	%r4, [q2c_double_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB67_2;
	bra.uni 	BB67_1;

BB67_1:
	cvta.to.global.u64 	%rd3, %rd1;
	shl.b32 	%r11, %r3, 1;
	mad.lo.s32 	%r12, %r11, %r2, %r1;
	add.s32 	%r13, %r12, %r3;
	add.s32 	%r14, %r2, %r4;
	mad.lo.s32 	%r15, %r11, %r14, %r1;
	add.s32 	%r16, %r15, %r3;
	mad.lo.s32 	%r17, %r2, %r3, %r1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r17, 32;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd6+16];
	ld.global.v2.f64 	{%fd3, %fd4}, [%rd6];
	mul.wide.s32 	%rd7, %r12, 16;
	add.s64 	%rd8, %rd3, %rd7;
	st.global.v2.f64 	[%rd8], {%fd3, %fd4};
	mul.wide.s32 	%rd9, %r13, 16;
	add.s64 	%rd10, %rd3, %rd9;
	neg.f64 	%fd8, %fd1;
	st.global.v2.f64 	[%rd10], {%fd8, %fd2};
	mul.wide.s32 	%rd11, %r15, 16;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.v2.f64 	[%rd12], {%fd1, %fd2};
	mul.wide.s32 	%rd13, %r16, 16;
	add.s64 	%rd14, %rd3, %rd13;
	neg.f64 	%fd10, %fd4;
	st.global.v2.f64 	[%rd14], {%fd3, %fd10};

BB67_2:
	ret;
}

	// .globl	c2q_float
.visible .entry c2q_float(
	.param .u64 c2q_float_param_0,
	.param .u64 c2q_float_param_1,
	.param .u32 c2q_float_param_2,
	.param .u32 c2q_float_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [c2q_float_param_0];
	ld.param.u64 	%rd2, [c2q_float_param_1];
	ld.param.u32 	%r3, [c2q_float_param_2];
	ld.param.u32 	%r4, [c2q_float_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB68_2;
	bra.uni 	BB68_1;

BB68_1:
	cvta.to.global.u64 	%rd3, %rd2;
	shl.b32 	%r11, %r3, 1;
	mad.lo.s32 	%r12, %r11, %r2, %r1;
	add.s32 	%r13, %r2, %r4;
	mad.lo.s32 	%r14, %r11, %r13, %r1;
	mad.lo.s32 	%r15, %r2, %r3, %r1;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r12, 8;
	add.s64 	%rd6, %rd3, %rd5;
	mul.wide.s32 	%rd7, %r14, 8;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.v2.f32 	{%f1, %f2}, [%rd6];
	ld.global.v2.f32 	{%f3, %f4}, [%rd8];
	mul.wide.s32 	%rd9, %r15, 16;
	add.s64 	%rd10, %rd4, %rd9;
	st.global.v4.f32 	[%rd10], {%f1, %f2, %f3, %f4};

BB68_2:
	ret;
}

	// .globl	c2q_double
.visible .entry c2q_double(
	.param .u64 c2q_double_param_0,
	.param .u64 c2q_double_param_1,
	.param .u32 c2q_double_param_2,
	.param .u32 c2q_double_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [c2q_double_param_0];
	ld.param.u64 	%rd2, [c2q_double_param_1];
	ld.param.u32 	%r3, [c2q_double_param_2];
	ld.param.u32 	%r4, [c2q_double_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB69_2;
	bra.uni 	BB69_1;

BB69_1:
	cvta.to.global.u64 	%rd3, %rd2;
	shl.b32 	%r11, %r3, 1;
	mad.lo.s32 	%r12, %r11, %r2, %r1;
	add.s32 	%r13, %r2, %r4;
	mad.lo.s32 	%r14, %r11, %r13, %r1;
	mad.lo.s32 	%r15, %r2, %r3, %r1;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r12, 16;
	add.s64 	%rd6, %rd3, %rd5;
	mul.wide.s32 	%rd7, %r14, 16;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd6];
	ld.global.v2.f64 	{%fd3, %fd4}, [%rd8];
	mul.wide.s32 	%rd9, %r15, 32;
	add.s64 	%rd10, %rd4, %rd9;
	st.global.v2.f64 	[%rd10], {%fd1, %fd2};
	st.global.v2.f64 	[%rd10+16], {%fd3, %fd4};

BB69_2:
	ret;
}

	// .globl	transpose_pre_float
.visible .entry transpose_pre_float(
	.param .u64 transpose_pre_float_param_0,
	.param .u32 transpose_pre_float_param_1,
	.param .u32 transpose_pre_float_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [transpose_pre_float_param_0];
	ld.param.u32 	%r4, [transpose_pre_float_param_1];
	ld.param.u32 	%r5, [transpose_pre_float_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r9, %r10, %r11;
	setp.lt.s32	%p1, %r1, %r4;
	setp.lt.s32	%p2, %r2, %r5;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB70_3;
	bra.uni 	BB70_1;

BB70_1:
	shr.s32 	%r12, %r4, 1;
	shr.s32 	%r13, %r5, 1;
	mad.lo.s32 	%r3, %r2, %r4, %r1;
	setp.lt.s32	%p4, %r2, %r13;
	setp.ge.s32	%p5, %r1, %r12;
	and.pred  	%p6, %p4, %p5;
	setp.ge.s32	%p7, %r2, %r13;
	setp.lt.s32	%p8, %r1, %r12;
	and.pred  	%p9, %p8, %p7;
	or.pred  	%p10, %p6, %p9;
	@!%p10 bra 	BB70_3;
	bra.uni 	BB70_2;

BB70_2:
	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r3, 8;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	neg.f32 	%f5, %f1;
	st.global.v2.f32 	[%rd4], {%f5, %f2};

BB70_3:
	ret;
}

	// .globl	transpose_pre_double
.visible .entry transpose_pre_double(
	.param .u64 transpose_pre_double_param_0,
	.param .u32 transpose_pre_double_param_1,
	.param .u32 transpose_pre_double_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [transpose_pre_double_param_0];
	ld.param.u32 	%r4, [transpose_pre_double_param_1];
	ld.param.u32 	%r5, [transpose_pre_double_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r9, %r10, %r11;
	setp.lt.s32	%p1, %r1, %r4;
	setp.lt.s32	%p2, %r2, %r5;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB71_3;
	bra.uni 	BB71_1;

BB71_1:
	shr.s32 	%r12, %r4, 1;
	shr.s32 	%r13, %r5, 1;
	mad.lo.s32 	%r3, %r2, %r4, %r1;
	setp.lt.s32	%p4, %r2, %r13;
	setp.ge.s32	%p5, %r1, %r12;
	and.pred  	%p6, %p4, %p5;
	setp.ge.s32	%p7, %r2, %r13;
	setp.lt.s32	%p8, %r1, %r12;
	and.pred  	%p9, %p8, %p7;
	or.pred  	%p10, %p6, %p9;
	@!%p10 bra 	BB71_3;
	bra.uni 	BB71_2;

BB71_2:
	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r3, 16;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd4];
	neg.f64 	%fd5, %fd1;
	st.global.v2.f64 	[%rd4], {%fd5, %fd2};

BB71_3:
	ret;
}

	// .globl	transpose_float
.visible .entry transpose_float(
	.param .u64 transpose_float_param_0,
	.param .u64 transpose_float_param_1,
	.param .u32 transpose_float_param_2,
	.param .u32 transpose_float_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [transpose_float_param_0];
	ld.param.u64 	%rd2, [transpose_float_param_1];
	ld.param.u32 	%r3, [transpose_float_param_2];
	ld.param.u32 	%r4, [transpose_float_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB72_2;
	bra.uni 	BB72_1;

BB72_1:
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r11, 16;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.s32 	%rd7, %r12, 16;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	st.global.v4.f32 	[%rd6], {%f1, %f2, %f3, %f4};

BB72_2:
	ret;
}

	// .globl	transpose_double
.visible .entry transpose_double(
	.param .u64 transpose_double_param_0,
	.param .u64 transpose_double_param_1,
	.param .u32 transpose_double_param_2,
	.param .u32 transpose_double_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [transpose_double_param_0];
	ld.param.u64 	%rd2, [transpose_double_param_1];
	ld.param.u32 	%r3, [transpose_double_param_2];
	ld.param.u32 	%r4, [transpose_double_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB73_2;
	bra.uni 	BB73_1;

BB73_1:
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r11, 32;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.s32 	%rd7, %r12, 32;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd8];
	ld.global.v2.f64 	{%fd3, %fd4}, [%rd8+16];
	st.global.v2.f64 	[%rd6+16], {%fd3, %fd4};
	st.global.v2.f64 	[%rd6], {%fd1, %fd2};

BB73_2:
	ret;
}

	// .globl	ctranspose_float
.visible .entry ctranspose_float(
	.param .u64 ctranspose_float_param_0,
	.param .u64 ctranspose_float_param_1,
	.param .u32 ctranspose_float_param_2,
	.param .u32 ctranspose_float_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [ctranspose_float_param_0];
	ld.param.u64 	%rd2, [ctranspose_float_param_1];
	ld.param.u32 	%r3, [ctranspose_float_param_2];
	ld.param.u32 	%r4, [ctranspose_float_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB74_2;
	bra.uni 	BB74_1;

BB74_1:
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r12, 16;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd5];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r11, 16;
	add.s64 	%rd8, %rd6, %rd7;
	neg.f32 	%f9, %f4;
	neg.f32 	%f10, %f3;
	neg.f32 	%f11, %f2;
	st.global.v4.f32 	[%rd8], {%f1, %f11, %f10, %f9};

BB74_2:
	ret;
}

	// .globl	ctranspose_double
.visible .entry ctranspose_double(
	.param .u64 ctranspose_double_param_0,
	.param .u64 ctranspose_double_param_1,
	.param .u32 ctranspose_double_param_2,
	.param .u32 ctranspose_double_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [ctranspose_double_param_0];
	ld.param.u64 	%rd2, [ctranspose_double_param_1];
	ld.param.u32 	%r3, [ctranspose_double_param_2];
	ld.param.u32 	%r4, [ctranspose_double_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r8, %r9, %r10;
	setp.lt.s32	%p1, %r1, %r3;
	setp.lt.s32	%p2, %r2, %r4;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB75_2;
	bra.uni 	BB75_1;

BB75_1:
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r12, 32;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd5+16];
	ld.global.v2.f64 	{%fd5, %fd6}, [%rd5];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r11, 32;
	add.s64 	%rd8, %rd6, %rd7;
	neg.f64 	%fd9, %fd6;
	st.global.v2.f64 	[%rd8], {%fd5, %fd9};
	neg.f64 	%fd10, %fd2;
	neg.f64 	%fd11, %fd1;
	st.global.v2.f64 	[%rd8+16], {%fd11, %fd10};

BB75_2:
	ret;
}

	// .globl	actfunc_real_float
.visible .entry actfunc_real_float(
	.param .u64 actfunc_real_float_param_0,
	.param .u32 actfunc_real_float_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [actfunc_real_float_param_0];
	ld.param.u32 	%r2, [actfunc_real_float_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB76_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	setp.ltu.f32	%p2, %f1, 0f00000000;
	selp.f32	%f2, 0fBF800000, 0f3F800000, %p2;
	st.global.f32 	[%rd4], %f2;

BB76_2:
	ret;
}

	// .globl	actfunc_real_double
.visible .entry actfunc_real_double(
	.param .u64 actfunc_real_double_param_0,
	.param .u32 actfunc_real_double_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [actfunc_real_double_param_0];
	ld.param.u32 	%r2, [actfunc_real_double_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB77_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 8;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f64 	%fd1, [%rd4];
	setp.ltu.f64	%p2, %fd1, 0d0000000000000000;
	selp.f64	%fd2, 0dBFF0000000000000, 0d3FF0000000000000, %p2;
	st.global.f64 	[%rd4], %fd2;

BB77_2:
	ret;
}

	// .globl	actfunc_comp_float
.visible .entry actfunc_comp_float(
	.param .u64 actfunc_comp_float_param_0,
	.param .u32 actfunc_comp_float_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [actfunc_comp_float_param_0];
	ld.param.u32 	%r2, [actfunc_comp_float_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB78_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 8;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	setp.ltu.f32	%p2, %f1, 0f00000000;
	setp.ltu.f32	%p3, %f2, 0f00000000;
	selp.f32	%f5, 0fBF800000, 0f3F800000, %p3;
	selp.f32	%f6, 0fBF800000, 0f3F800000, %p2;
	st.global.v2.f32 	[%rd4], {%f6, %f5};

BB78_2:
	ret;
}

	// .globl	actfunc_comp_double
.visible .entry actfunc_comp_double(
	.param .u64 actfunc_comp_double_param_0,
	.param .u32 actfunc_comp_double_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [actfunc_comp_double_param_0];
	ld.param.u32 	%r2, [actfunc_comp_double_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB79_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 16;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd4];
	setp.ltu.f64	%p2, %fd1, 0d0000000000000000;
	setp.ltu.f64	%p3, %fd2, 0d0000000000000000;
	selp.f64	%fd5, 0dBFF0000000000000, 0d3FF0000000000000, %p3;
	selp.f64	%fd6, 0dBFF0000000000000, 0d3FF0000000000000, %p2;
	st.global.v2.f64 	[%rd4], {%fd6, %fd5};

BB79_2:
	ret;
}

	// .globl	actfunc_quat_float
.visible .entry actfunc_quat_float(
	.param .u64 actfunc_quat_float_param_0,
	.param .u32 actfunc_quat_float_param_1
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [actfunc_quat_float_param_0];
	ld.param.u32 	%r2, [actfunc_quat_float_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB80_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 16;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	setp.ltu.f32	%p2, %f1, 0f00000000;
	setp.ltu.f32	%p3, %f2, 0f00000000;
	setp.ltu.f32	%p4, %f3, 0f00000000;
	setp.ltu.f32	%p5, %f4, 0f00000000;
	selp.f32	%f9, 0fBF800000, 0f3F800000, %p5;
	selp.f32	%f10, 0fBF800000, 0f3F800000, %p4;
	selp.f32	%f11, 0fBF800000, 0f3F800000, %p3;
	selp.f32	%f12, 0fBF800000, 0f3F800000, %p2;
	st.global.v4.f32 	[%rd4], {%f12, %f11, %f10, %f9};

BB80_2:
	ret;
}

	// .globl	actfunc_quat_double
.visible .entry actfunc_quat_double(
	.param .u64 actfunc_quat_double_param_0,
	.param .u32 actfunc_quat_double_param_1
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<13>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [actfunc_quat_double_param_0];
	ld.param.u32 	%r2, [actfunc_quat_double_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB81_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 32;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd4];
	setp.ltu.f64	%p2, %fd1, 0d0000000000000000;
	setp.ltu.f64	%p3, %fd2, 0d0000000000000000;
	ld.global.v2.f64 	{%fd5, %fd6}, [%rd4+16];
	setp.ltu.f64	%p4, %fd5, 0d0000000000000000;
	setp.ltu.f64	%p5, %fd6, 0d0000000000000000;
	selp.f64	%fd9, 0dBFF0000000000000, 0d3FF0000000000000, %p3;
	selp.f64	%fd10, 0dBFF0000000000000, 0d3FF0000000000000, %p2;
	st.global.v2.f64 	[%rd4], {%fd10, %fd9};
	selp.f64	%fd11, 0dBFF0000000000000, 0d3FF0000000000000, %p5;
	selp.f64	%fd12, 0dBFF0000000000000, 0d3FF0000000000000, %p4;
	st.global.v2.f64 	[%rd4+16], {%fd12, %fd11};

BB81_2:
	ret;
}

	// .globl	actfunc_comp_multi_float
.visible .entry actfunc_comp_multi_float(
	.param .u64 actfunc_comp_multi_float_param_0,
	.param .u32 actfunc_comp_multi_float_param_1,
	.param .u32 actfunc_comp_multi_float_param_2
)
{
	.local .align 4 .b8 	__local_depot82[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<37>;
	.reg .f32 	%f<136>;
	.reg .b32 	%r<202>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<28>;


	mov.u64 	%rd27, __local_depot82;
	cvta.local.u64 	%SP, %rd27;
	ld.param.u64 	%rd14, [actfunc_comp_multi_float_param_0];
	ld.param.u32 	%r77, [actfunc_comp_multi_float_param_1];
	ld.param.u32 	%r78, [actfunc_comp_multi_float_param_2];
	add.u64 	%rd15, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd15;
	mov.u32 	%r79, %ntid.x;
	mov.u32 	%r80, %ctaid.x;
	mov.u32 	%r81, %tid.x;
	mad.lo.s32 	%r1, %r79, %r80, %r81;
	setp.ge.s32	%p1, %r1, %r78;
	@%p1 bra 	BB82_51;

	cvta.to.global.u64 	%rd16, %rd14;
	mul.wide.s32 	%rd17, %r1, 8;
	add.s64 	%rd2, %rd16, %rd17;
	ld.global.v2.f32 	{%f43, %f44}, [%rd2];
	abs.f32 	%f1, %f43;
	abs.f32 	%f2, %f44;
	setp.eq.f32	%p2, %f1, 0f00000000;
	setp.eq.f32	%p3, %f2, 0f00000000;
	and.pred  	%p4, %p2, %p3;
	mov.b32 	 %r2, %f43;
	mov.b32 	 %r82, %f44;
	and.b32  	%r3, %r82, -2147483648;
	@%p4 bra 	BB82_5;
	bra.uni 	BB82_2;

BB82_5:
	shr.s32 	%r89, %r2, 31;
	and.b32  	%r90, %r89, 1078530011;
	or.b32  	%r91, %r90, %r3;
	mov.b32 	 %f124, %r91;
	bra.uni 	BB82_6;

BB82_2:
	setp.eq.f32	%p5, %f1, 0f7F800000;
	setp.eq.f32	%p6, %f2, 0f7F800000;
	and.pred  	%p7, %p5, %p6;
	@%p7 bra 	BB82_4;
	bra.uni 	BB82_3;

BB82_4:
	shr.s32 	%r85, %r2, 31;
	and.b32  	%r86, %r85, 13483017;
	add.s32 	%r87, %r86, 1061752795;
	or.b32  	%r88, %r87, %r3;
	mov.b32 	 %f124, %r88;
	bra.uni 	BB82_6;

BB82_3:
	max.f32 	%f47, %f2, %f1;
	min.f32 	%f48, %f2, %f1;
	div.rn.f32 	%f49, %f48, %f47;
	mul.rn.f32 	%f50, %f49, %f49;
	mov.f32 	%f51, 0fC0B59883;
	mov.f32 	%f52, 0fBF52C7EA;
	fma.rn.f32 	%f53, %f50, %f52, %f51;
	mov.f32 	%f54, 0fC0D21907;
	fma.rn.f32 	%f55, %f53, %f50, %f54;
	mul.f32 	%f56, %f50, %f55;
	mul.f32 	%f57, %f49, %f56;
	add.f32 	%f58, %f50, 0f41355DC0;
	mov.f32 	%f59, 0f41E6BD60;
	fma.rn.f32 	%f60, %f58, %f50, %f59;
	mov.f32 	%f61, 0f419D92C8;
	fma.rn.f32 	%f62, %f60, %f50, %f61;
	rcp.rn.f32 	%f63, %f62;
	fma.rn.f32 	%f64, %f57, %f63, %f49;
	mov.f32 	%f65, 0f3FC90FDB;
	sub.f32 	%f66, %f65, %f64;
	setp.gt.f32	%p8, %f2, %f1;
	selp.f32	%f67, %f66, %f64, %p8;
	mov.f32 	%f68, 0f40490FDB;
	sub.f32 	%f69, %f68, %f67;
	setp.lt.s32	%p9, %r2, 0;
	selp.f32	%f70, %f69, %f67, %p9;
	mov.b32 	 %r83, %f70;
	or.b32  	%r84, %r83, %r3;
	mov.b32 	 %f71, %r84;
	add.f32 	%f72, %f1, %f2;
	setp.gtu.f32	%p10, %f72, 0f7F800000;
	selp.f32	%f124, %f72, %f71, %p10;

BB82_6:
	cvt.rn.f64.s32	%fd1, %r77;
	mov.f64 	%fd2, 0d400921FB54442D18;
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvt.rn.f32.f64	%f73, %fd3;
	add.f32 	%f74, %f73, %f73;
	add.f32 	%f75, %f73, %f124;
	div.rn.f32 	%f76, %f75, %f74;
	cvt.rmi.f32.f32	%f77, %f76;
	mul.f32 	%f7, %f74, %f77;
	abs.f32 	%f8, %f7;
	setp.neu.f32	%p11, %f8, 0f7F800000;
	mov.f32 	%f131, %f7;
	@%p11 bra 	BB82_8;

	mov.f32 	%f78, 0f00000000;
	mul.rn.f32 	%f9, %f7, %f78;
	mov.f32 	%f131, %f9;

BB82_8:
	mov.f32 	%f10, %f131;
	mul.f32 	%f79, %f10, 0f3F22F983;
	cvt.rni.s32.f32	%r191, %f79;
	cvt.rn.f32.s32	%f80, %r191;
	neg.f32 	%f81, %f80;
	mov.f32 	%f82, 0f3FC90FDA;
	fma.rn.f32 	%f83, %f81, %f82, %f10;
	mov.f32 	%f84, 0f33A22168;
	fma.rn.f32 	%f85, %f81, %f84, %f83;
	mov.f32 	%f86, 0f27C234C5;
	fma.rn.f32 	%f125, %f81, %f86, %f85;
	abs.f32 	%f87, %f10;
	add.s64 	%rd3, %rd1, 24;
	setp.leu.f32	%p12, %f87, 0f47CE4780;
	@%p12 bra 	BB82_18;

	mov.b32 	 %r5, %f10;
	shr.u32 	%r6, %r5, 23;
	bfe.u32 	%r94, %r5, 23, 8;
	add.s32 	%r95, %r94, -128;
	shl.b32 	%r96, %r5, 8;
	or.b32  	%r7, %r96, -2147483648;
	shr.u32 	%r8, %r95, 5;
	mov.u32 	%r183, 0;
	mov.u64 	%rd22, __cudart_i2opi_f;
	mov.u32 	%r182, -6;
	mov.u64 	%rd26, %rd1;

BB82_10:
	.pragma "nounroll";
	mov.u64 	%rd5, %rd26;
	ld.const.u32 	%r99, [%rd22];
	// inline asm
	{
	mad.lo.cc.u32   %r97, %r99, %r7, %r183;
	madc.hi.u32     %r183, %r99, %r7,  0;
	}
	// inline asm
	st.local.u32 	[%rd5], %r97;
	add.s64 	%rd6, %rd5, 4;
	add.s64 	%rd22, %rd22, 4;
	add.s32 	%r182, %r182, 1;
	setp.ne.s32	%p13, %r182, 0;
	mov.u64 	%rd26, %rd6;
	@%p13 bra 	BB82_10;

	and.b32  	%r13, %r5, -2147483648;
	st.local.u32 	[%rd3], %r183;
	mov.u32 	%r102, 6;
	sub.s32 	%r103, %r102, %r8;
	mul.wide.s32 	%rd19, %r103, 4;
	add.s64 	%rd8, %rd1, %rd19;
	ld.local.u32 	%r184, [%rd8];
	ld.local.u32 	%r185, [%rd8+-4];
	and.b32  	%r16, %r6, 31;
	setp.eq.s32	%p14, %r16, 0;
	@%p14 bra 	BB82_13;

	mov.u32 	%r104, 32;
	sub.s32 	%r105, %r104, %r16;
	shr.u32 	%r106, %r185, %r105;
	shl.b32 	%r107, %r184, %r16;
	add.s32 	%r184, %r106, %r107;
	ld.local.u32 	%r108, [%rd8+-8];
	shr.u32 	%r109, %r108, %r105;
	shl.b32 	%r110, %r185, %r16;
	add.s32 	%r185, %r109, %r110;

BB82_13:
	shr.u32 	%r111, %r185, 30;
	shl.b32 	%r112, %r184, 2;
	add.s32 	%r186, %r111, %r112;
	shl.b32 	%r22, %r185, 2;
	shr.u32 	%r113, %r186, 31;
	shr.u32 	%r114, %r184, 30;
	add.s32 	%r23, %r113, %r114;
	setp.eq.s32	%p15, %r113, 0;
	mov.u32 	%r187, %r13;
	mov.u32 	%r188, %r22;
	@%p15 bra 	BB82_15;

	not.b32 	%r115, %r186;
	neg.s32 	%r24, %r22;
	setp.eq.s32	%p16, %r22, 0;
	selp.u32	%r116, 1, 0, %p16;
	add.s32 	%r186, %r116, %r115;
	xor.b32  	%r26, %r13, -2147483648;
	mov.u32 	%r187, %r26;
	mov.u32 	%r188, %r24;

BB82_15:
	mov.u32 	%r28, %r187;
	neg.s32 	%r117, %r23;
	setp.eq.s32	%p17, %r13, 0;
	selp.b32	%r191, %r23, %r117, %p17;
	clz.b32 	%r190, %r186;
	setp.eq.s32	%p18, %r190, 0;
	shl.b32 	%r118, %r186, %r190;
	mov.u32 	%r119, 32;
	sub.s32 	%r120, %r119, %r190;
	shr.u32 	%r121, %r188, %r120;
	add.s32 	%r122, %r121, %r118;
	selp.b32	%r32, %r186, %r122, %p18;
	mov.u32 	%r123, -921707870;
	mul.hi.u32 	%r189, %r32, %r123;
	setp.lt.s32	%p19, %r189, 1;
	@%p19 bra 	BB82_17;

	mul.lo.s32 	%r124, %r32, -921707870;
	shr.u32 	%r125, %r124, 31;
	shl.b32 	%r126, %r189, 1;
	add.s32 	%r189, %r125, %r126;
	add.s32 	%r190, %r190, 1;

BB82_17:
	mov.u32 	%r127, 126;
	sub.s32 	%r128, %r127, %r190;
	shl.b32 	%r129, %r128, 23;
	add.s32 	%r130, %r189, 1;
	shr.u32 	%r131, %r130, 7;
	add.s32 	%r132, %r131, 1;
	shr.u32 	%r133, %r132, 1;
	add.s32 	%r134, %r133, %r129;
	or.b32  	%r135, %r134, %r28;
	mov.b32 	 %f125, %r135;

BB82_18:
	mul.rn.f32 	%f14, %f125, %f125;
	add.s32 	%r39, %r191, 1;
	and.b32  	%r40, %r39, 1;
	setp.eq.s32	%p20, %r40, 0;
	@%p20 bra 	BB82_20;

	mov.f32 	%f88, 0fBAB6061A;
	mov.f32 	%f89, 0f37CCF5CE;
	fma.rn.f32 	%f126, %f89, %f14, %f88;
	bra.uni 	BB82_21;

BB82_20:
	mov.f32 	%f90, 0f3C08839E;
	mov.f32 	%f91, 0fB94CA1F9;
	fma.rn.f32 	%f126, %f91, %f14, %f90;

BB82_21:
	@%p20 bra 	BB82_23;

	mov.f32 	%f92, 0f3D2AAAA5;
	fma.rn.f32 	%f93, %f126, %f14, %f92;
	mov.f32 	%f94, 0fBF000000;
	fma.rn.f32 	%f127, %f93, %f14, %f94;
	bra.uni 	BB82_24;

BB82_23:
	mov.f32 	%f95, 0fBE2AAAA3;
	fma.rn.f32 	%f96, %f126, %f14, %f95;
	mov.f32 	%f97, 0f00000000;
	fma.rn.f32 	%f127, %f96, %f14, %f97;

BB82_24:
	fma.rn.f32 	%f128, %f127, %f125, %f125;
	@%p20 bra 	BB82_26;

	mov.f32 	%f98, 0f3F800000;
	fma.rn.f32 	%f128, %f127, %f14, %f98;

BB82_26:
	and.b32  	%r136, %r39, 2;
	setp.eq.s32	%p23, %r136, 0;
	@%p23 bra 	BB82_28;

	mov.f32 	%f99, 0f00000000;
	mov.f32 	%f100, 0fBF800000;
	fma.rn.f32 	%f128, %f128, %f100, %f99;

BB82_28:
	mov.f32 	%f130, %f7;
	@%p11 bra 	BB82_30;

	mov.f32 	%f101, 0f00000000;
	mul.rn.f32 	%f130, %f7, %f101;

BB82_30:
	mul.f32 	%f102, %f130, 0f3F22F983;
	cvt.rni.s32.f32	%r201, %f102;
	cvt.rn.f32.s32	%f103, %r201;
	neg.f32 	%f104, %f103;
	fma.rn.f32 	%f106, %f104, %f82, %f130;
	fma.rn.f32 	%f108, %f104, %f84, %f106;
	fma.rn.f32 	%f132, %f104, %f86, %f108;
	abs.f32 	%f110, %f130;
	setp.leu.f32	%p25, %f110, 0f47CE4780;
	@%p25 bra 	BB82_40;

	mov.b32 	 %r42, %f130;
	shr.u32 	%r43, %r42, 23;
	bfe.u32 	%r139, %r42, 23, 8;
	add.s32 	%r140, %r139, -128;
	shl.b32 	%r141, %r42, 8;
	or.b32  	%r44, %r141, -2147483648;
	shr.u32 	%r45, %r140, 5;
	mov.u32 	%r193, 0;
	mov.u64 	%rd23, __cudart_i2opi_f;
	mov.u32 	%r192, -6;
	mov.u64 	%rd25, %rd1;

BB82_32:
	.pragma "nounroll";
	ld.const.u32 	%r144, [%rd23];
	// inline asm
	{
	mad.lo.cc.u32   %r142, %r144, %r44, %r193;
	madc.hi.u32     %r193, %r144, %r44,  0;
	}
	// inline asm
	st.local.u32 	[%rd25], %r142;
	add.s64 	%rd25, %rd25, 4;
	add.s64 	%rd23, %rd23, 4;
	add.s32 	%r192, %r192, 1;
	setp.ne.s32	%p26, %r192, 0;
	@%p26 bra 	BB82_32;

	and.b32  	%r50, %r42, -2147483648;
	st.local.u32 	[%rd3], %r193;
	mov.u32 	%r147, 6;
	sub.s32 	%r148, %r147, %r45;
	mul.wide.s32 	%rd21, %r148, 4;
	add.s64 	%rd13, %rd1, %rd21;
	ld.local.u32 	%r194, [%rd13];
	ld.local.u32 	%r195, [%rd13+-4];
	and.b32  	%r53, %r43, 31;
	setp.eq.s32	%p27, %r53, 0;
	@%p27 bra 	BB82_35;

	mov.u32 	%r149, 32;
	sub.s32 	%r150, %r149, %r53;
	shr.u32 	%r151, %r195, %r150;
	shl.b32 	%r152, %r194, %r53;
	add.s32 	%r194, %r151, %r152;
	ld.local.u32 	%r153, [%rd13+-8];
	shr.u32 	%r154, %r153, %r150;
	shl.b32 	%r155, %r195, %r53;
	add.s32 	%r195, %r154, %r155;

BB82_35:
	shr.u32 	%r156, %r195, 30;
	shl.b32 	%r157, %r194, 2;
	add.s32 	%r196, %r156, %r157;
	shl.b32 	%r59, %r195, 2;
	shr.u32 	%r158, %r196, 31;
	shr.u32 	%r159, %r194, 30;
	add.s32 	%r60, %r158, %r159;
	setp.eq.s32	%p28, %r158, 0;
	mov.u32 	%r197, %r50;
	mov.u32 	%r198, %r59;
	@%p28 bra 	BB82_37;

	not.b32 	%r160, %r196;
	neg.s32 	%r61, %r59;
	setp.eq.s32	%p29, %r59, 0;
	selp.u32	%r161, 1, 0, %p29;
	add.s32 	%r196, %r161, %r160;
	xor.b32  	%r63, %r50, -2147483648;
	mov.u32 	%r197, %r63;
	mov.u32 	%r198, %r61;

BB82_37:
	mov.u32 	%r65, %r197;
	neg.s32 	%r162, %r60;
	setp.eq.s32	%p30, %r50, 0;
	selp.b32	%r201, %r60, %r162, %p30;
	clz.b32 	%r200, %r196;
	setp.eq.s32	%p31, %r200, 0;
	shl.b32 	%r163, %r196, %r200;
	mov.u32 	%r164, 32;
	sub.s32 	%r165, %r164, %r200;
	shr.u32 	%r166, %r198, %r165;
	add.s32 	%r167, %r166, %r163;
	selp.b32	%r69, %r196, %r167, %p31;
	mov.u32 	%r168, -921707870;
	mul.hi.u32 	%r199, %r69, %r168;
	setp.lt.s32	%p32, %r199, 1;
	@%p32 bra 	BB82_39;

	mul.lo.s32 	%r169, %r69, -921707870;
	shr.u32 	%r170, %r169, 31;
	shl.b32 	%r171, %r199, 1;
	add.s32 	%r199, %r170, %r171;
	add.s32 	%r200, %r200, 1;

BB82_39:
	mov.u32 	%r172, 126;
	sub.s32 	%r173, %r172, %r200;
	shl.b32 	%r174, %r173, 23;
	add.s32 	%r175, %r199, 1;
	shr.u32 	%r176, %r175, 7;
	add.s32 	%r177, %r176, 1;
	shr.u32 	%r178, %r177, 1;
	add.s32 	%r179, %r178, %r174;
	or.b32  	%r180, %r179, %r65;
	mov.b32 	 %f132, %r180;

BB82_40:
	mul.rn.f32 	%f31, %f132, %f132;
	and.b32  	%r76, %r201, 1;
	setp.eq.s32	%p33, %r76, 0;
	@%p33 bra 	BB82_42;

	mov.f32 	%f111, 0fBAB6061A;
	mov.f32 	%f112, 0f37CCF5CE;
	fma.rn.f32 	%f133, %f112, %f31, %f111;
	bra.uni 	BB82_43;

BB82_42:
	mov.f32 	%f113, 0f3C08839E;
	mov.f32 	%f114, 0fB94CA1F9;
	fma.rn.f32 	%f133, %f114, %f31, %f113;

BB82_43:
	@%p33 bra 	BB82_45;

	mov.f32 	%f115, 0f3D2AAAA5;
	fma.rn.f32 	%f116, %f133, %f31, %f115;
	mov.f32 	%f117, 0fBF000000;
	fma.rn.f32 	%f134, %f116, %f31, %f117;
	bra.uni 	BB82_46;

BB82_45:
	mov.f32 	%f118, 0fBE2AAAA3;
	fma.rn.f32 	%f119, %f133, %f31, %f118;
	mov.f32 	%f120, 0f00000000;
	fma.rn.f32 	%f134, %f119, %f31, %f120;

BB82_46:
	fma.rn.f32 	%f135, %f134, %f132, %f132;
	@%p33 bra 	BB82_48;

	mov.f32 	%f121, 0f3F800000;
	fma.rn.f32 	%f135, %f134, %f31, %f121;

BB82_48:
	and.b32  	%r181, %r201, 2;
	setp.eq.s32	%p36, %r181, 0;
	@%p36 bra 	BB82_50;

	mov.f32 	%f122, 0f00000000;
	mov.f32 	%f123, 0fBF800000;
	fma.rn.f32 	%f135, %f135, %f123, %f122;

BB82_50:
	st.global.v2.f32 	[%rd2], {%f128, %f135};

BB82_51:
	ret;
}

	// .globl	actfunc_comp_multi_double
.visible .entry actfunc_comp_multi_double(
	.param .u64 actfunc_comp_multi_double_param_0,
	.param .u32 actfunc_comp_multi_double_param_1,
	.param .u32 actfunc_comp_multi_double_param_2
)
{
	.local .align 4 .b8 	__local_depot83[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<23>;
	.reg .b32 	%r<49>;
	.reg .f64 	%fd<182>;
	.reg .b64 	%rd<18>;


	mov.u64 	%rd17, __local_depot83;
	cvta.local.u64 	%SP, %rd17;
	ld.param.u64 	%rd4, [actfunc_comp_multi_double_param_0];
	ld.param.u32 	%r11, [actfunc_comp_multi_double_param_2];
	add.u64 	%rd5, %SP, 4;
	cvta.to.local.u64 	%rd1, %rd5;
	add.u64 	%rd6, %SP, 0;
	cvta.to.local.u64 	%rd2, %rd6;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r14;
	setp.ge.s32	%p1, %r1, %r11;
	@%p1 bra 	BB83_19;

	cvta.to.global.u64 	%rd7, %rd4;
	mul.wide.s32 	%rd8, %r1, 16;
	add.s64 	%rd3, %rd7, %rd8;
	ld.global.v2.f64 	{%fd25, %fd26}, [%rd3];
	abs.f64 	%fd1, %fd25;
	abs.f64 	%fd2, %fd26;
	setp.eq.f64	%p2, %fd1, 0d0000000000000000;
	setp.eq.f64	%p3, %fd2, 0d0000000000000000;
	and.pred  	%p4, %p2, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd25;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd26;
	}
	and.b32  	%r3, %r15, -2147483648;
	@%p4 bra 	BB83_5;
	bra.uni 	BB83_2;

BB83_5:
	setp.lt.s32	%p12, %r2, 0;
	selp.f64	%fd81, 0d400921FB54442D18, 0d0000000000000000, %p12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd81;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd81;
	}
	or.b32  	%r24, %r23, %r3;
	mov.b64 	%fd174, {%r22, %r24};
	bra.uni 	BB83_6;

BB83_2:
	setp.eq.f64	%p5, %fd1, 0d7FF0000000000000;
	setp.eq.f64	%p6, %fd2, 0d7FF0000000000000;
	and.pred  	%p7, %p5, %p6;
	@%p7 bra 	BB83_4;
	bra.uni 	BB83_3;

BB83_4:
	setp.lt.s32	%p11, %r2, 0;
	selp.f64	%fd80, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd80;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd80;
	}
	or.b32  	%r21, %r20, %r3;
	mov.b64 	%fd174, {%r19, %r21};
	bra.uni 	BB83_6;

BB83_3:
	setp.lt.s32	%p8, %r2, 0;
	min.f64 	%fd29, %fd2, %fd1;
	max.f64 	%fd30, %fd2, %fd1;
	div.rn.f64 	%fd31, %fd29, %fd30;
	mul.f64 	%fd32, %fd31, %fd31;
	mov.f64 	%fd33, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd34, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd35, %fd34, %fd32, %fd33;
	mov.f64 	%fd36, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd37, %fd35, %fd32, %fd36;
	mov.f64 	%fd38, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd39, %fd37, %fd32, %fd38;
	mov.f64 	%fd40, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd41, %fd39, %fd32, %fd40;
	mov.f64 	%fd42, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd43, %fd41, %fd32, %fd42;
	mov.f64 	%fd44, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd45, %fd43, %fd32, %fd44;
	mov.f64 	%fd46, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd47, %fd45, %fd32, %fd46;
	mov.f64 	%fd48, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd49, %fd47, %fd32, %fd48;
	mov.f64 	%fd50, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd51, %fd49, %fd32, %fd50;
	mov.f64 	%fd52, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd53, %fd51, %fd32, %fd52;
	mov.f64 	%fd54, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd55, %fd53, %fd32, %fd54;
	mov.f64 	%fd56, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd57, %fd55, %fd32, %fd56;
	mov.f64 	%fd58, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd59, %fd57, %fd32, %fd58;
	mov.f64 	%fd60, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd61, %fd59, %fd32, %fd60;
	mov.f64 	%fd62, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd63, %fd61, %fd32, %fd62;
	mov.f64 	%fd64, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd65, %fd63, %fd32, %fd64;
	mov.f64 	%fd66, 0d3FC99999999840D2;
	fma.rn.f64 	%fd67, %fd65, %fd32, %fd66;
	mov.f64 	%fd68, 0dBFD555555555544C;
	fma.rn.f64 	%fd69, %fd67, %fd32, %fd68;
	mul.f64 	%fd70, %fd32, %fd69;
	fma.rn.f64 	%fd71, %fd70, %fd31, %fd31;
	mov.f64 	%fd72, 0d3FF921FB54442D18;
	sub.f64 	%fd73, %fd72, %fd71;
	setp.gt.f64	%p9, %fd2, %fd1;
	selp.f64	%fd74, %fd73, %fd71, %p9;
	mov.f64 	%fd75, 0d400921FB54442D18;
	sub.f64 	%fd76, %fd75, %fd74;
	selp.f64	%fd77, %fd76, %fd74, %p8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd77;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd77;
	}
	or.b32  	%r18, %r17, %r3;
	mov.b64 	%fd78, {%r16, %r18};
	add.f64 	%fd79, %fd1, %fd2;
	setp.gtu.f64	%p10, %fd79, 0d7FF0000000000000;
	selp.f64	%fd174, %fd79, %fd78, %p10;

BB83_6:
	ld.param.u32 	%r42, [actfunc_comp_multi_double_param_1];
	cvt.rn.f64.s32	%fd82, %r42;
	mov.f64 	%fd83, 0d400921FB54442D18;
	div.rn.f64 	%fd84, %fd83, %fd82;
	add.f64 	%fd85, %fd84, %fd84;
	add.f64 	%fd86, %fd84, %fd174;
	div.rn.f64 	%fd87, %fd86, %fd85;
	cvt.rmi.f64.f64	%fd88, %fd87;
	mul.f64 	%fd7, %fd85, %fd88;
	abs.f64 	%fd8, %fd7;
	setp.neu.f64	%p13, %fd8, 0d7FF0000000000000;
	mov.f64 	%fd179, %fd7;
	@%p13 bra 	BB83_8;

	mov.f64 	%fd89, 0d0000000000000000;
	mul.rn.f64 	%fd9, %fd7, %fd89;
	mov.f64 	%fd179, %fd9;

BB83_8:
	mov.f64 	%fd10, %fd179;
	mul.f64 	%fd90, %fd10, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r47, %fd90;
	st.local.u32 	[%rd2], %r47;
	cvt.rn.f64.s32	%fd91, %r47;
	neg.f64 	%fd92, %fd91;
	mov.f64 	%fd93, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd94, %fd92, %fd93, %fd10;
	mov.f64 	%fd95, 0d3C91A62633145C00;
	fma.rn.f64 	%fd96, %fd92, %fd95, %fd94;
	mov.f64 	%fd97, 0d397B839A252049C0;
	fma.rn.f64 	%fd175, %fd92, %fd97, %fd96;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd10;
	}
	and.b32  	%r26, %r25, 2145386496;
	setp.lt.u32	%p14, %r26, 1105199104;
	@%p14 bra 	BB83_10;

	add.u64 	%rd16, %SP, 0;
	// Callseq Start 74
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd10;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd16;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd175, [retval0+0];
	
	//{
	}// Callseq End 74
	ld.local.u32 	%r47, [%rd2];

BB83_10:
	mul.rn.f64 	%fd98, %fd175, %fd175;
	mov.f64 	%fd99, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd100, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd101, %fd100, %fd98, %fd99;
	mov.f64 	%fd102, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd103, %fd101, %fd98, %fd102;
	mov.f64 	%fd104, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd105, %fd103, %fd98, %fd104;
	mov.f64 	%fd106, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd107, %fd105, %fd98, %fd106;
	mov.f64 	%fd108, 0d3FA5555555555551;
	fma.rn.f64 	%fd109, %fd107, %fd98, %fd108;
	mov.f64 	%fd110, 0dBFE0000000000000;
	fma.rn.f64 	%fd111, %fd109, %fd98, %fd110;
	mov.f64 	%fd112, 0d3FF0000000000000;
	fma.rn.f64 	%fd113, %fd111, %fd98, %fd112;
	mov.f64 	%fd114, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd115, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd116, %fd115, %fd98, %fd114;
	mov.f64 	%fd117, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd118, %fd116, %fd98, %fd117;
	mov.f64 	%fd119, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd120, %fd118, %fd98, %fd119;
	mov.f64 	%fd121, 0d3F81111111110818;
	fma.rn.f64 	%fd122, %fd120, %fd98, %fd121;
	mov.f64 	%fd123, 0dBFC5555555555554;
	fma.rn.f64 	%fd124, %fd122, %fd98, %fd123;
	mov.f64 	%fd125, 0d0000000000000000;
	fma.rn.f64 	%fd126, %fd124, %fd98, %fd125;
	fma.rn.f64 	%fd127, %fd126, %fd175, %fd175;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd127;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd127;
	}
	xor.b32  	%r29, %r27, -2147483648;
	mov.b64 	%fd128, {%r28, %r29};
	and.b32  	%r30, %r47, 1;
	setp.eq.b32	%p15, %r30, 1;
	not.pred 	%p16, %p15;
	selp.f64	%fd176, %fd113, %fd128, %p16;
	and.b32  	%r31, %r47, 2;
	setp.eq.s32	%p17, %r31, 0;
	@%p17 bra 	BB83_12;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd176;
	}
	xor.b32  	%r33, %r32, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd176;
	}
	mov.b64 	%fd176, {%r34, %r33};

BB83_12:
	mov.f64 	%fd178, %fd7;
	@%p13 bra 	BB83_14;

	mul.rn.f64 	%fd178, %fd7, %fd125;

BB83_14:
	mov.f64 	%fd170, 0d397B839A252049C0;
	mov.f64 	%fd169, 0d3C91A62633145C00;
	mov.f64 	%fd168, 0d3FF921FB54442D18;
	mul.f64 	%fd130, %fd178, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r48, %fd130;
	st.local.u32 	[%rd1], %r48;
	cvt.rn.f64.s32	%fd131, %r48;
	neg.f64 	%fd132, %fd131;
	fma.rn.f64 	%fd134, %fd132, %fd168, %fd178;
	fma.rn.f64 	%fd136, %fd132, %fd169, %fd134;
	fma.rn.f64 	%fd180, %fd132, %fd170, %fd136;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd178;
	}
	and.b32  	%r36, %r35, 2145386496;
	setp.lt.u32	%p19, %r36, 1105199104;
	@%p19 bra 	BB83_16;

	add.u64 	%rd11, %SP, 4;
	// Callseq Start 75
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd178;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd11;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd180, [retval0+0];
	
	//{
	}// Callseq End 75
	ld.local.u32 	%r48, [%rd1];

BB83_16:
	mov.f64 	%fd173, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd172, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd171, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd138, %fd180, %fd180;
	fma.rn.f64 	%fd141, %fd171, %fd138, %fd172;
	fma.rn.f64 	%fd143, %fd141, %fd138, %fd173;
	fma.rn.f64 	%fd145, %fd143, %fd138, %fd104;
	fma.rn.f64 	%fd147, %fd145, %fd138, %fd106;
	fma.rn.f64 	%fd149, %fd147, %fd138, %fd108;
	fma.rn.f64 	%fd151, %fd149, %fd138, %fd110;
	fma.rn.f64 	%fd153, %fd151, %fd138, %fd112;
	fma.rn.f64 	%fd156, %fd115, %fd138, %fd114;
	fma.rn.f64 	%fd158, %fd156, %fd138, %fd117;
	fma.rn.f64 	%fd160, %fd158, %fd138, %fd119;
	fma.rn.f64 	%fd162, %fd160, %fd138, %fd121;
	fma.rn.f64 	%fd164, %fd162, %fd138, %fd123;
	fma.rn.f64 	%fd166, %fd164, %fd138, %fd125;
	fma.rn.f64 	%fd167, %fd166, %fd180, %fd180;
	and.b32  	%r37, %r48, 1;
	setp.eq.b32	%p20, %r37, 1;
	not.pred 	%p21, %p20;
	selp.f64	%fd181, %fd167, %fd153, %p21;
	and.b32  	%r38, %r48, 2;
	setp.eq.s32	%p22, %r38, 0;
	@%p22 bra 	BB83_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd181;
	}
	xor.b32  	%r40, %r39, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd181;
	}
	mov.b64 	%fd181, {%r41, %r40};

BB83_18:
	ld.param.u64 	%rd15, [actfunc_comp_multi_double_param_0];
	mov.u32 	%r46, %tid.x;
	mov.u32 	%r45, %ctaid.x;
	mov.u32 	%r44, %ntid.x;
	mad.lo.s32 	%r43, %r44, %r45, %r46;
	mul.wide.s32 	%rd14, %r43, 16;
	cvta.to.global.u64 	%rd13, %rd15;
	add.s64 	%rd12, %rd13, %rd14;
	st.global.v2.f64 	[%rd12], {%fd176, %fd181};

BB83_19:
	ret;
}

	// .globl	actfunc_quat_multi_float
.visible .entry actfunc_quat_multi_float(
	.param .u64 actfunc_quat_multi_float_param_0,
	.param .u32 actfunc_quat_multi_float_param_1,
	.param .u32 actfunc_quat_multi_float_param_2,
	.param .u32 actfunc_quat_multi_float_param_3,
	.param .u32 actfunc_quat_multi_float_param_4
)
{
	.local .align 4 .b8 	__local_depot84[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<672>;
	.reg .f32 	%f<2534>;
	.reg .b32 	%r<4457>;
	.reg .f64 	%fd<28>;
	.reg .b64 	%rd<492>;


	mov.u64 	%rd491, __local_depot84;
	cvta.local.u64 	%SP, %rd491;
	ld.param.u64 	%rd244, [actfunc_quat_multi_float_param_0];
	ld.param.u32 	%r1763, [actfunc_quat_multi_float_param_4];
	add.u64 	%rd245, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd245;
	mov.u32 	%r1764, %ntid.x;
	mov.u32 	%r1765, %ctaid.x;
	mov.u32 	%r1766, %tid.x;
	mad.lo.s32 	%r1, %r1764, %r1765, %r1766;
	setp.ge.s32	%p1, %r1, %r1763;
	@%p1 bra 	BB84_1090;

	cvta.to.global.u64 	%rd246, %rd244;
	mul.wide.s32 	%rd247, %r1, 16;
	add.s64 	%rd2, %rd246, %rd247;
	ld.global.v4.f32 	{%f925, %f926, %f927, %f928}, [%rd2];
	mul.f32 	%f929, %f926, %f926;
	fma.rn.f32 	%f930, %f925, %f925, %f929;
	fma.rn.f32 	%f931, %f927, %f927, %f930;
	fma.rn.f32 	%f932, %f928, %f928, %f931;
	sqrt.rn.f32 	%f5, %f932;
	setp.eq.f32	%p2, %f5, 0f00000000;
	add.s64 	%rd3, %rd1, 24;
	mov.f32 	%f2389, 0f00000000;
	mov.f32 	%f2388, %f2389;
	mov.f32 	%f2387, %f2389;
	@%p2 bra 	BB84_555;

	div.rn.f32 	%f6, %f925, %f5;
	div.rn.f32 	%f7, %f927, %f5;
	div.rn.f32 	%f8, %f926, %f5;
	mul.f32 	%f934, %f8, %f7;
	div.rn.f32 	%f9, %f928, %f5;
	mul.f32 	%f935, %f6, %f9;
	sub.f32 	%f936, %f934, %f935;
	add.f32 	%f10, %f936, %f936;
	setp.gt.f32	%p3, %f10, 0f3F800000;
	mov.f32 	%f933, 0f3F800000;
	mov.f32 	%f2234, %f933;
	@%p3 bra 	BB84_4;

	setp.lt.f32	%p4, %f10, 0fBF800000;
	selp.f32	%f11, 0fBF800000, %f10, %p4;
	mov.f32 	%f2234, %f11;

BB84_4:
	mov.f32 	%f12, %f2234;
	abs.f32 	%f937, %f12;
	sub.f32 	%f939, %f933, %f937;
	mul.f32 	%f940, %f939, 0f3F000000;
	sqrt.rn.f32 	%f941, %f940;
	setp.gt.f32	%p5, %f937, 0f3F11EB85;
	selp.f32	%f942, %f941, %f937, %p5;
	mul.f32 	%f943, %f942, %f942;
	mov.f32 	%f944, 0f3C94D2E9;
	mov.f32 	%f945, 0f3D53F941;
	fma.rn.f32 	%f946, %f945, %f943, %f944;
	mov.f32 	%f947, 0f3D3F841F;
	fma.rn.f32 	%f948, %f946, %f943, %f947;
	mov.f32 	%f949, 0f3D994929;
	fma.rn.f32 	%f950, %f948, %f943, %f949;
	mov.f32 	%f951, 0f3E2AAB94;
	fma.rn.f32 	%f952, %f950, %f943, %f951;
	mul.f32 	%f953, %f943, %f952;
	fma.rn.f32 	%f954, %f953, %f942, %f942;
	mov.f32 	%f955, 0f3FC90FDB;
	mov.f32 	%f956, 0fC0000000;
	fma.rn.f32 	%f957, %f956, %f954, %f955;
	selp.f32	%f958, %f957, %f954, %p5;
	setp.gtu.f32	%p6, %f958, 0f7F800000;
	mov.b32 	 %r1767, %f958;
	mov.b32 	 %r1768, %f12;
	and.b32  	%r1769, %r1768, -2147483648;
	or.b32  	%r1770, %r1767, %r1769;
	mov.b32 	 %f959, %r1770;
	selp.f32	%f960, %f958, %f959, %p6;
	mul.f32 	%f13, %f960, 0fBF000000;
	cvt.f64.f32	%fd1, %f13;
	setp.neu.f64	%p7, %fd1, 0d3FE921FB54442D18;
	setp.neu.f64	%p8, %fd1, 0dBFE921FB54442D18;
	and.pred  	%p9, %p7, %p8;
	mul.f32 	%f961, %f8, %f8;
	mul.f32 	%f14, %f6, %f6;
	sub.f32 	%f15, %f14, %f961;
	@%p9 bra 	BB84_10;
	bra.uni 	BB84_5;

BB84_10:
	fma.rn.f32 	%f999, %f7, %f7, %f15;
	mul.f32 	%f21, %f9, %f9;
	sub.f32 	%f1000, %f999, %f21;
	mul.f32 	%f1001, %f6, %f8;
	fma.rn.f32 	%f1002, %f6, %f8, %f1001;
	fma.rn.f32 	%f1003, %f9, %f7, %f1002;
	fma.rn.f32 	%f1004, %f7, %f9, %f1003;
	abs.f32 	%f22, %f1000;
	abs.f32 	%f23, %f1004;
	setp.eq.f32	%p19, %f22, 0f00000000;
	setp.eq.f32	%p20, %f23, 0f00000000;
	and.pred  	%p21, %p19, %p20;
	mov.b32 	 %r4, %f1000;
	mov.b32 	 %r1781, %f1004;
	and.b32  	%r5, %r1781, -2147483648;
	@%p21 bra 	BB84_14;
	bra.uni 	BB84_11;

BB84_14:
	shr.s32 	%r1788, %r4, 31;
	and.b32  	%r1789, %r1788, 1078530011;
	or.b32  	%r1790, %r1789, %r5;
	mov.b32 	 %f2235, %r1790;
	bra.uni 	BB84_15;

BB84_5:
	mul.f32 	%f962, %f7, %f7;
	sub.f32 	%f963, %f15, %f962;
	fma.rn.f32 	%f964, %f9, %f9, %f963;
	mul.f32 	%f965, %f6, %f7;
	mul.f32 	%f966, %f8, %f9;
	sub.f32 	%f967, %f965, %f966;
	fma.rn.f32 	%f968, %f6, %f7, %f967;
	sub.f32 	%f969, %f968, %f966;
	abs.f32 	%f16, %f964;
	abs.f32 	%f17, %f969;
	setp.eq.f32	%p10, %f16, 0f00000000;
	setp.eq.f32	%p11, %f17, 0f00000000;
	and.pred  	%p12, %p10, %p11;
	mov.b32 	 %r2, %f964;
	mov.b32 	 %r1771, %f969;
	and.b32  	%r3, %r1771, -2147483648;
	@%p12 bra 	BB84_9;
	bra.uni 	BB84_6;

BB84_9:
	shr.s32 	%r1778, %r2, 31;
	and.b32  	%r1779, %r1778, 1078530011;
	or.b32  	%r1780, %r3, %r1779;
	mov.b32 	 %f2236, %r1780;
	mov.f32 	%f2386, 0f00000000;
	bra.uni 	BB84_21;

BB84_11:
	setp.eq.f32	%p22, %f22, 0f7F800000;
	setp.eq.f32	%p23, %f23, 0f7F800000;
	and.pred  	%p24, %p22, %p23;
	@%p24 bra 	BB84_13;
	bra.uni 	BB84_12;

BB84_13:
	shr.s32 	%r1784, %r4, 31;
	and.b32  	%r1785, %r1784, 13483017;
	add.s32 	%r1786, %r1785, 1061752795;
	or.b32  	%r1787, %r1786, %r5;
	mov.b32 	 %f2235, %r1787;
	bra.uni 	BB84_15;

BB84_6:
	setp.eq.f32	%p13, %f16, 0f7F800000;
	setp.eq.f32	%p14, %f17, 0f7F800000;
	and.pred  	%p15, %p13, %p14;
	@%p15 bra 	BB84_8;
	bra.uni 	BB84_7;

BB84_8:
	shr.s32 	%r1774, %r2, 31;
	and.b32  	%r1775, %r1774, 13483017;
	add.s32 	%r1776, %r1775, 1061752795;
	or.b32  	%r1777, %r1776, %r3;
	mov.b32 	 %f2236, %r1777;
	mov.f32 	%f2386, 0f00000000;
	bra.uni 	BB84_21;

BB84_12:
	mov.f32 	%f2220, 0f3FC90FDB;
	max.f32 	%f1005, %f23, %f22;
	min.f32 	%f1006, %f23, %f22;
	div.rn.f32 	%f1007, %f1006, %f1005;
	mul.rn.f32 	%f1008, %f1007, %f1007;
	mov.f32 	%f1009, 0fC0B59883;
	mov.f32 	%f1010, 0fBF52C7EA;
	fma.rn.f32 	%f1011, %f1008, %f1010, %f1009;
	mov.f32 	%f1012, 0fC0D21907;
	fma.rn.f32 	%f1013, %f1011, %f1008, %f1012;
	mul.f32 	%f1014, %f1008, %f1013;
	mul.f32 	%f1015, %f1007, %f1014;
	add.f32 	%f1016, %f1008, 0f41355DC0;
	mov.f32 	%f1017, 0f41E6BD60;
	fma.rn.f32 	%f1018, %f1016, %f1008, %f1017;
	mov.f32 	%f1019, 0f419D92C8;
	fma.rn.f32 	%f1020, %f1018, %f1008, %f1019;
	rcp.rn.f32 	%f1021, %f1020;
	fma.rn.f32 	%f1022, %f1015, %f1021, %f1007;
	sub.f32 	%f1024, %f2220, %f1022;
	setp.gt.f32	%p25, %f23, %f22;
	selp.f32	%f1025, %f1024, %f1022, %p25;
	mov.f32 	%f1026, 0f40490FDB;
	sub.f32 	%f1027, %f1026, %f1025;
	setp.lt.s32	%p26, %r4, 0;
	selp.f32	%f1028, %f1027, %f1025, %p26;
	mov.b32 	 %r1782, %f1028;
	or.b32  	%r1783, %r1782, %r5;
	mov.b32 	 %f1029, %r1783;
	add.f32 	%f1030, %f22, %f23;
	setp.gtu.f32	%p27, %f1030, 0f7F800000;
	selp.f32	%f2235, %f1030, %f1029, %p27;

BB84_15:
	mul.f32 	%f2223, %f9, %f9;
	mul.f32 	%f2222, %f6, %f6;
	mul.f32 	%f2386, %f2235, 0f3F000000;
	fma.rn.f32 	%f1031, %f8, %f8, %f2222;
	mul.f32 	%f1032, %f7, %f7;
	sub.f32 	%f1033, %f1031, %f1032;
	sub.f32 	%f1034, %f1033, %f2223;
	mul.f32 	%f1035, %f9, %f8;
	fma.rn.f32 	%f1036, %f6, %f7, %f1035;
	fma.rn.f32 	%f1037, %f6, %f7, %f1036;
	add.f32 	%f1038, %f1037, %f1035;
	abs.f32 	%f29, %f1034;
	abs.f32 	%f30, %f1038;
	setp.eq.f32	%p28, %f29, 0f00000000;
	setp.eq.f32	%p29, %f30, 0f00000000;
	and.pred  	%p30, %p28, %p29;
	mov.b32 	 %r6, %f1034;
	mov.b32 	 %r1791, %f1038;
	and.b32  	%r7, %r1791, -2147483648;
	@%p30 bra 	BB84_19;
	bra.uni 	BB84_16;

BB84_19:
	shr.s32 	%r1798, %r6, 31;
	and.b32  	%r1799, %r1798, 1078530011;
	or.b32  	%r1800, %r1799, %r7;
	mov.b32 	 %f2236, %r1800;
	bra.uni 	BB84_20;

BB84_16:
	setp.eq.f32	%p31, %f29, 0f7F800000;
	setp.eq.f32	%p32, %f30, 0f7F800000;
	and.pred  	%p33, %p31, %p32;
	@%p33 bra 	BB84_18;
	bra.uni 	BB84_17;

BB84_18:
	shr.s32 	%r1794, %r6, 31;
	and.b32  	%r1795, %r1794, 13483017;
	add.s32 	%r1796, %r1795, 1061752795;
	or.b32  	%r1797, %r1796, %r7;
	mov.b32 	 %f2236, %r1797;

BB84_20:
	bra.uni 	BB84_21;

BB84_7:
	mov.f32 	%f2219, 0f3FC90FDB;
	max.f32 	%f971, %f17, %f16;
	min.f32 	%f972, %f17, %f16;
	div.rn.f32 	%f973, %f972, %f971;
	mul.rn.f32 	%f974, %f973, %f973;
	mov.f32 	%f975, 0fC0B59883;
	mov.f32 	%f976, 0fBF52C7EA;
	fma.rn.f32 	%f977, %f974, %f976, %f975;
	mov.f32 	%f978, 0fC0D21907;
	fma.rn.f32 	%f979, %f977, %f974, %f978;
	mul.f32 	%f980, %f974, %f979;
	mul.f32 	%f981, %f973, %f980;
	add.f32 	%f982, %f974, 0f41355DC0;
	mov.f32 	%f983, 0f41E6BD60;
	fma.rn.f32 	%f984, %f982, %f974, %f983;
	mov.f32 	%f985, 0f419D92C8;
	fma.rn.f32 	%f986, %f984, %f974, %f985;
	rcp.rn.f32 	%f987, %f986;
	fma.rn.f32 	%f988, %f981, %f987, %f973;
	sub.f32 	%f990, %f2219, %f988;
	setp.gt.f32	%p16, %f17, %f16;
	selp.f32	%f991, %f990, %f988, %p16;
	mov.f32 	%f992, 0f40490FDB;
	sub.f32 	%f993, %f992, %f991;
	setp.lt.s32	%p17, %r2, 0;
	selp.f32	%f994, %f993, %f991, %p17;
	mov.b32 	 %r1772, %f994;
	or.b32  	%r1773, %r1772, %r3;
	mov.b32 	 %f995, %r1773;
	add.f32 	%f996, %f16, %f17;
	setp.gtu.f32	%p18, %f996, 0f7F800000;
	selp.f32	%f2236, %f996, %f995, %p18;
	mov.f32 	%f2386, 0f00000000;
	bra.uni 	BB84_21;

BB84_17:
	mov.f32 	%f2221, 0f3FC90FDB;
	max.f32 	%f1039, %f30, %f29;
	min.f32 	%f1040, %f30, %f29;
	div.rn.f32 	%f1041, %f1040, %f1039;
	mul.rn.f32 	%f1042, %f1041, %f1041;
	mov.f32 	%f1043, 0fC0B59883;
	mov.f32 	%f1044, 0fBF52C7EA;
	fma.rn.f32 	%f1045, %f1042, %f1044, %f1043;
	mov.f32 	%f1046, 0fC0D21907;
	fma.rn.f32 	%f1047, %f1045, %f1042, %f1046;
	mul.f32 	%f1048, %f1042, %f1047;
	mul.f32 	%f1049, %f1041, %f1048;
	add.f32 	%f1050, %f1042, 0f41355DC0;
	mov.f32 	%f1051, 0f41E6BD60;
	fma.rn.f32 	%f1052, %f1050, %f1042, %f1051;
	mov.f32 	%f1053, 0f419D92C8;
	fma.rn.f32 	%f1054, %f1052, %f1042, %f1053;
	rcp.rn.f32 	%f1055, %f1054;
	fma.rn.f32 	%f1056, %f1049, %f1055, %f1041;
	sub.f32 	%f1058, %f2221, %f1056;
	setp.gt.f32	%p34, %f30, %f29;
	selp.f32	%f1059, %f1058, %f1056, %p34;
	mov.f32 	%f1060, 0f40490FDB;
	sub.f32 	%f1061, %f1060, %f1059;
	setp.lt.s32	%p35, %r6, 0;
	selp.f32	%f1062, %f1061, %f1059, %p35;
	mov.b32 	 %r1792, %f1062;
	or.b32  	%r1793, %r1792, %r7;
	mov.b32 	 %f1063, %r1793;
	add.f32 	%f1064, %f29, %f30;
	setp.gtu.f32	%p36, %f1064, 0f7F800000;
	selp.f32	%f2236, %f1064, %f1063, %p36;

BB84_21:
	mov.f32 	%f34, %f2386;
	mul.f32 	%f36, %f2236, 0f3F000000;
	abs.f32 	%f37, %f34;
	setp.neu.f32	%p37, %f37, 0f7F800000;
	mov.f32 	%f2385, %f34;
	@%p37 bra 	BB84_23;

	mov.f32 	%f1065, 0f00000000;
	mul.rn.f32 	%f38, %f34, %f1065;
	mov.f32 	%f2385, %f38;

BB84_23:
	mov.f32 	%f39, %f2385;
	mul.f32 	%f1066, %f39, 0f3F22F983;
	cvt.rni.s32.f32	%r3986, %f1066;
	cvt.rn.f32.s32	%f1067, %r3986;
	neg.f32 	%f1068, %f1067;
	mov.f32 	%f1069, 0f3FC90FDA;
	fma.rn.f32 	%f1070, %f1068, %f1069, %f39;
	mov.f32 	%f1071, 0f33A22168;
	fma.rn.f32 	%f1072, %f1068, %f1071, %f1070;
	mov.f32 	%f1073, 0f27C234C5;
	fma.rn.f32 	%f2237, %f1068, %f1073, %f1072;
	abs.f32 	%f1074, %f39;
	setp.leu.f32	%p38, %f1074, 0f47CE4780;
	@%p38 bra 	BB84_33;

	mov.b32 	 %r9, %f39;
	shr.u32 	%r10, %r9, 23;
	bfe.u32 	%r1803, %r9, 23, 8;
	add.s32 	%r1804, %r1803, -128;
	shl.b32 	%r1805, %r9, 8;
	or.b32  	%r11, %r1805, -2147483648;
	shr.u32 	%r12, %r1804, 5;
	mov.u32 	%r3978, 0;
	mov.u64 	%rd348, __cudart_i2opi_f;
	mov.u32 	%r3977, -6;
	mov.u64 	%rd490, %rd1;

BB84_25:
	.pragma "nounroll";
	mov.u64 	%rd5, %rd490;
	ld.const.u32 	%r1808, [%rd348];
	// inline asm
	{
	mad.lo.cc.u32   %r1806, %r1808, %r11, %r3978;
	madc.hi.u32     %r3978, %r1808, %r11,  0;
	}
	// inline asm
	st.local.u32 	[%rd5], %r1806;
	add.s64 	%rd6, %rd5, 4;
	add.s64 	%rd348, %rd348, 4;
	add.s32 	%r3977, %r3977, 1;
	setp.ne.s32	%p39, %r3977, 0;
	mov.u64 	%rd490, %rd6;
	@%p39 bra 	BB84_25;

	and.b32  	%r17, %r9, -2147483648;
	st.local.u32 	[%rd3], %r3978;
	mov.u32 	%r1811, 6;
	sub.s32 	%r1812, %r1811, %r12;
	mul.wide.s32 	%rd249, %r1812, 4;
	add.s64 	%rd8, %rd1, %rd249;
	ld.local.u32 	%r3979, [%rd8];
	ld.local.u32 	%r3980, [%rd8+-4];
	and.b32  	%r20, %r10, 31;
	setp.eq.s32	%p40, %r20, 0;
	@%p40 bra 	BB84_28;

	mov.u32 	%r1813, 32;
	sub.s32 	%r1814, %r1813, %r20;
	shr.u32 	%r1815, %r3980, %r1814;
	shl.b32 	%r1816, %r3979, %r20;
	add.s32 	%r3979, %r1815, %r1816;
	ld.local.u32 	%r1817, [%rd8+-8];
	shr.u32 	%r1818, %r1817, %r1814;
	shl.b32 	%r1819, %r3980, %r20;
	add.s32 	%r3980, %r1818, %r1819;

BB84_28:
	shr.u32 	%r1820, %r3980, 30;
	shl.b32 	%r1821, %r3979, 2;
	add.s32 	%r3981, %r1820, %r1821;
	shl.b32 	%r26, %r3980, 2;
	shr.u32 	%r1822, %r3981, 31;
	shr.u32 	%r1823, %r3979, 30;
	add.s32 	%r27, %r1822, %r1823;
	setp.eq.s32	%p41, %r1822, 0;
	mov.u32 	%r3982, %r17;
	mov.u32 	%r3983, %r26;
	@%p41 bra 	BB84_30;

	not.b32 	%r1824, %r3981;
	neg.s32 	%r28, %r26;
	setp.eq.s32	%p42, %r26, 0;
	selp.u32	%r1825, 1, 0, %p42;
	add.s32 	%r3981, %r1825, %r1824;
	xor.b32  	%r30, %r17, -2147483648;
	mov.u32 	%r3982, %r30;
	mov.u32 	%r3983, %r28;

BB84_30:
	mov.u32 	%r32, %r3982;
	neg.s32 	%r1826, %r27;
	setp.eq.s32	%p43, %r17, 0;
	selp.b32	%r3986, %r27, %r1826, %p43;
	clz.b32 	%r3985, %r3981;
	setp.eq.s32	%p44, %r3985, 0;
	shl.b32 	%r1827, %r3981, %r3985;
	mov.u32 	%r1828, 32;
	sub.s32 	%r1829, %r1828, %r3985;
	shr.u32 	%r1830, %r3983, %r1829;
	add.s32 	%r1831, %r1830, %r1827;
	selp.b32	%r36, %r3981, %r1831, %p44;
	mov.u32 	%r1832, -921707870;
	mul.hi.u32 	%r3984, %r36, %r1832;
	setp.lt.s32	%p45, %r3984, 1;
	@%p45 bra 	BB84_32;

	mul.lo.s32 	%r1833, %r36, -921707870;
	shr.u32 	%r1834, %r1833, 31;
	shl.b32 	%r1835, %r3984, 1;
	add.s32 	%r3984, %r1834, %r1835;
	add.s32 	%r3985, %r3985, 1;

BB84_32:
	mov.u32 	%r1836, 126;
	sub.s32 	%r1837, %r1836, %r3985;
	shl.b32 	%r1838, %r1837, 23;
	add.s32 	%r1839, %r3984, 1;
	shr.u32 	%r1840, %r1839, 7;
	add.s32 	%r1841, %r1840, 1;
	shr.u32 	%r1842, %r1841, 1;
	add.s32 	%r1843, %r1842, %r1838;
	or.b32  	%r1844, %r1843, %r32;
	mov.b32 	 %f2237, %r1844;

BB84_33:
	mul.rn.f32 	%f43, %f2237, %f2237;
	add.s32 	%r43, %r3986, 1;
	and.b32  	%r44, %r43, 1;
	setp.eq.s32	%p46, %r44, 0;
	@%p46 bra 	BB84_35;

	mov.f32 	%f1075, 0fBAB6061A;
	mov.f32 	%f1076, 0f37CCF5CE;
	fma.rn.f32 	%f2238, %f1076, %f43, %f1075;
	bra.uni 	BB84_36;

BB84_35:
	mov.f32 	%f1077, 0f3C08839E;
	mov.f32 	%f1078, 0fB94CA1F9;
	fma.rn.f32 	%f2238, %f1078, %f43, %f1077;

BB84_36:
	@%p46 bra 	BB84_38;

	mov.f32 	%f1079, 0f3D2AAAA5;
	fma.rn.f32 	%f1080, %f2238, %f43, %f1079;
	mov.f32 	%f1081, 0fBF000000;
	fma.rn.f32 	%f2239, %f1080, %f43, %f1081;
	bra.uni 	BB84_39;

BB84_38:
	mov.f32 	%f1082, 0fBE2AAAA3;
	fma.rn.f32 	%f1083, %f2238, %f43, %f1082;
	mov.f32 	%f1084, 0f00000000;
	fma.rn.f32 	%f2239, %f1083, %f43, %f1084;

BB84_39:
	fma.rn.f32 	%f2240, %f2239, %f2237, %f2237;
	@%p46 bra 	BB84_41;

	mul.rn.f32 	%f2224, %f2237, %f2237;
	mov.f32 	%f1085, 0f3F800000;
	fma.rn.f32 	%f2240, %f2239, %f2224, %f1085;

BB84_41:
	and.b32  	%r1845, %r43, 2;
	setp.eq.s32	%p49, %r1845, 0;
	@%p49 bra 	BB84_43;

	mov.f32 	%f1086, 0f00000000;
	mov.f32 	%f1087, 0fBF800000;
	fma.rn.f32 	%f2240, %f2240, %f1087, %f1086;

BB84_43:
	abs.f32 	%f55, %f36;
	setp.neu.f32	%p50, %f55, 0f7F800000;
	mov.f32 	%f2339, %f36;
	@%p50 bra 	BB84_45;

	mov.f32 	%f1088, 0f00000000;
	mul.rn.f32 	%f56, %f36, %f1088;
	mov.f32 	%f2339, %f56;

BB84_45:
	mov.f32 	%f57, %f2339;
	mul.f32 	%f1089, %f57, 0f3F22F983;
	cvt.rni.s32.f32	%r3996, %f1089;
	cvt.rn.f32.s32	%f1090, %r3996;
	neg.f32 	%f1091, %f1090;
	fma.rn.f32 	%f1093, %f1091, %f1069, %f57;
	fma.rn.f32 	%f1095, %f1091, %f1071, %f1093;
	fma.rn.f32 	%f2241, %f1091, %f1073, %f1095;
	abs.f32 	%f1097, %f57;
	setp.leu.f32	%p51, %f1097, 0f47CE4780;
	@%p51 bra 	BB84_55;

	mov.b32 	 %r46, %f57;
	shr.u32 	%r47, %r46, 23;
	bfe.u32 	%r1848, %r46, 23, 8;
	add.s32 	%r1849, %r1848, -128;
	shl.b32 	%r1850, %r46, 8;
	or.b32  	%r48, %r1850, -2147483648;
	shr.u32 	%r49, %r1849, 5;
	mov.u32 	%r3988, 0;
	mov.u64 	%rd349, __cudart_i2opi_f;
	mov.u32 	%r3987, -6;
	mov.u64 	%rd489, %rd1;

BB84_47:
	.pragma "nounroll";
	ld.const.u32 	%r1853, [%rd349];
	// inline asm
	{
	mad.lo.cc.u32   %r1851, %r1853, %r48, %r3988;
	madc.hi.u32     %r3988, %r1853, %r48,  0;
	}
	// inline asm
	st.local.u32 	[%rd489], %r1851;
	add.s64 	%rd489, %rd489, 4;
	add.s64 	%rd349, %rd349, 4;
	add.s32 	%r3987, %r3987, 1;
	setp.ne.s32	%p52, %r3987, 0;
	@%p52 bra 	BB84_47;

	and.b32  	%r54, %r46, -2147483648;
	st.local.u32 	[%rd3], %r3988;
	mov.u32 	%r1856, 6;
	sub.s32 	%r1857, %r1856, %r49;
	mul.wide.s32 	%rd251, %r1857, 4;
	add.s64 	%rd13, %rd1, %rd251;
	ld.local.u32 	%r3989, [%rd13];
	ld.local.u32 	%r3990, [%rd13+-4];
	and.b32  	%r57, %r47, 31;
	setp.eq.s32	%p53, %r57, 0;
	@%p53 bra 	BB84_50;

	mov.u32 	%r1858, 32;
	sub.s32 	%r1859, %r1858, %r57;
	shr.u32 	%r1860, %r3990, %r1859;
	shl.b32 	%r1861, %r3989, %r57;
	add.s32 	%r3989, %r1860, %r1861;
	ld.local.u32 	%r1862, [%rd13+-8];
	shr.u32 	%r1863, %r1862, %r1859;
	shl.b32 	%r1864, %r3990, %r57;
	add.s32 	%r3990, %r1863, %r1864;

BB84_50:
	shr.u32 	%r1865, %r3990, 30;
	shl.b32 	%r1866, %r3989, 2;
	add.s32 	%r3991, %r1865, %r1866;
	shl.b32 	%r63, %r3990, 2;
	shr.u32 	%r1867, %r3991, 31;
	shr.u32 	%r1868, %r3989, 30;
	add.s32 	%r64, %r1867, %r1868;
	setp.eq.s32	%p54, %r1867, 0;
	mov.u32 	%r3992, %r54;
	mov.u32 	%r3993, %r63;
	@%p54 bra 	BB84_52;

	not.b32 	%r1869, %r3991;
	neg.s32 	%r65, %r63;
	setp.eq.s32	%p55, %r63, 0;
	selp.u32	%r1870, 1, 0, %p55;
	add.s32 	%r3991, %r1870, %r1869;
	xor.b32  	%r67, %r54, -2147483648;
	mov.u32 	%r3992, %r67;
	mov.u32 	%r3993, %r65;

BB84_52:
	mov.u32 	%r69, %r3992;
	neg.s32 	%r1871, %r64;
	setp.eq.s32	%p56, %r54, 0;
	selp.b32	%r3996, %r64, %r1871, %p56;
	clz.b32 	%r3995, %r3991;
	setp.eq.s32	%p57, %r3995, 0;
	shl.b32 	%r1872, %r3991, %r3995;
	mov.u32 	%r1873, 32;
	sub.s32 	%r1874, %r1873, %r3995;
	shr.u32 	%r1875, %r3993, %r1874;
	add.s32 	%r1876, %r1875, %r1872;
	selp.b32	%r73, %r3991, %r1876, %p57;
	mov.u32 	%r1877, -921707870;
	mul.hi.u32 	%r3994, %r73, %r1877;
	setp.lt.s32	%p58, %r3994, 1;
	@%p58 bra 	BB84_54;

	mul.lo.s32 	%r1878, %r73, -921707870;
	shr.u32 	%r1879, %r1878, 31;
	shl.b32 	%r1880, %r3994, 1;
	add.s32 	%r3994, %r1879, %r1880;
	add.s32 	%r3995, %r3995, 1;

BB84_54:
	mov.u32 	%r1881, 126;
	sub.s32 	%r1882, %r1881, %r3995;
	shl.b32 	%r1883, %r1882, 23;
	add.s32 	%r1884, %r3994, 1;
	shr.u32 	%r1885, %r1884, 7;
	add.s32 	%r1886, %r1885, 1;
	shr.u32 	%r1887, %r1886, 1;
	add.s32 	%r1888, %r1887, %r1883;
	or.b32  	%r1889, %r1888, %r69;
	mov.b32 	 %f2241, %r1889;

BB84_55:
	mul.rn.f32 	%f61, %f2241, %f2241;
	add.s32 	%r80, %r3996, 1;
	and.b32  	%r81, %r80, 1;
	setp.eq.s32	%p59, %r81, 0;
	@%p59 bra 	BB84_57;

	mov.f32 	%f1098, 0fBAB6061A;
	mov.f32 	%f1099, 0f37CCF5CE;
	fma.rn.f32 	%f2242, %f1099, %f61, %f1098;
	bra.uni 	BB84_58;

BB84_57:
	mov.f32 	%f1100, 0f3C08839E;
	mov.f32 	%f1101, 0fB94CA1F9;
	fma.rn.f32 	%f2242, %f1101, %f61, %f1100;

BB84_58:
	@%p59 bra 	BB84_60;

	mul.rn.f32 	%f2225, %f2241, %f2241;
	mov.f32 	%f1102, 0f3D2AAAA5;
	fma.rn.f32 	%f1103, %f2242, %f2225, %f1102;
	mov.f32 	%f1104, 0fBF000000;
	fma.rn.f32 	%f2243, %f1103, %f2225, %f1104;
	bra.uni 	BB84_61;

BB84_60:
	mul.rn.f32 	%f2227, %f2241, %f2241;
	mov.f32 	%f1105, 0fBE2AAAA3;
	fma.rn.f32 	%f1106, %f2242, %f2227, %f1105;
	mov.f32 	%f1107, 0f00000000;
	fma.rn.f32 	%f2243, %f1106, %f2227, %f1107;

BB84_61:
	fma.rn.f32 	%f2244, %f2243, %f2241, %f2241;
	@%p59 bra 	BB84_63;

	mul.rn.f32 	%f2226, %f2241, %f2241;
	mov.f32 	%f1108, 0f3F800000;
	fma.rn.f32 	%f2244, %f2243, %f2226, %f1108;

BB84_63:
	and.b32  	%r1890, %r80, 2;
	setp.eq.s32	%p62, %r1890, 0;
	@%p62 bra 	BB84_65;

	mov.f32 	%f1109, 0f00000000;
	mov.f32 	%f1110, 0fBF800000;
	fma.rn.f32 	%f2244, %f2244, %f1110, %f1109;

BB84_65:
	mul.f32 	%f73, %f2240, %f2244;
	abs.f32 	%f74, %f13;
	setp.neu.f32	%p63, %f74, 0f7F800000;
	mov.f32 	%f2358, %f13;
	@%p63 bra 	BB84_67;

	mov.f32 	%f1111, 0f00000000;
	mul.rn.f32 	%f75, %f13, %f1111;
	mov.f32 	%f2358, %f75;

BB84_67:
	mov.f32 	%f76, %f2358;
	mul.f32 	%f1112, %f76, 0f3F22F983;
	cvt.rni.s32.f32	%r4006, %f1112;
	cvt.rn.f32.s32	%f1113, %r4006;
	neg.f32 	%f1114, %f1113;
	fma.rn.f32 	%f1116, %f1114, %f1069, %f76;
	fma.rn.f32 	%f1118, %f1114, %f1071, %f1116;
	fma.rn.f32 	%f2245, %f1114, %f1073, %f1118;
	abs.f32 	%f1120, %f76;
	setp.leu.f32	%p64, %f1120, 0f47CE4780;
	@%p64 bra 	BB84_77;

	mov.b32 	 %r83, %f76;
	shr.u32 	%r84, %r83, 23;
	bfe.u32 	%r1893, %r83, 23, 8;
	add.s32 	%r1894, %r1893, -128;
	shl.b32 	%r1895, %r83, 8;
	or.b32  	%r85, %r1895, -2147483648;
	shr.u32 	%r86, %r1894, 5;
	mov.u32 	%r3998, 0;
	mov.u64 	%rd350, __cudart_i2opi_f;
	mov.u32 	%r3997, -6;
	mov.u64 	%rd488, %rd1;

BB84_69:
	.pragma "nounroll";
	ld.const.u32 	%r1898, [%rd350];
	// inline asm
	{
	mad.lo.cc.u32   %r1896, %r1898, %r85, %r3998;
	madc.hi.u32     %r3998, %r1898, %r85,  0;
	}
	// inline asm
	st.local.u32 	[%rd488], %r1896;
	add.s64 	%rd488, %rd488, 4;
	add.s64 	%rd350, %rd350, 4;
	add.s32 	%r3997, %r3997, 1;
	setp.ne.s32	%p65, %r3997, 0;
	@%p65 bra 	BB84_69;

	and.b32  	%r91, %r83, -2147483648;
	st.local.u32 	[%rd3], %r3998;
	mov.u32 	%r1901, 6;
	sub.s32 	%r1902, %r1901, %r86;
	mul.wide.s32 	%rd253, %r1902, 4;
	add.s64 	%rd18, %rd1, %rd253;
	ld.local.u32 	%r3999, [%rd18];
	ld.local.u32 	%r4000, [%rd18+-4];
	and.b32  	%r94, %r84, 31;
	setp.eq.s32	%p66, %r94, 0;
	@%p66 bra 	BB84_72;

	mov.u32 	%r1903, 32;
	sub.s32 	%r1904, %r1903, %r94;
	shr.u32 	%r1905, %r4000, %r1904;
	shl.b32 	%r1906, %r3999, %r94;
	add.s32 	%r3999, %r1905, %r1906;
	ld.local.u32 	%r1907, [%rd18+-8];
	shr.u32 	%r1908, %r1907, %r1904;
	shl.b32 	%r1909, %r4000, %r94;
	add.s32 	%r4000, %r1908, %r1909;

BB84_72:
	shr.u32 	%r1910, %r4000, 30;
	shl.b32 	%r1911, %r3999, 2;
	add.s32 	%r4001, %r1910, %r1911;
	shl.b32 	%r100, %r4000, 2;
	shr.u32 	%r1912, %r4001, 31;
	shr.u32 	%r1913, %r3999, 30;
	add.s32 	%r101, %r1912, %r1913;
	setp.eq.s32	%p67, %r1912, 0;
	mov.u32 	%r4002, %r91;
	mov.u32 	%r4003, %r100;
	@%p67 bra 	BB84_74;

	not.b32 	%r1914, %r4001;
	neg.s32 	%r102, %r100;
	setp.eq.s32	%p68, %r100, 0;
	selp.u32	%r1915, 1, 0, %p68;
	add.s32 	%r4001, %r1915, %r1914;
	xor.b32  	%r104, %r91, -2147483648;
	mov.u32 	%r4002, %r104;
	mov.u32 	%r4003, %r102;

BB84_74:
	mov.u32 	%r106, %r4002;
	neg.s32 	%r1916, %r101;
	setp.eq.s32	%p69, %r91, 0;
	selp.b32	%r4006, %r101, %r1916, %p69;
	clz.b32 	%r4005, %r4001;
	setp.eq.s32	%p70, %r4005, 0;
	shl.b32 	%r1917, %r4001, %r4005;
	mov.u32 	%r1918, 32;
	sub.s32 	%r1919, %r1918, %r4005;
	shr.u32 	%r1920, %r4003, %r1919;
	add.s32 	%r1921, %r1920, %r1917;
	selp.b32	%r110, %r4001, %r1921, %p70;
	mov.u32 	%r1922, -921707870;
	mul.hi.u32 	%r4004, %r110, %r1922;
	setp.lt.s32	%p71, %r4004, 1;
	@%p71 bra 	BB84_76;

	mul.lo.s32 	%r1923, %r110, -921707870;
	shr.u32 	%r1924, %r1923, 31;
	shl.b32 	%r1925, %r4004, 1;
	add.s32 	%r4004, %r1924, %r1925;
	add.s32 	%r4005, %r4005, 1;

BB84_76:
	mov.u32 	%r1926, 126;
	sub.s32 	%r1927, %r1926, %r4005;
	shl.b32 	%r1928, %r1927, 23;
	add.s32 	%r1929, %r4004, 1;
	shr.u32 	%r1930, %r1929, 7;
	add.s32 	%r1931, %r1930, 1;
	shr.u32 	%r1932, %r1931, 1;
	add.s32 	%r1933, %r1932, %r1928;
	or.b32  	%r1934, %r1933, %r106;
	mov.b32 	 %f2245, %r1934;

BB84_77:
	mul.rn.f32 	%f80, %f2245, %f2245;
	add.s32 	%r117, %r4006, 1;
	and.b32  	%r118, %r117, 1;
	setp.eq.s32	%p72, %r118, 0;
	@%p72 bra 	BB84_79;

	mov.f32 	%f1121, 0fBAB6061A;
	mov.f32 	%f1122, 0f37CCF5CE;
	fma.rn.f32 	%f2246, %f1122, %f80, %f1121;
	bra.uni 	BB84_80;

BB84_79:
	mov.f32 	%f1123, 0f3C08839E;
	mov.f32 	%f1124, 0fB94CA1F9;
	fma.rn.f32 	%f2246, %f1124, %f80, %f1123;

BB84_80:
	@%p72 bra 	BB84_82;

	mul.rn.f32 	%f2228, %f2245, %f2245;
	mov.f32 	%f1125, 0f3D2AAAA5;
	fma.rn.f32 	%f1126, %f2246, %f2228, %f1125;
	mov.f32 	%f1127, 0fBF000000;
	fma.rn.f32 	%f2247, %f1126, %f2228, %f1127;
	bra.uni 	BB84_83;

BB84_82:
	mul.rn.f32 	%f2230, %f2245, %f2245;
	mov.f32 	%f1128, 0fBE2AAAA3;
	fma.rn.f32 	%f1129, %f2246, %f2230, %f1128;
	mov.f32 	%f1130, 0f00000000;
	fma.rn.f32 	%f2247, %f1129, %f2230, %f1130;

BB84_83:
	fma.rn.f32 	%f2248, %f2247, %f2245, %f2245;
	@%p72 bra 	BB84_85;

	mul.rn.f32 	%f2229, %f2245, %f2245;
	mov.f32 	%f1131, 0f3F800000;
	fma.rn.f32 	%f2248, %f2247, %f2229, %f1131;

BB84_85:
	and.b32  	%r1935, %r117, 2;
	setp.eq.s32	%p75, %r1935, 0;
	@%p75 bra 	BB84_87;

	mov.f32 	%f1132, 0f00000000;
	mov.f32 	%f1133, 0fBF800000;
	fma.rn.f32 	%f2248, %f2248, %f1133, %f1132;

BB84_87:
	mul.f32 	%f92, %f73, %f2248;
	mov.f32 	%f2384, %f34;
	@%p37 bra 	BB84_89;

	mov.f32 	%f1134, 0f00000000;
	mul.rn.f32 	%f2384, %f34, %f1134;

BB84_89:
	mul.f32 	%f1135, %f2384, 0f3F22F983;
	cvt.rni.s32.f32	%r4016, %f1135;
	cvt.rn.f32.s32	%f1136, %r4016;
	neg.f32 	%f1137, %f1136;
	fma.rn.f32 	%f1139, %f1137, %f1069, %f2384;
	fma.rn.f32 	%f1141, %f1137, %f1071, %f1139;
	fma.rn.f32 	%f2249, %f1137, %f1073, %f1141;
	abs.f32 	%f1143, %f2384;
	setp.leu.f32	%p77, %f1143, 0f47CE4780;
	@%p77 bra 	BB84_99;

	mov.b32 	 %r120, %f2384;
	shr.u32 	%r121, %r120, 23;
	bfe.u32 	%r1938, %r120, 23, 8;
	add.s32 	%r1939, %r1938, -128;
	shl.b32 	%r1940, %r120, 8;
	or.b32  	%r122, %r1940, -2147483648;
	shr.u32 	%r123, %r1939, 5;
	mov.u32 	%r4008, 0;
	mov.u64 	%rd351, __cudart_i2opi_f;
	mov.u32 	%r4007, -6;
	mov.u64 	%rd487, %rd1;

BB84_91:
	.pragma "nounroll";
	ld.const.u32 	%r1943, [%rd351];
	// inline asm
	{
	mad.lo.cc.u32   %r1941, %r1943, %r122, %r4008;
	madc.hi.u32     %r4008, %r1943, %r122,  0;
	}
	// inline asm
	st.local.u32 	[%rd487], %r1941;
	add.s64 	%rd487, %rd487, 4;
	add.s64 	%rd351, %rd351, 4;
	add.s32 	%r4007, %r4007, 1;
	setp.ne.s32	%p78, %r4007, 0;
	@%p78 bra 	BB84_91;

	and.b32  	%r128, %r120, -2147483648;
	st.local.u32 	[%rd3], %r4008;
	mov.u32 	%r1946, 6;
	sub.s32 	%r1947, %r1946, %r123;
	mul.wide.s32 	%rd255, %r1947, 4;
	add.s64 	%rd23, %rd1, %rd255;
	ld.local.u32 	%r4009, [%rd23];
	ld.local.u32 	%r4010, [%rd23+-4];
	and.b32  	%r131, %r121, 31;
	setp.eq.s32	%p79, %r131, 0;
	@%p79 bra 	BB84_94;

	mov.u32 	%r1948, 32;
	sub.s32 	%r1949, %r1948, %r131;
	shr.u32 	%r1950, %r4010, %r1949;
	shl.b32 	%r1951, %r4009, %r131;
	add.s32 	%r4009, %r1950, %r1951;
	ld.local.u32 	%r1952, [%rd23+-8];
	shr.u32 	%r1953, %r1952, %r1949;
	shl.b32 	%r1954, %r4010, %r131;
	add.s32 	%r4010, %r1953, %r1954;

BB84_94:
	shr.u32 	%r1955, %r4010, 30;
	shl.b32 	%r1956, %r4009, 2;
	add.s32 	%r4011, %r1955, %r1956;
	shl.b32 	%r137, %r4010, 2;
	shr.u32 	%r1957, %r4011, 31;
	shr.u32 	%r1958, %r4009, 30;
	add.s32 	%r138, %r1957, %r1958;
	setp.eq.s32	%p80, %r1957, 0;
	mov.u32 	%r4012, %r128;
	mov.u32 	%r4013, %r137;
	@%p80 bra 	BB84_96;

	not.b32 	%r1959, %r4011;
	neg.s32 	%r139, %r137;
	setp.eq.s32	%p81, %r137, 0;
	selp.u32	%r1960, 1, 0, %p81;
	add.s32 	%r4011, %r1960, %r1959;
	xor.b32  	%r141, %r128, -2147483648;
	mov.u32 	%r4012, %r141;
	mov.u32 	%r4013, %r139;

BB84_96:
	mov.u32 	%r143, %r4012;
	neg.s32 	%r1961, %r138;
	setp.eq.s32	%p82, %r128, 0;
	selp.b32	%r4016, %r138, %r1961, %p82;
	clz.b32 	%r4015, %r4011;
	setp.eq.s32	%p83, %r4015, 0;
	shl.b32 	%r1962, %r4011, %r4015;
	mov.u32 	%r1963, 32;
	sub.s32 	%r1964, %r1963, %r4015;
	shr.u32 	%r1965, %r4013, %r1964;
	add.s32 	%r1966, %r1965, %r1962;
	selp.b32	%r147, %r4011, %r1966, %p83;
	mov.u32 	%r1967, -921707870;
	mul.hi.u32 	%r4014, %r147, %r1967;
	setp.lt.s32	%p84, %r4014, 1;
	@%p84 bra 	BB84_98;

	mul.lo.s32 	%r1968, %r147, -921707870;
	shr.u32 	%r1969, %r1968, 31;
	shl.b32 	%r1970, %r4014, 1;
	add.s32 	%r4014, %r1969, %r1970;
	add.s32 	%r4015, %r4015, 1;

BB84_98:
	mov.u32 	%r1971, 126;
	sub.s32 	%r1972, %r1971, %r4015;
	shl.b32 	%r1973, %r1972, 23;
	add.s32 	%r1974, %r4014, 1;
	shr.u32 	%r1975, %r1974, 7;
	add.s32 	%r1976, %r1975, 1;
	shr.u32 	%r1977, %r1976, 1;
	add.s32 	%r1978, %r1977, %r1973;
	or.b32  	%r1979, %r1978, %r143;
	mov.b32 	 %f2249, %r1979;

BB84_99:
	mul.rn.f32 	%f98, %f2249, %f2249;
	and.b32  	%r154, %r4016, 1;
	setp.eq.s32	%p85, %r154, 0;
	@%p85 bra 	BB84_101;

	mov.f32 	%f1144, 0fBAB6061A;
	mov.f32 	%f1145, 0f37CCF5CE;
	fma.rn.f32 	%f2250, %f1145, %f98, %f1144;
	bra.uni 	BB84_102;

BB84_101:
	mov.f32 	%f1146, 0f3C08839E;
	mov.f32 	%f1147, 0fB94CA1F9;
	fma.rn.f32 	%f2250, %f1147, %f98, %f1146;

BB84_102:
	@%p85 bra 	BB84_104;

	mov.f32 	%f1148, 0f3D2AAAA5;
	fma.rn.f32 	%f1149, %f2250, %f98, %f1148;
	mov.f32 	%f1150, 0fBF000000;
	fma.rn.f32 	%f2251, %f1149, %f98, %f1150;
	bra.uni 	BB84_105;

BB84_104:
	mov.f32 	%f1151, 0fBE2AAAA3;
	fma.rn.f32 	%f1152, %f2250, %f98, %f1151;
	mov.f32 	%f1153, 0f00000000;
	fma.rn.f32 	%f2251, %f1152, %f98, %f1153;

BB84_105:
	fma.rn.f32 	%f2252, %f2251, %f2249, %f2249;
	@%p85 bra 	BB84_107;

	mul.rn.f32 	%f2231, %f2249, %f2249;
	mov.f32 	%f1154, 0f3F800000;
	fma.rn.f32 	%f2252, %f2251, %f2231, %f1154;

BB84_107:
	and.b32  	%r1980, %r4016, 2;
	setp.eq.s32	%p88, %r1980, 0;
	@%p88 bra 	BB84_109;

	mov.f32 	%f1155, 0f00000000;
	mov.f32 	%f1156, 0fBF800000;
	fma.rn.f32 	%f2252, %f2252, %f1156, %f1155;

BB84_109:
	mov.f32 	%f2338, %f36;
	@%p50 bra 	BB84_111;

	mov.f32 	%f1157, 0f00000000;
	mul.rn.f32 	%f2338, %f36, %f1157;

BB84_111:
	mul.f32 	%f1158, %f2338, 0f3F22F983;
	cvt.rni.s32.f32	%r4026, %f1158;
	cvt.rn.f32.s32	%f1159, %r4026;
	neg.f32 	%f1160, %f1159;
	fma.rn.f32 	%f1162, %f1160, %f1069, %f2338;
	fma.rn.f32 	%f1164, %f1160, %f1071, %f1162;
	fma.rn.f32 	%f2253, %f1160, %f1073, %f1164;
	abs.f32 	%f1166, %f2338;
	setp.leu.f32	%p90, %f1166, 0f47CE4780;
	@%p90 bra 	BB84_121;

	mov.b32 	 %r156, %f2338;
	shr.u32 	%r157, %r156, 23;
	bfe.u32 	%r1983, %r156, 23, 8;
	add.s32 	%r1984, %r1983, -128;
	shl.b32 	%r1985, %r156, 8;
	or.b32  	%r158, %r1985, -2147483648;
	shr.u32 	%r159, %r1984, 5;
	mov.u32 	%r4018, 0;
	mov.u64 	%rd352, __cudart_i2opi_f;
	mov.u32 	%r4017, -6;
	mov.u64 	%rd486, %rd1;

BB84_113:
	.pragma "nounroll";
	ld.const.u32 	%r1988, [%rd352];
	// inline asm
	{
	mad.lo.cc.u32   %r1986, %r1988, %r158, %r4018;
	madc.hi.u32     %r4018, %r1988, %r158,  0;
	}
	// inline asm
	st.local.u32 	[%rd486], %r1986;
	add.s64 	%rd486, %rd486, 4;
	add.s64 	%rd352, %rd352, 4;
	add.s32 	%r4017, %r4017, 1;
	setp.ne.s32	%p91, %r4017, 0;
	@%p91 bra 	BB84_113;

	and.b32  	%r164, %r156, -2147483648;
	st.local.u32 	[%rd3], %r4018;
	mov.u32 	%r1991, 6;
	sub.s32 	%r1992, %r1991, %r159;
	mul.wide.s32 	%rd257, %r1992, 4;
	add.s64 	%rd28, %rd1, %rd257;
	ld.local.u32 	%r4019, [%rd28];
	ld.local.u32 	%r4020, [%rd28+-4];
	and.b32  	%r167, %r157, 31;
	setp.eq.s32	%p92, %r167, 0;
	@%p92 bra 	BB84_116;

	mov.u32 	%r1993, 32;
	sub.s32 	%r1994, %r1993, %r167;
	shr.u32 	%r1995, %r4020, %r1994;
	shl.b32 	%r1996, %r4019, %r167;
	add.s32 	%r4019, %r1995, %r1996;
	ld.local.u32 	%r1997, [%rd28+-8];
	shr.u32 	%r1998, %r1997, %r1994;
	shl.b32 	%r1999, %r4020, %r167;
	add.s32 	%r4020, %r1998, %r1999;

BB84_116:
	shr.u32 	%r2000, %r4020, 30;
	shl.b32 	%r2001, %r4019, 2;
	add.s32 	%r4021, %r2000, %r2001;
	shl.b32 	%r173, %r4020, 2;
	shr.u32 	%r2002, %r4021, 31;
	shr.u32 	%r2003, %r4019, 30;
	add.s32 	%r174, %r2002, %r2003;
	setp.eq.s32	%p93, %r2002, 0;
	mov.u32 	%r4022, %r164;
	mov.u32 	%r4023, %r173;
	@%p93 bra 	BB84_118;

	not.b32 	%r2004, %r4021;
	neg.s32 	%r175, %r173;
	setp.eq.s32	%p94, %r173, 0;
	selp.u32	%r2005, 1, 0, %p94;
	add.s32 	%r4021, %r2005, %r2004;
	xor.b32  	%r177, %r164, -2147483648;
	mov.u32 	%r4022, %r177;
	mov.u32 	%r4023, %r175;

BB84_118:
	mov.u32 	%r179, %r4022;
	neg.s32 	%r2006, %r174;
	setp.eq.s32	%p95, %r164, 0;
	selp.b32	%r4026, %r174, %r2006, %p95;
	clz.b32 	%r4025, %r4021;
	setp.eq.s32	%p96, %r4025, 0;
	shl.b32 	%r2007, %r4021, %r4025;
	mov.u32 	%r2008, 32;
	sub.s32 	%r2009, %r2008, %r4025;
	shr.u32 	%r2010, %r4023, %r2009;
	add.s32 	%r2011, %r2010, %r2007;
	selp.b32	%r183, %r4021, %r2011, %p96;
	mov.u32 	%r2012, -921707870;
	mul.hi.u32 	%r4024, %r183, %r2012;
	setp.lt.s32	%p97, %r4024, 1;
	@%p97 bra 	BB84_120;

	mul.lo.s32 	%r2013, %r183, -921707870;
	shr.u32 	%r2014, %r2013, 31;
	shl.b32 	%r2015, %r4024, 1;
	add.s32 	%r4024, %r2014, %r2015;
	add.s32 	%r4025, %r4025, 1;

BB84_120:
	mov.u32 	%r2016, 126;
	sub.s32 	%r2017, %r2016, %r4025;
	shl.b32 	%r2018, %r2017, 23;
	add.s32 	%r2019, %r4024, 1;
	shr.u32 	%r2020, %r2019, 7;
	add.s32 	%r2021, %r2020, 1;
	shr.u32 	%r2022, %r2021, 1;
	add.s32 	%r2023, %r2022, %r2018;
	or.b32  	%r2024, %r2023, %r179;
	mov.b32 	 %f2253, %r2024;

BB84_121:
	mul.rn.f32 	%f115, %f2253, %f2253;
	and.b32  	%r190, %r4026, 1;
	setp.eq.s32	%p98, %r190, 0;
	@%p98 bra 	BB84_123;

	mov.f32 	%f1167, 0fBAB6061A;
	mov.f32 	%f1168, 0f37CCF5CE;
	fma.rn.f32 	%f2254, %f1168, %f115, %f1167;
	bra.uni 	BB84_124;

BB84_123:
	mov.f32 	%f1169, 0f3C08839E;
	mov.f32 	%f1170, 0fB94CA1F9;
	fma.rn.f32 	%f2254, %f1170, %f115, %f1169;

BB84_124:
	@%p98 bra 	BB84_126;

	mov.f32 	%f1171, 0f3D2AAAA5;
	fma.rn.f32 	%f1172, %f2254, %f115, %f1171;
	mov.f32 	%f1173, 0fBF000000;
	fma.rn.f32 	%f2255, %f1172, %f115, %f1173;
	bra.uni 	BB84_127;

BB84_126:
	mov.f32 	%f1174, 0fBE2AAAA3;
	fma.rn.f32 	%f1175, %f2254, %f115, %f1174;
	mov.f32 	%f1176, 0f00000000;
	fma.rn.f32 	%f2255, %f1175, %f115, %f1176;

BB84_127:
	fma.rn.f32 	%f2256, %f2255, %f2253, %f2253;
	@%p98 bra 	BB84_129;

	mul.rn.f32 	%f2232, %f2253, %f2253;
	mov.f32 	%f1177, 0f3F800000;
	fma.rn.f32 	%f2256, %f2255, %f2232, %f1177;

BB84_129:
	and.b32  	%r2025, %r4026, 2;
	setp.eq.s32	%p101, %r2025, 0;
	@%p101 bra 	BB84_131;

	mov.f32 	%f1178, 0f00000000;
	mov.f32 	%f1179, 0fBF800000;
	fma.rn.f32 	%f2256, %f2256, %f1179, %f1178;

BB84_131:
	mul.f32 	%f127, %f2252, %f2256;
	mov.f32 	%f2357, %f13;
	@%p63 bra 	BB84_133;

	mov.f32 	%f1180, 0f00000000;
	mul.rn.f32 	%f2357, %f13, %f1180;

BB84_133:
	mul.f32 	%f1181, %f2357, 0f3F22F983;
	cvt.rni.s32.f32	%r4036, %f1181;
	cvt.rn.f32.s32	%f1182, %r4036;
	neg.f32 	%f1183, %f1182;
	fma.rn.f32 	%f1185, %f1183, %f1069, %f2357;
	fma.rn.f32 	%f1187, %f1183, %f1071, %f1185;
	fma.rn.f32 	%f2257, %f1183, %f1073, %f1187;
	abs.f32 	%f1189, %f2357;
	setp.leu.f32	%p103, %f1189, 0f47CE4780;
	@%p103 bra 	BB84_143;

	mov.b32 	 %r192, %f2357;
	shr.u32 	%r193, %r192, 23;
	bfe.u32 	%r2028, %r192, 23, 8;
	add.s32 	%r2029, %r2028, -128;
	shl.b32 	%r2030, %r192, 8;
	or.b32  	%r194, %r2030, -2147483648;
	shr.u32 	%r195, %r2029, 5;
	mov.u32 	%r4028, 0;
	mov.u64 	%rd353, __cudart_i2opi_f;
	mov.u32 	%r4027, -6;
	mov.u64 	%rd485, %rd1;

BB84_135:
	.pragma "nounroll";
	ld.const.u32 	%r2033, [%rd353];
	// inline asm
	{
	mad.lo.cc.u32   %r2031, %r2033, %r194, %r4028;
	madc.hi.u32     %r4028, %r2033, %r194,  0;
	}
	// inline asm
	st.local.u32 	[%rd485], %r2031;
	add.s64 	%rd485, %rd485, 4;
	add.s64 	%rd353, %rd353, 4;
	add.s32 	%r4027, %r4027, 1;
	setp.ne.s32	%p104, %r4027, 0;
	@%p104 bra 	BB84_135;

	and.b32  	%r200, %r192, -2147483648;
	st.local.u32 	[%rd3], %r4028;
	mov.u32 	%r2036, 6;
	sub.s32 	%r2037, %r2036, %r195;
	mul.wide.s32 	%rd259, %r2037, 4;
	add.s64 	%rd33, %rd1, %rd259;
	ld.local.u32 	%r4029, [%rd33];
	ld.local.u32 	%r4030, [%rd33+-4];
	and.b32  	%r203, %r193, 31;
	setp.eq.s32	%p105, %r203, 0;
	@%p105 bra 	BB84_138;

	mov.u32 	%r2038, 32;
	sub.s32 	%r2039, %r2038, %r203;
	shr.u32 	%r2040, %r4030, %r2039;
	shl.b32 	%r2041, %r4029, %r203;
	add.s32 	%r4029, %r2040, %r2041;
	ld.local.u32 	%r2042, [%rd33+-8];
	shr.u32 	%r2043, %r2042, %r2039;
	shl.b32 	%r2044, %r4030, %r203;
	add.s32 	%r4030, %r2043, %r2044;

BB84_138:
	shr.u32 	%r2045, %r4030, 30;
	shl.b32 	%r2046, %r4029, 2;
	add.s32 	%r4031, %r2045, %r2046;
	shl.b32 	%r209, %r4030, 2;
	shr.u32 	%r2047, %r4031, 31;
	shr.u32 	%r2048, %r4029, 30;
	add.s32 	%r210, %r2047, %r2048;
	setp.eq.s32	%p106, %r2047, 0;
	mov.u32 	%r4032, %r200;
	mov.u32 	%r4033, %r209;
	@%p106 bra 	BB84_140;

	not.b32 	%r2049, %r4031;
	neg.s32 	%r211, %r209;
	setp.eq.s32	%p107, %r209, 0;
	selp.u32	%r2050, 1, 0, %p107;
	add.s32 	%r4031, %r2050, %r2049;
	xor.b32  	%r213, %r200, -2147483648;
	mov.u32 	%r4032, %r213;
	mov.u32 	%r4033, %r211;

BB84_140:
	mov.u32 	%r215, %r4032;
	neg.s32 	%r2051, %r210;
	setp.eq.s32	%p108, %r200, 0;
	selp.b32	%r4036, %r210, %r2051, %p108;
	clz.b32 	%r4035, %r4031;
	setp.eq.s32	%p109, %r4035, 0;
	shl.b32 	%r2052, %r4031, %r4035;
	mov.u32 	%r2053, 32;
	sub.s32 	%r2054, %r2053, %r4035;
	shr.u32 	%r2055, %r4033, %r2054;
	add.s32 	%r2056, %r2055, %r2052;
	selp.b32	%r219, %r4031, %r2056, %p109;
	mov.u32 	%r2057, -921707870;
	mul.hi.u32 	%r4034, %r219, %r2057;
	setp.lt.s32	%p110, %r4034, 1;
	@%p110 bra 	BB84_142;

	mul.lo.s32 	%r2058, %r219, -921707870;
	shr.u32 	%r2059, %r2058, 31;
	shl.b32 	%r2060, %r4034, 1;
	add.s32 	%r4034, %r2059, %r2060;
	add.s32 	%r4035, %r4035, 1;

BB84_142:
	mov.u32 	%r2061, 126;
	sub.s32 	%r2062, %r2061, %r4035;
	shl.b32 	%r2063, %r2062, 23;
	add.s32 	%r2064, %r4034, 1;
	shr.u32 	%r2065, %r2064, 7;
	add.s32 	%r2066, %r2065, 1;
	shr.u32 	%r2067, %r2066, 1;
	add.s32 	%r2068, %r2067, %r2063;
	or.b32  	%r2069, %r2068, %r215;
	mov.b32 	 %f2257, %r2069;

BB84_143:
	mul.rn.f32 	%f133, %f2257, %f2257;
	and.b32  	%r226, %r4036, 1;
	setp.eq.s32	%p111, %r226, 0;
	@%p111 bra 	BB84_145;

	mov.f32 	%f1190, 0fBAB6061A;
	mov.f32 	%f1191, 0f37CCF5CE;
	fma.rn.f32 	%f2258, %f1191, %f133, %f1190;
	bra.uni 	BB84_146;

BB84_145:
	mov.f32 	%f1192, 0f3C08839E;
	mov.f32 	%f1193, 0fB94CA1F9;
	fma.rn.f32 	%f2258, %f1193, %f133, %f1192;

BB84_146:
	@%p111 bra 	BB84_148;

	mov.f32 	%f1194, 0f3D2AAAA5;
	fma.rn.f32 	%f1195, %f2258, %f133, %f1194;
	mov.f32 	%f1196, 0fBF000000;
	fma.rn.f32 	%f2259, %f1195, %f133, %f1196;
	bra.uni 	BB84_149;

BB84_148:
	mov.f32 	%f1197, 0fBE2AAAA3;
	fma.rn.f32 	%f1198, %f2258, %f133, %f1197;
	mov.f32 	%f1199, 0f00000000;
	fma.rn.f32 	%f2259, %f1198, %f133, %f1199;

BB84_149:
	fma.rn.f32 	%f2260, %f2259, %f2257, %f2257;
	@%p111 bra 	BB84_151;

	mul.rn.f32 	%f2233, %f2257, %f2257;
	mov.f32 	%f1200, 0f3F800000;
	fma.rn.f32 	%f2260, %f2259, %f2233, %f1200;

BB84_151:
	and.b32  	%r2070, %r4036, 2;
	setp.eq.s32	%p114, %r2070, 0;
	@%p114 bra 	BB84_153;

	mov.f32 	%f1201, 0f00000000;
	mov.f32 	%f1202, 0fBF800000;
	fma.rn.f32 	%f2260, %f2260, %f1202, %f1201;

BB84_153:
	fma.rn.f32 	%f145, %f127, %f2260, %f92;
	mov.f32 	%f2383, %f34;
	@%p37 bra 	BB84_155;

	mov.f32 	%f1203, 0f00000000;
	mul.rn.f32 	%f2383, %f34, %f1203;

BB84_155:
	mul.f32 	%f1204, %f2383, 0f3F22F983;
	cvt.rni.s32.f32	%r4046, %f1204;
	cvt.rn.f32.s32	%f1205, %r4046;
	neg.f32 	%f1206, %f1205;
	fma.rn.f32 	%f1208, %f1206, %f1069, %f2383;
	fma.rn.f32 	%f1210, %f1206, %f1071, %f1208;
	fma.rn.f32 	%f2261, %f1206, %f1073, %f1210;
	abs.f32 	%f1212, %f2383;
	setp.leu.f32	%p116, %f1212, 0f47CE4780;
	@%p116 bra 	BB84_165;

	mov.b32 	 %r228, %f2383;
	shr.u32 	%r229, %r228, 23;
	bfe.u32 	%r2073, %r228, 23, 8;
	add.s32 	%r2074, %r2073, -128;
	shl.b32 	%r2075, %r228, 8;
	or.b32  	%r230, %r2075, -2147483648;
	shr.u32 	%r231, %r2074, 5;
	mov.u32 	%r4038, 0;
	mov.u64 	%rd354, __cudart_i2opi_f;
	mov.u32 	%r4037, -6;
	mov.u64 	%rd484, %rd1;

BB84_157:
	.pragma "nounroll";
	ld.const.u32 	%r2078, [%rd354];
	// inline asm
	{
	mad.lo.cc.u32   %r2076, %r2078, %r230, %r4038;
	madc.hi.u32     %r4038, %r2078, %r230,  0;
	}
	// inline asm
	st.local.u32 	[%rd484], %r2076;
	add.s64 	%rd484, %rd484, 4;
	add.s64 	%rd354, %rd354, 4;
	add.s32 	%r4037, %r4037, 1;
	setp.ne.s32	%p117, %r4037, 0;
	@%p117 bra 	BB84_157;

	and.b32  	%r236, %r228, -2147483648;
	st.local.u32 	[%rd3], %r4038;
	mov.u32 	%r2081, 6;
	sub.s32 	%r2082, %r2081, %r231;
	mul.wide.s32 	%rd261, %r2082, 4;
	add.s64 	%rd38, %rd1, %rd261;
	ld.local.u32 	%r4039, [%rd38];
	ld.local.u32 	%r4040, [%rd38+-4];
	and.b32  	%r239, %r229, 31;
	setp.eq.s32	%p118, %r239, 0;
	@%p118 bra 	BB84_160;

	mov.u32 	%r2083, 32;
	sub.s32 	%r2084, %r2083, %r239;
	shr.u32 	%r2085, %r4040, %r2084;
	shl.b32 	%r2086, %r4039, %r239;
	add.s32 	%r4039, %r2085, %r2086;
	ld.local.u32 	%r2087, [%rd38+-8];
	shr.u32 	%r2088, %r2087, %r2084;
	shl.b32 	%r2089, %r4040, %r239;
	add.s32 	%r4040, %r2088, %r2089;

BB84_160:
	shr.u32 	%r2090, %r4040, 30;
	shl.b32 	%r2091, %r4039, 2;
	add.s32 	%r4041, %r2090, %r2091;
	shl.b32 	%r245, %r4040, 2;
	shr.u32 	%r2092, %r4041, 31;
	shr.u32 	%r2093, %r4039, 30;
	add.s32 	%r246, %r2092, %r2093;
	setp.eq.s32	%p119, %r2092, 0;
	mov.u32 	%r4042, %r236;
	mov.u32 	%r4043, %r245;
	@%p119 bra 	BB84_162;

	not.b32 	%r2094, %r4041;
	neg.s32 	%r247, %r245;
	setp.eq.s32	%p120, %r245, 0;
	selp.u32	%r2095, 1, 0, %p120;
	add.s32 	%r4041, %r2095, %r2094;
	xor.b32  	%r249, %r236, -2147483648;
	mov.u32 	%r4042, %r249;
	mov.u32 	%r4043, %r247;

BB84_162:
	mov.u32 	%r251, %r4042;
	neg.s32 	%r2096, %r246;
	setp.eq.s32	%p121, %r236, 0;
	selp.b32	%r4046, %r246, %r2096, %p121;
	clz.b32 	%r4045, %r4041;
	setp.eq.s32	%p122, %r4045, 0;
	shl.b32 	%r2097, %r4041, %r4045;
	mov.u32 	%r2098, 32;
	sub.s32 	%r2099, %r2098, %r4045;
	shr.u32 	%r2100, %r4043, %r2099;
	add.s32 	%r2101, %r2100, %r2097;
	selp.b32	%r255, %r4041, %r2101, %p122;
	mov.u32 	%r2102, -921707870;
	mul.hi.u32 	%r4044, %r255, %r2102;
	setp.lt.s32	%p123, %r4044, 1;
	@%p123 bra 	BB84_164;

	mul.lo.s32 	%r2103, %r255, -921707870;
	shr.u32 	%r2104, %r2103, 31;
	shl.b32 	%r2105, %r4044, 1;
	add.s32 	%r4044, %r2104, %r2105;
	add.s32 	%r4045, %r4045, 1;

BB84_164:
	mov.u32 	%r2106, 126;
	sub.s32 	%r2107, %r2106, %r4045;
	shl.b32 	%r2108, %r2107, 23;
	add.s32 	%r2109, %r4044, 1;
	shr.u32 	%r2110, %r2109, 7;
	add.s32 	%r2111, %r2110, 1;
	shr.u32 	%r2112, %r2111, 1;
	add.s32 	%r2113, %r2112, %r2108;
	or.b32  	%r2114, %r2113, %r251;
	mov.b32 	 %f2261, %r2114;

BB84_165:
	mul.rn.f32 	%f151, %f2261, %f2261;
	and.b32  	%r262, %r4046, 1;
	setp.eq.s32	%p124, %r262, 0;
	@%p124 bra 	BB84_167;

	mov.f32 	%f1213, 0fBAB6061A;
	mov.f32 	%f1214, 0f37CCF5CE;
	fma.rn.f32 	%f2262, %f1214, %f151, %f1213;
	bra.uni 	BB84_168;

BB84_167:
	mov.f32 	%f1215, 0f3C08839E;
	mov.f32 	%f1216, 0fB94CA1F9;
	fma.rn.f32 	%f2262, %f1216, %f151, %f1215;

BB84_168:
	@%p124 bra 	BB84_170;

	mov.f32 	%f1217, 0f3D2AAAA5;
	fma.rn.f32 	%f1218, %f2262, %f151, %f1217;
	mov.f32 	%f1219, 0fBF000000;
	fma.rn.f32 	%f2263, %f1218, %f151, %f1219;
	bra.uni 	BB84_171;

BB84_170:
	mov.f32 	%f1220, 0fBE2AAAA3;
	fma.rn.f32 	%f1221, %f2262, %f151, %f1220;
	mov.f32 	%f1222, 0f00000000;
	fma.rn.f32 	%f2263, %f1221, %f151, %f1222;

BB84_171:
	fma.rn.f32 	%f2264, %f2263, %f2261, %f2261;
	@%p124 bra 	BB84_173;

	mov.f32 	%f1223, 0f3F800000;
	fma.rn.f32 	%f2264, %f2263, %f151, %f1223;

BB84_173:
	and.b32  	%r2115, %r4046, 2;
	setp.eq.s32	%p127, %r2115, 0;
	@%p127 bra 	BB84_175;

	mov.f32 	%f1224, 0f00000000;
	mov.f32 	%f1225, 0fBF800000;
	fma.rn.f32 	%f2264, %f2264, %f1225, %f1224;

BB84_175:
	mov.f32 	%f2337, %f36;
	@%p50 bra 	BB84_177;

	mov.f32 	%f1226, 0f00000000;
	mul.rn.f32 	%f2337, %f36, %f1226;

BB84_177:
	mul.f32 	%f1227, %f2337, 0f3F22F983;
	cvt.rni.s32.f32	%r4056, %f1227;
	cvt.rn.f32.s32	%f1228, %r4056;
	neg.f32 	%f1229, %f1228;
	fma.rn.f32 	%f1231, %f1229, %f1069, %f2337;
	fma.rn.f32 	%f1233, %f1229, %f1071, %f1231;
	fma.rn.f32 	%f2265, %f1229, %f1073, %f1233;
	abs.f32 	%f1235, %f2337;
	setp.leu.f32	%p129, %f1235, 0f47CE4780;
	@%p129 bra 	BB84_187;

	mov.b32 	 %r264, %f2337;
	shr.u32 	%r265, %r264, 23;
	bfe.u32 	%r2118, %r264, 23, 8;
	add.s32 	%r2119, %r2118, -128;
	shl.b32 	%r2120, %r264, 8;
	or.b32  	%r266, %r2120, -2147483648;
	shr.u32 	%r267, %r2119, 5;
	mov.u32 	%r4048, 0;
	mov.u64 	%rd355, __cudart_i2opi_f;
	mov.u32 	%r4047, -6;
	mov.u64 	%rd483, %rd1;

BB84_179:
	.pragma "nounroll";
	ld.const.u32 	%r2123, [%rd355];
	// inline asm
	{
	mad.lo.cc.u32   %r2121, %r2123, %r266, %r4048;
	madc.hi.u32     %r4048, %r2123, %r266,  0;
	}
	// inline asm
	st.local.u32 	[%rd483], %r2121;
	add.s64 	%rd483, %rd483, 4;
	add.s64 	%rd355, %rd355, 4;
	add.s32 	%r4047, %r4047, 1;
	setp.ne.s32	%p130, %r4047, 0;
	@%p130 bra 	BB84_179;

	and.b32  	%r272, %r264, -2147483648;
	st.local.u32 	[%rd3], %r4048;
	mov.u32 	%r2126, 6;
	sub.s32 	%r2127, %r2126, %r267;
	mul.wide.s32 	%rd263, %r2127, 4;
	add.s64 	%rd43, %rd1, %rd263;
	ld.local.u32 	%r4049, [%rd43];
	ld.local.u32 	%r4050, [%rd43+-4];
	and.b32  	%r275, %r265, 31;
	setp.eq.s32	%p131, %r275, 0;
	@%p131 bra 	BB84_182;

	mov.u32 	%r2128, 32;
	sub.s32 	%r2129, %r2128, %r275;
	shr.u32 	%r2130, %r4050, %r2129;
	shl.b32 	%r2131, %r4049, %r275;
	add.s32 	%r4049, %r2130, %r2131;
	ld.local.u32 	%r2132, [%rd43+-8];
	shr.u32 	%r2133, %r2132, %r2129;
	shl.b32 	%r2134, %r4050, %r275;
	add.s32 	%r4050, %r2133, %r2134;

BB84_182:
	shr.u32 	%r2135, %r4050, 30;
	shl.b32 	%r2136, %r4049, 2;
	add.s32 	%r4051, %r2135, %r2136;
	shl.b32 	%r281, %r4050, 2;
	shr.u32 	%r2137, %r4051, 31;
	shr.u32 	%r2138, %r4049, 30;
	add.s32 	%r282, %r2137, %r2138;
	setp.eq.s32	%p132, %r2137, 0;
	mov.u32 	%r4052, %r272;
	mov.u32 	%r4053, %r281;
	@%p132 bra 	BB84_184;

	not.b32 	%r2139, %r4051;
	neg.s32 	%r283, %r281;
	setp.eq.s32	%p133, %r281, 0;
	selp.u32	%r2140, 1, 0, %p133;
	add.s32 	%r4051, %r2140, %r2139;
	xor.b32  	%r285, %r272, -2147483648;
	mov.u32 	%r4052, %r285;
	mov.u32 	%r4053, %r283;

BB84_184:
	mov.u32 	%r287, %r4052;
	neg.s32 	%r2141, %r282;
	setp.eq.s32	%p134, %r272, 0;
	selp.b32	%r4056, %r282, %r2141, %p134;
	clz.b32 	%r4055, %r4051;
	setp.eq.s32	%p135, %r4055, 0;
	shl.b32 	%r2142, %r4051, %r4055;
	mov.u32 	%r2143, 32;
	sub.s32 	%r2144, %r2143, %r4055;
	shr.u32 	%r2145, %r4053, %r2144;
	add.s32 	%r2146, %r2145, %r2142;
	selp.b32	%r291, %r4051, %r2146, %p135;
	mov.u32 	%r2147, -921707870;
	mul.hi.u32 	%r4054, %r291, %r2147;
	setp.lt.s32	%p136, %r4054, 1;
	@%p136 bra 	BB84_186;

	mul.lo.s32 	%r2148, %r291, -921707870;
	shr.u32 	%r2149, %r2148, 31;
	shl.b32 	%r2150, %r4054, 1;
	add.s32 	%r4054, %r2149, %r2150;
	add.s32 	%r4055, %r4055, 1;

BB84_186:
	mov.u32 	%r2151, 126;
	sub.s32 	%r2152, %r2151, %r4055;
	shl.b32 	%r2153, %r2152, 23;
	add.s32 	%r2154, %r4054, 1;
	shr.u32 	%r2155, %r2154, 7;
	add.s32 	%r2156, %r2155, 1;
	shr.u32 	%r2157, %r2156, 1;
	add.s32 	%r2158, %r2157, %r2153;
	or.b32  	%r2159, %r2158, %r287;
	mov.b32 	 %f2265, %r2159;

BB84_187:
	mul.rn.f32 	%f168, %f2265, %f2265;
	add.s32 	%r298, %r4056, 1;
	and.b32  	%r299, %r298, 1;
	setp.eq.s32	%p137, %r299, 0;
	@%p137 bra 	BB84_189;

	mov.f32 	%f1236, 0fBAB6061A;
	mov.f32 	%f1237, 0f37CCF5CE;
	fma.rn.f32 	%f2266, %f1237, %f168, %f1236;
	bra.uni 	BB84_190;

BB84_189:
	mov.f32 	%f1238, 0f3C08839E;
	mov.f32 	%f1239, 0fB94CA1F9;
	fma.rn.f32 	%f2266, %f1239, %f168, %f1238;

BB84_190:
	@%p137 bra 	BB84_192;

	mov.f32 	%f1240, 0f3D2AAAA5;
	fma.rn.f32 	%f1241, %f2266, %f168, %f1240;
	mov.f32 	%f1242, 0fBF000000;
	fma.rn.f32 	%f2267, %f1241, %f168, %f1242;
	bra.uni 	BB84_193;

BB84_192:
	mov.f32 	%f1243, 0fBE2AAAA3;
	fma.rn.f32 	%f1244, %f2266, %f168, %f1243;
	mov.f32 	%f1245, 0f00000000;
	fma.rn.f32 	%f2267, %f1244, %f168, %f1245;

BB84_193:
	fma.rn.f32 	%f2268, %f2267, %f2265, %f2265;
	@%p137 bra 	BB84_195;

	mov.f32 	%f1246, 0f3F800000;
	fma.rn.f32 	%f2268, %f2267, %f168, %f1246;

BB84_195:
	and.b32  	%r2160, %r298, 2;
	setp.eq.s32	%p140, %r2160, 0;
	@%p140 bra 	BB84_197;

	mov.f32 	%f1247, 0f00000000;
	mov.f32 	%f1248, 0fBF800000;
	fma.rn.f32 	%f2268, %f2268, %f1248, %f1247;

BB84_197:
	mul.f32 	%f180, %f2264, %f2268;
	mov.f32 	%f2356, %f13;
	@%p63 bra 	BB84_199;

	mov.f32 	%f1249, 0f00000000;
	mul.rn.f32 	%f2356, %f13, %f1249;

BB84_199:
	mul.f32 	%f1250, %f2356, 0f3F22F983;
	cvt.rni.s32.f32	%r4066, %f1250;
	cvt.rn.f32.s32	%f1251, %r4066;
	neg.f32 	%f1252, %f1251;
	fma.rn.f32 	%f1254, %f1252, %f1069, %f2356;
	fma.rn.f32 	%f1256, %f1252, %f1071, %f1254;
	fma.rn.f32 	%f2269, %f1252, %f1073, %f1256;
	abs.f32 	%f1258, %f2356;
	setp.leu.f32	%p142, %f1258, 0f47CE4780;
	@%p142 bra 	BB84_209;

	mov.b32 	 %r301, %f2356;
	shr.u32 	%r302, %r301, 23;
	bfe.u32 	%r2163, %r301, 23, 8;
	add.s32 	%r2164, %r2163, -128;
	shl.b32 	%r2165, %r301, 8;
	or.b32  	%r303, %r2165, -2147483648;
	shr.u32 	%r304, %r2164, 5;
	mov.u32 	%r4058, 0;
	mov.u64 	%rd356, __cudart_i2opi_f;
	mov.u32 	%r4057, -6;
	mov.u64 	%rd482, %rd1;

BB84_201:
	.pragma "nounroll";
	ld.const.u32 	%r2168, [%rd356];
	// inline asm
	{
	mad.lo.cc.u32   %r2166, %r2168, %r303, %r4058;
	madc.hi.u32     %r4058, %r2168, %r303,  0;
	}
	// inline asm
	st.local.u32 	[%rd482], %r2166;
	add.s64 	%rd482, %rd482, 4;
	add.s64 	%rd356, %rd356, 4;
	add.s32 	%r4057, %r4057, 1;
	setp.ne.s32	%p143, %r4057, 0;
	@%p143 bra 	BB84_201;

	and.b32  	%r309, %r301, -2147483648;
	st.local.u32 	[%rd3], %r4058;
	mov.u32 	%r2171, 6;
	sub.s32 	%r2172, %r2171, %r304;
	mul.wide.s32 	%rd265, %r2172, 4;
	add.s64 	%rd48, %rd1, %rd265;
	ld.local.u32 	%r4059, [%rd48];
	ld.local.u32 	%r4060, [%rd48+-4];
	and.b32  	%r312, %r302, 31;
	setp.eq.s32	%p144, %r312, 0;
	@%p144 bra 	BB84_204;

	mov.u32 	%r2173, 32;
	sub.s32 	%r2174, %r2173, %r312;
	shr.u32 	%r2175, %r4060, %r2174;
	shl.b32 	%r2176, %r4059, %r312;
	add.s32 	%r4059, %r2175, %r2176;
	ld.local.u32 	%r2177, [%rd48+-8];
	shr.u32 	%r2178, %r2177, %r2174;
	shl.b32 	%r2179, %r4060, %r312;
	add.s32 	%r4060, %r2178, %r2179;

BB84_204:
	shr.u32 	%r2180, %r4060, 30;
	shl.b32 	%r2181, %r4059, 2;
	add.s32 	%r4061, %r2180, %r2181;
	shl.b32 	%r318, %r4060, 2;
	shr.u32 	%r2182, %r4061, 31;
	shr.u32 	%r2183, %r4059, 30;
	add.s32 	%r319, %r2182, %r2183;
	setp.eq.s32	%p145, %r2182, 0;
	mov.u32 	%r4062, %r309;
	mov.u32 	%r4063, %r318;
	@%p145 bra 	BB84_206;

	not.b32 	%r2184, %r4061;
	neg.s32 	%r320, %r318;
	setp.eq.s32	%p146, %r318, 0;
	selp.u32	%r2185, 1, 0, %p146;
	add.s32 	%r4061, %r2185, %r2184;
	xor.b32  	%r322, %r309, -2147483648;
	mov.u32 	%r4062, %r322;
	mov.u32 	%r4063, %r320;

BB84_206:
	mov.u32 	%r324, %r4062;
	neg.s32 	%r2186, %r319;
	setp.eq.s32	%p147, %r309, 0;
	selp.b32	%r4066, %r319, %r2186, %p147;
	clz.b32 	%r4065, %r4061;
	setp.eq.s32	%p148, %r4065, 0;
	shl.b32 	%r2187, %r4061, %r4065;
	mov.u32 	%r2188, 32;
	sub.s32 	%r2189, %r2188, %r4065;
	shr.u32 	%r2190, %r4063, %r2189;
	add.s32 	%r2191, %r2190, %r2187;
	selp.b32	%r328, %r4061, %r2191, %p148;
	mov.u32 	%r2192, -921707870;
	mul.hi.u32 	%r4064, %r328, %r2192;
	setp.lt.s32	%p149, %r4064, 1;
	@%p149 bra 	BB84_208;

	mul.lo.s32 	%r2193, %r328, -921707870;
	shr.u32 	%r2194, %r2193, 31;
	shl.b32 	%r2195, %r4064, 1;
	add.s32 	%r4064, %r2194, %r2195;
	add.s32 	%r4065, %r4065, 1;

BB84_208:
	mov.u32 	%r2196, 126;
	sub.s32 	%r2197, %r2196, %r4065;
	shl.b32 	%r2198, %r2197, 23;
	add.s32 	%r2199, %r4064, 1;
	shr.u32 	%r2200, %r2199, 7;
	add.s32 	%r2201, %r2200, 1;
	shr.u32 	%r2202, %r2201, 1;
	add.s32 	%r2203, %r2202, %r2198;
	or.b32  	%r2204, %r2203, %r324;
	mov.b32 	 %f2269, %r2204;

BB84_209:
	mul.rn.f32 	%f186, %f2269, %f2269;
	add.s32 	%r335, %r4066, 1;
	and.b32  	%r336, %r335, 1;
	setp.eq.s32	%p150, %r336, 0;
	@%p150 bra 	BB84_211;

	mov.f32 	%f1259, 0fBAB6061A;
	mov.f32 	%f1260, 0f37CCF5CE;
	fma.rn.f32 	%f2270, %f1260, %f186, %f1259;
	bra.uni 	BB84_212;

BB84_211:
	mov.f32 	%f1261, 0f3C08839E;
	mov.f32 	%f1262, 0fB94CA1F9;
	fma.rn.f32 	%f2270, %f1262, %f186, %f1261;

BB84_212:
	@%p150 bra 	BB84_214;

	mov.f32 	%f1263, 0f3D2AAAA5;
	fma.rn.f32 	%f1264, %f2270, %f186, %f1263;
	mov.f32 	%f1265, 0fBF000000;
	fma.rn.f32 	%f2271, %f1264, %f186, %f1265;
	bra.uni 	BB84_215;

BB84_214:
	mov.f32 	%f1266, 0fBE2AAAA3;
	fma.rn.f32 	%f1267, %f2270, %f186, %f1266;
	mov.f32 	%f1268, 0f00000000;
	fma.rn.f32 	%f2271, %f1267, %f186, %f1268;

BB84_215:
	fma.rn.f32 	%f2272, %f2271, %f2269, %f2269;
	@%p150 bra 	BB84_217;

	mov.f32 	%f1269, 0f3F800000;
	fma.rn.f32 	%f2272, %f2271, %f186, %f1269;

BB84_217:
	and.b32  	%r2205, %r335, 2;
	setp.eq.s32	%p153, %r2205, 0;
	@%p153 bra 	BB84_219;

	mov.f32 	%f1270, 0f00000000;
	mov.f32 	%f1271, 0fBF800000;
	fma.rn.f32 	%f2272, %f2272, %f1271, %f1270;

BB84_219:
	mul.f32 	%f198, %f180, %f2272;
	mov.f32 	%f2382, %f34;
	@%p37 bra 	BB84_221;

	mov.f32 	%f1272, 0f00000000;
	mul.rn.f32 	%f2382, %f34, %f1272;

BB84_221:
	mul.f32 	%f1273, %f2382, 0f3F22F983;
	cvt.rni.s32.f32	%r4076, %f1273;
	cvt.rn.f32.s32	%f1274, %r4076;
	neg.f32 	%f1275, %f1274;
	fma.rn.f32 	%f1277, %f1275, %f1069, %f2382;
	fma.rn.f32 	%f1279, %f1275, %f1071, %f1277;
	fma.rn.f32 	%f2273, %f1275, %f1073, %f1279;
	abs.f32 	%f1281, %f2382;
	setp.leu.f32	%p155, %f1281, 0f47CE4780;
	@%p155 bra 	BB84_231;

	mov.b32 	 %r338, %f2382;
	shr.u32 	%r339, %r338, 23;
	bfe.u32 	%r2208, %r338, 23, 8;
	add.s32 	%r2209, %r2208, -128;
	shl.b32 	%r2210, %r338, 8;
	or.b32  	%r340, %r2210, -2147483648;
	shr.u32 	%r341, %r2209, 5;
	mov.u32 	%r4068, 0;
	mov.u64 	%rd357, __cudart_i2opi_f;
	mov.u32 	%r4067, -6;
	mov.u64 	%rd481, %rd1;

BB84_223:
	.pragma "nounroll";
	ld.const.u32 	%r2213, [%rd357];
	// inline asm
	{
	mad.lo.cc.u32   %r2211, %r2213, %r340, %r4068;
	madc.hi.u32     %r4068, %r2213, %r340,  0;
	}
	// inline asm
	st.local.u32 	[%rd481], %r2211;
	add.s64 	%rd481, %rd481, 4;
	add.s64 	%rd357, %rd357, 4;
	add.s32 	%r4067, %r4067, 1;
	setp.ne.s32	%p156, %r4067, 0;
	@%p156 bra 	BB84_223;

	and.b32  	%r346, %r338, -2147483648;
	st.local.u32 	[%rd3], %r4068;
	mov.u32 	%r2216, 6;
	sub.s32 	%r2217, %r2216, %r341;
	mul.wide.s32 	%rd267, %r2217, 4;
	add.s64 	%rd53, %rd1, %rd267;
	ld.local.u32 	%r4069, [%rd53];
	ld.local.u32 	%r4070, [%rd53+-4];
	and.b32  	%r349, %r339, 31;
	setp.eq.s32	%p157, %r349, 0;
	@%p157 bra 	BB84_226;

	mov.u32 	%r2218, 32;
	sub.s32 	%r2219, %r2218, %r349;
	shr.u32 	%r2220, %r4070, %r2219;
	shl.b32 	%r2221, %r4069, %r349;
	add.s32 	%r4069, %r2220, %r2221;
	ld.local.u32 	%r2222, [%rd53+-8];
	shr.u32 	%r2223, %r2222, %r2219;
	shl.b32 	%r2224, %r4070, %r349;
	add.s32 	%r4070, %r2223, %r2224;

BB84_226:
	shr.u32 	%r2225, %r4070, 30;
	shl.b32 	%r2226, %r4069, 2;
	add.s32 	%r4071, %r2225, %r2226;
	shl.b32 	%r355, %r4070, 2;
	shr.u32 	%r2227, %r4071, 31;
	shr.u32 	%r2228, %r4069, 30;
	add.s32 	%r356, %r2227, %r2228;
	setp.eq.s32	%p158, %r2227, 0;
	mov.u32 	%r4072, %r346;
	mov.u32 	%r4073, %r355;
	@%p158 bra 	BB84_228;

	not.b32 	%r2229, %r4071;
	neg.s32 	%r357, %r355;
	setp.eq.s32	%p159, %r355, 0;
	selp.u32	%r2230, 1, 0, %p159;
	add.s32 	%r4071, %r2230, %r2229;
	xor.b32  	%r359, %r346, -2147483648;
	mov.u32 	%r4072, %r359;
	mov.u32 	%r4073, %r357;

BB84_228:
	mov.u32 	%r361, %r4072;
	neg.s32 	%r2231, %r356;
	setp.eq.s32	%p160, %r346, 0;
	selp.b32	%r4076, %r356, %r2231, %p160;
	clz.b32 	%r4075, %r4071;
	setp.eq.s32	%p161, %r4075, 0;
	shl.b32 	%r2232, %r4071, %r4075;
	mov.u32 	%r2233, 32;
	sub.s32 	%r2234, %r2233, %r4075;
	shr.u32 	%r2235, %r4073, %r2234;
	add.s32 	%r2236, %r2235, %r2232;
	selp.b32	%r365, %r4071, %r2236, %p161;
	mov.u32 	%r2237, -921707870;
	mul.hi.u32 	%r4074, %r365, %r2237;
	setp.lt.s32	%p162, %r4074, 1;
	@%p162 bra 	BB84_230;

	mul.lo.s32 	%r2238, %r365, -921707870;
	shr.u32 	%r2239, %r2238, 31;
	shl.b32 	%r2240, %r4074, 1;
	add.s32 	%r4074, %r2239, %r2240;
	add.s32 	%r4075, %r4075, 1;

BB84_230:
	mov.u32 	%r2241, 126;
	sub.s32 	%r2242, %r2241, %r4075;
	shl.b32 	%r2243, %r2242, 23;
	add.s32 	%r2244, %r4074, 1;
	shr.u32 	%r2245, %r2244, 7;
	add.s32 	%r2246, %r2245, 1;
	shr.u32 	%r2247, %r2246, 1;
	add.s32 	%r2248, %r2247, %r2243;
	or.b32  	%r2249, %r2248, %r361;
	mov.b32 	 %f2273, %r2249;

BB84_231:
	mul.rn.f32 	%f204, %f2273, %f2273;
	add.s32 	%r372, %r4076, 1;
	and.b32  	%r373, %r372, 1;
	setp.eq.s32	%p163, %r373, 0;
	@%p163 bra 	BB84_233;

	mov.f32 	%f1282, 0fBAB6061A;
	mov.f32 	%f1283, 0f37CCF5CE;
	fma.rn.f32 	%f2274, %f1283, %f204, %f1282;
	bra.uni 	BB84_234;

BB84_233:
	mov.f32 	%f1284, 0f3C08839E;
	mov.f32 	%f1285, 0fB94CA1F9;
	fma.rn.f32 	%f2274, %f1285, %f204, %f1284;

BB84_234:
	@%p163 bra 	BB84_236;

	mov.f32 	%f1286, 0f3D2AAAA5;
	fma.rn.f32 	%f1287, %f2274, %f204, %f1286;
	mov.f32 	%f1288, 0fBF000000;
	fma.rn.f32 	%f2275, %f1287, %f204, %f1288;
	bra.uni 	BB84_237;

BB84_236:
	mov.f32 	%f1289, 0fBE2AAAA3;
	fma.rn.f32 	%f1290, %f2274, %f204, %f1289;
	mov.f32 	%f1291, 0f00000000;
	fma.rn.f32 	%f2275, %f1290, %f204, %f1291;

BB84_237:
	fma.rn.f32 	%f2276, %f2275, %f2273, %f2273;
	@%p163 bra 	BB84_239;

	mov.f32 	%f1292, 0f3F800000;
	fma.rn.f32 	%f2276, %f2275, %f204, %f1292;

BB84_239:
	and.b32  	%r2250, %r372, 2;
	setp.eq.s32	%p166, %r2250, 0;
	@%p166 bra 	BB84_241;

	mov.f32 	%f1293, 0f00000000;
	mov.f32 	%f1294, 0fBF800000;
	fma.rn.f32 	%f2276, %f2276, %f1294, %f1293;

BB84_241:
	mov.f32 	%f2336, %f36;
	@%p50 bra 	BB84_243;

	mov.f32 	%f1295, 0f00000000;
	mul.rn.f32 	%f2336, %f36, %f1295;

BB84_243:
	mul.f32 	%f1296, %f2336, 0f3F22F983;
	cvt.rni.s32.f32	%r4086, %f1296;
	cvt.rn.f32.s32	%f1297, %r4086;
	neg.f32 	%f1298, %f1297;
	fma.rn.f32 	%f1300, %f1298, %f1069, %f2336;
	fma.rn.f32 	%f1302, %f1298, %f1071, %f1300;
	fma.rn.f32 	%f2277, %f1298, %f1073, %f1302;
	abs.f32 	%f1304, %f2336;
	setp.leu.f32	%p168, %f1304, 0f47CE4780;
	@%p168 bra 	BB84_253;

	mov.b32 	 %r375, %f2336;
	shr.u32 	%r376, %r375, 23;
	bfe.u32 	%r2253, %r375, 23, 8;
	add.s32 	%r2254, %r2253, -128;
	shl.b32 	%r2255, %r375, 8;
	or.b32  	%r377, %r2255, -2147483648;
	shr.u32 	%r378, %r2254, 5;
	mov.u32 	%r4078, 0;
	mov.u64 	%rd358, __cudart_i2opi_f;
	mov.u32 	%r4077, -6;
	mov.u64 	%rd480, %rd1;

BB84_245:
	.pragma "nounroll";
	ld.const.u32 	%r2258, [%rd358];
	// inline asm
	{
	mad.lo.cc.u32   %r2256, %r2258, %r377, %r4078;
	madc.hi.u32     %r4078, %r2258, %r377,  0;
	}
	// inline asm
	st.local.u32 	[%rd480], %r2256;
	add.s64 	%rd480, %rd480, 4;
	add.s64 	%rd358, %rd358, 4;
	add.s32 	%r4077, %r4077, 1;
	setp.ne.s32	%p169, %r4077, 0;
	@%p169 bra 	BB84_245;

	and.b32  	%r383, %r375, -2147483648;
	st.local.u32 	[%rd3], %r4078;
	mov.u32 	%r2261, 6;
	sub.s32 	%r2262, %r2261, %r378;
	mul.wide.s32 	%rd269, %r2262, 4;
	add.s64 	%rd58, %rd1, %rd269;
	ld.local.u32 	%r4079, [%rd58];
	ld.local.u32 	%r4080, [%rd58+-4];
	and.b32  	%r386, %r376, 31;
	setp.eq.s32	%p170, %r386, 0;
	@%p170 bra 	BB84_248;

	mov.u32 	%r2263, 32;
	sub.s32 	%r2264, %r2263, %r386;
	shr.u32 	%r2265, %r4080, %r2264;
	shl.b32 	%r2266, %r4079, %r386;
	add.s32 	%r4079, %r2265, %r2266;
	ld.local.u32 	%r2267, [%rd58+-8];
	shr.u32 	%r2268, %r2267, %r2264;
	shl.b32 	%r2269, %r4080, %r386;
	add.s32 	%r4080, %r2268, %r2269;

BB84_248:
	shr.u32 	%r2270, %r4080, 30;
	shl.b32 	%r2271, %r4079, 2;
	add.s32 	%r4081, %r2270, %r2271;
	shl.b32 	%r392, %r4080, 2;
	shr.u32 	%r2272, %r4081, 31;
	shr.u32 	%r2273, %r4079, 30;
	add.s32 	%r393, %r2272, %r2273;
	setp.eq.s32	%p171, %r2272, 0;
	mov.u32 	%r4082, %r383;
	mov.u32 	%r4083, %r392;
	@%p171 bra 	BB84_250;

	not.b32 	%r2274, %r4081;
	neg.s32 	%r394, %r392;
	setp.eq.s32	%p172, %r392, 0;
	selp.u32	%r2275, 1, 0, %p172;
	add.s32 	%r4081, %r2275, %r2274;
	xor.b32  	%r396, %r383, -2147483648;
	mov.u32 	%r4082, %r396;
	mov.u32 	%r4083, %r394;

BB84_250:
	mov.u32 	%r398, %r4082;
	neg.s32 	%r2276, %r393;
	setp.eq.s32	%p173, %r383, 0;
	selp.b32	%r4086, %r393, %r2276, %p173;
	clz.b32 	%r4085, %r4081;
	setp.eq.s32	%p174, %r4085, 0;
	shl.b32 	%r2277, %r4081, %r4085;
	mov.u32 	%r2278, 32;
	sub.s32 	%r2279, %r2278, %r4085;
	shr.u32 	%r2280, %r4083, %r2279;
	add.s32 	%r2281, %r2280, %r2277;
	selp.b32	%r402, %r4081, %r2281, %p174;
	mov.u32 	%r2282, -921707870;
	mul.hi.u32 	%r4084, %r402, %r2282;
	setp.lt.s32	%p175, %r4084, 1;
	@%p175 bra 	BB84_252;

	mul.lo.s32 	%r2283, %r402, -921707870;
	shr.u32 	%r2284, %r2283, 31;
	shl.b32 	%r2285, %r4084, 1;
	add.s32 	%r4084, %r2284, %r2285;
	add.s32 	%r4085, %r4085, 1;

BB84_252:
	mov.u32 	%r2286, 126;
	sub.s32 	%r2287, %r2286, %r4085;
	shl.b32 	%r2288, %r2287, 23;
	add.s32 	%r2289, %r4084, 1;
	shr.u32 	%r2290, %r2289, 7;
	add.s32 	%r2291, %r2290, 1;
	shr.u32 	%r2292, %r2291, 1;
	add.s32 	%r2293, %r2292, %r2288;
	or.b32  	%r2294, %r2293, %r398;
	mov.b32 	 %f2277, %r2294;

BB84_253:
	mul.rn.f32 	%f221, %f2277, %f2277;
	and.b32  	%r409, %r4086, 1;
	setp.eq.s32	%p176, %r409, 0;
	@%p176 bra 	BB84_255;

	mov.f32 	%f1305, 0fBAB6061A;
	mov.f32 	%f1306, 0f37CCF5CE;
	fma.rn.f32 	%f2278, %f1306, %f221, %f1305;
	bra.uni 	BB84_256;

BB84_255:
	mov.f32 	%f1307, 0f3C08839E;
	mov.f32 	%f1308, 0fB94CA1F9;
	fma.rn.f32 	%f2278, %f1308, %f221, %f1307;

BB84_256:
	@%p176 bra 	BB84_258;

	mov.f32 	%f1309, 0f3D2AAAA5;
	fma.rn.f32 	%f1310, %f2278, %f221, %f1309;
	mov.f32 	%f1311, 0fBF000000;
	fma.rn.f32 	%f2279, %f1310, %f221, %f1311;
	bra.uni 	BB84_259;

BB84_258:
	mov.f32 	%f1312, 0fBE2AAAA3;
	fma.rn.f32 	%f1313, %f2278, %f221, %f1312;
	mov.f32 	%f1314, 0f00000000;
	fma.rn.f32 	%f2279, %f1313, %f221, %f1314;

BB84_259:
	fma.rn.f32 	%f2280, %f2279, %f2277, %f2277;
	@%p176 bra 	BB84_261;

	mov.f32 	%f1315, 0f3F800000;
	fma.rn.f32 	%f2280, %f2279, %f221, %f1315;

BB84_261:
	and.b32  	%r2295, %r4086, 2;
	setp.eq.s32	%p179, %r2295, 0;
	@%p179 bra 	BB84_263;

	mov.f32 	%f1316, 0f00000000;
	mov.f32 	%f1317, 0fBF800000;
	fma.rn.f32 	%f2280, %f2280, %f1317, %f1316;

BB84_263:
	mul.f32 	%f233, %f2276, %f2280;
	mov.f32 	%f2355, %f13;
	@%p63 bra 	BB84_265;

	mov.f32 	%f1318, 0f00000000;
	mul.rn.f32 	%f2355, %f13, %f1318;

BB84_265:
	mul.f32 	%f1319, %f2355, 0f3F22F983;
	cvt.rni.s32.f32	%r4096, %f1319;
	cvt.rn.f32.s32	%f1320, %r4096;
	neg.f32 	%f1321, %f1320;
	fma.rn.f32 	%f1323, %f1321, %f1069, %f2355;
	fma.rn.f32 	%f1325, %f1321, %f1071, %f1323;
	fma.rn.f32 	%f2281, %f1321, %f1073, %f1325;
	abs.f32 	%f1327, %f2355;
	setp.leu.f32	%p181, %f1327, 0f47CE4780;
	@%p181 bra 	BB84_275;

	mov.b32 	 %r411, %f2355;
	shr.u32 	%r412, %r411, 23;
	bfe.u32 	%r2298, %r411, 23, 8;
	add.s32 	%r2299, %r2298, -128;
	shl.b32 	%r2300, %r411, 8;
	or.b32  	%r413, %r2300, -2147483648;
	shr.u32 	%r414, %r2299, 5;
	mov.u32 	%r4088, 0;
	mov.u64 	%rd359, __cudart_i2opi_f;
	mov.u32 	%r4087, -6;
	mov.u64 	%rd479, %rd1;

BB84_267:
	.pragma "nounroll";
	ld.const.u32 	%r2303, [%rd359];
	// inline asm
	{
	mad.lo.cc.u32   %r2301, %r2303, %r413, %r4088;
	madc.hi.u32     %r4088, %r2303, %r413,  0;
	}
	// inline asm
	st.local.u32 	[%rd479], %r2301;
	add.s64 	%rd479, %rd479, 4;
	add.s64 	%rd359, %rd359, 4;
	add.s32 	%r4087, %r4087, 1;
	setp.ne.s32	%p182, %r4087, 0;
	@%p182 bra 	BB84_267;

	and.b32  	%r419, %r411, -2147483648;
	st.local.u32 	[%rd3], %r4088;
	mov.u32 	%r2306, 6;
	sub.s32 	%r2307, %r2306, %r414;
	mul.wide.s32 	%rd271, %r2307, 4;
	add.s64 	%rd63, %rd1, %rd271;
	ld.local.u32 	%r4089, [%rd63];
	ld.local.u32 	%r4090, [%rd63+-4];
	and.b32  	%r422, %r412, 31;
	setp.eq.s32	%p183, %r422, 0;
	@%p183 bra 	BB84_270;

	mov.u32 	%r2308, 32;
	sub.s32 	%r2309, %r2308, %r422;
	shr.u32 	%r2310, %r4090, %r2309;
	shl.b32 	%r2311, %r4089, %r422;
	add.s32 	%r4089, %r2310, %r2311;
	ld.local.u32 	%r2312, [%rd63+-8];
	shr.u32 	%r2313, %r2312, %r2309;
	shl.b32 	%r2314, %r4090, %r422;
	add.s32 	%r4090, %r2313, %r2314;

BB84_270:
	shr.u32 	%r2315, %r4090, 30;
	shl.b32 	%r2316, %r4089, 2;
	add.s32 	%r4091, %r2315, %r2316;
	shl.b32 	%r428, %r4090, 2;
	shr.u32 	%r2317, %r4091, 31;
	shr.u32 	%r2318, %r4089, 30;
	add.s32 	%r429, %r2317, %r2318;
	setp.eq.s32	%p184, %r2317, 0;
	mov.u32 	%r4092, %r419;
	mov.u32 	%r4093, %r428;
	@%p184 bra 	BB84_272;

	not.b32 	%r2319, %r4091;
	neg.s32 	%r430, %r428;
	setp.eq.s32	%p185, %r428, 0;
	selp.u32	%r2320, 1, 0, %p185;
	add.s32 	%r4091, %r2320, %r2319;
	xor.b32  	%r432, %r419, -2147483648;
	mov.u32 	%r4092, %r432;
	mov.u32 	%r4093, %r430;

BB84_272:
	mov.u32 	%r434, %r4092;
	neg.s32 	%r2321, %r429;
	setp.eq.s32	%p186, %r419, 0;
	selp.b32	%r4096, %r429, %r2321, %p186;
	clz.b32 	%r4095, %r4091;
	setp.eq.s32	%p187, %r4095, 0;
	shl.b32 	%r2322, %r4091, %r4095;
	mov.u32 	%r2323, 32;
	sub.s32 	%r2324, %r2323, %r4095;
	shr.u32 	%r2325, %r4093, %r2324;
	add.s32 	%r2326, %r2325, %r2322;
	selp.b32	%r438, %r4091, %r2326, %p187;
	mov.u32 	%r2327, -921707870;
	mul.hi.u32 	%r4094, %r438, %r2327;
	setp.lt.s32	%p188, %r4094, 1;
	@%p188 bra 	BB84_274;

	mul.lo.s32 	%r2328, %r438, -921707870;
	shr.u32 	%r2329, %r2328, 31;
	shl.b32 	%r2330, %r4094, 1;
	add.s32 	%r4094, %r2329, %r2330;
	add.s32 	%r4095, %r4095, 1;

BB84_274:
	mov.u32 	%r2331, 126;
	sub.s32 	%r2332, %r2331, %r4095;
	shl.b32 	%r2333, %r2332, 23;
	add.s32 	%r2334, %r4094, 1;
	shr.u32 	%r2335, %r2334, 7;
	add.s32 	%r2336, %r2335, 1;
	shr.u32 	%r2337, %r2336, 1;
	add.s32 	%r2338, %r2337, %r2333;
	or.b32  	%r2339, %r2338, %r434;
	mov.b32 	 %f2281, %r2339;

BB84_275:
	mul.rn.f32 	%f239, %f2281, %f2281;
	and.b32  	%r445, %r4096, 1;
	setp.eq.s32	%p189, %r445, 0;
	@%p189 bra 	BB84_277;

	mov.f32 	%f1328, 0fBAB6061A;
	mov.f32 	%f1329, 0f37CCF5CE;
	fma.rn.f32 	%f2282, %f1329, %f239, %f1328;
	bra.uni 	BB84_278;

BB84_277:
	mov.f32 	%f1330, 0f3C08839E;
	mov.f32 	%f1331, 0fB94CA1F9;
	fma.rn.f32 	%f2282, %f1331, %f239, %f1330;

BB84_278:
	@%p189 bra 	BB84_280;

	mov.f32 	%f1332, 0f3D2AAAA5;
	fma.rn.f32 	%f1333, %f2282, %f239, %f1332;
	mov.f32 	%f1334, 0fBF000000;
	fma.rn.f32 	%f2283, %f1333, %f239, %f1334;
	bra.uni 	BB84_281;

BB84_280:
	mov.f32 	%f1335, 0fBE2AAAA3;
	fma.rn.f32 	%f1336, %f2282, %f239, %f1335;
	mov.f32 	%f1337, 0f00000000;
	fma.rn.f32 	%f2283, %f1336, %f239, %f1337;

BB84_281:
	fma.rn.f32 	%f2284, %f2283, %f2281, %f2281;
	@%p189 bra 	BB84_283;

	mov.f32 	%f1338, 0f3F800000;
	fma.rn.f32 	%f2284, %f2283, %f239, %f1338;

BB84_283:
	and.b32  	%r2340, %r4096, 2;
	setp.eq.s32	%p192, %r2340, 0;
	@%p192 bra 	BB84_285;

	mov.f32 	%f1339, 0f00000000;
	mov.f32 	%f1340, 0fBF800000;
	fma.rn.f32 	%f2284, %f2284, %f1340, %f1339;

BB84_285:
	mul.f32 	%f1341, %f233, %f2284;
	sub.f32 	%f251, %f198, %f1341;
	mov.f32 	%f2381, %f34;
	@%p37 bra 	BB84_287;

	mov.f32 	%f1342, 0f00000000;
	mul.rn.f32 	%f2381, %f34, %f1342;

BB84_287:
	mul.f32 	%f1343, %f2381, 0f3F22F983;
	cvt.rni.s32.f32	%r4106, %f1343;
	cvt.rn.f32.s32	%f1344, %r4106;
	neg.f32 	%f1345, %f1344;
	fma.rn.f32 	%f1347, %f1345, %f1069, %f2381;
	fma.rn.f32 	%f1349, %f1345, %f1071, %f1347;
	fma.rn.f32 	%f2285, %f1345, %f1073, %f1349;
	abs.f32 	%f1351, %f2381;
	setp.leu.f32	%p194, %f1351, 0f47CE4780;
	@%p194 bra 	BB84_297;

	mov.b32 	 %r447, %f2381;
	shr.u32 	%r448, %r447, 23;
	bfe.u32 	%r2343, %r447, 23, 8;
	add.s32 	%r2344, %r2343, -128;
	shl.b32 	%r2345, %r447, 8;
	or.b32  	%r449, %r2345, -2147483648;
	shr.u32 	%r450, %r2344, 5;
	mov.u32 	%r4098, 0;
	mov.u64 	%rd360, __cudart_i2opi_f;
	mov.u32 	%r4097, -6;
	mov.u64 	%rd478, %rd1;

BB84_289:
	.pragma "nounroll";
	ld.const.u32 	%r2348, [%rd360];
	// inline asm
	{
	mad.lo.cc.u32   %r2346, %r2348, %r449, %r4098;
	madc.hi.u32     %r4098, %r2348, %r449,  0;
	}
	// inline asm
	st.local.u32 	[%rd478], %r2346;
	add.s64 	%rd478, %rd478, 4;
	add.s64 	%rd360, %rd360, 4;
	add.s32 	%r4097, %r4097, 1;
	setp.ne.s32	%p195, %r4097, 0;
	@%p195 bra 	BB84_289;

	and.b32  	%r455, %r447, -2147483648;
	st.local.u32 	[%rd3], %r4098;
	mov.u32 	%r2351, 6;
	sub.s32 	%r2352, %r2351, %r450;
	mul.wide.s32 	%rd273, %r2352, 4;
	add.s64 	%rd68, %rd1, %rd273;
	ld.local.u32 	%r4099, [%rd68];
	ld.local.u32 	%r4100, [%rd68+-4];
	and.b32  	%r458, %r448, 31;
	setp.eq.s32	%p196, %r458, 0;
	@%p196 bra 	BB84_292;

	mov.u32 	%r2353, 32;
	sub.s32 	%r2354, %r2353, %r458;
	shr.u32 	%r2355, %r4100, %r2354;
	shl.b32 	%r2356, %r4099, %r458;
	add.s32 	%r4099, %r2355, %r2356;
	ld.local.u32 	%r2357, [%rd68+-8];
	shr.u32 	%r2358, %r2357, %r2354;
	shl.b32 	%r2359, %r4100, %r458;
	add.s32 	%r4100, %r2358, %r2359;

BB84_292:
	shr.u32 	%r2360, %r4100, 30;
	shl.b32 	%r2361, %r4099, 2;
	add.s32 	%r4101, %r2360, %r2361;
	shl.b32 	%r464, %r4100, 2;
	shr.u32 	%r2362, %r4101, 31;
	shr.u32 	%r2363, %r4099, 30;
	add.s32 	%r465, %r2362, %r2363;
	setp.eq.s32	%p197, %r2362, 0;
	mov.u32 	%r4102, %r455;
	mov.u32 	%r4103, %r464;
	@%p197 bra 	BB84_294;

	not.b32 	%r2364, %r4101;
	neg.s32 	%r466, %r464;
	setp.eq.s32	%p198, %r464, 0;
	selp.u32	%r2365, 1, 0, %p198;
	add.s32 	%r4101, %r2365, %r2364;
	xor.b32  	%r468, %r455, -2147483648;
	mov.u32 	%r4102, %r468;
	mov.u32 	%r4103, %r466;

BB84_294:
	mov.u32 	%r470, %r4102;
	neg.s32 	%r2366, %r465;
	setp.eq.s32	%p199, %r455, 0;
	selp.b32	%r4106, %r465, %r2366, %p199;
	clz.b32 	%r4105, %r4101;
	setp.eq.s32	%p200, %r4105, 0;
	shl.b32 	%r2367, %r4101, %r4105;
	mov.u32 	%r2368, 32;
	sub.s32 	%r2369, %r2368, %r4105;
	shr.u32 	%r2370, %r4103, %r2369;
	add.s32 	%r2371, %r2370, %r2367;
	selp.b32	%r474, %r4101, %r2371, %p200;
	mov.u32 	%r2372, -921707870;
	mul.hi.u32 	%r4104, %r474, %r2372;
	setp.lt.s32	%p201, %r4104, 1;
	@%p201 bra 	BB84_296;

	mul.lo.s32 	%r2373, %r474, -921707870;
	shr.u32 	%r2374, %r2373, 31;
	shl.b32 	%r2375, %r4104, 1;
	add.s32 	%r4104, %r2374, %r2375;
	add.s32 	%r4105, %r4105, 1;

BB84_296:
	mov.u32 	%r2376, 126;
	sub.s32 	%r2377, %r2376, %r4105;
	shl.b32 	%r2378, %r2377, 23;
	add.s32 	%r2379, %r4104, 1;
	shr.u32 	%r2380, %r2379, 7;
	add.s32 	%r2381, %r2380, 1;
	shr.u32 	%r2382, %r2381, 1;
	add.s32 	%r2383, %r2382, %r2378;
	or.b32  	%r2384, %r2383, %r470;
	mov.b32 	 %f2285, %r2384;

BB84_297:
	mul.rn.f32 	%f257, %f2285, %f2285;
	add.s32 	%r481, %r4106, 1;
	and.b32  	%r482, %r481, 1;
	setp.eq.s32	%p202, %r482, 0;
	@%p202 bra 	BB84_299;

	mov.f32 	%f1352, 0fBAB6061A;
	mov.f32 	%f1353, 0f37CCF5CE;
	fma.rn.f32 	%f2286, %f1353, %f257, %f1352;
	bra.uni 	BB84_300;

BB84_299:
	mov.f32 	%f1354, 0f3C08839E;
	mov.f32 	%f1355, 0fB94CA1F9;
	fma.rn.f32 	%f2286, %f1355, %f257, %f1354;

BB84_300:
	@%p202 bra 	BB84_302;

	mov.f32 	%f1356, 0f3D2AAAA5;
	fma.rn.f32 	%f1357, %f2286, %f257, %f1356;
	mov.f32 	%f1358, 0fBF000000;
	fma.rn.f32 	%f2287, %f1357, %f257, %f1358;
	bra.uni 	BB84_303;

BB84_302:
	mov.f32 	%f1359, 0fBE2AAAA3;
	fma.rn.f32 	%f1360, %f2286, %f257, %f1359;
	mov.f32 	%f1361, 0f00000000;
	fma.rn.f32 	%f2287, %f1360, %f257, %f1361;

BB84_303:
	fma.rn.f32 	%f2288, %f2287, %f2285, %f2285;
	@%p202 bra 	BB84_305;

	mov.f32 	%f1362, 0f3F800000;
	fma.rn.f32 	%f2288, %f2287, %f257, %f1362;

BB84_305:
	and.b32  	%r2385, %r481, 2;
	setp.eq.s32	%p205, %r2385, 0;
	@%p205 bra 	BB84_307;

	mov.f32 	%f1363, 0f00000000;
	mov.f32 	%f1364, 0fBF800000;
	fma.rn.f32 	%f2288, %f2288, %f1364, %f1363;

BB84_307:
	mov.f32 	%f2335, %f36;
	@%p50 bra 	BB84_309;

	mov.f32 	%f1365, 0f00000000;
	mul.rn.f32 	%f2335, %f36, %f1365;

BB84_309:
	mul.f32 	%f1366, %f2335, 0f3F22F983;
	cvt.rni.s32.f32	%r4116, %f1366;
	cvt.rn.f32.s32	%f1367, %r4116;
	neg.f32 	%f1368, %f1367;
	fma.rn.f32 	%f1370, %f1368, %f1069, %f2335;
	fma.rn.f32 	%f1372, %f1368, %f1071, %f1370;
	fma.rn.f32 	%f2289, %f1368, %f1073, %f1372;
	abs.f32 	%f1374, %f2335;
	setp.leu.f32	%p207, %f1374, 0f47CE4780;
	@%p207 bra 	BB84_319;

	mov.b32 	 %r484, %f2335;
	shr.u32 	%r485, %r484, 23;
	bfe.u32 	%r2388, %r484, 23, 8;
	add.s32 	%r2389, %r2388, -128;
	shl.b32 	%r2390, %r484, 8;
	or.b32  	%r486, %r2390, -2147483648;
	shr.u32 	%r487, %r2389, 5;
	mov.u32 	%r4108, 0;
	mov.u64 	%rd361, __cudart_i2opi_f;
	mov.u32 	%r4107, -6;
	mov.u64 	%rd477, %rd1;

BB84_311:
	.pragma "nounroll";
	ld.const.u32 	%r2393, [%rd361];
	// inline asm
	{
	mad.lo.cc.u32   %r2391, %r2393, %r486, %r4108;
	madc.hi.u32     %r4108, %r2393, %r486,  0;
	}
	// inline asm
	st.local.u32 	[%rd477], %r2391;
	add.s64 	%rd477, %rd477, 4;
	add.s64 	%rd361, %rd361, 4;
	add.s32 	%r4107, %r4107, 1;
	setp.ne.s32	%p208, %r4107, 0;
	@%p208 bra 	BB84_311;

	and.b32  	%r492, %r484, -2147483648;
	st.local.u32 	[%rd3], %r4108;
	mov.u32 	%r2396, 6;
	sub.s32 	%r2397, %r2396, %r487;
	mul.wide.s32 	%rd275, %r2397, 4;
	add.s64 	%rd73, %rd1, %rd275;
	ld.local.u32 	%r4109, [%rd73];
	ld.local.u32 	%r4110, [%rd73+-4];
	and.b32  	%r495, %r485, 31;
	setp.eq.s32	%p209, %r495, 0;
	@%p209 bra 	BB84_314;

	mov.u32 	%r2398, 32;
	sub.s32 	%r2399, %r2398, %r495;
	shr.u32 	%r2400, %r4110, %r2399;
	shl.b32 	%r2401, %r4109, %r495;
	add.s32 	%r4109, %r2400, %r2401;
	ld.local.u32 	%r2402, [%rd73+-8];
	shr.u32 	%r2403, %r2402, %r2399;
	shl.b32 	%r2404, %r4110, %r495;
	add.s32 	%r4110, %r2403, %r2404;

BB84_314:
	shr.u32 	%r2405, %r4110, 30;
	shl.b32 	%r2406, %r4109, 2;
	add.s32 	%r4111, %r2405, %r2406;
	shl.b32 	%r501, %r4110, 2;
	shr.u32 	%r2407, %r4111, 31;
	shr.u32 	%r2408, %r4109, 30;
	add.s32 	%r502, %r2407, %r2408;
	setp.eq.s32	%p210, %r2407, 0;
	mov.u32 	%r4112, %r492;
	mov.u32 	%r4113, %r501;
	@%p210 bra 	BB84_316;

	not.b32 	%r2409, %r4111;
	neg.s32 	%r503, %r501;
	setp.eq.s32	%p211, %r501, 0;
	selp.u32	%r2410, 1, 0, %p211;
	add.s32 	%r4111, %r2410, %r2409;
	xor.b32  	%r505, %r492, -2147483648;
	mov.u32 	%r4112, %r505;
	mov.u32 	%r4113, %r503;

BB84_316:
	mov.u32 	%r507, %r4112;
	neg.s32 	%r2411, %r502;
	setp.eq.s32	%p212, %r492, 0;
	selp.b32	%r4116, %r502, %r2411, %p212;
	clz.b32 	%r4115, %r4111;
	setp.eq.s32	%p213, %r4115, 0;
	shl.b32 	%r2412, %r4111, %r4115;
	mov.u32 	%r2413, 32;
	sub.s32 	%r2414, %r2413, %r4115;
	shr.u32 	%r2415, %r4113, %r2414;
	add.s32 	%r2416, %r2415, %r2412;
	selp.b32	%r511, %r4111, %r2416, %p213;
	mov.u32 	%r2417, -921707870;
	mul.hi.u32 	%r4114, %r511, %r2417;
	setp.lt.s32	%p214, %r4114, 1;
	@%p214 bra 	BB84_318;

	mul.lo.s32 	%r2418, %r511, -921707870;
	shr.u32 	%r2419, %r2418, 31;
	shl.b32 	%r2420, %r4114, 1;
	add.s32 	%r4114, %r2419, %r2420;
	add.s32 	%r4115, %r4115, 1;

BB84_318:
	mov.u32 	%r2421, 126;
	sub.s32 	%r2422, %r2421, %r4115;
	shl.b32 	%r2423, %r2422, 23;
	add.s32 	%r2424, %r4114, 1;
	shr.u32 	%r2425, %r2424, 7;
	add.s32 	%r2426, %r2425, 1;
	shr.u32 	%r2427, %r2426, 1;
	add.s32 	%r2428, %r2427, %r2423;
	or.b32  	%r2429, %r2428, %r507;
	mov.b32 	 %f2289, %r2429;

BB84_319:
	mul.rn.f32 	%f274, %f2289, %f2289;
	and.b32  	%r518, %r4116, 1;
	setp.eq.s32	%p215, %r518, 0;
	@%p215 bra 	BB84_321;

	mov.f32 	%f1375, 0fBAB6061A;
	mov.f32 	%f1376, 0f37CCF5CE;
	fma.rn.f32 	%f2290, %f1376, %f274, %f1375;
	bra.uni 	BB84_322;

BB84_321:
	mov.f32 	%f1377, 0f3C08839E;
	mov.f32 	%f1378, 0fB94CA1F9;
	fma.rn.f32 	%f2290, %f1378, %f274, %f1377;

BB84_322:
	@%p215 bra 	BB84_324;

	mov.f32 	%f1379, 0f3D2AAAA5;
	fma.rn.f32 	%f1380, %f2290, %f274, %f1379;
	mov.f32 	%f1381, 0fBF000000;
	fma.rn.f32 	%f2291, %f1380, %f274, %f1381;
	bra.uni 	BB84_325;

BB84_324:
	mov.f32 	%f1382, 0fBE2AAAA3;
	fma.rn.f32 	%f1383, %f2290, %f274, %f1382;
	mov.f32 	%f1384, 0f00000000;
	fma.rn.f32 	%f2291, %f1383, %f274, %f1384;

BB84_325:
	fma.rn.f32 	%f2292, %f2291, %f2289, %f2289;
	@%p215 bra 	BB84_327;

	mov.f32 	%f1385, 0f3F800000;
	fma.rn.f32 	%f2292, %f2291, %f274, %f1385;

BB84_327:
	and.b32  	%r2430, %r4116, 2;
	setp.eq.s32	%p218, %r2430, 0;
	@%p218 bra 	BB84_329;

	mov.f32 	%f1386, 0f00000000;
	mov.f32 	%f1387, 0fBF800000;
	fma.rn.f32 	%f2292, %f2292, %f1387, %f1386;

BB84_329:
	mul.f32 	%f286, %f2288, %f2292;
	mov.f32 	%f2354, %f13;
	@%p63 bra 	BB84_331;

	mov.f32 	%f1388, 0f00000000;
	mul.rn.f32 	%f2354, %f13, %f1388;

BB84_331:
	mul.f32 	%f1389, %f2354, 0f3F22F983;
	cvt.rni.s32.f32	%r4126, %f1389;
	cvt.rn.f32.s32	%f1390, %r4126;
	neg.f32 	%f1391, %f1390;
	fma.rn.f32 	%f1393, %f1391, %f1069, %f2354;
	fma.rn.f32 	%f1395, %f1391, %f1071, %f1393;
	fma.rn.f32 	%f2293, %f1391, %f1073, %f1395;
	abs.f32 	%f1397, %f2354;
	setp.leu.f32	%p220, %f1397, 0f47CE4780;
	@%p220 bra 	BB84_341;

	mov.b32 	 %r520, %f2354;
	shr.u32 	%r521, %r520, 23;
	bfe.u32 	%r2433, %r520, 23, 8;
	add.s32 	%r2434, %r2433, -128;
	shl.b32 	%r2435, %r520, 8;
	or.b32  	%r522, %r2435, -2147483648;
	shr.u32 	%r523, %r2434, 5;
	mov.u32 	%r4118, 0;
	mov.u64 	%rd362, __cudart_i2opi_f;
	mov.u32 	%r4117, -6;
	mov.u64 	%rd476, %rd1;

BB84_333:
	.pragma "nounroll";
	ld.const.u32 	%r2438, [%rd362];
	// inline asm
	{
	mad.lo.cc.u32   %r2436, %r2438, %r522, %r4118;
	madc.hi.u32     %r4118, %r2438, %r522,  0;
	}
	// inline asm
	st.local.u32 	[%rd476], %r2436;
	add.s64 	%rd476, %rd476, 4;
	add.s64 	%rd362, %rd362, 4;
	add.s32 	%r4117, %r4117, 1;
	setp.ne.s32	%p221, %r4117, 0;
	@%p221 bra 	BB84_333;

	and.b32  	%r528, %r520, -2147483648;
	st.local.u32 	[%rd3], %r4118;
	mov.u32 	%r2441, 6;
	sub.s32 	%r2442, %r2441, %r523;
	mul.wide.s32 	%rd277, %r2442, 4;
	add.s64 	%rd78, %rd1, %rd277;
	ld.local.u32 	%r4119, [%rd78];
	ld.local.u32 	%r4120, [%rd78+-4];
	and.b32  	%r531, %r521, 31;
	setp.eq.s32	%p222, %r531, 0;
	@%p222 bra 	BB84_336;

	mov.u32 	%r2443, 32;
	sub.s32 	%r2444, %r2443, %r531;
	shr.u32 	%r2445, %r4120, %r2444;
	shl.b32 	%r2446, %r4119, %r531;
	add.s32 	%r4119, %r2445, %r2446;
	ld.local.u32 	%r2447, [%rd78+-8];
	shr.u32 	%r2448, %r2447, %r2444;
	shl.b32 	%r2449, %r4120, %r531;
	add.s32 	%r4120, %r2448, %r2449;

BB84_336:
	shr.u32 	%r2450, %r4120, 30;
	shl.b32 	%r2451, %r4119, 2;
	add.s32 	%r4121, %r2450, %r2451;
	shl.b32 	%r537, %r4120, 2;
	shr.u32 	%r2452, %r4121, 31;
	shr.u32 	%r2453, %r4119, 30;
	add.s32 	%r538, %r2452, %r2453;
	setp.eq.s32	%p223, %r2452, 0;
	mov.u32 	%r4122, %r528;
	mov.u32 	%r4123, %r537;
	@%p223 bra 	BB84_338;

	not.b32 	%r2454, %r4121;
	neg.s32 	%r539, %r537;
	setp.eq.s32	%p224, %r537, 0;
	selp.u32	%r2455, 1, 0, %p224;
	add.s32 	%r4121, %r2455, %r2454;
	xor.b32  	%r541, %r528, -2147483648;
	mov.u32 	%r4122, %r541;
	mov.u32 	%r4123, %r539;

BB84_338:
	mov.u32 	%r543, %r4122;
	neg.s32 	%r2456, %r538;
	setp.eq.s32	%p225, %r528, 0;
	selp.b32	%r4126, %r538, %r2456, %p225;
	clz.b32 	%r4125, %r4121;
	setp.eq.s32	%p226, %r4125, 0;
	shl.b32 	%r2457, %r4121, %r4125;
	mov.u32 	%r2458, 32;
	sub.s32 	%r2459, %r2458, %r4125;
	shr.u32 	%r2460, %r4123, %r2459;
	add.s32 	%r2461, %r2460, %r2457;
	selp.b32	%r547, %r4121, %r2461, %p226;
	mov.u32 	%r2462, -921707870;
	mul.hi.u32 	%r4124, %r547, %r2462;
	setp.lt.s32	%p227, %r4124, 1;
	@%p227 bra 	BB84_340;

	mul.lo.s32 	%r2463, %r547, -921707870;
	shr.u32 	%r2464, %r2463, 31;
	shl.b32 	%r2465, %r4124, 1;
	add.s32 	%r4124, %r2464, %r2465;
	add.s32 	%r4125, %r4125, 1;

BB84_340:
	mov.u32 	%r2466, 126;
	sub.s32 	%r2467, %r2466, %r4125;
	shl.b32 	%r2468, %r2467, 23;
	add.s32 	%r2469, %r4124, 1;
	shr.u32 	%r2470, %r2469, 7;
	add.s32 	%r2471, %r2470, 1;
	shr.u32 	%r2472, %r2471, 1;
	add.s32 	%r2473, %r2472, %r2468;
	or.b32  	%r2474, %r2473, %r543;
	mov.b32 	 %f2293, %r2474;

BB84_341:
	mul.rn.f32 	%f292, %f2293, %f2293;
	add.s32 	%r554, %r4126, 1;
	and.b32  	%r555, %r554, 1;
	setp.eq.s32	%p228, %r555, 0;
	@%p228 bra 	BB84_343;

	mov.f32 	%f1398, 0fBAB6061A;
	mov.f32 	%f1399, 0f37CCF5CE;
	fma.rn.f32 	%f2294, %f1399, %f292, %f1398;
	bra.uni 	BB84_344;

BB84_343:
	mov.f32 	%f1400, 0f3C08839E;
	mov.f32 	%f1401, 0fB94CA1F9;
	fma.rn.f32 	%f2294, %f1401, %f292, %f1400;

BB84_344:
	@%p228 bra 	BB84_346;

	mov.f32 	%f1402, 0f3D2AAAA5;
	fma.rn.f32 	%f1403, %f2294, %f292, %f1402;
	mov.f32 	%f1404, 0fBF000000;
	fma.rn.f32 	%f2295, %f1403, %f292, %f1404;
	bra.uni 	BB84_347;

BB84_346:
	mov.f32 	%f1405, 0fBE2AAAA3;
	fma.rn.f32 	%f1406, %f2294, %f292, %f1405;
	mov.f32 	%f1407, 0f00000000;
	fma.rn.f32 	%f2295, %f1406, %f292, %f1407;

BB84_347:
	fma.rn.f32 	%f2296, %f2295, %f2293, %f2293;
	@%p228 bra 	BB84_349;

	mov.f32 	%f1408, 0f3F800000;
	fma.rn.f32 	%f2296, %f2295, %f292, %f1408;

BB84_349:
	and.b32  	%r2475, %r554, 2;
	setp.eq.s32	%p231, %r2475, 0;
	@%p231 bra 	BB84_351;

	mov.f32 	%f1409, 0f00000000;
	mov.f32 	%f1410, 0fBF800000;
	fma.rn.f32 	%f2296, %f2296, %f1410, %f1409;

BB84_351:
	mul.f32 	%f304, %f286, %f2296;
	mov.f32 	%f2380, %f34;
	@%p37 bra 	BB84_353;

	mov.f32 	%f1411, 0f00000000;
	mul.rn.f32 	%f2380, %f34, %f1411;

BB84_353:
	mul.f32 	%f1412, %f2380, 0f3F22F983;
	cvt.rni.s32.f32	%r4136, %f1412;
	cvt.rn.f32.s32	%f1413, %r4136;
	neg.f32 	%f1414, %f1413;
	fma.rn.f32 	%f1416, %f1414, %f1069, %f2380;
	fma.rn.f32 	%f1418, %f1414, %f1071, %f1416;
	fma.rn.f32 	%f2297, %f1414, %f1073, %f1418;
	abs.f32 	%f1420, %f2380;
	setp.leu.f32	%p233, %f1420, 0f47CE4780;
	@%p233 bra 	BB84_363;

	mov.b32 	 %r557, %f2380;
	shr.u32 	%r558, %r557, 23;
	bfe.u32 	%r2478, %r557, 23, 8;
	add.s32 	%r2479, %r2478, -128;
	shl.b32 	%r2480, %r557, 8;
	or.b32  	%r559, %r2480, -2147483648;
	shr.u32 	%r560, %r2479, 5;
	mov.u32 	%r4128, 0;
	mov.u64 	%rd363, __cudart_i2opi_f;
	mov.u32 	%r4127, -6;
	mov.u64 	%rd475, %rd1;

BB84_355:
	.pragma "nounroll";
	ld.const.u32 	%r2483, [%rd363];
	// inline asm
	{
	mad.lo.cc.u32   %r2481, %r2483, %r559, %r4128;
	madc.hi.u32     %r4128, %r2483, %r559,  0;
	}
	// inline asm
	st.local.u32 	[%rd475], %r2481;
	add.s64 	%rd475, %rd475, 4;
	add.s64 	%rd363, %rd363, 4;
	add.s32 	%r4127, %r4127, 1;
	setp.ne.s32	%p234, %r4127, 0;
	@%p234 bra 	BB84_355;

	and.b32  	%r565, %r557, -2147483648;
	st.local.u32 	[%rd3], %r4128;
	mov.u32 	%r2486, 6;
	sub.s32 	%r2487, %r2486, %r560;
	mul.wide.s32 	%rd279, %r2487, 4;
	add.s64 	%rd83, %rd1, %rd279;
	ld.local.u32 	%r4129, [%rd83];
	ld.local.u32 	%r4130, [%rd83+-4];
	and.b32  	%r568, %r558, 31;
	setp.eq.s32	%p235, %r568, 0;
	@%p235 bra 	BB84_358;

	mov.u32 	%r2488, 32;
	sub.s32 	%r2489, %r2488, %r568;
	shr.u32 	%r2490, %r4130, %r2489;
	shl.b32 	%r2491, %r4129, %r568;
	add.s32 	%r4129, %r2490, %r2491;
	ld.local.u32 	%r2492, [%rd83+-8];
	shr.u32 	%r2493, %r2492, %r2489;
	shl.b32 	%r2494, %r4130, %r568;
	add.s32 	%r4130, %r2493, %r2494;

BB84_358:
	shr.u32 	%r2495, %r4130, 30;
	shl.b32 	%r2496, %r4129, 2;
	add.s32 	%r4131, %r2495, %r2496;
	shl.b32 	%r574, %r4130, 2;
	shr.u32 	%r2497, %r4131, 31;
	shr.u32 	%r2498, %r4129, 30;
	add.s32 	%r575, %r2497, %r2498;
	setp.eq.s32	%p236, %r2497, 0;
	mov.u32 	%r4132, %r565;
	mov.u32 	%r4133, %r574;
	@%p236 bra 	BB84_360;

	not.b32 	%r2499, %r4131;
	neg.s32 	%r576, %r574;
	setp.eq.s32	%p237, %r574, 0;
	selp.u32	%r2500, 1, 0, %p237;
	add.s32 	%r4131, %r2500, %r2499;
	xor.b32  	%r578, %r565, -2147483648;
	mov.u32 	%r4132, %r578;
	mov.u32 	%r4133, %r576;

BB84_360:
	mov.u32 	%r580, %r4132;
	neg.s32 	%r2501, %r575;
	setp.eq.s32	%p238, %r565, 0;
	selp.b32	%r4136, %r575, %r2501, %p238;
	clz.b32 	%r4135, %r4131;
	setp.eq.s32	%p239, %r4135, 0;
	shl.b32 	%r2502, %r4131, %r4135;
	mov.u32 	%r2503, 32;
	sub.s32 	%r2504, %r2503, %r4135;
	shr.u32 	%r2505, %r4133, %r2504;
	add.s32 	%r2506, %r2505, %r2502;
	selp.b32	%r584, %r4131, %r2506, %p239;
	mov.u32 	%r2507, -921707870;
	mul.hi.u32 	%r4134, %r584, %r2507;
	setp.lt.s32	%p240, %r4134, 1;
	@%p240 bra 	BB84_362;

	mul.lo.s32 	%r2508, %r584, -921707870;
	shr.u32 	%r2509, %r2508, 31;
	shl.b32 	%r2510, %r4134, 1;
	add.s32 	%r4134, %r2509, %r2510;
	add.s32 	%r4135, %r4135, 1;

BB84_362:
	mov.u32 	%r2511, 126;
	sub.s32 	%r2512, %r2511, %r4135;
	shl.b32 	%r2513, %r2512, 23;
	add.s32 	%r2514, %r4134, 1;
	shr.u32 	%r2515, %r2514, 7;
	add.s32 	%r2516, %r2515, 1;
	shr.u32 	%r2517, %r2516, 1;
	add.s32 	%r2518, %r2517, %r2513;
	or.b32  	%r2519, %r2518, %r580;
	mov.b32 	 %f2297, %r2519;

BB84_363:
	mul.rn.f32 	%f310, %f2297, %f2297;
	and.b32  	%r591, %r4136, 1;
	setp.eq.s32	%p241, %r591, 0;
	@%p241 bra 	BB84_365;

	mov.f32 	%f1421, 0fBAB6061A;
	mov.f32 	%f1422, 0f37CCF5CE;
	fma.rn.f32 	%f2298, %f1422, %f310, %f1421;
	bra.uni 	BB84_366;

BB84_365:
	mov.f32 	%f1423, 0f3C08839E;
	mov.f32 	%f1424, 0fB94CA1F9;
	fma.rn.f32 	%f2298, %f1424, %f310, %f1423;

BB84_366:
	@%p241 bra 	BB84_368;

	mov.f32 	%f1425, 0f3D2AAAA5;
	fma.rn.f32 	%f1426, %f2298, %f310, %f1425;
	mov.f32 	%f1427, 0fBF000000;
	fma.rn.f32 	%f2299, %f1426, %f310, %f1427;
	bra.uni 	BB84_369;

BB84_368:
	mov.f32 	%f1428, 0fBE2AAAA3;
	fma.rn.f32 	%f1429, %f2298, %f310, %f1428;
	mov.f32 	%f1430, 0f00000000;
	fma.rn.f32 	%f2299, %f1429, %f310, %f1430;

BB84_369:
	fma.rn.f32 	%f2300, %f2299, %f2297, %f2297;
	@%p241 bra 	BB84_371;

	mov.f32 	%f1431, 0f3F800000;
	fma.rn.f32 	%f2300, %f2299, %f310, %f1431;

BB84_371:
	and.b32  	%r2520, %r4136, 2;
	setp.eq.s32	%p244, %r2520, 0;
	@%p244 bra 	BB84_373;

	mov.f32 	%f1432, 0f00000000;
	mov.f32 	%f1433, 0fBF800000;
	fma.rn.f32 	%f2300, %f2300, %f1433, %f1432;

BB84_373:
	mov.f32 	%f2334, %f36;
	@%p50 bra 	BB84_375;

	mov.f32 	%f1434, 0f00000000;
	mul.rn.f32 	%f2334, %f36, %f1434;

BB84_375:
	mul.f32 	%f1435, %f2334, 0f3F22F983;
	cvt.rni.s32.f32	%r4146, %f1435;
	cvt.rn.f32.s32	%f1436, %r4146;
	neg.f32 	%f1437, %f1436;
	fma.rn.f32 	%f1439, %f1437, %f1069, %f2334;
	fma.rn.f32 	%f1441, %f1437, %f1071, %f1439;
	fma.rn.f32 	%f2301, %f1437, %f1073, %f1441;
	abs.f32 	%f1443, %f2334;
	setp.leu.f32	%p246, %f1443, 0f47CE4780;
	@%p246 bra 	BB84_385;

	mov.b32 	 %r593, %f2334;
	shr.u32 	%r594, %r593, 23;
	bfe.u32 	%r2523, %r593, 23, 8;
	add.s32 	%r2524, %r2523, -128;
	shl.b32 	%r2525, %r593, 8;
	or.b32  	%r595, %r2525, -2147483648;
	shr.u32 	%r596, %r2524, 5;
	mov.u32 	%r4138, 0;
	mov.u64 	%rd364, __cudart_i2opi_f;
	mov.u32 	%r4137, -6;
	mov.u64 	%rd474, %rd1;

BB84_377:
	.pragma "nounroll";
	ld.const.u32 	%r2528, [%rd364];
	// inline asm
	{
	mad.lo.cc.u32   %r2526, %r2528, %r595, %r4138;
	madc.hi.u32     %r4138, %r2528, %r595,  0;
	}
	// inline asm
	st.local.u32 	[%rd474], %r2526;
	add.s64 	%rd474, %rd474, 4;
	add.s64 	%rd364, %rd364, 4;
	add.s32 	%r4137, %r4137, 1;
	setp.ne.s32	%p247, %r4137, 0;
	@%p247 bra 	BB84_377;

	and.b32  	%r601, %r593, -2147483648;
	st.local.u32 	[%rd3], %r4138;
	mov.u32 	%r2531, 6;
	sub.s32 	%r2532, %r2531, %r596;
	mul.wide.s32 	%rd281, %r2532, 4;
	add.s64 	%rd88, %rd1, %rd281;
	ld.local.u32 	%r4139, [%rd88];
	ld.local.u32 	%r4140, [%rd88+-4];
	and.b32  	%r604, %r594, 31;
	setp.eq.s32	%p248, %r604, 0;
	@%p248 bra 	BB84_380;

	mov.u32 	%r2533, 32;
	sub.s32 	%r2534, %r2533, %r604;
	shr.u32 	%r2535, %r4140, %r2534;
	shl.b32 	%r2536, %r4139, %r604;
	add.s32 	%r4139, %r2535, %r2536;
	ld.local.u32 	%r2537, [%rd88+-8];
	shr.u32 	%r2538, %r2537, %r2534;
	shl.b32 	%r2539, %r4140, %r604;
	add.s32 	%r4140, %r2538, %r2539;

BB84_380:
	shr.u32 	%r2540, %r4140, 30;
	shl.b32 	%r2541, %r4139, 2;
	add.s32 	%r4141, %r2540, %r2541;
	shl.b32 	%r610, %r4140, 2;
	shr.u32 	%r2542, %r4141, 31;
	shr.u32 	%r2543, %r4139, 30;
	add.s32 	%r611, %r2542, %r2543;
	setp.eq.s32	%p249, %r2542, 0;
	mov.u32 	%r4142, %r601;
	mov.u32 	%r4143, %r610;
	@%p249 bra 	BB84_382;

	not.b32 	%r2544, %r4141;
	neg.s32 	%r612, %r610;
	setp.eq.s32	%p250, %r610, 0;
	selp.u32	%r2545, 1, 0, %p250;
	add.s32 	%r4141, %r2545, %r2544;
	xor.b32  	%r614, %r601, -2147483648;
	mov.u32 	%r4142, %r614;
	mov.u32 	%r4143, %r612;

BB84_382:
	mov.u32 	%r616, %r4142;
	neg.s32 	%r2546, %r611;
	setp.eq.s32	%p251, %r601, 0;
	selp.b32	%r4146, %r611, %r2546, %p251;
	clz.b32 	%r4145, %r4141;
	setp.eq.s32	%p252, %r4145, 0;
	shl.b32 	%r2547, %r4141, %r4145;
	mov.u32 	%r2548, 32;
	sub.s32 	%r2549, %r2548, %r4145;
	shr.u32 	%r2550, %r4143, %r2549;
	add.s32 	%r2551, %r2550, %r2547;
	selp.b32	%r620, %r4141, %r2551, %p252;
	mov.u32 	%r2552, -921707870;
	mul.hi.u32 	%r4144, %r620, %r2552;
	setp.lt.s32	%p253, %r4144, 1;
	@%p253 bra 	BB84_384;

	mul.lo.s32 	%r2553, %r620, -921707870;
	shr.u32 	%r2554, %r2553, 31;
	shl.b32 	%r2555, %r4144, 1;
	add.s32 	%r4144, %r2554, %r2555;
	add.s32 	%r4145, %r4145, 1;

BB84_384:
	mov.u32 	%r2556, 126;
	sub.s32 	%r2557, %r2556, %r4145;
	shl.b32 	%r2558, %r2557, 23;
	add.s32 	%r2559, %r4144, 1;
	shr.u32 	%r2560, %r2559, 7;
	add.s32 	%r2561, %r2560, 1;
	shr.u32 	%r2562, %r2561, 1;
	add.s32 	%r2563, %r2562, %r2558;
	or.b32  	%r2564, %r2563, %r616;
	mov.b32 	 %f2301, %r2564;

BB84_385:
	mul.rn.f32 	%f327, %f2301, %f2301;
	add.s32 	%r627, %r4146, 1;
	and.b32  	%r628, %r627, 1;
	setp.eq.s32	%p254, %r628, 0;
	@%p254 bra 	BB84_387;

	mov.f32 	%f1444, 0fBAB6061A;
	mov.f32 	%f1445, 0f37CCF5CE;
	fma.rn.f32 	%f2302, %f1445, %f327, %f1444;
	bra.uni 	BB84_388;

BB84_387:
	mov.f32 	%f1446, 0f3C08839E;
	mov.f32 	%f1447, 0fB94CA1F9;
	fma.rn.f32 	%f2302, %f1447, %f327, %f1446;

BB84_388:
	@%p254 bra 	BB84_390;

	mov.f32 	%f1448, 0f3D2AAAA5;
	fma.rn.f32 	%f1449, %f2302, %f327, %f1448;
	mov.f32 	%f1450, 0fBF000000;
	fma.rn.f32 	%f2303, %f1449, %f327, %f1450;
	bra.uni 	BB84_391;

BB84_390:
	mov.f32 	%f1451, 0fBE2AAAA3;
	fma.rn.f32 	%f1452, %f2302, %f327, %f1451;
	mov.f32 	%f1453, 0f00000000;
	fma.rn.f32 	%f2303, %f1452, %f327, %f1453;

BB84_391:
	fma.rn.f32 	%f2304, %f2303, %f2301, %f2301;
	@%p254 bra 	BB84_393;

	mov.f32 	%f1454, 0f3F800000;
	fma.rn.f32 	%f2304, %f2303, %f327, %f1454;

BB84_393:
	and.b32  	%r2565, %r627, 2;
	setp.eq.s32	%p257, %r2565, 0;
	@%p257 bra 	BB84_395;

	mov.f32 	%f1455, 0f00000000;
	mov.f32 	%f1456, 0fBF800000;
	fma.rn.f32 	%f2304, %f2304, %f1456, %f1455;

BB84_395:
	mul.f32 	%f339, %f2300, %f2304;
	mov.f32 	%f2353, %f13;
	@%p63 bra 	BB84_397;

	mov.f32 	%f1457, 0f00000000;
	mul.rn.f32 	%f2353, %f13, %f1457;

BB84_397:
	mul.f32 	%f1458, %f2353, 0f3F22F983;
	cvt.rni.s32.f32	%r4156, %f1458;
	cvt.rn.f32.s32	%f1459, %r4156;
	neg.f32 	%f1460, %f1459;
	fma.rn.f32 	%f1462, %f1460, %f1069, %f2353;
	fma.rn.f32 	%f1464, %f1460, %f1071, %f1462;
	fma.rn.f32 	%f2305, %f1460, %f1073, %f1464;
	abs.f32 	%f1466, %f2353;
	setp.leu.f32	%p259, %f1466, 0f47CE4780;
	@%p259 bra 	BB84_407;

	mov.b32 	 %r630, %f2353;
	shr.u32 	%r631, %r630, 23;
	bfe.u32 	%r2568, %r630, 23, 8;
	add.s32 	%r2569, %r2568, -128;
	shl.b32 	%r2570, %r630, 8;
	or.b32  	%r632, %r2570, -2147483648;
	shr.u32 	%r633, %r2569, 5;
	mov.u32 	%r4148, 0;
	mov.u64 	%rd365, __cudart_i2opi_f;
	mov.u32 	%r4147, -6;
	mov.u64 	%rd473, %rd1;

BB84_399:
	.pragma "nounroll";
	ld.const.u32 	%r2573, [%rd365];
	// inline asm
	{
	mad.lo.cc.u32   %r2571, %r2573, %r632, %r4148;
	madc.hi.u32     %r4148, %r2573, %r632,  0;
	}
	// inline asm
	st.local.u32 	[%rd473], %r2571;
	add.s64 	%rd473, %rd473, 4;
	add.s64 	%rd365, %rd365, 4;
	add.s32 	%r4147, %r4147, 1;
	setp.ne.s32	%p260, %r4147, 0;
	@%p260 bra 	BB84_399;

	and.b32  	%r638, %r630, -2147483648;
	st.local.u32 	[%rd3], %r4148;
	mov.u32 	%r2576, 6;
	sub.s32 	%r2577, %r2576, %r633;
	mul.wide.s32 	%rd283, %r2577, 4;
	add.s64 	%rd93, %rd1, %rd283;
	ld.local.u32 	%r4149, [%rd93];
	ld.local.u32 	%r4150, [%rd93+-4];
	and.b32  	%r641, %r631, 31;
	setp.eq.s32	%p261, %r641, 0;
	@%p261 bra 	BB84_402;

	mov.u32 	%r2578, 32;
	sub.s32 	%r2579, %r2578, %r641;
	shr.u32 	%r2580, %r4150, %r2579;
	shl.b32 	%r2581, %r4149, %r641;
	add.s32 	%r4149, %r2580, %r2581;
	ld.local.u32 	%r2582, [%rd93+-8];
	shr.u32 	%r2583, %r2582, %r2579;
	shl.b32 	%r2584, %r4150, %r641;
	add.s32 	%r4150, %r2583, %r2584;

BB84_402:
	shr.u32 	%r2585, %r4150, 30;
	shl.b32 	%r2586, %r4149, 2;
	add.s32 	%r4151, %r2585, %r2586;
	shl.b32 	%r647, %r4150, 2;
	shr.u32 	%r2587, %r4151, 31;
	shr.u32 	%r2588, %r4149, 30;
	add.s32 	%r648, %r2587, %r2588;
	setp.eq.s32	%p262, %r2587, 0;
	mov.u32 	%r4152, %r638;
	mov.u32 	%r4153, %r647;
	@%p262 bra 	BB84_404;

	not.b32 	%r2589, %r4151;
	neg.s32 	%r649, %r647;
	setp.eq.s32	%p263, %r647, 0;
	selp.u32	%r2590, 1, 0, %p263;
	add.s32 	%r4151, %r2590, %r2589;
	xor.b32  	%r651, %r638, -2147483648;
	mov.u32 	%r4152, %r651;
	mov.u32 	%r4153, %r649;

BB84_404:
	mov.u32 	%r653, %r4152;
	neg.s32 	%r2591, %r648;
	setp.eq.s32	%p264, %r638, 0;
	selp.b32	%r4156, %r648, %r2591, %p264;
	clz.b32 	%r4155, %r4151;
	setp.eq.s32	%p265, %r4155, 0;
	shl.b32 	%r2592, %r4151, %r4155;
	mov.u32 	%r2593, 32;
	sub.s32 	%r2594, %r2593, %r4155;
	shr.u32 	%r2595, %r4153, %r2594;
	add.s32 	%r2596, %r2595, %r2592;
	selp.b32	%r657, %r4151, %r2596, %p265;
	mov.u32 	%r2597, -921707870;
	mul.hi.u32 	%r4154, %r657, %r2597;
	setp.lt.s32	%p266, %r4154, 1;
	@%p266 bra 	BB84_406;

	mul.lo.s32 	%r2598, %r657, -921707870;
	shr.u32 	%r2599, %r2598, 31;
	shl.b32 	%r2600, %r4154, 1;
	add.s32 	%r4154, %r2599, %r2600;
	add.s32 	%r4155, %r4155, 1;

BB84_406:
	mov.u32 	%r2601, 126;
	sub.s32 	%r2602, %r2601, %r4155;
	shl.b32 	%r2603, %r2602, 23;
	add.s32 	%r2604, %r4154, 1;
	shr.u32 	%r2605, %r2604, 7;
	add.s32 	%r2606, %r2605, 1;
	shr.u32 	%r2607, %r2606, 1;
	add.s32 	%r2608, %r2607, %r2603;
	or.b32  	%r2609, %r2608, %r653;
	mov.b32 	 %f2305, %r2609;

BB84_407:
	mul.rn.f32 	%f345, %f2305, %f2305;
	and.b32  	%r664, %r4156, 1;
	setp.eq.s32	%p267, %r664, 0;
	@%p267 bra 	BB84_409;

	mov.f32 	%f1467, 0fBAB6061A;
	mov.f32 	%f1468, 0f37CCF5CE;
	fma.rn.f32 	%f2306, %f1468, %f345, %f1467;
	bra.uni 	BB84_410;

BB84_409:
	mov.f32 	%f1469, 0f3C08839E;
	mov.f32 	%f1470, 0fB94CA1F9;
	fma.rn.f32 	%f2306, %f1470, %f345, %f1469;

BB84_410:
	@%p267 bra 	BB84_412;

	mov.f32 	%f1471, 0f3D2AAAA5;
	fma.rn.f32 	%f1472, %f2306, %f345, %f1471;
	mov.f32 	%f1473, 0fBF000000;
	fma.rn.f32 	%f2307, %f1472, %f345, %f1473;
	bra.uni 	BB84_413;

BB84_412:
	mov.f32 	%f1474, 0fBE2AAAA3;
	fma.rn.f32 	%f1475, %f2306, %f345, %f1474;
	mov.f32 	%f1476, 0f00000000;
	fma.rn.f32 	%f2307, %f1475, %f345, %f1476;

BB84_413:
	fma.rn.f32 	%f2308, %f2307, %f2305, %f2305;
	@%p267 bra 	BB84_415;

	mov.f32 	%f1477, 0f3F800000;
	fma.rn.f32 	%f2308, %f2307, %f345, %f1477;

BB84_415:
	and.b32  	%r2610, %r4156, 2;
	setp.eq.s32	%p270, %r2610, 0;
	@%p270 bra 	BB84_417;

	mov.f32 	%f1478, 0f00000000;
	mov.f32 	%f1479, 0fBF800000;
	fma.rn.f32 	%f2308, %f2308, %f1479, %f1478;

BB84_417:
	mul.f32 	%f1480, %f339, %f2308;
	sub.f32 	%f357, %f304, %f1480;
	mov.f32 	%f2379, %f34;
	@%p37 bra 	BB84_419;

	mov.f32 	%f1481, 0f00000000;
	mul.rn.f32 	%f2379, %f34, %f1481;

BB84_419:
	mul.f32 	%f1482, %f2379, 0f3F22F983;
	cvt.rni.s32.f32	%r4166, %f1482;
	cvt.rn.f32.s32	%f1483, %r4166;
	neg.f32 	%f1484, %f1483;
	fma.rn.f32 	%f1486, %f1484, %f1069, %f2379;
	fma.rn.f32 	%f1488, %f1484, %f1071, %f1486;
	fma.rn.f32 	%f2309, %f1484, %f1073, %f1488;
	abs.f32 	%f1490, %f2379;
	setp.leu.f32	%p272, %f1490, 0f47CE4780;
	@%p272 bra 	BB84_429;

	mov.b32 	 %r666, %f2379;
	shr.u32 	%r667, %r666, 23;
	bfe.u32 	%r2613, %r666, 23, 8;
	add.s32 	%r2614, %r2613, -128;
	shl.b32 	%r2615, %r666, 8;
	or.b32  	%r668, %r2615, -2147483648;
	shr.u32 	%r669, %r2614, 5;
	mov.u32 	%r4158, 0;
	mov.u64 	%rd366, __cudart_i2opi_f;
	mov.u32 	%r4157, -6;
	mov.u64 	%rd472, %rd1;

BB84_421:
	.pragma "nounroll";
	ld.const.u32 	%r2618, [%rd366];
	// inline asm
	{
	mad.lo.cc.u32   %r2616, %r2618, %r668, %r4158;
	madc.hi.u32     %r4158, %r2618, %r668,  0;
	}
	// inline asm
	st.local.u32 	[%rd472], %r2616;
	add.s64 	%rd472, %rd472, 4;
	add.s64 	%rd366, %rd366, 4;
	add.s32 	%r4157, %r4157, 1;
	setp.ne.s32	%p273, %r4157, 0;
	@%p273 bra 	BB84_421;

	and.b32  	%r674, %r666, -2147483648;
	st.local.u32 	[%rd3], %r4158;
	mov.u32 	%r2621, 6;
	sub.s32 	%r2622, %r2621, %r669;
	mul.wide.s32 	%rd285, %r2622, 4;
	add.s64 	%rd98, %rd1, %rd285;
	ld.local.u32 	%r4159, [%rd98];
	ld.local.u32 	%r4160, [%rd98+-4];
	and.b32  	%r677, %r667, 31;
	setp.eq.s32	%p274, %r677, 0;
	@%p274 bra 	BB84_424;

	mov.u32 	%r2623, 32;
	sub.s32 	%r2624, %r2623, %r677;
	shr.u32 	%r2625, %r4160, %r2624;
	shl.b32 	%r2626, %r4159, %r677;
	add.s32 	%r4159, %r2625, %r2626;
	ld.local.u32 	%r2627, [%rd98+-8];
	shr.u32 	%r2628, %r2627, %r2624;
	shl.b32 	%r2629, %r4160, %r677;
	add.s32 	%r4160, %r2628, %r2629;

BB84_424:
	shr.u32 	%r2630, %r4160, 30;
	shl.b32 	%r2631, %r4159, 2;
	add.s32 	%r4161, %r2630, %r2631;
	shl.b32 	%r683, %r4160, 2;
	shr.u32 	%r2632, %r4161, 31;
	shr.u32 	%r2633, %r4159, 30;
	add.s32 	%r684, %r2632, %r2633;
	setp.eq.s32	%p275, %r2632, 0;
	mov.u32 	%r4162, %r674;
	mov.u32 	%r4163, %r683;
	@%p275 bra 	BB84_426;

	not.b32 	%r2634, %r4161;
	neg.s32 	%r685, %r683;
	setp.eq.s32	%p276, %r683, 0;
	selp.u32	%r2635, 1, 0, %p276;
	add.s32 	%r4161, %r2635, %r2634;
	xor.b32  	%r687, %r674, -2147483648;
	mov.u32 	%r4162, %r687;
	mov.u32 	%r4163, %r685;

BB84_426:
	mov.u32 	%r689, %r4162;
	neg.s32 	%r2636, %r684;
	setp.eq.s32	%p277, %r674, 0;
	selp.b32	%r4166, %r684, %r2636, %p277;
	clz.b32 	%r4165, %r4161;
	setp.eq.s32	%p278, %r4165, 0;
	shl.b32 	%r2637, %r4161, %r4165;
	mov.u32 	%r2638, 32;
	sub.s32 	%r2639, %r2638, %r4165;
	shr.u32 	%r2640, %r4163, %r2639;
	add.s32 	%r2641, %r2640, %r2637;
	selp.b32	%r693, %r4161, %r2641, %p278;
	mov.u32 	%r2642, -921707870;
	mul.hi.u32 	%r4164, %r693, %r2642;
	setp.lt.s32	%p279, %r4164, 1;
	@%p279 bra 	BB84_428;

	mul.lo.s32 	%r2643, %r693, -921707870;
	shr.u32 	%r2644, %r2643, 31;
	shl.b32 	%r2645, %r4164, 1;
	add.s32 	%r4164, %r2644, %r2645;
	add.s32 	%r4165, %r4165, 1;

BB84_428:
	mov.u32 	%r2646, 126;
	sub.s32 	%r2647, %r2646, %r4165;
	shl.b32 	%r2648, %r2647, 23;
	add.s32 	%r2649, %r4164, 1;
	shr.u32 	%r2650, %r2649, 7;
	add.s32 	%r2651, %r2650, 1;
	shr.u32 	%r2652, %r2651, 1;
	add.s32 	%r2653, %r2652, %r2648;
	or.b32  	%r2654, %r2653, %r689;
	mov.b32 	 %f2309, %r2654;

BB84_429:
	mul.rn.f32 	%f363, %f2309, %f2309;
	add.s32 	%r700, %r4166, 1;
	and.b32  	%r701, %r700, 1;
	setp.eq.s32	%p280, %r701, 0;
	@%p280 bra 	BB84_431;

	mov.f32 	%f1491, 0fBAB6061A;
	mov.f32 	%f1492, 0f37CCF5CE;
	fma.rn.f32 	%f2310, %f1492, %f363, %f1491;
	bra.uni 	BB84_432;

BB84_431:
	mov.f32 	%f1493, 0f3C08839E;
	mov.f32 	%f1494, 0fB94CA1F9;
	fma.rn.f32 	%f2310, %f1494, %f363, %f1493;

BB84_432:
	@%p280 bra 	BB84_434;

	mov.f32 	%f1495, 0f3D2AAAA5;
	fma.rn.f32 	%f1496, %f2310, %f363, %f1495;
	mov.f32 	%f1497, 0fBF000000;
	fma.rn.f32 	%f2311, %f1496, %f363, %f1497;
	bra.uni 	BB84_435;

BB84_434:
	mov.f32 	%f1498, 0fBE2AAAA3;
	fma.rn.f32 	%f1499, %f2310, %f363, %f1498;
	mov.f32 	%f1500, 0f00000000;
	fma.rn.f32 	%f2311, %f1499, %f363, %f1500;

BB84_435:
	fma.rn.f32 	%f2312, %f2311, %f2309, %f2309;
	@%p280 bra 	BB84_437;

	mov.f32 	%f1501, 0f3F800000;
	fma.rn.f32 	%f2312, %f2311, %f363, %f1501;

BB84_437:
	and.b32  	%r2655, %r700, 2;
	setp.eq.s32	%p283, %r2655, 0;
	@%p283 bra 	BB84_439;

	mov.f32 	%f1502, 0f00000000;
	mov.f32 	%f1503, 0fBF800000;
	fma.rn.f32 	%f2312, %f2312, %f1503, %f1502;

BB84_439:
	mov.f32 	%f2333, %f36;
	@%p50 bra 	BB84_441;

	mov.f32 	%f1504, 0f00000000;
	mul.rn.f32 	%f2333, %f36, %f1504;

BB84_441:
	mul.f32 	%f1505, %f2333, 0f3F22F983;
	cvt.rni.s32.f32	%r4176, %f1505;
	cvt.rn.f32.s32	%f1506, %r4176;
	neg.f32 	%f1507, %f1506;
	fma.rn.f32 	%f1509, %f1507, %f1069, %f2333;
	fma.rn.f32 	%f1511, %f1507, %f1071, %f1509;
	fma.rn.f32 	%f2313, %f1507, %f1073, %f1511;
	abs.f32 	%f1513, %f2333;
	setp.leu.f32	%p285, %f1513, 0f47CE4780;
	@%p285 bra 	BB84_451;

	mov.b32 	 %r703, %f2333;
	shr.u32 	%r704, %r703, 23;
	bfe.u32 	%r2658, %r703, 23, 8;
	add.s32 	%r2659, %r2658, -128;
	shl.b32 	%r2660, %r703, 8;
	or.b32  	%r705, %r2660, -2147483648;
	shr.u32 	%r706, %r2659, 5;
	mov.u32 	%r4168, 0;
	mov.u64 	%rd367, __cudart_i2opi_f;
	mov.u32 	%r4167, -6;
	mov.u64 	%rd471, %rd1;

BB84_443:
	.pragma "nounroll";
	ld.const.u32 	%r2663, [%rd367];
	// inline asm
	{
	mad.lo.cc.u32   %r2661, %r2663, %r705, %r4168;
	madc.hi.u32     %r4168, %r2663, %r705,  0;
	}
	// inline asm
	st.local.u32 	[%rd471], %r2661;
	add.s64 	%rd471, %rd471, 4;
	add.s64 	%rd367, %rd367, 4;
	add.s32 	%r4167, %r4167, 1;
	setp.ne.s32	%p286, %r4167, 0;
	@%p286 bra 	BB84_443;

	and.b32  	%r711, %r703, -2147483648;
	st.local.u32 	[%rd3], %r4168;
	mov.u32 	%r2666, 6;
	sub.s32 	%r2667, %r2666, %r706;
	mul.wide.s32 	%rd287, %r2667, 4;
	add.s64 	%rd103, %rd1, %rd287;
	ld.local.u32 	%r4169, [%rd103];
	ld.local.u32 	%r4170, [%rd103+-4];
	and.b32  	%r714, %r704, 31;
	setp.eq.s32	%p287, %r714, 0;
	@%p287 bra 	BB84_446;

	mov.u32 	%r2668, 32;
	sub.s32 	%r2669, %r2668, %r714;
	shr.u32 	%r2670, %r4170, %r2669;
	shl.b32 	%r2671, %r4169, %r714;
	add.s32 	%r4169, %r2670, %r2671;
	ld.local.u32 	%r2672, [%rd103+-8];
	shr.u32 	%r2673, %r2672, %r2669;
	shl.b32 	%r2674, %r4170, %r714;
	add.s32 	%r4170, %r2673, %r2674;

BB84_446:
	shr.u32 	%r2675, %r4170, 30;
	shl.b32 	%r2676, %r4169, 2;
	add.s32 	%r4171, %r2675, %r2676;
	shl.b32 	%r720, %r4170, 2;
	shr.u32 	%r2677, %r4171, 31;
	shr.u32 	%r2678, %r4169, 30;
	add.s32 	%r721, %r2677, %r2678;
	setp.eq.s32	%p288, %r2677, 0;
	mov.u32 	%r4172, %r711;
	mov.u32 	%r4173, %r720;
	@%p288 bra 	BB84_448;

	not.b32 	%r2679, %r4171;
	neg.s32 	%r722, %r720;
	setp.eq.s32	%p289, %r720, 0;
	selp.u32	%r2680, 1, 0, %p289;
	add.s32 	%r4171, %r2680, %r2679;
	xor.b32  	%r724, %r711, -2147483648;
	mov.u32 	%r4172, %r724;
	mov.u32 	%r4173, %r722;

BB84_448:
	mov.u32 	%r726, %r4172;
	neg.s32 	%r2681, %r721;
	setp.eq.s32	%p290, %r711, 0;
	selp.b32	%r4176, %r721, %r2681, %p290;
	clz.b32 	%r4175, %r4171;
	setp.eq.s32	%p291, %r4175, 0;
	shl.b32 	%r2682, %r4171, %r4175;
	mov.u32 	%r2683, 32;
	sub.s32 	%r2684, %r2683, %r4175;
	shr.u32 	%r2685, %r4173, %r2684;
	add.s32 	%r2686, %r2685, %r2682;
	selp.b32	%r730, %r4171, %r2686, %p291;
	mov.u32 	%r2687, -921707870;
	mul.hi.u32 	%r4174, %r730, %r2687;
	setp.lt.s32	%p292, %r4174, 1;
	@%p292 bra 	BB84_450;

	mul.lo.s32 	%r2688, %r730, -921707870;
	shr.u32 	%r2689, %r2688, 31;
	shl.b32 	%r2690, %r4174, 1;
	add.s32 	%r4174, %r2689, %r2690;
	add.s32 	%r4175, %r4175, 1;

BB84_450:
	mov.u32 	%r2691, 126;
	sub.s32 	%r2692, %r2691, %r4175;
	shl.b32 	%r2693, %r2692, 23;
	add.s32 	%r2694, %r4174, 1;
	shr.u32 	%r2695, %r2694, 7;
	add.s32 	%r2696, %r2695, 1;
	shr.u32 	%r2697, %r2696, 1;
	add.s32 	%r2698, %r2697, %r2693;
	or.b32  	%r2699, %r2698, %r726;
	mov.b32 	 %f2313, %r2699;

BB84_451:
	mul.rn.f32 	%f380, %f2313, %f2313;
	add.s32 	%r737, %r4176, 1;
	and.b32  	%r738, %r737, 1;
	setp.eq.s32	%p293, %r738, 0;
	@%p293 bra 	BB84_453;

	mov.f32 	%f1514, 0fBAB6061A;
	mov.f32 	%f1515, 0f37CCF5CE;
	fma.rn.f32 	%f2314, %f1515, %f380, %f1514;
	bra.uni 	BB84_454;

BB84_453:
	mov.f32 	%f1516, 0f3C08839E;
	mov.f32 	%f1517, 0fB94CA1F9;
	fma.rn.f32 	%f2314, %f1517, %f380, %f1516;

BB84_454:
	@%p293 bra 	BB84_456;

	mov.f32 	%f1518, 0f3D2AAAA5;
	fma.rn.f32 	%f1519, %f2314, %f380, %f1518;
	mov.f32 	%f1520, 0fBF000000;
	fma.rn.f32 	%f2315, %f1519, %f380, %f1520;
	bra.uni 	BB84_457;

BB84_456:
	mov.f32 	%f1521, 0fBE2AAAA3;
	fma.rn.f32 	%f1522, %f2314, %f380, %f1521;
	mov.f32 	%f1523, 0f00000000;
	fma.rn.f32 	%f2315, %f1522, %f380, %f1523;

BB84_457:
	fma.rn.f32 	%f2316, %f2315, %f2313, %f2313;
	@%p293 bra 	BB84_459;

	mov.f32 	%f1524, 0f3F800000;
	fma.rn.f32 	%f2316, %f2315, %f380, %f1524;

BB84_459:
	and.b32  	%r2700, %r737, 2;
	setp.eq.s32	%p296, %r2700, 0;
	@%p296 bra 	BB84_461;

	mov.f32 	%f1525, 0f00000000;
	mov.f32 	%f1526, 0fBF800000;
	fma.rn.f32 	%f2316, %f2316, %f1526, %f1525;

BB84_461:
	mul.f32 	%f392, %f2312, %f2316;
	mov.f32 	%f2352, %f13;
	@%p63 bra 	BB84_463;

	mov.f32 	%f1527, 0f00000000;
	mul.rn.f32 	%f2352, %f13, %f1527;

BB84_463:
	mul.f32 	%f1528, %f2352, 0f3F22F983;
	cvt.rni.s32.f32	%r4186, %f1528;
	cvt.rn.f32.s32	%f1529, %r4186;
	neg.f32 	%f1530, %f1529;
	fma.rn.f32 	%f1532, %f1530, %f1069, %f2352;
	fma.rn.f32 	%f1534, %f1530, %f1071, %f1532;
	fma.rn.f32 	%f2317, %f1530, %f1073, %f1534;
	abs.f32 	%f1536, %f2352;
	setp.leu.f32	%p298, %f1536, 0f47CE4780;
	@%p298 bra 	BB84_473;

	mov.b32 	 %r740, %f2352;
	shr.u32 	%r741, %r740, 23;
	bfe.u32 	%r2703, %r740, 23, 8;
	add.s32 	%r2704, %r2703, -128;
	shl.b32 	%r2705, %r740, 8;
	or.b32  	%r742, %r2705, -2147483648;
	shr.u32 	%r743, %r2704, 5;
	mov.u32 	%r4178, 0;
	mov.u64 	%rd368, __cudart_i2opi_f;
	mov.u32 	%r4177, -6;
	mov.u64 	%rd470, %rd1;

BB84_465:
	.pragma "nounroll";
	ld.const.u32 	%r2708, [%rd368];
	// inline asm
	{
	mad.lo.cc.u32   %r2706, %r2708, %r742, %r4178;
	madc.hi.u32     %r4178, %r2708, %r742,  0;
	}
	// inline asm
	st.local.u32 	[%rd470], %r2706;
	add.s64 	%rd470, %rd470, 4;
	add.s64 	%rd368, %rd368, 4;
	add.s32 	%r4177, %r4177, 1;
	setp.ne.s32	%p299, %r4177, 0;
	@%p299 bra 	BB84_465;

	and.b32  	%r748, %r740, -2147483648;
	st.local.u32 	[%rd3], %r4178;
	mov.u32 	%r2711, 6;
	sub.s32 	%r2712, %r2711, %r743;
	mul.wide.s32 	%rd289, %r2712, 4;
	add.s64 	%rd108, %rd1, %rd289;
	ld.local.u32 	%r4179, [%rd108];
	ld.local.u32 	%r4180, [%rd108+-4];
	and.b32  	%r751, %r741, 31;
	setp.eq.s32	%p300, %r751, 0;
	@%p300 bra 	BB84_468;

	mov.u32 	%r2713, 32;
	sub.s32 	%r2714, %r2713, %r751;
	shr.u32 	%r2715, %r4180, %r2714;
	shl.b32 	%r2716, %r4179, %r751;
	add.s32 	%r4179, %r2715, %r2716;
	ld.local.u32 	%r2717, [%rd108+-8];
	shr.u32 	%r2718, %r2717, %r2714;
	shl.b32 	%r2719, %r4180, %r751;
	add.s32 	%r4180, %r2718, %r2719;

BB84_468:
	shr.u32 	%r2720, %r4180, 30;
	shl.b32 	%r2721, %r4179, 2;
	add.s32 	%r4181, %r2720, %r2721;
	shl.b32 	%r757, %r4180, 2;
	shr.u32 	%r2722, %r4181, 31;
	shr.u32 	%r2723, %r4179, 30;
	add.s32 	%r758, %r2722, %r2723;
	setp.eq.s32	%p301, %r2722, 0;
	mov.u32 	%r4182, %r748;
	mov.u32 	%r4183, %r757;
	@%p301 bra 	BB84_470;

	not.b32 	%r2724, %r4181;
	neg.s32 	%r759, %r757;
	setp.eq.s32	%p302, %r757, 0;
	selp.u32	%r2725, 1, 0, %p302;
	add.s32 	%r4181, %r2725, %r2724;
	xor.b32  	%r761, %r748, -2147483648;
	mov.u32 	%r4182, %r761;
	mov.u32 	%r4183, %r759;

BB84_470:
	mov.u32 	%r763, %r4182;
	neg.s32 	%r2726, %r758;
	setp.eq.s32	%p303, %r748, 0;
	selp.b32	%r4186, %r758, %r2726, %p303;
	clz.b32 	%r4185, %r4181;
	setp.eq.s32	%p304, %r4185, 0;
	shl.b32 	%r2727, %r4181, %r4185;
	mov.u32 	%r2728, 32;
	sub.s32 	%r2729, %r2728, %r4185;
	shr.u32 	%r2730, %r4183, %r2729;
	add.s32 	%r2731, %r2730, %r2727;
	selp.b32	%r767, %r4181, %r2731, %p304;
	mov.u32 	%r2732, -921707870;
	mul.hi.u32 	%r4184, %r767, %r2732;
	setp.lt.s32	%p305, %r4184, 1;
	@%p305 bra 	BB84_472;

	mul.lo.s32 	%r2733, %r767, -921707870;
	shr.u32 	%r2734, %r2733, 31;
	shl.b32 	%r2735, %r4184, 1;
	add.s32 	%r4184, %r2734, %r2735;
	add.s32 	%r4185, %r4185, 1;

BB84_472:
	mov.u32 	%r2736, 126;
	sub.s32 	%r2737, %r2736, %r4185;
	shl.b32 	%r2738, %r2737, 23;
	add.s32 	%r2739, %r4184, 1;
	shr.u32 	%r2740, %r2739, 7;
	add.s32 	%r2741, %r2740, 1;
	shr.u32 	%r2742, %r2741, 1;
	add.s32 	%r2743, %r2742, %r2738;
	or.b32  	%r2744, %r2743, %r763;
	mov.b32 	 %f2317, %r2744;

BB84_473:
	mul.rn.f32 	%f398, %f2317, %f2317;
	and.b32  	%r774, %r4186, 1;
	setp.eq.s32	%p306, %r774, 0;
	@%p306 bra 	BB84_475;

	mov.f32 	%f1537, 0fBAB6061A;
	mov.f32 	%f1538, 0f37CCF5CE;
	fma.rn.f32 	%f2318, %f1538, %f398, %f1537;
	bra.uni 	BB84_476;

BB84_475:
	mov.f32 	%f1539, 0f3C08839E;
	mov.f32 	%f1540, 0fB94CA1F9;
	fma.rn.f32 	%f2318, %f1540, %f398, %f1539;

BB84_476:
	@%p306 bra 	BB84_478;

	mov.f32 	%f1541, 0f3D2AAAA5;
	fma.rn.f32 	%f1542, %f2318, %f398, %f1541;
	mov.f32 	%f1543, 0fBF000000;
	fma.rn.f32 	%f2319, %f1542, %f398, %f1543;
	bra.uni 	BB84_479;

BB84_478:
	mov.f32 	%f1544, 0fBE2AAAA3;
	fma.rn.f32 	%f1545, %f2318, %f398, %f1544;
	mov.f32 	%f1546, 0f00000000;
	fma.rn.f32 	%f2319, %f1545, %f398, %f1546;

BB84_479:
	fma.rn.f32 	%f2320, %f2319, %f2317, %f2317;
	@%p306 bra 	BB84_481;

	mov.f32 	%f1547, 0f3F800000;
	fma.rn.f32 	%f2320, %f2319, %f398, %f1547;

BB84_481:
	and.b32  	%r2745, %r4186, 2;
	setp.eq.s32	%p309, %r2745, 0;
	@%p309 bra 	BB84_483;

	mov.f32 	%f1548, 0f00000000;
	mov.f32 	%f1549, 0fBF800000;
	fma.rn.f32 	%f2320, %f2320, %f1549, %f1548;

BB84_483:
	mul.f32 	%f410, %f392, %f2320;
	mov.f32 	%f2378, %f34;
	@%p37 bra 	BB84_485;

	mov.f32 	%f1550, 0f00000000;
	mul.rn.f32 	%f2378, %f34, %f1550;

BB84_485:
	mul.f32 	%f1551, %f2378, 0f3F22F983;
	cvt.rni.s32.f32	%r4196, %f1551;
	cvt.rn.f32.s32	%f1552, %r4196;
	neg.f32 	%f1553, %f1552;
	fma.rn.f32 	%f1555, %f1553, %f1069, %f2378;
	fma.rn.f32 	%f1557, %f1553, %f1071, %f1555;
	fma.rn.f32 	%f2321, %f1553, %f1073, %f1557;
	abs.f32 	%f1559, %f2378;
	setp.leu.f32	%p311, %f1559, 0f47CE4780;
	@%p311 bra 	BB84_495;

	mov.b32 	 %r776, %f2378;
	shr.u32 	%r777, %r776, 23;
	bfe.u32 	%r2748, %r776, 23, 8;
	add.s32 	%r2749, %r2748, -128;
	shl.b32 	%r2750, %r776, 8;
	or.b32  	%r778, %r2750, -2147483648;
	shr.u32 	%r779, %r2749, 5;
	mov.u32 	%r4188, 0;
	mov.u64 	%rd369, __cudart_i2opi_f;
	mov.u32 	%r4187, -6;
	mov.u64 	%rd469, %rd1;

BB84_487:
	.pragma "nounroll";
	ld.const.u32 	%r2753, [%rd369];
	// inline asm
	{
	mad.lo.cc.u32   %r2751, %r2753, %r778, %r4188;
	madc.hi.u32     %r4188, %r2753, %r778,  0;
	}
	// inline asm
	st.local.u32 	[%rd469], %r2751;
	add.s64 	%rd469, %rd469, 4;
	add.s64 	%rd369, %rd369, 4;
	add.s32 	%r4187, %r4187, 1;
	setp.ne.s32	%p312, %r4187, 0;
	@%p312 bra 	BB84_487;

	and.b32  	%r784, %r776, -2147483648;
	st.local.u32 	[%rd3], %r4188;
	mov.u32 	%r2756, 6;
	sub.s32 	%r2757, %r2756, %r779;
	mul.wide.s32 	%rd291, %r2757, 4;
	add.s64 	%rd113, %rd1, %rd291;
	ld.local.u32 	%r4189, [%rd113];
	ld.local.u32 	%r4190, [%rd113+-4];
	and.b32  	%r787, %r777, 31;
	setp.eq.s32	%p313, %r787, 0;
	@%p313 bra 	BB84_490;

	mov.u32 	%r2758, 32;
	sub.s32 	%r2759, %r2758, %r787;
	shr.u32 	%r2760, %r4190, %r2759;
	shl.b32 	%r2761, %r4189, %r787;
	add.s32 	%r4189, %r2760, %r2761;
	ld.local.u32 	%r2762, [%rd113+-8];
	shr.u32 	%r2763, %r2762, %r2759;
	shl.b32 	%r2764, %r4190, %r787;
	add.s32 	%r4190, %r2763, %r2764;

BB84_490:
	shr.u32 	%r2765, %r4190, 30;
	shl.b32 	%r2766, %r4189, 2;
	add.s32 	%r4191, %r2765, %r2766;
	shl.b32 	%r793, %r4190, 2;
	shr.u32 	%r2767, %r4191, 31;
	shr.u32 	%r2768, %r4189, 30;
	add.s32 	%r794, %r2767, %r2768;
	setp.eq.s32	%p314, %r2767, 0;
	mov.u32 	%r4192, %r784;
	mov.u32 	%r4193, %r793;
	@%p314 bra 	BB84_492;

	not.b32 	%r2769, %r4191;
	neg.s32 	%r795, %r793;
	setp.eq.s32	%p315, %r793, 0;
	selp.u32	%r2770, 1, 0, %p315;
	add.s32 	%r4191, %r2770, %r2769;
	xor.b32  	%r797, %r784, -2147483648;
	mov.u32 	%r4192, %r797;
	mov.u32 	%r4193, %r795;

BB84_492:
	mov.u32 	%r799, %r4192;
	neg.s32 	%r2771, %r794;
	setp.eq.s32	%p316, %r784, 0;
	selp.b32	%r4196, %r794, %r2771, %p316;
	clz.b32 	%r4195, %r4191;
	setp.eq.s32	%p317, %r4195, 0;
	shl.b32 	%r2772, %r4191, %r4195;
	mov.u32 	%r2773, 32;
	sub.s32 	%r2774, %r2773, %r4195;
	shr.u32 	%r2775, %r4193, %r2774;
	add.s32 	%r2776, %r2775, %r2772;
	selp.b32	%r803, %r4191, %r2776, %p317;
	mov.u32 	%r2777, -921707870;
	mul.hi.u32 	%r4194, %r803, %r2777;
	setp.lt.s32	%p318, %r4194, 1;
	@%p318 bra 	BB84_494;

	mul.lo.s32 	%r2778, %r803, -921707870;
	shr.u32 	%r2779, %r2778, 31;
	shl.b32 	%r2780, %r4194, 1;
	add.s32 	%r4194, %r2779, %r2780;
	add.s32 	%r4195, %r4195, 1;

BB84_494:
	mov.u32 	%r2781, 126;
	sub.s32 	%r2782, %r2781, %r4195;
	shl.b32 	%r2783, %r2782, 23;
	add.s32 	%r2784, %r4194, 1;
	shr.u32 	%r2785, %r2784, 7;
	add.s32 	%r2786, %r2785, 1;
	shr.u32 	%r2787, %r2786, 1;
	add.s32 	%r2788, %r2787, %r2783;
	or.b32  	%r2789, %r2788, %r799;
	mov.b32 	 %f2321, %r2789;

BB84_495:
	mul.rn.f32 	%f416, %f2321, %f2321;
	and.b32  	%r810, %r4196, 1;
	setp.eq.s32	%p319, %r810, 0;
	@%p319 bra 	BB84_497;

	mov.f32 	%f1560, 0fBAB6061A;
	mov.f32 	%f1561, 0f37CCF5CE;
	fma.rn.f32 	%f2322, %f1561, %f416, %f1560;
	bra.uni 	BB84_498;

BB84_497:
	mov.f32 	%f1562, 0f3C08839E;
	mov.f32 	%f1563, 0fB94CA1F9;
	fma.rn.f32 	%f2322, %f1563, %f416, %f1562;

BB84_498:
	@%p319 bra 	BB84_500;

	mov.f32 	%f1564, 0f3D2AAAA5;
	fma.rn.f32 	%f1565, %f2322, %f416, %f1564;
	mov.f32 	%f1566, 0fBF000000;
	fma.rn.f32 	%f2323, %f1565, %f416, %f1566;
	bra.uni 	BB84_501;

BB84_500:
	mov.f32 	%f1567, 0fBE2AAAA3;
	fma.rn.f32 	%f1568, %f2322, %f416, %f1567;
	mov.f32 	%f1569, 0f00000000;
	fma.rn.f32 	%f2323, %f1568, %f416, %f1569;

BB84_501:
	fma.rn.f32 	%f2324, %f2323, %f2321, %f2321;
	@%p319 bra 	BB84_503;

	mov.f32 	%f1570, 0f3F800000;
	fma.rn.f32 	%f2324, %f2323, %f416, %f1570;

BB84_503:
	and.b32  	%r2790, %r4196, 2;
	setp.eq.s32	%p322, %r2790, 0;
	@%p322 bra 	BB84_505;

	mov.f32 	%f1571, 0f00000000;
	mov.f32 	%f1572, 0fBF800000;
	fma.rn.f32 	%f2324, %f2324, %f1572, %f1571;

BB84_505:
	mov.f32 	%f2332, %f36;
	@%p50 bra 	BB84_507;

	mov.f32 	%f1573, 0f00000000;
	mul.rn.f32 	%f2332, %f36, %f1573;

BB84_507:
	mul.f32 	%f1574, %f2332, 0f3F22F983;
	cvt.rni.s32.f32	%r4206, %f1574;
	cvt.rn.f32.s32	%f1575, %r4206;
	neg.f32 	%f1576, %f1575;
	fma.rn.f32 	%f1578, %f1576, %f1069, %f2332;
	fma.rn.f32 	%f1580, %f1576, %f1071, %f1578;
	fma.rn.f32 	%f2340, %f1576, %f1073, %f1580;
	abs.f32 	%f1582, %f2332;
	setp.leu.f32	%p324, %f1582, 0f47CE4780;
	@%p324 bra 	BB84_517;

	mov.b32 	 %r812, %f2332;
	shr.u32 	%r813, %r812, 23;
	bfe.u32 	%r2793, %r812, 23, 8;
	add.s32 	%r2794, %r2793, -128;
	shl.b32 	%r2795, %r812, 8;
	or.b32  	%r814, %r2795, -2147483648;
	shr.u32 	%r815, %r2794, 5;
	mov.u32 	%r4198, 0;
	mov.u64 	%rd370, __cudart_i2opi_f;
	mov.u32 	%r4197, -6;
	mov.u64 	%rd468, %rd1;

BB84_509:
	.pragma "nounroll";
	ld.const.u32 	%r2798, [%rd370];
	// inline asm
	{
	mad.lo.cc.u32   %r2796, %r2798, %r814, %r4198;
	madc.hi.u32     %r4198, %r2798, %r814,  0;
	}
	// inline asm
	st.local.u32 	[%rd468], %r2796;
	add.s64 	%rd468, %rd468, 4;
	add.s64 	%rd370, %rd370, 4;
	add.s32 	%r4197, %r4197, 1;
	setp.ne.s32	%p325, %r4197, 0;
	@%p325 bra 	BB84_509;

	and.b32  	%r820, %r812, -2147483648;
	st.local.u32 	[%rd3], %r4198;
	mov.u32 	%r2801, 6;
	sub.s32 	%r2802, %r2801, %r815;
	mul.wide.s32 	%rd293, %r2802, 4;
	add.s64 	%rd118, %rd1, %rd293;
	ld.local.u32 	%r4199, [%rd118];
	ld.local.u32 	%r4200, [%rd118+-4];
	and.b32  	%r823, %r813, 31;
	setp.eq.s32	%p326, %r823, 0;
	@%p326 bra 	BB84_512;

	mov.u32 	%r2803, 32;
	sub.s32 	%r2804, %r2803, %r823;
	shr.u32 	%r2805, %r4200, %r2804;
	shl.b32 	%r2806, %r4199, %r823;
	add.s32 	%r4199, %r2805, %r2806;
	ld.local.u32 	%r2807, [%rd118+-8];
	shr.u32 	%r2808, %r2807, %r2804;
	shl.b32 	%r2809, %r4200, %r823;
	add.s32 	%r4200, %r2808, %r2809;

BB84_512:
	shr.u32 	%r2810, %r4200, 30;
	shl.b32 	%r2811, %r4199, 2;
	add.s32 	%r4201, %r2810, %r2811;
	shl.b32 	%r829, %r4200, 2;
	shr.u32 	%r2812, %r4201, 31;
	shr.u32 	%r2813, %r4199, 30;
	add.s32 	%r830, %r2812, %r2813;
	setp.eq.s32	%p327, %r2812, 0;
	mov.u32 	%r4202, %r820;
	mov.u32 	%r4203, %r829;
	@%p327 bra 	BB84_514;

	not.b32 	%r2814, %r4201;
	neg.s32 	%r831, %r829;
	setp.eq.s32	%p328, %r829, 0;
	selp.u32	%r2815, 1, 0, %p328;
	add.s32 	%r4201, %r2815, %r2814;
	xor.b32  	%r833, %r820, -2147483648;
	mov.u32 	%r4202, %r833;
	mov.u32 	%r4203, %r831;

BB84_514:
	mov.u32 	%r835, %r4202;
	neg.s32 	%r2816, %r830;
	setp.eq.s32	%p329, %r820, 0;
	selp.b32	%r4206, %r830, %r2816, %p329;
	clz.b32 	%r4205, %r4201;
	setp.eq.s32	%p330, %r4205, 0;
	shl.b32 	%r2817, %r4201, %r4205;
	mov.u32 	%r2818, 32;
	sub.s32 	%r2819, %r2818, %r4205;
	shr.u32 	%r2820, %r4203, %r2819;
	add.s32 	%r2821, %r2820, %r2817;
	selp.b32	%r839, %r4201, %r2821, %p330;
	mov.u32 	%r2822, -921707870;
	mul.hi.u32 	%r4204, %r839, %r2822;
	setp.lt.s32	%p331, %r4204, 1;
	@%p331 bra 	BB84_516;

	mul.lo.s32 	%r2823, %r839, -921707870;
	shr.u32 	%r2824, %r2823, 31;
	shl.b32 	%r2825, %r4204, 1;
	add.s32 	%r4204, %r2824, %r2825;
	add.s32 	%r4205, %r4205, 1;

BB84_516:
	mov.u32 	%r2826, 126;
	sub.s32 	%r2827, %r2826, %r4205;
	shl.b32 	%r2828, %r2827, 23;
	add.s32 	%r2829, %r4204, 1;
	shr.u32 	%r2830, %r2829, 7;
	add.s32 	%r2831, %r2830, 1;
	shr.u32 	%r2832, %r2831, 1;
	add.s32 	%r2833, %r2832, %r2828;
	or.b32  	%r2834, %r2833, %r835;
	mov.b32 	 %f2340, %r2834;

BB84_517:
	mul.rn.f32 	%f433, %f2340, %f2340;
	and.b32  	%r846, %r4206, 1;
	setp.eq.s32	%p332, %r846, 0;
	@%p332 bra 	BB84_519;

	mov.f32 	%f1583, 0fBAB6061A;
	mov.f32 	%f1584, 0f37CCF5CE;
	fma.rn.f32 	%f2341, %f1584, %f433, %f1583;
	bra.uni 	BB84_520;

BB84_519:
	mov.f32 	%f1585, 0f3C08839E;
	mov.f32 	%f1586, 0fB94CA1F9;
	fma.rn.f32 	%f2341, %f1586, %f433, %f1585;

BB84_520:
	@%p332 bra 	BB84_522;

	mov.f32 	%f1587, 0f3D2AAAA5;
	fma.rn.f32 	%f1588, %f2341, %f433, %f1587;
	mov.f32 	%f1589, 0fBF000000;
	fma.rn.f32 	%f2342, %f1588, %f433, %f1589;
	bra.uni 	BB84_523;

BB84_522:
	mov.f32 	%f1590, 0fBE2AAAA3;
	fma.rn.f32 	%f1591, %f2341, %f433, %f1590;
	mov.f32 	%f1592, 0f00000000;
	fma.rn.f32 	%f2342, %f1591, %f433, %f1592;

BB84_523:
	fma.rn.f32 	%f2343, %f2342, %f2340, %f2340;
	@%p332 bra 	BB84_525;

	mov.f32 	%f1593, 0f3F800000;
	fma.rn.f32 	%f2343, %f2342, %f433, %f1593;

BB84_525:
	and.b32  	%r2835, %r4206, 2;
	setp.eq.s32	%p335, %r2835, 0;
	@%p335 bra 	BB84_527;

	mov.f32 	%f1594, 0f00000000;
	mov.f32 	%f1595, 0fBF800000;
	fma.rn.f32 	%f2343, %f2343, %f1595, %f1594;

BB84_527:
	mul.f32 	%f445, %f2324, %f2343;
	mov.f32 	%f2351, %f13;
	@%p63 bra 	BB84_529;

	mov.f32 	%f1596, 0f00000000;
	mul.rn.f32 	%f2351, %f13, %f1596;

BB84_529:
	mul.f32 	%f1597, %f2351, 0f3F22F983;
	cvt.rni.s32.f32	%r4216, %f1597;
	cvt.rn.f32.s32	%f1598, %r4216;
	neg.f32 	%f1599, %f1598;
	fma.rn.f32 	%f1601, %f1599, %f1069, %f2351;
	fma.rn.f32 	%f1603, %f1599, %f1071, %f1601;
	fma.rn.f32 	%f2359, %f1599, %f1073, %f1603;
	abs.f32 	%f1605, %f2351;
	setp.leu.f32	%p337, %f1605, 0f47CE4780;
	@%p337 bra 	BB84_539;

	mov.b32 	 %r848, %f2351;
	shr.u32 	%r849, %r848, 23;
	bfe.u32 	%r2838, %r848, 23, 8;
	add.s32 	%r2839, %r2838, -128;
	shl.b32 	%r2840, %r848, 8;
	or.b32  	%r850, %r2840, -2147483648;
	shr.u32 	%r851, %r2839, 5;
	mov.u32 	%r4208, 0;
	mov.u64 	%rd371, __cudart_i2opi_f;
	mov.u32 	%r4207, -6;
	mov.u64 	%rd467, %rd1;

BB84_531:
	.pragma "nounroll";
	ld.const.u32 	%r2843, [%rd371];
	// inline asm
	{
	mad.lo.cc.u32   %r2841, %r2843, %r850, %r4208;
	madc.hi.u32     %r4208, %r2843, %r850,  0;
	}
	// inline asm
	st.local.u32 	[%rd467], %r2841;
	add.s64 	%rd467, %rd467, 4;
	add.s64 	%rd371, %rd371, 4;
	add.s32 	%r4207, %r4207, 1;
	setp.ne.s32	%p338, %r4207, 0;
	@%p338 bra 	BB84_531;

	and.b32  	%r856, %r848, -2147483648;
	st.local.u32 	[%rd3], %r4208;
	mov.u32 	%r2846, 6;
	sub.s32 	%r2847, %r2846, %r851;
	mul.wide.s32 	%rd295, %r2847, 4;
	add.s64 	%rd123, %rd1, %rd295;
	ld.local.u32 	%r4209, [%rd123];
	ld.local.u32 	%r4210, [%rd123+-4];
	and.b32  	%r859, %r849, 31;
	setp.eq.s32	%p339, %r859, 0;
	@%p339 bra 	BB84_534;

	mov.u32 	%r2848, 32;
	sub.s32 	%r2849, %r2848, %r859;
	shr.u32 	%r2850, %r4210, %r2849;
	shl.b32 	%r2851, %r4209, %r859;
	add.s32 	%r4209, %r2850, %r2851;
	ld.local.u32 	%r2852, [%rd123+-8];
	shr.u32 	%r2853, %r2852, %r2849;
	shl.b32 	%r2854, %r4210, %r859;
	add.s32 	%r4210, %r2853, %r2854;

BB84_534:
	shr.u32 	%r2855, %r4210, 30;
	shl.b32 	%r2856, %r4209, 2;
	add.s32 	%r4211, %r2855, %r2856;
	shl.b32 	%r865, %r4210, 2;
	shr.u32 	%r2857, %r4211, 31;
	shr.u32 	%r2858, %r4209, 30;
	add.s32 	%r866, %r2857, %r2858;
	setp.eq.s32	%p340, %r2857, 0;
	mov.u32 	%r4212, %r856;
	mov.u32 	%r4213, %r865;
	@%p340 bra 	BB84_536;

	not.b32 	%r2859, %r4211;
	neg.s32 	%r867, %r865;
	setp.eq.s32	%p341, %r865, 0;
	selp.u32	%r2860, 1, 0, %p341;
	add.s32 	%r4211, %r2860, %r2859;
	xor.b32  	%r869, %r856, -2147483648;
	mov.u32 	%r4212, %r869;
	mov.u32 	%r4213, %r867;

BB84_536:
	mov.u32 	%r871, %r4212;
	neg.s32 	%r2861, %r866;
	setp.eq.s32	%p342, %r856, 0;
	selp.b32	%r4216, %r866, %r2861, %p342;
	clz.b32 	%r4215, %r4211;
	setp.eq.s32	%p343, %r4215, 0;
	shl.b32 	%r2862, %r4211, %r4215;
	mov.u32 	%r2863, 32;
	sub.s32 	%r2864, %r2863, %r4215;
	shr.u32 	%r2865, %r4213, %r2864;
	add.s32 	%r2866, %r2865, %r2862;
	selp.b32	%r875, %r4211, %r2866, %p343;
	mov.u32 	%r2867, -921707870;
	mul.hi.u32 	%r4214, %r875, %r2867;
	setp.lt.s32	%p344, %r4214, 1;
	@%p344 bra 	BB84_538;

	mul.lo.s32 	%r2868, %r875, -921707870;
	shr.u32 	%r2869, %r2868, 31;
	shl.b32 	%r2870, %r4214, 1;
	add.s32 	%r4214, %r2869, %r2870;
	add.s32 	%r4215, %r4215, 1;

BB84_538:
	mov.u32 	%r2871, 126;
	sub.s32 	%r2872, %r2871, %r4215;
	shl.b32 	%r2873, %r2872, 23;
	add.s32 	%r2874, %r4214, 1;
	shr.u32 	%r2875, %r2874, 7;
	add.s32 	%r2876, %r2875, 1;
	shr.u32 	%r2877, %r2876, 1;
	add.s32 	%r2878, %r2877, %r2873;
	or.b32  	%r2879, %r2878, %r871;
	mov.b32 	 %f2359, %r2879;

BB84_539:
	mul.rn.f32 	%f451, %f2359, %f2359;
	add.s32 	%r882, %r4216, 1;
	and.b32  	%r883, %r882, 1;
	setp.eq.s32	%p345, %r883, 0;
	@%p345 bra 	BB84_541;

	mov.f32 	%f1606, 0fBAB6061A;
	mov.f32 	%f1607, 0f37CCF5CE;
	fma.rn.f32 	%f2360, %f1607, %f451, %f1606;
	bra.uni 	BB84_542;

BB84_541:
	mov.f32 	%f1608, 0f3C08839E;
	mov.f32 	%f1609, 0fB94CA1F9;
	fma.rn.f32 	%f2360, %f1609, %f451, %f1608;

BB84_542:
	@%p345 bra 	BB84_544;

	mov.f32 	%f1610, 0f3D2AAAA5;
	fma.rn.f32 	%f1611, %f2360, %f451, %f1610;
	mov.f32 	%f1612, 0fBF000000;
	fma.rn.f32 	%f2361, %f1611, %f451, %f1612;
	bra.uni 	BB84_545;

BB84_544:
	mov.f32 	%f1613, 0fBE2AAAA3;
	fma.rn.f32 	%f1614, %f2360, %f451, %f1613;
	mov.f32 	%f1615, 0f00000000;
	fma.rn.f32 	%f2361, %f1614, %f451, %f1615;

BB84_545:
	fma.rn.f32 	%f2362, %f2361, %f2359, %f2359;
	@%p345 bra 	BB84_547;

	mov.f32 	%f1616, 0f3F800000;
	fma.rn.f32 	%f2362, %f2361, %f451, %f1616;

BB84_547:
	and.b32  	%r2880, %r882, 2;
	setp.eq.s32	%p348, %r2880, 0;
	@%p348 bra 	BB84_549;

	mov.f32 	%f1617, 0f00000000;
	mov.f32 	%f1618, 0fBF800000;
	fma.rn.f32 	%f2362, %f2362, %f1618, %f1617;

BB84_549:
	fma.rn.f32 	%f463, %f445, %f2362, %f410;
	neg.f32 	%f1619, %f6;
	add.f32 	%f1620, %f145, %f6;
	abs.f32 	%f1621, %f1620;
	cvt.f64.f32	%fd2, %f1621;
	abs.f32 	%f1622, %f1619;
	abs.f32 	%f1623, %f145;
	max.f32 	%f1624, %f1623, %f1622;
	cvt.f64.f32	%fd3, %f1624;
	mul.f64 	%fd4, %fd3, 0d3EE9000000000000;
	setp.geu.f64	%p349, %fd2, %fd4;
	mov.f32 	%f2377, %f34;
	@%p349 bra 	BB84_554;

	neg.f32 	%f1625, %f8;
	add.f32 	%f1626, %f251, %f8;
	abs.f32 	%f1627, %f1626;
	cvt.f64.f32	%fd5, %f1627;
	abs.f32 	%f1628, %f1625;
	abs.f32 	%f1629, %f251;
	max.f32 	%f1630, %f1629, %f1628;
	cvt.f64.f32	%fd6, %f1630;
	mul.f64 	%fd7, %fd6, 0d3EE9000000000000;
	setp.geu.f64	%p350, %fd5, %fd7;
	mov.f32 	%f2377, %f34;
	@%p350 bra 	BB84_554;

	neg.f32 	%f1631, %f7;
	add.f32 	%f1632, %f357, %f7;
	abs.f32 	%f1633, %f1632;
	cvt.f64.f32	%fd8, %f1633;
	abs.f32 	%f1634, %f1631;
	abs.f32 	%f1635, %f357;
	max.f32 	%f1636, %f1635, %f1634;
	cvt.f64.f32	%fd9, %f1636;
	mul.f64 	%fd10, %fd9, 0d3EE9000000000000;
	setp.geu.f64	%p351, %fd8, %fd10;
	mov.f32 	%f2377, %f34;
	@%p351 bra 	BB84_554;

	neg.f32 	%f1637, %f9;
	add.f32 	%f1638, %f463, %f9;
	abs.f32 	%f1639, %f1638;
	cvt.f64.f32	%fd11, %f1639;
	abs.f32 	%f1640, %f1637;
	abs.f32 	%f1641, %f463;
	max.f32 	%f1642, %f1641, %f1640;
	cvt.f64.f32	%fd12, %f1642;
	mul.f64 	%fd13, %fd12, 0d3EE9000000000000;
	setp.geu.f64	%p352, %fd11, %fd13;
	mov.f32 	%f2377, %f34;
	@%p352 bra 	BB84_554;

	cvt.f64.f32	%fd14, %f34;
	setp.ltu.f32	%p353, %f34, 0f00000000;
	selp.f64	%fd15, 0dC00921FB54442D18, 0d400921FB54442D18, %p353;
	sub.f64 	%fd16, %fd14, %fd15;
	cvt.rn.f32.f64	%f2377, %fd16;

BB84_554:
	mov.f32 	%f2389, %f13;
	mov.f32 	%f2388, %f36;
	mov.f32 	%f2387, %f2377;

BB84_555:
	ld.param.u32 	%r3970, [actfunc_quat_multi_float_param_1];
	cvt.rn.f64.s32	%fd17, %r3970;
	mov.f64 	%fd18, 0d401921FB54442D18;
	div.rn.f64 	%fd19, %fd18, %fd17;
	cvt.rn.f32.f64	%f474, %fd19;
	mul.f32 	%f475, %f474, 0f3F000000;
	sub.f32 	%f1643, %f2387, %f475;
	add.f32 	%f1644, %f1643, 0f40490FDB;
	div.rn.f32 	%f476, %f1644, %f474;
	abs.f32 	%f1645, %f476;
	mov.b32 	 %r2881, %f476;
	and.b32  	%r2882, %r2881, -2147483648;
	or.b32  	%r2883, %r2882, 1056964608;
	mov.b32 	 %f1646, %r2883;
	add.f32 	%f1647, %f476, %f1646;
	cvt.rzi.f32.f32	%f1648, %f1647;
	setp.gt.f32	%p354, %f1645, 0f4B000000;
	selp.f32	%f2390, %f476, %f1648, %p354;
	setp.geu.f32	%p355, %f1645, 0f3F000000;
	@%p355 bra 	BB84_557;

	cvt.rzi.f32.f32	%f2390, %f476;

BB84_557:
	ld.param.u32 	%r3971, [actfunc_quat_multi_float_param_2];
	fma.rn.f32 	%f1649, %f474, %f2390, 0fC0490FDB;
	add.f32 	%f480, %f475, %f1649;
	cvt.rn.f64.s32	%fd20, %r3971;
	div.rn.f64 	%fd22, %fd18, %fd20;
	mul.f64 	%fd23, %fd22, 0d3FE0000000000000;
	cvt.rn.f32.f64	%f481, %fd23;
	mul.f32 	%f482, %f481, 0f3F000000;
	sub.f32 	%f1650, %f2388, %f482;
	add.f32 	%f1651, %f1650, 0f3FC90FDB;
	div.rn.f32 	%f483, %f1651, %f481;
	abs.f32 	%f1652, %f483;
	mov.b32 	 %r2884, %f483;
	and.b32  	%r2885, %r2884, -2147483648;
	or.b32  	%r2886, %r2885, 1056964608;
	mov.b32 	 %f1653, %r2886;
	add.f32 	%f1654, %f483, %f1653;
	cvt.rzi.f32.f32	%f1655, %f1654;
	setp.gt.f32	%p356, %f1652, 0f4B000000;
	selp.f32	%f2391, %f483, %f1655, %p356;
	setp.geu.f32	%p357, %f1652, 0f3F000000;
	@%p357 bra 	BB84_559;

	cvt.rzi.f32.f32	%f2391, %f483;

BB84_559:
	ld.param.u32 	%r3972, [actfunc_quat_multi_float_param_3];
	fma.rn.f32 	%f1656, %f481, %f2391, 0fBFC90FDB;
	add.f32 	%f487, %f482, %f1656;
	cvt.rn.f64.s32	%fd24, %r3972;
	div.rn.f64 	%fd26, %fd18, %fd24;
	mul.f64 	%fd27, %fd26, 0d3FD0000000000000;
	cvt.rn.f32.f64	%f488, %fd27;
	mul.f32 	%f489, %f488, 0f3F000000;
	sub.f32 	%f1657, %f2389, %f489;
	add.f32 	%f1658, %f1657, 0f3F490FDB;
	div.rn.f32 	%f490, %f1658, %f488;
	abs.f32 	%f1659, %f490;
	mov.b32 	 %r2887, %f490;
	and.b32  	%r2888, %r2887, -2147483648;
	or.b32  	%r2889, %r2888, 1056964608;
	mov.b32 	 %f1660, %r2889;
	add.f32 	%f1661, %f490, %f1660;
	cvt.rzi.f32.f32	%f1662, %f1661;
	setp.gt.f32	%p358, %f1659, 0f4B000000;
	selp.f32	%f2392, %f490, %f1662, %p358;
	setp.geu.f32	%p359, %f1659, 0f3F000000;
	@%p359 bra 	BB84_561;

	cvt.rzi.f32.f32	%f2392, %f490;

BB84_561:
	fma.rn.f32 	%f1663, %f488, %f2392, 0fBF490FDB;
	add.f32 	%f494, %f489, %f1663;
	abs.f32 	%f495, %f480;
	setp.neu.f32	%p360, %f495, 0f7F800000;
	mov.f32 	%f2491, %f480;
	@%p360 bra 	BB84_563;

	mov.f32 	%f1664, 0f00000000;
	mul.rn.f32 	%f496, %f480, %f1664;
	mov.f32 	%f2491, %f496;

BB84_563:
	mov.f32 	%f497, %f2491;
	mul.f32 	%f1665, %f497, 0f3F22F983;
	cvt.rni.s32.f32	%r4226, %f1665;
	cvt.rn.f32.s32	%f1666, %r4226;
	neg.f32 	%f1667, %f1666;
	mov.f32 	%f1668, 0f3FC90FDA;
	fma.rn.f32 	%f1669, %f1667, %f1668, %f497;
	mov.f32 	%f1670, 0f33A22168;
	fma.rn.f32 	%f1671, %f1667, %f1670, %f1669;
	mov.f32 	%f1672, 0f27C234C5;
	fma.rn.f32 	%f2393, %f1667, %f1672, %f1671;
	abs.f32 	%f1673, %f497;
	setp.leu.f32	%p361, %f1673, 0f47CE4780;
	@%p361 bra 	BB84_573;

	mov.b32 	 %r885, %f497;
	shr.u32 	%r886, %r885, 23;
	bfe.u32 	%r2892, %r885, 23, 8;
	add.s32 	%r2893, %r2892, -128;
	shl.b32 	%r2894, %r885, 8;
	or.b32  	%r887, %r2894, -2147483648;
	shr.u32 	%r888, %r2893, 5;
	mov.u32 	%r4218, 0;
	mov.u64 	%rd372, __cudart_i2opi_f;
	mov.u32 	%r4217, -6;
	mov.u64 	%rd466, %rd1;

BB84_565:
	.pragma "nounroll";
	ld.const.u32 	%r2897, [%rd372];
	// inline asm
	{
	mad.lo.cc.u32   %r2895, %r2897, %r887, %r4218;
	madc.hi.u32     %r4218, %r2897, %r887,  0;
	}
	// inline asm
	st.local.u32 	[%rd466], %r2895;
	add.s64 	%rd466, %rd466, 4;
	add.s64 	%rd372, %rd372, 4;
	add.s32 	%r4217, %r4217, 1;
	setp.ne.s32	%p362, %r4217, 0;
	@%p362 bra 	BB84_565;

	and.b32  	%r893, %r885, -2147483648;
	st.local.u32 	[%rd3], %r4218;
	mov.u32 	%r2900, 6;
	sub.s32 	%r2901, %r2900, %r888;
	mul.wide.s32 	%rd297, %r2901, 4;
	add.s64 	%rd128, %rd1, %rd297;
	ld.local.u32 	%r4219, [%rd128];
	ld.local.u32 	%r4220, [%rd128+-4];
	and.b32  	%r896, %r886, 31;
	setp.eq.s32	%p363, %r896, 0;
	@%p363 bra 	BB84_568;

	mov.u32 	%r2902, 32;
	sub.s32 	%r2903, %r2902, %r896;
	shr.u32 	%r2904, %r4220, %r2903;
	shl.b32 	%r2905, %r4219, %r896;
	add.s32 	%r4219, %r2904, %r2905;
	ld.local.u32 	%r2906, [%rd128+-8];
	shr.u32 	%r2907, %r2906, %r2903;
	shl.b32 	%r2908, %r4220, %r896;
	add.s32 	%r4220, %r2907, %r2908;

BB84_568:
	shr.u32 	%r2909, %r4220, 30;
	shl.b32 	%r2910, %r4219, 2;
	add.s32 	%r4221, %r2909, %r2910;
	shl.b32 	%r902, %r4220, 2;
	shr.u32 	%r2911, %r4221, 31;
	shr.u32 	%r2912, %r4219, 30;
	add.s32 	%r903, %r2911, %r2912;
	setp.eq.s32	%p364, %r2911, 0;
	mov.u32 	%r4222, %r893;
	mov.u32 	%r4223, %r902;
	@%p364 bra 	BB84_570;

	not.b32 	%r2913, %r4221;
	neg.s32 	%r904, %r902;
	setp.eq.s32	%p365, %r902, 0;
	selp.u32	%r2914, 1, 0, %p365;
	add.s32 	%r4221, %r2914, %r2913;
	xor.b32  	%r906, %r893, -2147483648;
	mov.u32 	%r4222, %r906;
	mov.u32 	%r4223, %r904;

BB84_570:
	mov.u32 	%r908, %r4222;
	neg.s32 	%r2915, %r903;
	setp.eq.s32	%p366, %r893, 0;
	selp.b32	%r4226, %r903, %r2915, %p366;
	clz.b32 	%r4225, %r4221;
	setp.eq.s32	%p367, %r4225, 0;
	shl.b32 	%r2916, %r4221, %r4225;
	mov.u32 	%r2917, 32;
	sub.s32 	%r2918, %r2917, %r4225;
	shr.u32 	%r2919, %r4223, %r2918;
	add.s32 	%r2920, %r2919, %r2916;
	selp.b32	%r912, %r4221, %r2920, %p367;
	mov.u32 	%r2921, -921707870;
	mul.hi.u32 	%r4224, %r912, %r2921;
	setp.lt.s32	%p368, %r4224, 1;
	@%p368 bra 	BB84_572;

	mul.lo.s32 	%r2922, %r912, -921707870;
	shr.u32 	%r2923, %r2922, 31;
	shl.b32 	%r2924, %r4224, 1;
	add.s32 	%r4224, %r2923, %r2924;
	add.s32 	%r4225, %r4225, 1;

BB84_572:
	mov.u32 	%r2925, 126;
	sub.s32 	%r2926, %r2925, %r4225;
	shl.b32 	%r2927, %r2926, 23;
	add.s32 	%r2928, %r4224, 1;
	shr.u32 	%r2929, %r2928, 7;
	add.s32 	%r2930, %r2929, 1;
	shr.u32 	%r2931, %r2930, 1;
	add.s32 	%r2932, %r2931, %r2927;
	or.b32  	%r2933, %r2932, %r908;
	mov.b32 	 %f2393, %r2933;

BB84_573:
	mul.rn.f32 	%f501, %f2393, %f2393;
	add.s32 	%r919, %r4226, 1;
	and.b32  	%r920, %r919, 1;
	setp.eq.s32	%p369, %r920, 0;
	@%p369 bra 	BB84_575;

	mov.f32 	%f1674, 0fBAB6061A;
	mov.f32 	%f1675, 0f37CCF5CE;
	fma.rn.f32 	%f2394, %f1675, %f501, %f1674;
	bra.uni 	BB84_576;

BB84_575:
	mov.f32 	%f1676, 0f3C08839E;
	mov.f32 	%f1677, 0fB94CA1F9;
	fma.rn.f32 	%f2394, %f1677, %f501, %f1676;

BB84_576:
	@%p369 bra 	BB84_578;

	mov.f32 	%f1678, 0f3D2AAAA5;
	fma.rn.f32 	%f1679, %f2394, %f501, %f1678;
	mov.f32 	%f1680, 0fBF000000;
	fma.rn.f32 	%f2395, %f1679, %f501, %f1680;
	bra.uni 	BB84_579;

BB84_578:
	mov.f32 	%f1681, 0fBE2AAAA3;
	fma.rn.f32 	%f1682, %f2394, %f501, %f1681;
	mov.f32 	%f1683, 0f00000000;
	fma.rn.f32 	%f2395, %f1682, %f501, %f1683;

BB84_579:
	fma.rn.f32 	%f2396, %f2395, %f2393, %f2393;
	@%p369 bra 	BB84_581;

	mov.f32 	%f1684, 0f3F800000;
	fma.rn.f32 	%f2396, %f2395, %f501, %f1684;

BB84_581:
	and.b32  	%r2934, %r919, 2;
	setp.eq.s32	%p372, %r2934, 0;
	@%p372 bra 	BB84_583;

	mov.f32 	%f1685, 0f00000000;
	mov.f32 	%f1686, 0fBF800000;
	fma.rn.f32 	%f2396, %f2396, %f1686, %f1685;

BB84_583:
	abs.f32 	%f513, %f487;
	setp.neu.f32	%p373, %f513, 0f7F800000;
	mov.f32 	%f2510, %f487;
	@%p373 bra 	BB84_585;

	mov.f32 	%f1687, 0f00000000;
	mul.rn.f32 	%f514, %f487, %f1687;
	mov.f32 	%f2510, %f514;

BB84_585:
	mov.f32 	%f515, %f2510;
	mul.f32 	%f1688, %f515, 0f3F22F983;
	cvt.rni.s32.f32	%r4236, %f1688;
	cvt.rn.f32.s32	%f1689, %r4236;
	neg.f32 	%f1690, %f1689;
	fma.rn.f32 	%f1692, %f1690, %f1668, %f515;
	fma.rn.f32 	%f1694, %f1690, %f1670, %f1692;
	fma.rn.f32 	%f2397, %f1690, %f1672, %f1694;
	abs.f32 	%f1696, %f515;
	setp.leu.f32	%p374, %f1696, 0f47CE4780;
	@%p374 bra 	BB84_595;

	mov.b32 	 %r922, %f515;
	shr.u32 	%r923, %r922, 23;
	bfe.u32 	%r2937, %r922, 23, 8;
	add.s32 	%r2938, %r2937, -128;
	shl.b32 	%r2939, %r922, 8;
	or.b32  	%r924, %r2939, -2147483648;
	shr.u32 	%r925, %r2938, 5;
	mov.u32 	%r4228, 0;
	mov.u64 	%rd373, __cudart_i2opi_f;
	mov.u32 	%r4227, -6;
	mov.u64 	%rd465, %rd1;

BB84_587:
	.pragma "nounroll";
	ld.const.u32 	%r2942, [%rd373];
	// inline asm
	{
	mad.lo.cc.u32   %r2940, %r2942, %r924, %r4228;
	madc.hi.u32     %r4228, %r2942, %r924,  0;
	}
	// inline asm
	st.local.u32 	[%rd465], %r2940;
	add.s64 	%rd465, %rd465, 4;
	add.s64 	%rd373, %rd373, 4;
	add.s32 	%r4227, %r4227, 1;
	setp.ne.s32	%p375, %r4227, 0;
	@%p375 bra 	BB84_587;

	and.b32  	%r930, %r922, -2147483648;
	st.local.u32 	[%rd3], %r4228;
	mov.u32 	%r2945, 6;
	sub.s32 	%r2946, %r2945, %r925;
	mul.wide.s32 	%rd299, %r2946, 4;
	add.s64 	%rd133, %rd1, %rd299;
	ld.local.u32 	%r4229, [%rd133];
	ld.local.u32 	%r4230, [%rd133+-4];
	and.b32  	%r933, %r923, 31;
	setp.eq.s32	%p376, %r933, 0;
	@%p376 bra 	BB84_590;

	mov.u32 	%r2947, 32;
	sub.s32 	%r2948, %r2947, %r933;
	shr.u32 	%r2949, %r4230, %r2948;
	shl.b32 	%r2950, %r4229, %r933;
	add.s32 	%r4229, %r2949, %r2950;
	ld.local.u32 	%r2951, [%rd133+-8];
	shr.u32 	%r2952, %r2951, %r2948;
	shl.b32 	%r2953, %r4230, %r933;
	add.s32 	%r4230, %r2952, %r2953;

BB84_590:
	shr.u32 	%r2954, %r4230, 30;
	shl.b32 	%r2955, %r4229, 2;
	add.s32 	%r4231, %r2954, %r2955;
	shl.b32 	%r939, %r4230, 2;
	shr.u32 	%r2956, %r4231, 31;
	shr.u32 	%r2957, %r4229, 30;
	add.s32 	%r940, %r2956, %r2957;
	setp.eq.s32	%p377, %r2956, 0;
	mov.u32 	%r4232, %r930;
	mov.u32 	%r4233, %r939;
	@%p377 bra 	BB84_592;

	not.b32 	%r2958, %r4231;
	neg.s32 	%r941, %r939;
	setp.eq.s32	%p378, %r939, 0;
	selp.u32	%r2959, 1, 0, %p378;
	add.s32 	%r4231, %r2959, %r2958;
	xor.b32  	%r943, %r930, -2147483648;
	mov.u32 	%r4232, %r943;
	mov.u32 	%r4233, %r941;

BB84_592:
	mov.u32 	%r945, %r4232;
	neg.s32 	%r2960, %r940;
	setp.eq.s32	%p379, %r930, 0;
	selp.b32	%r4236, %r940, %r2960, %p379;
	clz.b32 	%r4235, %r4231;
	setp.eq.s32	%p380, %r4235, 0;
	shl.b32 	%r2961, %r4231, %r4235;
	mov.u32 	%r2962, 32;
	sub.s32 	%r2963, %r2962, %r4235;
	shr.u32 	%r2964, %r4233, %r2963;
	add.s32 	%r2965, %r2964, %r2961;
	selp.b32	%r949, %r4231, %r2965, %p380;
	mov.u32 	%r2966, -921707870;
	mul.hi.u32 	%r4234, %r949, %r2966;
	setp.lt.s32	%p381, %r4234, 1;
	@%p381 bra 	BB84_594;

	mul.lo.s32 	%r2967, %r949, -921707870;
	shr.u32 	%r2968, %r2967, 31;
	shl.b32 	%r2969, %r4234, 1;
	add.s32 	%r4234, %r2968, %r2969;
	add.s32 	%r4235, %r4235, 1;

BB84_594:
	mov.u32 	%r2970, 126;
	sub.s32 	%r2971, %r2970, %r4235;
	shl.b32 	%r2972, %r2971, 23;
	add.s32 	%r2973, %r4234, 1;
	shr.u32 	%r2974, %r2973, 7;
	add.s32 	%r2975, %r2974, 1;
	shr.u32 	%r2976, %r2975, 1;
	add.s32 	%r2977, %r2976, %r2972;
	or.b32  	%r2978, %r2977, %r945;
	mov.b32 	 %f2397, %r2978;

BB84_595:
	mul.rn.f32 	%f519, %f2397, %f2397;
	add.s32 	%r956, %r4236, 1;
	and.b32  	%r957, %r956, 1;
	setp.eq.s32	%p382, %r957, 0;
	@%p382 bra 	BB84_597;

	mov.f32 	%f1697, 0fBAB6061A;
	mov.f32 	%f1698, 0f37CCF5CE;
	fma.rn.f32 	%f2398, %f1698, %f519, %f1697;
	bra.uni 	BB84_598;

BB84_597:
	mov.f32 	%f1699, 0f3C08839E;
	mov.f32 	%f1700, 0fB94CA1F9;
	fma.rn.f32 	%f2398, %f1700, %f519, %f1699;

BB84_598:
	@%p382 bra 	BB84_600;

	mov.f32 	%f1701, 0f3D2AAAA5;
	fma.rn.f32 	%f1702, %f2398, %f519, %f1701;
	mov.f32 	%f1703, 0fBF000000;
	fma.rn.f32 	%f2399, %f1702, %f519, %f1703;
	bra.uni 	BB84_601;

BB84_600:
	mov.f32 	%f1704, 0fBE2AAAA3;
	fma.rn.f32 	%f1705, %f2398, %f519, %f1704;
	mov.f32 	%f1706, 0f00000000;
	fma.rn.f32 	%f2399, %f1705, %f519, %f1706;

BB84_601:
	fma.rn.f32 	%f2400, %f2399, %f2397, %f2397;
	@%p382 bra 	BB84_603;

	mov.f32 	%f1707, 0f3F800000;
	fma.rn.f32 	%f2400, %f2399, %f519, %f1707;

BB84_603:
	and.b32  	%r2979, %r956, 2;
	setp.eq.s32	%p385, %r2979, 0;
	@%p385 bra 	BB84_605;

	mov.f32 	%f1708, 0f00000000;
	mov.f32 	%f1709, 0fBF800000;
	fma.rn.f32 	%f2400, %f2400, %f1709, %f1708;

BB84_605:
	mul.f32 	%f531, %f2396, %f2400;
	abs.f32 	%f532, %f494;
	setp.neu.f32	%p386, %f532, 0f7F800000;
	mov.f32 	%f2529, %f494;
	@%p386 bra 	BB84_607;

	mov.f32 	%f1710, 0f00000000;
	mul.rn.f32 	%f533, %f494, %f1710;
	mov.f32 	%f2529, %f533;

BB84_607:
	mov.f32 	%f534, %f2529;
	mul.f32 	%f1711, %f534, 0f3F22F983;
	cvt.rni.s32.f32	%r4246, %f1711;
	cvt.rn.f32.s32	%f1712, %r4246;
	neg.f32 	%f1713, %f1712;
	fma.rn.f32 	%f1715, %f1713, %f1668, %f534;
	fma.rn.f32 	%f1717, %f1713, %f1670, %f1715;
	fma.rn.f32 	%f2401, %f1713, %f1672, %f1717;
	abs.f32 	%f1719, %f534;
	setp.leu.f32	%p387, %f1719, 0f47CE4780;
	@%p387 bra 	BB84_617;

	mov.b32 	 %r959, %f534;
	shr.u32 	%r960, %r959, 23;
	bfe.u32 	%r2982, %r959, 23, 8;
	add.s32 	%r2983, %r2982, -128;
	shl.b32 	%r2984, %r959, 8;
	or.b32  	%r961, %r2984, -2147483648;
	shr.u32 	%r962, %r2983, 5;
	mov.u32 	%r4238, 0;
	mov.u64 	%rd374, __cudart_i2opi_f;
	mov.u32 	%r4237, -6;
	mov.u64 	%rd464, %rd1;

BB84_609:
	.pragma "nounroll";
	ld.const.u32 	%r2987, [%rd374];
	// inline asm
	{
	mad.lo.cc.u32   %r2985, %r2987, %r961, %r4238;
	madc.hi.u32     %r4238, %r2987, %r961,  0;
	}
	// inline asm
	st.local.u32 	[%rd464], %r2985;
	add.s64 	%rd464, %rd464, 4;
	add.s64 	%rd374, %rd374, 4;
	add.s32 	%r4237, %r4237, 1;
	setp.ne.s32	%p388, %r4237, 0;
	@%p388 bra 	BB84_609;

	and.b32  	%r967, %r959, -2147483648;
	st.local.u32 	[%rd3], %r4238;
	mov.u32 	%r2990, 6;
	sub.s32 	%r2991, %r2990, %r962;
	mul.wide.s32 	%rd301, %r2991, 4;
	add.s64 	%rd138, %rd1, %rd301;
	ld.local.u32 	%r4239, [%rd138];
	ld.local.u32 	%r4240, [%rd138+-4];
	and.b32  	%r970, %r960, 31;
	setp.eq.s32	%p389, %r970, 0;
	@%p389 bra 	BB84_612;

	mov.u32 	%r2992, 32;
	sub.s32 	%r2993, %r2992, %r970;
	shr.u32 	%r2994, %r4240, %r2993;
	shl.b32 	%r2995, %r4239, %r970;
	add.s32 	%r4239, %r2994, %r2995;
	ld.local.u32 	%r2996, [%rd138+-8];
	shr.u32 	%r2997, %r2996, %r2993;
	shl.b32 	%r2998, %r4240, %r970;
	add.s32 	%r4240, %r2997, %r2998;

BB84_612:
	shr.u32 	%r2999, %r4240, 30;
	shl.b32 	%r3000, %r4239, 2;
	add.s32 	%r4241, %r2999, %r3000;
	shl.b32 	%r976, %r4240, 2;
	shr.u32 	%r3001, %r4241, 31;
	shr.u32 	%r3002, %r4239, 30;
	add.s32 	%r977, %r3001, %r3002;
	setp.eq.s32	%p390, %r3001, 0;
	mov.u32 	%r4242, %r967;
	mov.u32 	%r4243, %r976;
	@%p390 bra 	BB84_614;

	not.b32 	%r3003, %r4241;
	neg.s32 	%r978, %r976;
	setp.eq.s32	%p391, %r976, 0;
	selp.u32	%r3004, 1, 0, %p391;
	add.s32 	%r4241, %r3004, %r3003;
	xor.b32  	%r980, %r967, -2147483648;
	mov.u32 	%r4242, %r980;
	mov.u32 	%r4243, %r978;

BB84_614:
	mov.u32 	%r982, %r4242;
	neg.s32 	%r3005, %r977;
	setp.eq.s32	%p392, %r967, 0;
	selp.b32	%r4246, %r977, %r3005, %p392;
	clz.b32 	%r4245, %r4241;
	setp.eq.s32	%p393, %r4245, 0;
	shl.b32 	%r3006, %r4241, %r4245;
	mov.u32 	%r3007, 32;
	sub.s32 	%r3008, %r3007, %r4245;
	shr.u32 	%r3009, %r4243, %r3008;
	add.s32 	%r3010, %r3009, %r3006;
	selp.b32	%r986, %r4241, %r3010, %p393;
	mov.u32 	%r3011, -921707870;
	mul.hi.u32 	%r4244, %r986, %r3011;
	setp.lt.s32	%p394, %r4244, 1;
	@%p394 bra 	BB84_616;

	mul.lo.s32 	%r3012, %r986, -921707870;
	shr.u32 	%r3013, %r3012, 31;
	shl.b32 	%r3014, %r4244, 1;
	add.s32 	%r4244, %r3013, %r3014;
	add.s32 	%r4245, %r4245, 1;

BB84_616:
	mov.u32 	%r3015, 126;
	sub.s32 	%r3016, %r3015, %r4245;
	shl.b32 	%r3017, %r3016, 23;
	add.s32 	%r3018, %r4244, 1;
	shr.u32 	%r3019, %r3018, 7;
	add.s32 	%r3020, %r3019, 1;
	shr.u32 	%r3021, %r3020, 1;
	add.s32 	%r3022, %r3021, %r3017;
	or.b32  	%r3023, %r3022, %r982;
	mov.b32 	 %f2401, %r3023;

BB84_617:
	mul.rn.f32 	%f538, %f2401, %f2401;
	add.s32 	%r993, %r4246, 1;
	and.b32  	%r994, %r993, 1;
	setp.eq.s32	%p395, %r994, 0;
	@%p395 bra 	BB84_619;

	mov.f32 	%f1720, 0fBAB6061A;
	mov.f32 	%f1721, 0f37CCF5CE;
	fma.rn.f32 	%f2402, %f1721, %f538, %f1720;
	bra.uni 	BB84_620;

BB84_619:
	mov.f32 	%f1722, 0f3C08839E;
	mov.f32 	%f1723, 0fB94CA1F9;
	fma.rn.f32 	%f2402, %f1723, %f538, %f1722;

BB84_620:
	@%p395 bra 	BB84_622;

	mov.f32 	%f1724, 0f3D2AAAA5;
	fma.rn.f32 	%f1725, %f2402, %f538, %f1724;
	mov.f32 	%f1726, 0fBF000000;
	fma.rn.f32 	%f2403, %f1725, %f538, %f1726;
	bra.uni 	BB84_623;

BB84_622:
	mov.f32 	%f1727, 0fBE2AAAA3;
	fma.rn.f32 	%f1728, %f2402, %f538, %f1727;
	mov.f32 	%f1729, 0f00000000;
	fma.rn.f32 	%f2403, %f1728, %f538, %f1729;

BB84_623:
	fma.rn.f32 	%f2404, %f2403, %f2401, %f2401;
	@%p395 bra 	BB84_625;

	mov.f32 	%f1730, 0f3F800000;
	fma.rn.f32 	%f2404, %f2403, %f538, %f1730;

BB84_625:
	and.b32  	%r3024, %r993, 2;
	setp.eq.s32	%p398, %r3024, 0;
	@%p398 bra 	BB84_627;

	mov.f32 	%f1731, 0f00000000;
	mov.f32 	%f1732, 0fBF800000;
	fma.rn.f32 	%f2404, %f2404, %f1732, %f1731;

BB84_627:
	mul.f32 	%f550, %f531, %f2404;
	mov.f32 	%f2490, %f480;
	@%p360 bra 	BB84_629;

	mov.f32 	%f1733, 0f00000000;
	mul.rn.f32 	%f2490, %f480, %f1733;

BB84_629:
	mul.f32 	%f1734, %f2490, 0f3F22F983;
	cvt.rni.s32.f32	%r4256, %f1734;
	cvt.rn.f32.s32	%f1735, %r4256;
	neg.f32 	%f1736, %f1735;
	fma.rn.f32 	%f1738, %f1736, %f1668, %f2490;
	fma.rn.f32 	%f1740, %f1736, %f1670, %f1738;
	fma.rn.f32 	%f2405, %f1736, %f1672, %f1740;
	abs.f32 	%f1742, %f2490;
	setp.leu.f32	%p400, %f1742, 0f47CE4780;
	@%p400 bra 	BB84_639;

	mov.b32 	 %r996, %f2490;
	shr.u32 	%r997, %r996, 23;
	bfe.u32 	%r3027, %r996, 23, 8;
	add.s32 	%r3028, %r3027, -128;
	shl.b32 	%r3029, %r996, 8;
	or.b32  	%r998, %r3029, -2147483648;
	shr.u32 	%r999, %r3028, 5;
	mov.u32 	%r4248, 0;
	mov.u64 	%rd375, __cudart_i2opi_f;
	mov.u32 	%r4247, -6;
	mov.u64 	%rd463, %rd1;

BB84_631:
	.pragma "nounroll";
	ld.const.u32 	%r3032, [%rd375];
	// inline asm
	{
	mad.lo.cc.u32   %r3030, %r3032, %r998, %r4248;
	madc.hi.u32     %r4248, %r3032, %r998,  0;
	}
	// inline asm
	st.local.u32 	[%rd463], %r3030;
	add.s64 	%rd463, %rd463, 4;
	add.s64 	%rd375, %rd375, 4;
	add.s32 	%r4247, %r4247, 1;
	setp.ne.s32	%p401, %r4247, 0;
	@%p401 bra 	BB84_631;

	and.b32  	%r1004, %r996, -2147483648;
	st.local.u32 	[%rd3], %r4248;
	mov.u32 	%r3035, 6;
	sub.s32 	%r3036, %r3035, %r999;
	mul.wide.s32 	%rd303, %r3036, 4;
	add.s64 	%rd143, %rd1, %rd303;
	ld.local.u32 	%r4249, [%rd143];
	ld.local.u32 	%r4250, [%rd143+-4];
	and.b32  	%r1007, %r997, 31;
	setp.eq.s32	%p402, %r1007, 0;
	@%p402 bra 	BB84_634;

	mov.u32 	%r3037, 32;
	sub.s32 	%r3038, %r3037, %r1007;
	shr.u32 	%r3039, %r4250, %r3038;
	shl.b32 	%r3040, %r4249, %r1007;
	add.s32 	%r4249, %r3039, %r3040;
	ld.local.u32 	%r3041, [%rd143+-8];
	shr.u32 	%r3042, %r3041, %r3038;
	shl.b32 	%r3043, %r4250, %r1007;
	add.s32 	%r4250, %r3042, %r3043;

BB84_634:
	shr.u32 	%r3044, %r4250, 30;
	shl.b32 	%r3045, %r4249, 2;
	add.s32 	%r4251, %r3044, %r3045;
	shl.b32 	%r1013, %r4250, 2;
	shr.u32 	%r3046, %r4251, 31;
	shr.u32 	%r3047, %r4249, 30;
	add.s32 	%r1014, %r3046, %r3047;
	setp.eq.s32	%p403, %r3046, 0;
	mov.u32 	%r4252, %r1004;
	mov.u32 	%r4253, %r1013;
	@%p403 bra 	BB84_636;

	not.b32 	%r3048, %r4251;
	neg.s32 	%r1015, %r1013;
	setp.eq.s32	%p404, %r1013, 0;
	selp.u32	%r3049, 1, 0, %p404;
	add.s32 	%r4251, %r3049, %r3048;
	xor.b32  	%r1017, %r1004, -2147483648;
	mov.u32 	%r4252, %r1017;
	mov.u32 	%r4253, %r1015;

BB84_636:
	mov.u32 	%r1019, %r4252;
	neg.s32 	%r3050, %r1014;
	setp.eq.s32	%p405, %r1004, 0;
	selp.b32	%r4256, %r1014, %r3050, %p405;
	clz.b32 	%r4255, %r4251;
	setp.eq.s32	%p406, %r4255, 0;
	shl.b32 	%r3051, %r4251, %r4255;
	mov.u32 	%r3052, 32;
	sub.s32 	%r3053, %r3052, %r4255;
	shr.u32 	%r3054, %r4253, %r3053;
	add.s32 	%r3055, %r3054, %r3051;
	selp.b32	%r1023, %r4251, %r3055, %p406;
	mov.u32 	%r3056, -921707870;
	mul.hi.u32 	%r4254, %r1023, %r3056;
	setp.lt.s32	%p407, %r4254, 1;
	@%p407 bra 	BB84_638;

	mul.lo.s32 	%r3057, %r1023, -921707870;
	shr.u32 	%r3058, %r3057, 31;
	shl.b32 	%r3059, %r4254, 1;
	add.s32 	%r4254, %r3058, %r3059;
	add.s32 	%r4255, %r4255, 1;

BB84_638:
	mov.u32 	%r3060, 126;
	sub.s32 	%r3061, %r3060, %r4255;
	shl.b32 	%r3062, %r3061, 23;
	add.s32 	%r3063, %r4254, 1;
	shr.u32 	%r3064, %r3063, 7;
	add.s32 	%r3065, %r3064, 1;
	shr.u32 	%r3066, %r3065, 1;
	add.s32 	%r3067, %r3066, %r3062;
	or.b32  	%r3068, %r3067, %r1019;
	mov.b32 	 %f2405, %r3068;

BB84_639:
	mul.rn.f32 	%f556, %f2405, %f2405;
	and.b32  	%r1030, %r4256, 1;
	setp.eq.s32	%p408, %r1030, 0;
	@%p408 bra 	BB84_641;

	mov.f32 	%f1743, 0fBAB6061A;
	mov.f32 	%f1744, 0f37CCF5CE;
	fma.rn.f32 	%f2406, %f1744, %f556, %f1743;
	bra.uni 	BB84_642;

BB84_641:
	mov.f32 	%f1745, 0f3C08839E;
	mov.f32 	%f1746, 0fB94CA1F9;
	fma.rn.f32 	%f2406, %f1746, %f556, %f1745;

BB84_642:
	@%p408 bra 	BB84_644;

	mov.f32 	%f1747, 0f3D2AAAA5;
	fma.rn.f32 	%f1748, %f2406, %f556, %f1747;
	mov.f32 	%f1749, 0fBF000000;
	fma.rn.f32 	%f2407, %f1748, %f556, %f1749;
	bra.uni 	BB84_645;

BB84_644:
	mov.f32 	%f1750, 0fBE2AAAA3;
	fma.rn.f32 	%f1751, %f2406, %f556, %f1750;
	mov.f32 	%f1752, 0f00000000;
	fma.rn.f32 	%f2407, %f1751, %f556, %f1752;

BB84_645:
	fma.rn.f32 	%f2408, %f2407, %f2405, %f2405;
	@%p408 bra 	BB84_647;

	mov.f32 	%f1753, 0f3F800000;
	fma.rn.f32 	%f2408, %f2407, %f556, %f1753;

BB84_647:
	and.b32  	%r3069, %r4256, 2;
	setp.eq.s32	%p411, %r3069, 0;
	@%p411 bra 	BB84_649;

	mov.f32 	%f1754, 0f00000000;
	mov.f32 	%f1755, 0fBF800000;
	fma.rn.f32 	%f2408, %f2408, %f1755, %f1754;

BB84_649:
	mov.f32 	%f2509, %f487;
	@%p373 bra 	BB84_651;

	mov.f32 	%f1756, 0f00000000;
	mul.rn.f32 	%f2509, %f487, %f1756;

BB84_651:
	mul.f32 	%f1757, %f2509, 0f3F22F983;
	cvt.rni.s32.f32	%r4266, %f1757;
	cvt.rn.f32.s32	%f1758, %r4266;
	neg.f32 	%f1759, %f1758;
	fma.rn.f32 	%f1761, %f1759, %f1668, %f2509;
	fma.rn.f32 	%f1763, %f1759, %f1670, %f1761;
	fma.rn.f32 	%f2409, %f1759, %f1672, %f1763;
	abs.f32 	%f1765, %f2509;
	setp.leu.f32	%p413, %f1765, 0f47CE4780;
	@%p413 bra 	BB84_661;

	mov.b32 	 %r1032, %f2509;
	shr.u32 	%r1033, %r1032, 23;
	bfe.u32 	%r3072, %r1032, 23, 8;
	add.s32 	%r3073, %r3072, -128;
	shl.b32 	%r3074, %r1032, 8;
	or.b32  	%r1034, %r3074, -2147483648;
	shr.u32 	%r1035, %r3073, 5;
	mov.u32 	%r4258, 0;
	mov.u64 	%rd376, __cudart_i2opi_f;
	mov.u32 	%r4257, -6;
	mov.u64 	%rd462, %rd1;

BB84_653:
	.pragma "nounroll";
	ld.const.u32 	%r3077, [%rd376];
	// inline asm
	{
	mad.lo.cc.u32   %r3075, %r3077, %r1034, %r4258;
	madc.hi.u32     %r4258, %r3077, %r1034,  0;
	}
	// inline asm
	st.local.u32 	[%rd462], %r3075;
	add.s64 	%rd462, %rd462, 4;
	add.s64 	%rd376, %rd376, 4;
	add.s32 	%r4257, %r4257, 1;
	setp.ne.s32	%p414, %r4257, 0;
	@%p414 bra 	BB84_653;

	and.b32  	%r1040, %r1032, -2147483648;
	st.local.u32 	[%rd3], %r4258;
	mov.u32 	%r3080, 6;
	sub.s32 	%r3081, %r3080, %r1035;
	mul.wide.s32 	%rd305, %r3081, 4;
	add.s64 	%rd148, %rd1, %rd305;
	ld.local.u32 	%r4259, [%rd148];
	ld.local.u32 	%r4260, [%rd148+-4];
	and.b32  	%r1043, %r1033, 31;
	setp.eq.s32	%p415, %r1043, 0;
	@%p415 bra 	BB84_656;

	mov.u32 	%r3082, 32;
	sub.s32 	%r3083, %r3082, %r1043;
	shr.u32 	%r3084, %r4260, %r3083;
	shl.b32 	%r3085, %r4259, %r1043;
	add.s32 	%r4259, %r3084, %r3085;
	ld.local.u32 	%r3086, [%rd148+-8];
	shr.u32 	%r3087, %r3086, %r3083;
	shl.b32 	%r3088, %r4260, %r1043;
	add.s32 	%r4260, %r3087, %r3088;

BB84_656:
	shr.u32 	%r3089, %r4260, 30;
	shl.b32 	%r3090, %r4259, 2;
	add.s32 	%r4261, %r3089, %r3090;
	shl.b32 	%r1049, %r4260, 2;
	shr.u32 	%r3091, %r4261, 31;
	shr.u32 	%r3092, %r4259, 30;
	add.s32 	%r1050, %r3091, %r3092;
	setp.eq.s32	%p416, %r3091, 0;
	mov.u32 	%r4262, %r1040;
	mov.u32 	%r4263, %r1049;
	@%p416 bra 	BB84_658;

	not.b32 	%r3093, %r4261;
	neg.s32 	%r1051, %r1049;
	setp.eq.s32	%p417, %r1049, 0;
	selp.u32	%r3094, 1, 0, %p417;
	add.s32 	%r4261, %r3094, %r3093;
	xor.b32  	%r1053, %r1040, -2147483648;
	mov.u32 	%r4262, %r1053;
	mov.u32 	%r4263, %r1051;

BB84_658:
	mov.u32 	%r1055, %r4262;
	neg.s32 	%r3095, %r1050;
	setp.eq.s32	%p418, %r1040, 0;
	selp.b32	%r4266, %r1050, %r3095, %p418;
	clz.b32 	%r4265, %r4261;
	setp.eq.s32	%p419, %r4265, 0;
	shl.b32 	%r3096, %r4261, %r4265;
	mov.u32 	%r3097, 32;
	sub.s32 	%r3098, %r3097, %r4265;
	shr.u32 	%r3099, %r4263, %r3098;
	add.s32 	%r3100, %r3099, %r3096;
	selp.b32	%r1059, %r4261, %r3100, %p419;
	mov.u32 	%r3101, -921707870;
	mul.hi.u32 	%r4264, %r1059, %r3101;
	setp.lt.s32	%p420, %r4264, 1;
	@%p420 bra 	BB84_660;

	mul.lo.s32 	%r3102, %r1059, -921707870;
	shr.u32 	%r3103, %r3102, 31;
	shl.b32 	%r3104, %r4264, 1;
	add.s32 	%r4264, %r3103, %r3104;
	add.s32 	%r4265, %r4265, 1;

BB84_660:
	mov.u32 	%r3105, 126;
	sub.s32 	%r3106, %r3105, %r4265;
	shl.b32 	%r3107, %r3106, 23;
	add.s32 	%r3108, %r4264, 1;
	shr.u32 	%r3109, %r3108, 7;
	add.s32 	%r3110, %r3109, 1;
	shr.u32 	%r3111, %r3110, 1;
	add.s32 	%r3112, %r3111, %r3107;
	or.b32  	%r3113, %r3112, %r1055;
	mov.b32 	 %f2409, %r3113;

BB84_661:
	mul.rn.f32 	%f573, %f2409, %f2409;
	and.b32  	%r1066, %r4266, 1;
	setp.eq.s32	%p421, %r1066, 0;
	@%p421 bra 	BB84_663;

	mov.f32 	%f1766, 0fBAB6061A;
	mov.f32 	%f1767, 0f37CCF5CE;
	fma.rn.f32 	%f2410, %f1767, %f573, %f1766;
	bra.uni 	BB84_664;

BB84_663:
	mov.f32 	%f1768, 0f3C08839E;
	mov.f32 	%f1769, 0fB94CA1F9;
	fma.rn.f32 	%f2410, %f1769, %f573, %f1768;

BB84_664:
	@%p421 bra 	BB84_666;

	mov.f32 	%f1770, 0f3D2AAAA5;
	fma.rn.f32 	%f1771, %f2410, %f573, %f1770;
	mov.f32 	%f1772, 0fBF000000;
	fma.rn.f32 	%f2411, %f1771, %f573, %f1772;
	bra.uni 	BB84_667;

BB84_666:
	mov.f32 	%f1773, 0fBE2AAAA3;
	fma.rn.f32 	%f1774, %f2410, %f573, %f1773;
	mov.f32 	%f1775, 0f00000000;
	fma.rn.f32 	%f2411, %f1774, %f573, %f1775;

BB84_667:
	fma.rn.f32 	%f2412, %f2411, %f2409, %f2409;
	@%p421 bra 	BB84_669;

	mov.f32 	%f1776, 0f3F800000;
	fma.rn.f32 	%f2412, %f2411, %f573, %f1776;

BB84_669:
	and.b32  	%r3114, %r4266, 2;
	setp.eq.s32	%p424, %r3114, 0;
	@%p424 bra 	BB84_671;

	mov.f32 	%f1777, 0f00000000;
	mov.f32 	%f1778, 0fBF800000;
	fma.rn.f32 	%f2412, %f2412, %f1778, %f1777;

BB84_671:
	mul.f32 	%f585, %f2408, %f2412;
	mov.f32 	%f2528, %f494;
	@%p386 bra 	BB84_673;

	mov.f32 	%f1779, 0f00000000;
	mul.rn.f32 	%f2528, %f494, %f1779;

BB84_673:
	mul.f32 	%f1780, %f2528, 0f3F22F983;
	cvt.rni.s32.f32	%r4276, %f1780;
	cvt.rn.f32.s32	%f1781, %r4276;
	neg.f32 	%f1782, %f1781;
	fma.rn.f32 	%f1784, %f1782, %f1668, %f2528;
	fma.rn.f32 	%f1786, %f1782, %f1670, %f1784;
	fma.rn.f32 	%f2413, %f1782, %f1672, %f1786;
	abs.f32 	%f1788, %f2528;
	setp.leu.f32	%p426, %f1788, 0f47CE4780;
	@%p426 bra 	BB84_683;

	mov.b32 	 %r1068, %f2528;
	shr.u32 	%r1069, %r1068, 23;
	bfe.u32 	%r3117, %r1068, 23, 8;
	add.s32 	%r3118, %r3117, -128;
	shl.b32 	%r3119, %r1068, 8;
	or.b32  	%r1070, %r3119, -2147483648;
	shr.u32 	%r1071, %r3118, 5;
	mov.u32 	%r4268, 0;
	mov.u64 	%rd377, __cudart_i2opi_f;
	mov.u32 	%r4267, -6;
	mov.u64 	%rd461, %rd1;

BB84_675:
	.pragma "nounroll";
	ld.const.u32 	%r3122, [%rd377];
	// inline asm
	{
	mad.lo.cc.u32   %r3120, %r3122, %r1070, %r4268;
	madc.hi.u32     %r4268, %r3122, %r1070,  0;
	}
	// inline asm
	st.local.u32 	[%rd461], %r3120;
	add.s64 	%rd461, %rd461, 4;
	add.s64 	%rd377, %rd377, 4;
	add.s32 	%r4267, %r4267, 1;
	setp.ne.s32	%p427, %r4267, 0;
	@%p427 bra 	BB84_675;

	and.b32  	%r1076, %r1068, -2147483648;
	st.local.u32 	[%rd3], %r4268;
	mov.u32 	%r3125, 6;
	sub.s32 	%r3126, %r3125, %r1071;
	mul.wide.s32 	%rd307, %r3126, 4;
	add.s64 	%rd153, %rd1, %rd307;
	ld.local.u32 	%r4269, [%rd153];
	ld.local.u32 	%r4270, [%rd153+-4];
	and.b32  	%r1079, %r1069, 31;
	setp.eq.s32	%p428, %r1079, 0;
	@%p428 bra 	BB84_678;

	mov.u32 	%r3127, 32;
	sub.s32 	%r3128, %r3127, %r1079;
	shr.u32 	%r3129, %r4270, %r3128;
	shl.b32 	%r3130, %r4269, %r1079;
	add.s32 	%r4269, %r3129, %r3130;
	ld.local.u32 	%r3131, [%rd153+-8];
	shr.u32 	%r3132, %r3131, %r3128;
	shl.b32 	%r3133, %r4270, %r1079;
	add.s32 	%r4270, %r3132, %r3133;

BB84_678:
	shr.u32 	%r3134, %r4270, 30;
	shl.b32 	%r3135, %r4269, 2;
	add.s32 	%r4271, %r3134, %r3135;
	shl.b32 	%r1085, %r4270, 2;
	shr.u32 	%r3136, %r4271, 31;
	shr.u32 	%r3137, %r4269, 30;
	add.s32 	%r1086, %r3136, %r3137;
	setp.eq.s32	%p429, %r3136, 0;
	mov.u32 	%r4272, %r1076;
	mov.u32 	%r4273, %r1085;
	@%p429 bra 	BB84_680;

	not.b32 	%r3138, %r4271;
	neg.s32 	%r1087, %r1085;
	setp.eq.s32	%p430, %r1085, 0;
	selp.u32	%r3139, 1, 0, %p430;
	add.s32 	%r4271, %r3139, %r3138;
	xor.b32  	%r1089, %r1076, -2147483648;
	mov.u32 	%r4272, %r1089;
	mov.u32 	%r4273, %r1087;

BB84_680:
	mov.u32 	%r1091, %r4272;
	neg.s32 	%r3140, %r1086;
	setp.eq.s32	%p431, %r1076, 0;
	selp.b32	%r4276, %r1086, %r3140, %p431;
	clz.b32 	%r4275, %r4271;
	setp.eq.s32	%p432, %r4275, 0;
	shl.b32 	%r3141, %r4271, %r4275;
	mov.u32 	%r3142, 32;
	sub.s32 	%r3143, %r3142, %r4275;
	shr.u32 	%r3144, %r4273, %r3143;
	add.s32 	%r3145, %r3144, %r3141;
	selp.b32	%r1095, %r4271, %r3145, %p432;
	mov.u32 	%r3146, -921707870;
	mul.hi.u32 	%r4274, %r1095, %r3146;
	setp.lt.s32	%p433, %r4274, 1;
	@%p433 bra 	BB84_682;

	mul.lo.s32 	%r3147, %r1095, -921707870;
	shr.u32 	%r3148, %r3147, 31;
	shl.b32 	%r3149, %r4274, 1;
	add.s32 	%r4274, %r3148, %r3149;
	add.s32 	%r4275, %r4275, 1;

BB84_682:
	mov.u32 	%r3150, 126;
	sub.s32 	%r3151, %r3150, %r4275;
	shl.b32 	%r3152, %r3151, 23;
	add.s32 	%r3153, %r4274, 1;
	shr.u32 	%r3154, %r3153, 7;
	add.s32 	%r3155, %r3154, 1;
	shr.u32 	%r3156, %r3155, 1;
	add.s32 	%r3157, %r3156, %r3152;
	or.b32  	%r3158, %r3157, %r1091;
	mov.b32 	 %f2413, %r3158;

BB84_683:
	mul.rn.f32 	%f591, %f2413, %f2413;
	and.b32  	%r1102, %r4276, 1;
	setp.eq.s32	%p434, %r1102, 0;
	@%p434 bra 	BB84_685;

	mov.f32 	%f1789, 0fBAB6061A;
	mov.f32 	%f1790, 0f37CCF5CE;
	fma.rn.f32 	%f2414, %f1790, %f591, %f1789;
	bra.uni 	BB84_686;

BB84_685:
	mov.f32 	%f1791, 0f3C08839E;
	mov.f32 	%f1792, 0fB94CA1F9;
	fma.rn.f32 	%f2414, %f1792, %f591, %f1791;

BB84_686:
	@%p434 bra 	BB84_688;

	mov.f32 	%f1793, 0f3D2AAAA5;
	fma.rn.f32 	%f1794, %f2414, %f591, %f1793;
	mov.f32 	%f1795, 0fBF000000;
	fma.rn.f32 	%f2415, %f1794, %f591, %f1795;
	bra.uni 	BB84_689;

BB84_688:
	mov.f32 	%f1796, 0fBE2AAAA3;
	fma.rn.f32 	%f1797, %f2414, %f591, %f1796;
	mov.f32 	%f1798, 0f00000000;
	fma.rn.f32 	%f2415, %f1797, %f591, %f1798;

BB84_689:
	fma.rn.f32 	%f2416, %f2415, %f2413, %f2413;
	@%p434 bra 	BB84_691;

	mov.f32 	%f1799, 0f3F800000;
	fma.rn.f32 	%f2416, %f2415, %f591, %f1799;

BB84_691:
	and.b32  	%r3159, %r4276, 2;
	setp.eq.s32	%p437, %r3159, 0;
	@%p437 bra 	BB84_693;

	mov.f32 	%f1800, 0f00000000;
	mov.f32 	%f1801, 0fBF800000;
	fma.rn.f32 	%f2416, %f2416, %f1801, %f1800;

BB84_693:
	fma.rn.f32 	%f603, %f585, %f2416, %f550;
	mov.f32 	%f2489, %f480;
	@%p360 bra 	BB84_695;

	mov.f32 	%f1802, 0f00000000;
	mul.rn.f32 	%f2489, %f480, %f1802;

BB84_695:
	mul.f32 	%f1803, %f2489, 0f3F22F983;
	cvt.rni.s32.f32	%r4286, %f1803;
	cvt.rn.f32.s32	%f1804, %r4286;
	neg.f32 	%f1805, %f1804;
	fma.rn.f32 	%f1807, %f1805, %f1668, %f2489;
	fma.rn.f32 	%f1809, %f1805, %f1670, %f1807;
	fma.rn.f32 	%f2417, %f1805, %f1672, %f1809;
	abs.f32 	%f1811, %f2489;
	setp.leu.f32	%p439, %f1811, 0f47CE4780;
	@%p439 bra 	BB84_705;

	mov.b32 	 %r1104, %f2489;
	shr.u32 	%r1105, %r1104, 23;
	bfe.u32 	%r3162, %r1104, 23, 8;
	add.s32 	%r3163, %r3162, -128;
	shl.b32 	%r3164, %r1104, 8;
	or.b32  	%r1106, %r3164, -2147483648;
	shr.u32 	%r1107, %r3163, 5;
	mov.u32 	%r4278, 0;
	mov.u64 	%rd378, __cudart_i2opi_f;
	mov.u32 	%r4277, -6;
	mov.u64 	%rd460, %rd1;

BB84_697:
	.pragma "nounroll";
	ld.const.u32 	%r3167, [%rd378];
	// inline asm
	{
	mad.lo.cc.u32   %r3165, %r3167, %r1106, %r4278;
	madc.hi.u32     %r4278, %r3167, %r1106,  0;
	}
	// inline asm
	st.local.u32 	[%rd460], %r3165;
	add.s64 	%rd460, %rd460, 4;
	add.s64 	%rd378, %rd378, 4;
	add.s32 	%r4277, %r4277, 1;
	setp.ne.s32	%p440, %r4277, 0;
	@%p440 bra 	BB84_697;

	and.b32  	%r1112, %r1104, -2147483648;
	st.local.u32 	[%rd3], %r4278;
	mov.u32 	%r3170, 6;
	sub.s32 	%r3171, %r3170, %r1107;
	mul.wide.s32 	%rd309, %r3171, 4;
	add.s64 	%rd158, %rd1, %rd309;
	ld.local.u32 	%r4279, [%rd158];
	ld.local.u32 	%r4280, [%rd158+-4];
	and.b32  	%r1115, %r1105, 31;
	setp.eq.s32	%p441, %r1115, 0;
	@%p441 bra 	BB84_700;

	mov.u32 	%r3172, 32;
	sub.s32 	%r3173, %r3172, %r1115;
	shr.u32 	%r3174, %r4280, %r3173;
	shl.b32 	%r3175, %r4279, %r1115;
	add.s32 	%r4279, %r3174, %r3175;
	ld.local.u32 	%r3176, [%rd158+-8];
	shr.u32 	%r3177, %r3176, %r3173;
	shl.b32 	%r3178, %r4280, %r1115;
	add.s32 	%r4280, %r3177, %r3178;

BB84_700:
	shr.u32 	%r3179, %r4280, 30;
	shl.b32 	%r3180, %r4279, 2;
	add.s32 	%r4281, %r3179, %r3180;
	shl.b32 	%r1121, %r4280, 2;
	shr.u32 	%r3181, %r4281, 31;
	shr.u32 	%r3182, %r4279, 30;
	add.s32 	%r1122, %r3181, %r3182;
	setp.eq.s32	%p442, %r3181, 0;
	mov.u32 	%r4282, %r1112;
	mov.u32 	%r4283, %r1121;
	@%p442 bra 	BB84_702;

	not.b32 	%r3183, %r4281;
	neg.s32 	%r1123, %r1121;
	setp.eq.s32	%p443, %r1121, 0;
	selp.u32	%r3184, 1, 0, %p443;
	add.s32 	%r4281, %r3184, %r3183;
	xor.b32  	%r1125, %r1112, -2147483648;
	mov.u32 	%r4282, %r1125;
	mov.u32 	%r4283, %r1123;

BB84_702:
	mov.u32 	%r1127, %r4282;
	neg.s32 	%r3185, %r1122;
	setp.eq.s32	%p444, %r1112, 0;
	selp.b32	%r4286, %r1122, %r3185, %p444;
	clz.b32 	%r4285, %r4281;
	setp.eq.s32	%p445, %r4285, 0;
	shl.b32 	%r3186, %r4281, %r4285;
	mov.u32 	%r3187, 32;
	sub.s32 	%r3188, %r3187, %r4285;
	shr.u32 	%r3189, %r4283, %r3188;
	add.s32 	%r3190, %r3189, %r3186;
	selp.b32	%r1131, %r4281, %r3190, %p445;
	mov.u32 	%r3191, -921707870;
	mul.hi.u32 	%r4284, %r1131, %r3191;
	setp.lt.s32	%p446, %r4284, 1;
	@%p446 bra 	BB84_704;

	mul.lo.s32 	%r3192, %r1131, -921707870;
	shr.u32 	%r3193, %r3192, 31;
	shl.b32 	%r3194, %r4284, 1;
	add.s32 	%r4284, %r3193, %r3194;
	add.s32 	%r4285, %r4285, 1;

BB84_704:
	mov.u32 	%r3195, 126;
	sub.s32 	%r3196, %r3195, %r4285;
	shl.b32 	%r3197, %r3196, 23;
	add.s32 	%r3198, %r4284, 1;
	shr.u32 	%r3199, %r3198, 7;
	add.s32 	%r3200, %r3199, 1;
	shr.u32 	%r3201, %r3200, 1;
	add.s32 	%r3202, %r3201, %r3197;
	or.b32  	%r3203, %r3202, %r1127;
	mov.b32 	 %f2417, %r3203;

BB84_705:
	mul.rn.f32 	%f609, %f2417, %f2417;
	and.b32  	%r1138, %r4286, 1;
	setp.eq.s32	%p447, %r1138, 0;
	@%p447 bra 	BB84_707;

	mov.f32 	%f1812, 0fBAB6061A;
	mov.f32 	%f1813, 0f37CCF5CE;
	fma.rn.f32 	%f2418, %f1813, %f609, %f1812;
	bra.uni 	BB84_708;

BB84_707:
	mov.f32 	%f1814, 0f3C08839E;
	mov.f32 	%f1815, 0fB94CA1F9;
	fma.rn.f32 	%f2418, %f1815, %f609, %f1814;

BB84_708:
	@%p447 bra 	BB84_710;

	mov.f32 	%f1816, 0f3D2AAAA5;
	fma.rn.f32 	%f1817, %f2418, %f609, %f1816;
	mov.f32 	%f1818, 0fBF000000;
	fma.rn.f32 	%f2419, %f1817, %f609, %f1818;
	bra.uni 	BB84_711;

BB84_710:
	mov.f32 	%f1819, 0fBE2AAAA3;
	fma.rn.f32 	%f1820, %f2418, %f609, %f1819;
	mov.f32 	%f1821, 0f00000000;
	fma.rn.f32 	%f2419, %f1820, %f609, %f1821;

BB84_711:
	fma.rn.f32 	%f2420, %f2419, %f2417, %f2417;
	@%p447 bra 	BB84_713;

	mov.f32 	%f1822, 0f3F800000;
	fma.rn.f32 	%f2420, %f2419, %f609, %f1822;

BB84_713:
	and.b32  	%r3204, %r4286, 2;
	setp.eq.s32	%p450, %r3204, 0;
	@%p450 bra 	BB84_715;

	mov.f32 	%f1823, 0f00000000;
	mov.f32 	%f1824, 0fBF800000;
	fma.rn.f32 	%f2420, %f2420, %f1824, %f1823;

BB84_715:
	mov.f32 	%f2508, %f487;
	@%p373 bra 	BB84_717;

	mov.f32 	%f1825, 0f00000000;
	mul.rn.f32 	%f2508, %f487, %f1825;

BB84_717:
	mul.f32 	%f1826, %f2508, 0f3F22F983;
	cvt.rni.s32.f32	%r4296, %f1826;
	cvt.rn.f32.s32	%f1827, %r4296;
	neg.f32 	%f1828, %f1827;
	fma.rn.f32 	%f1830, %f1828, %f1668, %f2508;
	fma.rn.f32 	%f1832, %f1828, %f1670, %f1830;
	fma.rn.f32 	%f2421, %f1828, %f1672, %f1832;
	abs.f32 	%f1834, %f2508;
	setp.leu.f32	%p452, %f1834, 0f47CE4780;
	@%p452 bra 	BB84_727;

	mov.b32 	 %r1140, %f2508;
	shr.u32 	%r1141, %r1140, 23;
	bfe.u32 	%r3207, %r1140, 23, 8;
	add.s32 	%r3208, %r3207, -128;
	shl.b32 	%r3209, %r1140, 8;
	or.b32  	%r1142, %r3209, -2147483648;
	shr.u32 	%r1143, %r3208, 5;
	mov.u32 	%r4288, 0;
	mov.u64 	%rd379, __cudart_i2opi_f;
	mov.u32 	%r4287, -6;
	mov.u64 	%rd459, %rd1;

BB84_719:
	.pragma "nounroll";
	ld.const.u32 	%r3212, [%rd379];
	// inline asm
	{
	mad.lo.cc.u32   %r3210, %r3212, %r1142, %r4288;
	madc.hi.u32     %r4288, %r3212, %r1142,  0;
	}
	// inline asm
	st.local.u32 	[%rd459], %r3210;
	add.s64 	%rd459, %rd459, 4;
	add.s64 	%rd379, %rd379, 4;
	add.s32 	%r4287, %r4287, 1;
	setp.ne.s32	%p453, %r4287, 0;
	@%p453 bra 	BB84_719;

	and.b32  	%r1148, %r1140, -2147483648;
	st.local.u32 	[%rd3], %r4288;
	mov.u32 	%r3215, 6;
	sub.s32 	%r3216, %r3215, %r1143;
	mul.wide.s32 	%rd311, %r3216, 4;
	add.s64 	%rd163, %rd1, %rd311;
	ld.local.u32 	%r4289, [%rd163];
	ld.local.u32 	%r4290, [%rd163+-4];
	and.b32  	%r1151, %r1141, 31;
	setp.eq.s32	%p454, %r1151, 0;
	@%p454 bra 	BB84_722;

	mov.u32 	%r3217, 32;
	sub.s32 	%r3218, %r3217, %r1151;
	shr.u32 	%r3219, %r4290, %r3218;
	shl.b32 	%r3220, %r4289, %r1151;
	add.s32 	%r4289, %r3219, %r3220;
	ld.local.u32 	%r3221, [%rd163+-8];
	shr.u32 	%r3222, %r3221, %r3218;
	shl.b32 	%r3223, %r4290, %r1151;
	add.s32 	%r4290, %r3222, %r3223;

BB84_722:
	shr.u32 	%r3224, %r4290, 30;
	shl.b32 	%r3225, %r4289, 2;
	add.s32 	%r4291, %r3224, %r3225;
	shl.b32 	%r1157, %r4290, 2;
	shr.u32 	%r3226, %r4291, 31;
	shr.u32 	%r3227, %r4289, 30;
	add.s32 	%r1158, %r3226, %r3227;
	setp.eq.s32	%p455, %r3226, 0;
	mov.u32 	%r4292, %r1148;
	mov.u32 	%r4293, %r1157;
	@%p455 bra 	BB84_724;

	not.b32 	%r3228, %r4291;
	neg.s32 	%r1159, %r1157;
	setp.eq.s32	%p456, %r1157, 0;
	selp.u32	%r3229, 1, 0, %p456;
	add.s32 	%r4291, %r3229, %r3228;
	xor.b32  	%r1161, %r1148, -2147483648;
	mov.u32 	%r4292, %r1161;
	mov.u32 	%r4293, %r1159;

BB84_724:
	mov.u32 	%r1163, %r4292;
	neg.s32 	%r3230, %r1158;
	setp.eq.s32	%p457, %r1148, 0;
	selp.b32	%r4296, %r1158, %r3230, %p457;
	clz.b32 	%r4295, %r4291;
	setp.eq.s32	%p458, %r4295, 0;
	shl.b32 	%r3231, %r4291, %r4295;
	mov.u32 	%r3232, 32;
	sub.s32 	%r3233, %r3232, %r4295;
	shr.u32 	%r3234, %r4293, %r3233;
	add.s32 	%r3235, %r3234, %r3231;
	selp.b32	%r1167, %r4291, %r3235, %p458;
	mov.u32 	%r3236, -921707870;
	mul.hi.u32 	%r4294, %r1167, %r3236;
	setp.lt.s32	%p459, %r4294, 1;
	@%p459 bra 	BB84_726;

	mul.lo.s32 	%r3237, %r1167, -921707870;
	shr.u32 	%r3238, %r3237, 31;
	shl.b32 	%r3239, %r4294, 1;
	add.s32 	%r4294, %r3238, %r3239;
	add.s32 	%r4295, %r4295, 1;

BB84_726:
	mov.u32 	%r3240, 126;
	sub.s32 	%r3241, %r3240, %r4295;
	shl.b32 	%r3242, %r3241, 23;
	add.s32 	%r3243, %r4294, 1;
	shr.u32 	%r3244, %r3243, 7;
	add.s32 	%r3245, %r3244, 1;
	shr.u32 	%r3246, %r3245, 1;
	add.s32 	%r3247, %r3246, %r3242;
	or.b32  	%r3248, %r3247, %r1163;
	mov.b32 	 %f2421, %r3248;

BB84_727:
	mul.rn.f32 	%f626, %f2421, %f2421;
	add.s32 	%r1174, %r4296, 1;
	and.b32  	%r1175, %r1174, 1;
	setp.eq.s32	%p460, %r1175, 0;
	@%p460 bra 	BB84_729;

	mov.f32 	%f1835, 0fBAB6061A;
	mov.f32 	%f1836, 0f37CCF5CE;
	fma.rn.f32 	%f2422, %f1836, %f626, %f1835;
	bra.uni 	BB84_730;

BB84_729:
	mov.f32 	%f1837, 0f3C08839E;
	mov.f32 	%f1838, 0fB94CA1F9;
	fma.rn.f32 	%f2422, %f1838, %f626, %f1837;

BB84_730:
	@%p460 bra 	BB84_732;

	mov.f32 	%f1839, 0f3D2AAAA5;
	fma.rn.f32 	%f1840, %f2422, %f626, %f1839;
	mov.f32 	%f1841, 0fBF000000;
	fma.rn.f32 	%f2423, %f1840, %f626, %f1841;
	bra.uni 	BB84_733;

BB84_732:
	mov.f32 	%f1842, 0fBE2AAAA3;
	fma.rn.f32 	%f1843, %f2422, %f626, %f1842;
	mov.f32 	%f1844, 0f00000000;
	fma.rn.f32 	%f2423, %f1843, %f626, %f1844;

BB84_733:
	fma.rn.f32 	%f2424, %f2423, %f2421, %f2421;
	@%p460 bra 	BB84_735;

	mov.f32 	%f1845, 0f3F800000;
	fma.rn.f32 	%f2424, %f2423, %f626, %f1845;

BB84_735:
	and.b32  	%r3249, %r1174, 2;
	setp.eq.s32	%p463, %r3249, 0;
	@%p463 bra 	BB84_737;

	mov.f32 	%f1846, 0f00000000;
	mov.f32 	%f1847, 0fBF800000;
	fma.rn.f32 	%f2424, %f2424, %f1847, %f1846;

BB84_737:
	mul.f32 	%f638, %f2420, %f2424;
	mov.f32 	%f2527, %f494;
	@%p386 bra 	BB84_739;

	mov.f32 	%f1848, 0f00000000;
	mul.rn.f32 	%f2527, %f494, %f1848;

BB84_739:
	mul.f32 	%f1849, %f2527, 0f3F22F983;
	cvt.rni.s32.f32	%r4306, %f1849;
	cvt.rn.f32.s32	%f1850, %r4306;
	neg.f32 	%f1851, %f1850;
	fma.rn.f32 	%f1853, %f1851, %f1668, %f2527;
	fma.rn.f32 	%f1855, %f1851, %f1670, %f1853;
	fma.rn.f32 	%f2425, %f1851, %f1672, %f1855;
	abs.f32 	%f1857, %f2527;
	setp.leu.f32	%p465, %f1857, 0f47CE4780;
	@%p465 bra 	BB84_749;

	mov.b32 	 %r1177, %f2527;
	shr.u32 	%r1178, %r1177, 23;
	bfe.u32 	%r3252, %r1177, 23, 8;
	add.s32 	%r3253, %r3252, -128;
	shl.b32 	%r3254, %r1177, 8;
	or.b32  	%r1179, %r3254, -2147483648;
	shr.u32 	%r1180, %r3253, 5;
	mov.u32 	%r4298, 0;
	mov.u64 	%rd380, __cudart_i2opi_f;
	mov.u32 	%r4297, -6;
	mov.u64 	%rd458, %rd1;

BB84_741:
	.pragma "nounroll";
	ld.const.u32 	%r3257, [%rd380];
	// inline asm
	{
	mad.lo.cc.u32   %r3255, %r3257, %r1179, %r4298;
	madc.hi.u32     %r4298, %r3257, %r1179,  0;
	}
	// inline asm
	st.local.u32 	[%rd458], %r3255;
	add.s64 	%rd458, %rd458, 4;
	add.s64 	%rd380, %rd380, 4;
	add.s32 	%r4297, %r4297, 1;
	setp.ne.s32	%p466, %r4297, 0;
	@%p466 bra 	BB84_741;

	and.b32  	%r1185, %r1177, -2147483648;
	st.local.u32 	[%rd3], %r4298;
	mov.u32 	%r3260, 6;
	sub.s32 	%r3261, %r3260, %r1180;
	mul.wide.s32 	%rd313, %r3261, 4;
	add.s64 	%rd168, %rd1, %rd313;
	ld.local.u32 	%r4299, [%rd168];
	ld.local.u32 	%r4300, [%rd168+-4];
	and.b32  	%r1188, %r1178, 31;
	setp.eq.s32	%p467, %r1188, 0;
	@%p467 bra 	BB84_744;

	mov.u32 	%r3262, 32;
	sub.s32 	%r3263, %r3262, %r1188;
	shr.u32 	%r3264, %r4300, %r3263;
	shl.b32 	%r3265, %r4299, %r1188;
	add.s32 	%r4299, %r3264, %r3265;
	ld.local.u32 	%r3266, [%rd168+-8];
	shr.u32 	%r3267, %r3266, %r3263;
	shl.b32 	%r3268, %r4300, %r1188;
	add.s32 	%r4300, %r3267, %r3268;

BB84_744:
	shr.u32 	%r3269, %r4300, 30;
	shl.b32 	%r3270, %r4299, 2;
	add.s32 	%r4301, %r3269, %r3270;
	shl.b32 	%r1194, %r4300, 2;
	shr.u32 	%r3271, %r4301, 31;
	shr.u32 	%r3272, %r4299, 30;
	add.s32 	%r1195, %r3271, %r3272;
	setp.eq.s32	%p468, %r3271, 0;
	mov.u32 	%r4302, %r1185;
	mov.u32 	%r4303, %r1194;
	@%p468 bra 	BB84_746;

	not.b32 	%r3273, %r4301;
	neg.s32 	%r1196, %r1194;
	setp.eq.s32	%p469, %r1194, 0;
	selp.u32	%r3274, 1, 0, %p469;
	add.s32 	%r4301, %r3274, %r3273;
	xor.b32  	%r1198, %r1185, -2147483648;
	mov.u32 	%r4302, %r1198;
	mov.u32 	%r4303, %r1196;

BB84_746:
	mov.u32 	%r1200, %r4302;
	neg.s32 	%r3275, %r1195;
	setp.eq.s32	%p470, %r1185, 0;
	selp.b32	%r4306, %r1195, %r3275, %p470;
	clz.b32 	%r4305, %r4301;
	setp.eq.s32	%p471, %r4305, 0;
	shl.b32 	%r3276, %r4301, %r4305;
	mov.u32 	%r3277, 32;
	sub.s32 	%r3278, %r3277, %r4305;
	shr.u32 	%r3279, %r4303, %r3278;
	add.s32 	%r3280, %r3279, %r3276;
	selp.b32	%r1204, %r4301, %r3280, %p471;
	mov.u32 	%r3281, -921707870;
	mul.hi.u32 	%r4304, %r1204, %r3281;
	setp.lt.s32	%p472, %r4304, 1;
	@%p472 bra 	BB84_748;

	mul.lo.s32 	%r3282, %r1204, -921707870;
	shr.u32 	%r3283, %r3282, 31;
	shl.b32 	%r3284, %r4304, 1;
	add.s32 	%r4304, %r3283, %r3284;
	add.s32 	%r4305, %r4305, 1;

BB84_748:
	mov.u32 	%r3285, 126;
	sub.s32 	%r3286, %r3285, %r4305;
	shl.b32 	%r3287, %r3286, 23;
	add.s32 	%r3288, %r4304, 1;
	shr.u32 	%r3289, %r3288, 7;
	add.s32 	%r3290, %r3289, 1;
	shr.u32 	%r3291, %r3290, 1;
	add.s32 	%r3292, %r3291, %r3287;
	or.b32  	%r3293, %r3292, %r1200;
	mov.b32 	 %f2425, %r3293;

BB84_749:
	mul.rn.f32 	%f644, %f2425, %f2425;
	add.s32 	%r1211, %r4306, 1;
	and.b32  	%r1212, %r1211, 1;
	setp.eq.s32	%p473, %r1212, 0;
	@%p473 bra 	BB84_751;

	mov.f32 	%f1858, 0fBAB6061A;
	mov.f32 	%f1859, 0f37CCF5CE;
	fma.rn.f32 	%f2426, %f1859, %f644, %f1858;
	bra.uni 	BB84_752;

BB84_751:
	mov.f32 	%f1860, 0f3C08839E;
	mov.f32 	%f1861, 0fB94CA1F9;
	fma.rn.f32 	%f2426, %f1861, %f644, %f1860;

BB84_752:
	@%p473 bra 	BB84_754;

	mov.f32 	%f1862, 0f3D2AAAA5;
	fma.rn.f32 	%f1863, %f2426, %f644, %f1862;
	mov.f32 	%f1864, 0fBF000000;
	fma.rn.f32 	%f2427, %f1863, %f644, %f1864;
	bra.uni 	BB84_755;

BB84_754:
	mov.f32 	%f1865, 0fBE2AAAA3;
	fma.rn.f32 	%f1866, %f2426, %f644, %f1865;
	mov.f32 	%f1867, 0f00000000;
	fma.rn.f32 	%f2427, %f1866, %f644, %f1867;

BB84_755:
	fma.rn.f32 	%f2428, %f2427, %f2425, %f2425;
	@%p473 bra 	BB84_757;

	mov.f32 	%f1868, 0f3F800000;
	fma.rn.f32 	%f2428, %f2427, %f644, %f1868;

BB84_757:
	and.b32  	%r3294, %r1211, 2;
	setp.eq.s32	%p476, %r3294, 0;
	@%p476 bra 	BB84_759;

	mov.f32 	%f1869, 0f00000000;
	mov.f32 	%f1870, 0fBF800000;
	fma.rn.f32 	%f2428, %f2428, %f1870, %f1869;

BB84_759:
	mul.f32 	%f656, %f638, %f2428;
	mov.f32 	%f2488, %f480;
	@%p360 bra 	BB84_761;

	mov.f32 	%f1871, 0f00000000;
	mul.rn.f32 	%f2488, %f480, %f1871;

BB84_761:
	mul.f32 	%f1872, %f2488, 0f3F22F983;
	cvt.rni.s32.f32	%r4316, %f1872;
	cvt.rn.f32.s32	%f1873, %r4316;
	neg.f32 	%f1874, %f1873;
	fma.rn.f32 	%f1876, %f1874, %f1668, %f2488;
	fma.rn.f32 	%f1878, %f1874, %f1670, %f1876;
	fma.rn.f32 	%f2429, %f1874, %f1672, %f1878;
	abs.f32 	%f1880, %f2488;
	setp.leu.f32	%p478, %f1880, 0f47CE4780;
	@%p478 bra 	BB84_771;

	mov.b32 	 %r1214, %f2488;
	shr.u32 	%r1215, %r1214, 23;
	bfe.u32 	%r3297, %r1214, 23, 8;
	add.s32 	%r3298, %r3297, -128;
	shl.b32 	%r3299, %r1214, 8;
	or.b32  	%r1216, %r3299, -2147483648;
	shr.u32 	%r1217, %r3298, 5;
	mov.u32 	%r4308, 0;
	mov.u64 	%rd381, __cudart_i2opi_f;
	mov.u32 	%r4307, -6;
	mov.u64 	%rd457, %rd1;

BB84_763:
	.pragma "nounroll";
	ld.const.u32 	%r3302, [%rd381];
	// inline asm
	{
	mad.lo.cc.u32   %r3300, %r3302, %r1216, %r4308;
	madc.hi.u32     %r4308, %r3302, %r1216,  0;
	}
	// inline asm
	st.local.u32 	[%rd457], %r3300;
	add.s64 	%rd457, %rd457, 4;
	add.s64 	%rd381, %rd381, 4;
	add.s32 	%r4307, %r4307, 1;
	setp.ne.s32	%p479, %r4307, 0;
	@%p479 bra 	BB84_763;

	and.b32  	%r1222, %r1214, -2147483648;
	st.local.u32 	[%rd3], %r4308;
	mov.u32 	%r3305, 6;
	sub.s32 	%r3306, %r3305, %r1217;
	mul.wide.s32 	%rd315, %r3306, 4;
	add.s64 	%rd173, %rd1, %rd315;
	ld.local.u32 	%r4309, [%rd173];
	ld.local.u32 	%r4310, [%rd173+-4];
	and.b32  	%r1225, %r1215, 31;
	setp.eq.s32	%p480, %r1225, 0;
	@%p480 bra 	BB84_766;

	mov.u32 	%r3307, 32;
	sub.s32 	%r3308, %r3307, %r1225;
	shr.u32 	%r3309, %r4310, %r3308;
	shl.b32 	%r3310, %r4309, %r1225;
	add.s32 	%r4309, %r3309, %r3310;
	ld.local.u32 	%r3311, [%rd173+-8];
	shr.u32 	%r3312, %r3311, %r3308;
	shl.b32 	%r3313, %r4310, %r1225;
	add.s32 	%r4310, %r3312, %r3313;

BB84_766:
	shr.u32 	%r3314, %r4310, 30;
	shl.b32 	%r3315, %r4309, 2;
	add.s32 	%r4311, %r3314, %r3315;
	shl.b32 	%r1231, %r4310, 2;
	shr.u32 	%r3316, %r4311, 31;
	shr.u32 	%r3317, %r4309, 30;
	add.s32 	%r1232, %r3316, %r3317;
	setp.eq.s32	%p481, %r3316, 0;
	mov.u32 	%r4312, %r1222;
	mov.u32 	%r4313, %r1231;
	@%p481 bra 	BB84_768;

	not.b32 	%r3318, %r4311;
	neg.s32 	%r1233, %r1231;
	setp.eq.s32	%p482, %r1231, 0;
	selp.u32	%r3319, 1, 0, %p482;
	add.s32 	%r4311, %r3319, %r3318;
	xor.b32  	%r1235, %r1222, -2147483648;
	mov.u32 	%r4312, %r1235;
	mov.u32 	%r4313, %r1233;

BB84_768:
	mov.u32 	%r1237, %r4312;
	neg.s32 	%r3320, %r1232;
	setp.eq.s32	%p483, %r1222, 0;
	selp.b32	%r4316, %r1232, %r3320, %p483;
	clz.b32 	%r4315, %r4311;
	setp.eq.s32	%p484, %r4315, 0;
	shl.b32 	%r3321, %r4311, %r4315;
	mov.u32 	%r3322, 32;
	sub.s32 	%r3323, %r3322, %r4315;
	shr.u32 	%r3324, %r4313, %r3323;
	add.s32 	%r3325, %r3324, %r3321;
	selp.b32	%r1241, %r4311, %r3325, %p484;
	mov.u32 	%r3326, -921707870;
	mul.hi.u32 	%r4314, %r1241, %r3326;
	setp.lt.s32	%p485, %r4314, 1;
	@%p485 bra 	BB84_770;

	mul.lo.s32 	%r3327, %r1241, -921707870;
	shr.u32 	%r3328, %r3327, 31;
	shl.b32 	%r3329, %r4314, 1;
	add.s32 	%r4314, %r3328, %r3329;
	add.s32 	%r4315, %r4315, 1;

BB84_770:
	mov.u32 	%r3330, 126;
	sub.s32 	%r3331, %r3330, %r4315;
	shl.b32 	%r3332, %r3331, 23;
	add.s32 	%r3333, %r4314, 1;
	shr.u32 	%r3334, %r3333, 7;
	add.s32 	%r3335, %r3334, 1;
	shr.u32 	%r3336, %r3335, 1;
	add.s32 	%r3337, %r3336, %r3332;
	or.b32  	%r3338, %r3337, %r1237;
	mov.b32 	 %f2429, %r3338;

BB84_771:
	mul.rn.f32 	%f662, %f2429, %f2429;
	add.s32 	%r1248, %r4316, 1;
	and.b32  	%r1249, %r1248, 1;
	setp.eq.s32	%p486, %r1249, 0;
	@%p486 bra 	BB84_773;

	mov.f32 	%f1881, 0fBAB6061A;
	mov.f32 	%f1882, 0f37CCF5CE;
	fma.rn.f32 	%f2430, %f1882, %f662, %f1881;
	bra.uni 	BB84_774;

BB84_773:
	mov.f32 	%f1883, 0f3C08839E;
	mov.f32 	%f1884, 0fB94CA1F9;
	fma.rn.f32 	%f2430, %f1884, %f662, %f1883;

BB84_774:
	@%p486 bra 	BB84_776;

	mov.f32 	%f1885, 0f3D2AAAA5;
	fma.rn.f32 	%f1886, %f2430, %f662, %f1885;
	mov.f32 	%f1887, 0fBF000000;
	fma.rn.f32 	%f2431, %f1886, %f662, %f1887;
	bra.uni 	BB84_777;

BB84_776:
	mov.f32 	%f1888, 0fBE2AAAA3;
	fma.rn.f32 	%f1889, %f2430, %f662, %f1888;
	mov.f32 	%f1890, 0f00000000;
	fma.rn.f32 	%f2431, %f1889, %f662, %f1890;

BB84_777:
	fma.rn.f32 	%f2432, %f2431, %f2429, %f2429;
	@%p486 bra 	BB84_779;

	mov.f32 	%f1891, 0f3F800000;
	fma.rn.f32 	%f2432, %f2431, %f662, %f1891;

BB84_779:
	and.b32  	%r3339, %r1248, 2;
	setp.eq.s32	%p489, %r3339, 0;
	@%p489 bra 	BB84_781;

	mov.f32 	%f1892, 0f00000000;
	mov.f32 	%f1893, 0fBF800000;
	fma.rn.f32 	%f2432, %f2432, %f1893, %f1892;

BB84_781:
	mov.f32 	%f2507, %f487;
	@%p373 bra 	BB84_783;

	mov.f32 	%f1894, 0f00000000;
	mul.rn.f32 	%f2507, %f487, %f1894;

BB84_783:
	mul.f32 	%f1895, %f2507, 0f3F22F983;
	cvt.rni.s32.f32	%r4326, %f1895;
	cvt.rn.f32.s32	%f1896, %r4326;
	neg.f32 	%f1897, %f1896;
	fma.rn.f32 	%f1899, %f1897, %f1668, %f2507;
	fma.rn.f32 	%f1901, %f1897, %f1670, %f1899;
	fma.rn.f32 	%f2433, %f1897, %f1672, %f1901;
	abs.f32 	%f1903, %f2507;
	setp.leu.f32	%p491, %f1903, 0f47CE4780;
	@%p491 bra 	BB84_793;

	mov.b32 	 %r1251, %f2507;
	shr.u32 	%r1252, %r1251, 23;
	bfe.u32 	%r3342, %r1251, 23, 8;
	add.s32 	%r3343, %r3342, -128;
	shl.b32 	%r3344, %r1251, 8;
	or.b32  	%r1253, %r3344, -2147483648;
	shr.u32 	%r1254, %r3343, 5;
	mov.u32 	%r4318, 0;
	mov.u64 	%rd382, __cudart_i2opi_f;
	mov.u32 	%r4317, -6;
	mov.u64 	%rd456, %rd1;

BB84_785:
	.pragma "nounroll";
	ld.const.u32 	%r3347, [%rd382];
	// inline asm
	{
	mad.lo.cc.u32   %r3345, %r3347, %r1253, %r4318;
	madc.hi.u32     %r4318, %r3347, %r1253,  0;
	}
	// inline asm
	st.local.u32 	[%rd456], %r3345;
	add.s64 	%rd456, %rd456, 4;
	add.s64 	%rd382, %rd382, 4;
	add.s32 	%r4317, %r4317, 1;
	setp.ne.s32	%p492, %r4317, 0;
	@%p492 bra 	BB84_785;

	and.b32  	%r1259, %r1251, -2147483648;
	st.local.u32 	[%rd3], %r4318;
	mov.u32 	%r3350, 6;
	sub.s32 	%r3351, %r3350, %r1254;
	mul.wide.s32 	%rd317, %r3351, 4;
	add.s64 	%rd178, %rd1, %rd317;
	ld.local.u32 	%r4319, [%rd178];
	ld.local.u32 	%r4320, [%rd178+-4];
	and.b32  	%r1262, %r1252, 31;
	setp.eq.s32	%p493, %r1262, 0;
	@%p493 bra 	BB84_788;

	mov.u32 	%r3352, 32;
	sub.s32 	%r3353, %r3352, %r1262;
	shr.u32 	%r3354, %r4320, %r3353;
	shl.b32 	%r3355, %r4319, %r1262;
	add.s32 	%r4319, %r3354, %r3355;
	ld.local.u32 	%r3356, [%rd178+-8];
	shr.u32 	%r3357, %r3356, %r3353;
	shl.b32 	%r3358, %r4320, %r1262;
	add.s32 	%r4320, %r3357, %r3358;

BB84_788:
	shr.u32 	%r3359, %r4320, 30;
	shl.b32 	%r3360, %r4319, 2;
	add.s32 	%r4321, %r3359, %r3360;
	shl.b32 	%r1268, %r4320, 2;
	shr.u32 	%r3361, %r4321, 31;
	shr.u32 	%r3362, %r4319, 30;
	add.s32 	%r1269, %r3361, %r3362;
	setp.eq.s32	%p494, %r3361, 0;
	mov.u32 	%r4322, %r1259;
	mov.u32 	%r4323, %r1268;
	@%p494 bra 	BB84_790;

	not.b32 	%r3363, %r4321;
	neg.s32 	%r1270, %r1268;
	setp.eq.s32	%p495, %r1268, 0;
	selp.u32	%r3364, 1, 0, %p495;
	add.s32 	%r4321, %r3364, %r3363;
	xor.b32  	%r1272, %r1259, -2147483648;
	mov.u32 	%r4322, %r1272;
	mov.u32 	%r4323, %r1270;

BB84_790:
	mov.u32 	%r1274, %r4322;
	neg.s32 	%r3365, %r1269;
	setp.eq.s32	%p496, %r1259, 0;
	selp.b32	%r4326, %r1269, %r3365, %p496;
	clz.b32 	%r4325, %r4321;
	setp.eq.s32	%p497, %r4325, 0;
	shl.b32 	%r3366, %r4321, %r4325;
	mov.u32 	%r3367, 32;
	sub.s32 	%r3368, %r3367, %r4325;
	shr.u32 	%r3369, %r4323, %r3368;
	add.s32 	%r3370, %r3369, %r3366;
	selp.b32	%r1278, %r4321, %r3370, %p497;
	mov.u32 	%r3371, -921707870;
	mul.hi.u32 	%r4324, %r1278, %r3371;
	setp.lt.s32	%p498, %r4324, 1;
	@%p498 bra 	BB84_792;

	mul.lo.s32 	%r3372, %r1278, -921707870;
	shr.u32 	%r3373, %r3372, 31;
	shl.b32 	%r3374, %r4324, 1;
	add.s32 	%r4324, %r3373, %r3374;
	add.s32 	%r4325, %r4325, 1;

BB84_792:
	mov.u32 	%r3375, 126;
	sub.s32 	%r3376, %r3375, %r4325;
	shl.b32 	%r3377, %r3376, 23;
	add.s32 	%r3378, %r4324, 1;
	shr.u32 	%r3379, %r3378, 7;
	add.s32 	%r3380, %r3379, 1;
	shr.u32 	%r3381, %r3380, 1;
	add.s32 	%r3382, %r3381, %r3377;
	or.b32  	%r3383, %r3382, %r1274;
	mov.b32 	 %f2433, %r3383;

BB84_793:
	mul.rn.f32 	%f679, %f2433, %f2433;
	and.b32  	%r1285, %r4326, 1;
	setp.eq.s32	%p499, %r1285, 0;
	@%p499 bra 	BB84_795;

	mov.f32 	%f1904, 0fBAB6061A;
	mov.f32 	%f1905, 0f37CCF5CE;
	fma.rn.f32 	%f2434, %f1905, %f679, %f1904;
	bra.uni 	BB84_796;

BB84_795:
	mov.f32 	%f1906, 0f3C08839E;
	mov.f32 	%f1907, 0fB94CA1F9;
	fma.rn.f32 	%f2434, %f1907, %f679, %f1906;

BB84_796:
	@%p499 bra 	BB84_798;

	mov.f32 	%f1908, 0f3D2AAAA5;
	fma.rn.f32 	%f1909, %f2434, %f679, %f1908;
	mov.f32 	%f1910, 0fBF000000;
	fma.rn.f32 	%f2435, %f1909, %f679, %f1910;
	bra.uni 	BB84_799;

BB84_798:
	mov.f32 	%f1911, 0fBE2AAAA3;
	fma.rn.f32 	%f1912, %f2434, %f679, %f1911;
	mov.f32 	%f1913, 0f00000000;
	fma.rn.f32 	%f2435, %f1912, %f679, %f1913;

BB84_799:
	fma.rn.f32 	%f2436, %f2435, %f2433, %f2433;
	@%p499 bra 	BB84_801;

	mov.f32 	%f1914, 0f3F800000;
	fma.rn.f32 	%f2436, %f2435, %f679, %f1914;

BB84_801:
	and.b32  	%r3384, %r4326, 2;
	setp.eq.s32	%p502, %r3384, 0;
	@%p502 bra 	BB84_803;

	mov.f32 	%f1915, 0f00000000;
	mov.f32 	%f1916, 0fBF800000;
	fma.rn.f32 	%f2436, %f2436, %f1916, %f1915;

BB84_803:
	mul.f32 	%f691, %f2432, %f2436;
	mov.f32 	%f2526, %f494;
	@%p386 bra 	BB84_805;

	mov.f32 	%f1917, 0f00000000;
	mul.rn.f32 	%f2526, %f494, %f1917;

BB84_805:
	mul.f32 	%f1918, %f2526, 0f3F22F983;
	cvt.rni.s32.f32	%r4336, %f1918;
	cvt.rn.f32.s32	%f1919, %r4336;
	neg.f32 	%f1920, %f1919;
	fma.rn.f32 	%f1922, %f1920, %f1668, %f2526;
	fma.rn.f32 	%f1924, %f1920, %f1670, %f1922;
	fma.rn.f32 	%f2437, %f1920, %f1672, %f1924;
	abs.f32 	%f1926, %f2526;
	setp.leu.f32	%p504, %f1926, 0f47CE4780;
	@%p504 bra 	BB84_815;

	mov.b32 	 %r1287, %f2526;
	shr.u32 	%r1288, %r1287, 23;
	bfe.u32 	%r3387, %r1287, 23, 8;
	add.s32 	%r3388, %r3387, -128;
	shl.b32 	%r3389, %r1287, 8;
	or.b32  	%r1289, %r3389, -2147483648;
	shr.u32 	%r1290, %r3388, 5;
	mov.u32 	%r4328, 0;
	mov.u64 	%rd383, __cudart_i2opi_f;
	mov.u32 	%r4327, -6;
	mov.u64 	%rd455, %rd1;

BB84_807:
	.pragma "nounroll";
	ld.const.u32 	%r3392, [%rd383];
	// inline asm
	{
	mad.lo.cc.u32   %r3390, %r3392, %r1289, %r4328;
	madc.hi.u32     %r4328, %r3392, %r1289,  0;
	}
	// inline asm
	st.local.u32 	[%rd455], %r3390;
	add.s64 	%rd455, %rd455, 4;
	add.s64 	%rd383, %rd383, 4;
	add.s32 	%r4327, %r4327, 1;
	setp.ne.s32	%p505, %r4327, 0;
	@%p505 bra 	BB84_807;

	and.b32  	%r1295, %r1287, -2147483648;
	st.local.u32 	[%rd3], %r4328;
	mov.u32 	%r3395, 6;
	sub.s32 	%r3396, %r3395, %r1290;
	mul.wide.s32 	%rd319, %r3396, 4;
	add.s64 	%rd183, %rd1, %rd319;
	ld.local.u32 	%r4329, [%rd183];
	ld.local.u32 	%r4330, [%rd183+-4];
	and.b32  	%r1298, %r1288, 31;
	setp.eq.s32	%p506, %r1298, 0;
	@%p506 bra 	BB84_810;

	mov.u32 	%r3397, 32;
	sub.s32 	%r3398, %r3397, %r1298;
	shr.u32 	%r3399, %r4330, %r3398;
	shl.b32 	%r3400, %r4329, %r1298;
	add.s32 	%r4329, %r3399, %r3400;
	ld.local.u32 	%r3401, [%rd183+-8];
	shr.u32 	%r3402, %r3401, %r3398;
	shl.b32 	%r3403, %r4330, %r1298;
	add.s32 	%r4330, %r3402, %r3403;

BB84_810:
	shr.u32 	%r3404, %r4330, 30;
	shl.b32 	%r3405, %r4329, 2;
	add.s32 	%r4331, %r3404, %r3405;
	shl.b32 	%r1304, %r4330, 2;
	shr.u32 	%r3406, %r4331, 31;
	shr.u32 	%r3407, %r4329, 30;
	add.s32 	%r1305, %r3406, %r3407;
	setp.eq.s32	%p507, %r3406, 0;
	mov.u32 	%r4332, %r1295;
	mov.u32 	%r4333, %r1304;
	@%p507 bra 	BB84_812;

	not.b32 	%r3408, %r4331;
	neg.s32 	%r1306, %r1304;
	setp.eq.s32	%p508, %r1304, 0;
	selp.u32	%r3409, 1, 0, %p508;
	add.s32 	%r4331, %r3409, %r3408;
	xor.b32  	%r1308, %r1295, -2147483648;
	mov.u32 	%r4332, %r1308;
	mov.u32 	%r4333, %r1306;

BB84_812:
	mov.u32 	%r1310, %r4332;
	neg.s32 	%r3410, %r1305;
	setp.eq.s32	%p509, %r1295, 0;
	selp.b32	%r4336, %r1305, %r3410, %p509;
	clz.b32 	%r4335, %r4331;
	setp.eq.s32	%p510, %r4335, 0;
	shl.b32 	%r3411, %r4331, %r4335;
	mov.u32 	%r3412, 32;
	sub.s32 	%r3413, %r3412, %r4335;
	shr.u32 	%r3414, %r4333, %r3413;
	add.s32 	%r3415, %r3414, %r3411;
	selp.b32	%r1314, %r4331, %r3415, %p510;
	mov.u32 	%r3416, -921707870;
	mul.hi.u32 	%r4334, %r1314, %r3416;
	setp.lt.s32	%p511, %r4334, 1;
	@%p511 bra 	BB84_814;

	mul.lo.s32 	%r3417, %r1314, -921707870;
	shr.u32 	%r3418, %r3417, 31;
	shl.b32 	%r3419, %r4334, 1;
	add.s32 	%r4334, %r3418, %r3419;
	add.s32 	%r4335, %r4335, 1;

BB84_814:
	mov.u32 	%r3420, 126;
	sub.s32 	%r3421, %r3420, %r4335;
	shl.b32 	%r3422, %r3421, 23;
	add.s32 	%r3423, %r4334, 1;
	shr.u32 	%r3424, %r3423, 7;
	add.s32 	%r3425, %r3424, 1;
	shr.u32 	%r3426, %r3425, 1;
	add.s32 	%r3427, %r3426, %r3422;
	or.b32  	%r3428, %r3427, %r1310;
	mov.b32 	 %f2437, %r3428;

BB84_815:
	mul.rn.f32 	%f697, %f2437, %f2437;
	and.b32  	%r1321, %r4336, 1;
	setp.eq.s32	%p512, %r1321, 0;
	@%p512 bra 	BB84_817;

	mov.f32 	%f1927, 0fBAB6061A;
	mov.f32 	%f1928, 0f37CCF5CE;
	fma.rn.f32 	%f2438, %f1928, %f697, %f1927;
	bra.uni 	BB84_818;

BB84_817:
	mov.f32 	%f1929, 0f3C08839E;
	mov.f32 	%f1930, 0fB94CA1F9;
	fma.rn.f32 	%f2438, %f1930, %f697, %f1929;

BB84_818:
	@%p512 bra 	BB84_820;

	mov.f32 	%f1931, 0f3D2AAAA5;
	fma.rn.f32 	%f1932, %f2438, %f697, %f1931;
	mov.f32 	%f1933, 0fBF000000;
	fma.rn.f32 	%f2439, %f1932, %f697, %f1933;
	bra.uni 	BB84_821;

BB84_820:
	mov.f32 	%f1934, 0fBE2AAAA3;
	fma.rn.f32 	%f1935, %f2438, %f697, %f1934;
	mov.f32 	%f1936, 0f00000000;
	fma.rn.f32 	%f2439, %f1935, %f697, %f1936;

BB84_821:
	fma.rn.f32 	%f2440, %f2439, %f2437, %f2437;
	@%p512 bra 	BB84_823;

	mov.f32 	%f1937, 0f3F800000;
	fma.rn.f32 	%f2440, %f2439, %f697, %f1937;

BB84_823:
	and.b32  	%r3429, %r4336, 2;
	setp.eq.s32	%p515, %r3429, 0;
	@%p515 bra 	BB84_825;

	mov.f32 	%f1938, 0f00000000;
	mov.f32 	%f1939, 0fBF800000;
	fma.rn.f32 	%f2440, %f2440, %f1939, %f1938;

BB84_825:
	mul.f32 	%f1940, %f691, %f2440;
	sub.f32 	%f709, %f656, %f1940;
	mov.f32 	%f2487, %f480;
	@%p360 bra 	BB84_827;

	mov.f32 	%f1941, 0f00000000;
	mul.rn.f32 	%f2487, %f480, %f1941;

BB84_827:
	mul.f32 	%f1942, %f2487, 0f3F22F983;
	cvt.rni.s32.f32	%r4346, %f1942;
	cvt.rn.f32.s32	%f1943, %r4346;
	neg.f32 	%f1944, %f1943;
	fma.rn.f32 	%f1946, %f1944, %f1668, %f2487;
	fma.rn.f32 	%f1948, %f1944, %f1670, %f1946;
	fma.rn.f32 	%f2441, %f1944, %f1672, %f1948;
	abs.f32 	%f1950, %f2487;
	setp.leu.f32	%p517, %f1950, 0f47CE4780;
	@%p517 bra 	BB84_837;

	mov.b32 	 %r1323, %f2487;
	shr.u32 	%r1324, %r1323, 23;
	bfe.u32 	%r3432, %r1323, 23, 8;
	add.s32 	%r3433, %r3432, -128;
	shl.b32 	%r3434, %r1323, 8;
	or.b32  	%r1325, %r3434, -2147483648;
	shr.u32 	%r1326, %r3433, 5;
	mov.u32 	%r4338, 0;
	mov.u64 	%rd384, __cudart_i2opi_f;
	mov.u32 	%r4337, -6;
	mov.u64 	%rd454, %rd1;

BB84_829:
	.pragma "nounroll";
	ld.const.u32 	%r3437, [%rd384];
	// inline asm
	{
	mad.lo.cc.u32   %r3435, %r3437, %r1325, %r4338;
	madc.hi.u32     %r4338, %r3437, %r1325,  0;
	}
	// inline asm
	st.local.u32 	[%rd454], %r3435;
	add.s64 	%rd454, %rd454, 4;
	add.s64 	%rd384, %rd384, 4;
	add.s32 	%r4337, %r4337, 1;
	setp.ne.s32	%p518, %r4337, 0;
	@%p518 bra 	BB84_829;

	and.b32  	%r1331, %r1323, -2147483648;
	st.local.u32 	[%rd3], %r4338;
	mov.u32 	%r3440, 6;
	sub.s32 	%r3441, %r3440, %r1326;
	mul.wide.s32 	%rd321, %r3441, 4;
	add.s64 	%rd188, %rd1, %rd321;
	ld.local.u32 	%r4339, [%rd188];
	ld.local.u32 	%r4340, [%rd188+-4];
	and.b32  	%r1334, %r1324, 31;
	setp.eq.s32	%p519, %r1334, 0;
	@%p519 bra 	BB84_832;

	mov.u32 	%r3442, 32;
	sub.s32 	%r3443, %r3442, %r1334;
	shr.u32 	%r3444, %r4340, %r3443;
	shl.b32 	%r3445, %r4339, %r1334;
	add.s32 	%r4339, %r3444, %r3445;
	ld.local.u32 	%r3446, [%rd188+-8];
	shr.u32 	%r3447, %r3446, %r3443;
	shl.b32 	%r3448, %r4340, %r1334;
	add.s32 	%r4340, %r3447, %r3448;

BB84_832:
	shr.u32 	%r3449, %r4340, 30;
	shl.b32 	%r3450, %r4339, 2;
	add.s32 	%r4341, %r3449, %r3450;
	shl.b32 	%r1340, %r4340, 2;
	shr.u32 	%r3451, %r4341, 31;
	shr.u32 	%r3452, %r4339, 30;
	add.s32 	%r1341, %r3451, %r3452;
	setp.eq.s32	%p520, %r3451, 0;
	mov.u32 	%r4342, %r1331;
	mov.u32 	%r4343, %r1340;
	@%p520 bra 	BB84_834;

	not.b32 	%r3453, %r4341;
	neg.s32 	%r1342, %r1340;
	setp.eq.s32	%p521, %r1340, 0;
	selp.u32	%r3454, 1, 0, %p521;
	add.s32 	%r4341, %r3454, %r3453;
	xor.b32  	%r1344, %r1331, -2147483648;
	mov.u32 	%r4342, %r1344;
	mov.u32 	%r4343, %r1342;

BB84_834:
	mov.u32 	%r1346, %r4342;
	neg.s32 	%r3455, %r1341;
	setp.eq.s32	%p522, %r1331, 0;
	selp.b32	%r4346, %r1341, %r3455, %p522;
	clz.b32 	%r4345, %r4341;
	setp.eq.s32	%p523, %r4345, 0;
	shl.b32 	%r3456, %r4341, %r4345;
	mov.u32 	%r3457, 32;
	sub.s32 	%r3458, %r3457, %r4345;
	shr.u32 	%r3459, %r4343, %r3458;
	add.s32 	%r3460, %r3459, %r3456;
	selp.b32	%r1350, %r4341, %r3460, %p523;
	mov.u32 	%r3461, -921707870;
	mul.hi.u32 	%r4344, %r1350, %r3461;
	setp.lt.s32	%p524, %r4344, 1;
	@%p524 bra 	BB84_836;

	mul.lo.s32 	%r3462, %r1350, -921707870;
	shr.u32 	%r3463, %r3462, 31;
	shl.b32 	%r3464, %r4344, 1;
	add.s32 	%r4344, %r3463, %r3464;
	add.s32 	%r4345, %r4345, 1;

BB84_836:
	mov.u32 	%r3465, 126;
	sub.s32 	%r3466, %r3465, %r4345;
	shl.b32 	%r3467, %r3466, 23;
	add.s32 	%r3468, %r4344, 1;
	shr.u32 	%r3469, %r3468, 7;
	add.s32 	%r3470, %r3469, 1;
	shr.u32 	%r3471, %r3470, 1;
	add.s32 	%r3472, %r3471, %r3467;
	or.b32  	%r3473, %r3472, %r1346;
	mov.b32 	 %f2441, %r3473;

BB84_837:
	mul.rn.f32 	%f715, %f2441, %f2441;
	add.s32 	%r1357, %r4346, 1;
	and.b32  	%r1358, %r1357, 1;
	setp.eq.s32	%p525, %r1358, 0;
	@%p525 bra 	BB84_839;

	mov.f32 	%f1951, 0fBAB6061A;
	mov.f32 	%f1952, 0f37CCF5CE;
	fma.rn.f32 	%f2442, %f1952, %f715, %f1951;
	bra.uni 	BB84_840;

BB84_839:
	mov.f32 	%f1953, 0f3C08839E;
	mov.f32 	%f1954, 0fB94CA1F9;
	fma.rn.f32 	%f2442, %f1954, %f715, %f1953;

BB84_840:
	@%p525 bra 	BB84_842;

	mov.f32 	%f1955, 0f3D2AAAA5;
	fma.rn.f32 	%f1956, %f2442, %f715, %f1955;
	mov.f32 	%f1957, 0fBF000000;
	fma.rn.f32 	%f2443, %f1956, %f715, %f1957;
	bra.uni 	BB84_843;

BB84_842:
	mov.f32 	%f1958, 0fBE2AAAA3;
	fma.rn.f32 	%f1959, %f2442, %f715, %f1958;
	mov.f32 	%f1960, 0f00000000;
	fma.rn.f32 	%f2443, %f1959, %f715, %f1960;

BB84_843:
	fma.rn.f32 	%f2444, %f2443, %f2441, %f2441;
	@%p525 bra 	BB84_845;

	mov.f32 	%f1961, 0f3F800000;
	fma.rn.f32 	%f2444, %f2443, %f715, %f1961;

BB84_845:
	and.b32  	%r3474, %r1357, 2;
	setp.eq.s32	%p528, %r3474, 0;
	@%p528 bra 	BB84_847;

	mov.f32 	%f1962, 0f00000000;
	mov.f32 	%f1963, 0fBF800000;
	fma.rn.f32 	%f2444, %f2444, %f1963, %f1962;

BB84_847:
	mov.f32 	%f2506, %f487;
	@%p373 bra 	BB84_849;

	mov.f32 	%f1964, 0f00000000;
	mul.rn.f32 	%f2506, %f487, %f1964;

BB84_849:
	mul.f32 	%f1965, %f2506, 0f3F22F983;
	cvt.rni.s32.f32	%r4356, %f1965;
	cvt.rn.f32.s32	%f1966, %r4356;
	neg.f32 	%f1967, %f1966;
	fma.rn.f32 	%f1969, %f1967, %f1668, %f2506;
	fma.rn.f32 	%f1971, %f1967, %f1670, %f1969;
	fma.rn.f32 	%f2445, %f1967, %f1672, %f1971;
	abs.f32 	%f1973, %f2506;
	setp.leu.f32	%p530, %f1973, 0f47CE4780;
	@%p530 bra 	BB84_859;

	mov.b32 	 %r1360, %f2506;
	shr.u32 	%r1361, %r1360, 23;
	bfe.u32 	%r3477, %r1360, 23, 8;
	add.s32 	%r3478, %r3477, -128;
	shl.b32 	%r3479, %r1360, 8;
	or.b32  	%r1362, %r3479, -2147483648;
	shr.u32 	%r1363, %r3478, 5;
	mov.u32 	%r4348, 0;
	mov.u64 	%rd385, __cudart_i2opi_f;
	mov.u32 	%r4347, -6;
	mov.u64 	%rd453, %rd1;

BB84_851:
	.pragma "nounroll";
	ld.const.u32 	%r3482, [%rd385];
	// inline asm
	{
	mad.lo.cc.u32   %r3480, %r3482, %r1362, %r4348;
	madc.hi.u32     %r4348, %r3482, %r1362,  0;
	}
	// inline asm
	st.local.u32 	[%rd453], %r3480;
	add.s64 	%rd453, %rd453, 4;
	add.s64 	%rd385, %rd385, 4;
	add.s32 	%r4347, %r4347, 1;
	setp.ne.s32	%p531, %r4347, 0;
	@%p531 bra 	BB84_851;

	and.b32  	%r1368, %r1360, -2147483648;
	st.local.u32 	[%rd3], %r4348;
	mov.u32 	%r3485, 6;
	sub.s32 	%r3486, %r3485, %r1363;
	mul.wide.s32 	%rd323, %r3486, 4;
	add.s64 	%rd193, %rd1, %rd323;
	ld.local.u32 	%r4349, [%rd193];
	ld.local.u32 	%r4350, [%rd193+-4];
	and.b32  	%r1371, %r1361, 31;
	setp.eq.s32	%p532, %r1371, 0;
	@%p532 bra 	BB84_854;

	mov.u32 	%r3487, 32;
	sub.s32 	%r3488, %r3487, %r1371;
	shr.u32 	%r3489, %r4350, %r3488;
	shl.b32 	%r3490, %r4349, %r1371;
	add.s32 	%r4349, %r3489, %r3490;
	ld.local.u32 	%r3491, [%rd193+-8];
	shr.u32 	%r3492, %r3491, %r3488;
	shl.b32 	%r3493, %r4350, %r1371;
	add.s32 	%r4350, %r3492, %r3493;

BB84_854:
	shr.u32 	%r3494, %r4350, 30;
	shl.b32 	%r3495, %r4349, 2;
	add.s32 	%r4351, %r3494, %r3495;
	shl.b32 	%r1377, %r4350, 2;
	shr.u32 	%r3496, %r4351, 31;
	shr.u32 	%r3497, %r4349, 30;
	add.s32 	%r1378, %r3496, %r3497;
	setp.eq.s32	%p533, %r3496, 0;
	mov.u32 	%r4352, %r1368;
	mov.u32 	%r4353, %r1377;
	@%p533 bra 	BB84_856;

	not.b32 	%r3498, %r4351;
	neg.s32 	%r1379, %r1377;
	setp.eq.s32	%p534, %r1377, 0;
	selp.u32	%r3499, 1, 0, %p534;
	add.s32 	%r4351, %r3499, %r3498;
	xor.b32  	%r1381, %r1368, -2147483648;
	mov.u32 	%r4352, %r1381;
	mov.u32 	%r4353, %r1379;

BB84_856:
	mov.u32 	%r1383, %r4352;
	neg.s32 	%r3500, %r1378;
	setp.eq.s32	%p535, %r1368, 0;
	selp.b32	%r4356, %r1378, %r3500, %p535;
	clz.b32 	%r4355, %r4351;
	setp.eq.s32	%p536, %r4355, 0;
	shl.b32 	%r3501, %r4351, %r4355;
	mov.u32 	%r3502, 32;
	sub.s32 	%r3503, %r3502, %r4355;
	shr.u32 	%r3504, %r4353, %r3503;
	add.s32 	%r3505, %r3504, %r3501;
	selp.b32	%r1387, %r4351, %r3505, %p536;
	mov.u32 	%r3506, -921707870;
	mul.hi.u32 	%r4354, %r1387, %r3506;
	setp.lt.s32	%p537, %r4354, 1;
	@%p537 bra 	BB84_858;

	mul.lo.s32 	%r3507, %r1387, -921707870;
	shr.u32 	%r3508, %r3507, 31;
	shl.b32 	%r3509, %r4354, 1;
	add.s32 	%r4354, %r3508, %r3509;
	add.s32 	%r4355, %r4355, 1;

BB84_858:
	mov.u32 	%r3510, 126;
	sub.s32 	%r3511, %r3510, %r4355;
	shl.b32 	%r3512, %r3511, 23;
	add.s32 	%r3513, %r4354, 1;
	shr.u32 	%r3514, %r3513, 7;
	add.s32 	%r3515, %r3514, 1;
	shr.u32 	%r3516, %r3515, 1;
	add.s32 	%r3517, %r3516, %r3512;
	or.b32  	%r3518, %r3517, %r1383;
	mov.b32 	 %f2445, %r3518;

BB84_859:
	mul.rn.f32 	%f732, %f2445, %f2445;
	and.b32  	%r1394, %r4356, 1;
	setp.eq.s32	%p538, %r1394, 0;
	@%p538 bra 	BB84_861;

	mov.f32 	%f1974, 0fBAB6061A;
	mov.f32 	%f1975, 0f37CCF5CE;
	fma.rn.f32 	%f2446, %f1975, %f732, %f1974;
	bra.uni 	BB84_862;

BB84_861:
	mov.f32 	%f1976, 0f3C08839E;
	mov.f32 	%f1977, 0fB94CA1F9;
	fma.rn.f32 	%f2446, %f1977, %f732, %f1976;

BB84_862:
	@%p538 bra 	BB84_864;

	mov.f32 	%f1978, 0f3D2AAAA5;
	fma.rn.f32 	%f1979, %f2446, %f732, %f1978;
	mov.f32 	%f1980, 0fBF000000;
	fma.rn.f32 	%f2447, %f1979, %f732, %f1980;
	bra.uni 	BB84_865;

BB84_864:
	mov.f32 	%f1981, 0fBE2AAAA3;
	fma.rn.f32 	%f1982, %f2446, %f732, %f1981;
	mov.f32 	%f1983, 0f00000000;
	fma.rn.f32 	%f2447, %f1982, %f732, %f1983;

BB84_865:
	fma.rn.f32 	%f2448, %f2447, %f2445, %f2445;
	@%p538 bra 	BB84_867;

	mov.f32 	%f1984, 0f3F800000;
	fma.rn.f32 	%f2448, %f2447, %f732, %f1984;

BB84_867:
	and.b32  	%r3519, %r4356, 2;
	setp.eq.s32	%p541, %r3519, 0;
	@%p541 bra 	BB84_869;

	mov.f32 	%f1985, 0f00000000;
	mov.f32 	%f1986, 0fBF800000;
	fma.rn.f32 	%f2448, %f2448, %f1986, %f1985;

BB84_869:
	mul.f32 	%f744, %f2444, %f2448;
	mov.f32 	%f2525, %f494;
	@%p386 bra 	BB84_871;

	mov.f32 	%f1987, 0f00000000;
	mul.rn.f32 	%f2525, %f494, %f1987;

BB84_871:
	mul.f32 	%f1988, %f2525, 0f3F22F983;
	cvt.rni.s32.f32	%r4366, %f1988;
	cvt.rn.f32.s32	%f1989, %r4366;
	neg.f32 	%f1990, %f1989;
	fma.rn.f32 	%f1992, %f1990, %f1668, %f2525;
	fma.rn.f32 	%f1994, %f1990, %f1670, %f1992;
	fma.rn.f32 	%f2449, %f1990, %f1672, %f1994;
	abs.f32 	%f1996, %f2525;
	setp.leu.f32	%p543, %f1996, 0f47CE4780;
	@%p543 bra 	BB84_881;

	mov.b32 	 %r1396, %f2525;
	shr.u32 	%r1397, %r1396, 23;
	bfe.u32 	%r3522, %r1396, 23, 8;
	add.s32 	%r3523, %r3522, -128;
	shl.b32 	%r3524, %r1396, 8;
	or.b32  	%r1398, %r3524, -2147483648;
	shr.u32 	%r1399, %r3523, 5;
	mov.u32 	%r4358, 0;
	mov.u64 	%rd386, __cudart_i2opi_f;
	mov.u32 	%r4357, -6;
	mov.u64 	%rd452, %rd1;

BB84_873:
	.pragma "nounroll";
	ld.const.u32 	%r3527, [%rd386];
	// inline asm
	{
	mad.lo.cc.u32   %r3525, %r3527, %r1398, %r4358;
	madc.hi.u32     %r4358, %r3527, %r1398,  0;
	}
	// inline asm
	st.local.u32 	[%rd452], %r3525;
	add.s64 	%rd452, %rd452, 4;
	add.s64 	%rd386, %rd386, 4;
	add.s32 	%r4357, %r4357, 1;
	setp.ne.s32	%p544, %r4357, 0;
	@%p544 bra 	BB84_873;

	and.b32  	%r1404, %r1396, -2147483648;
	st.local.u32 	[%rd3], %r4358;
	mov.u32 	%r3530, 6;
	sub.s32 	%r3531, %r3530, %r1399;
	mul.wide.s32 	%rd325, %r3531, 4;
	add.s64 	%rd198, %rd1, %rd325;
	ld.local.u32 	%r4359, [%rd198];
	ld.local.u32 	%r4360, [%rd198+-4];
	and.b32  	%r1407, %r1397, 31;
	setp.eq.s32	%p545, %r1407, 0;
	@%p545 bra 	BB84_876;

	mov.u32 	%r3532, 32;
	sub.s32 	%r3533, %r3532, %r1407;
	shr.u32 	%r3534, %r4360, %r3533;
	shl.b32 	%r3535, %r4359, %r1407;
	add.s32 	%r4359, %r3534, %r3535;
	ld.local.u32 	%r3536, [%rd198+-8];
	shr.u32 	%r3537, %r3536, %r3533;
	shl.b32 	%r3538, %r4360, %r1407;
	add.s32 	%r4360, %r3537, %r3538;

BB84_876:
	shr.u32 	%r3539, %r4360, 30;
	shl.b32 	%r3540, %r4359, 2;
	add.s32 	%r4361, %r3539, %r3540;
	shl.b32 	%r1413, %r4360, 2;
	shr.u32 	%r3541, %r4361, 31;
	shr.u32 	%r3542, %r4359, 30;
	add.s32 	%r1414, %r3541, %r3542;
	setp.eq.s32	%p546, %r3541, 0;
	mov.u32 	%r4362, %r1404;
	mov.u32 	%r4363, %r1413;
	@%p546 bra 	BB84_878;

	not.b32 	%r3543, %r4361;
	neg.s32 	%r1415, %r1413;
	setp.eq.s32	%p547, %r1413, 0;
	selp.u32	%r3544, 1, 0, %p547;
	add.s32 	%r4361, %r3544, %r3543;
	xor.b32  	%r1417, %r1404, -2147483648;
	mov.u32 	%r4362, %r1417;
	mov.u32 	%r4363, %r1415;

BB84_878:
	mov.u32 	%r1419, %r4362;
	neg.s32 	%r3545, %r1414;
	setp.eq.s32	%p548, %r1404, 0;
	selp.b32	%r4366, %r1414, %r3545, %p548;
	clz.b32 	%r4365, %r4361;
	setp.eq.s32	%p549, %r4365, 0;
	shl.b32 	%r3546, %r4361, %r4365;
	mov.u32 	%r3547, 32;
	sub.s32 	%r3548, %r3547, %r4365;
	shr.u32 	%r3549, %r4363, %r3548;
	add.s32 	%r3550, %r3549, %r3546;
	selp.b32	%r1423, %r4361, %r3550, %p549;
	mov.u32 	%r3551, -921707870;
	mul.hi.u32 	%r4364, %r1423, %r3551;
	setp.lt.s32	%p550, %r4364, 1;
	@%p550 bra 	BB84_880;

	mul.lo.s32 	%r3552, %r1423, -921707870;
	shr.u32 	%r3553, %r3552, 31;
	shl.b32 	%r3554, %r4364, 1;
	add.s32 	%r4364, %r3553, %r3554;
	add.s32 	%r4365, %r4365, 1;

BB84_880:
	mov.u32 	%r3555, 126;
	sub.s32 	%r3556, %r3555, %r4365;
	shl.b32 	%r3557, %r3556, 23;
	add.s32 	%r3558, %r4364, 1;
	shr.u32 	%r3559, %r3558, 7;
	add.s32 	%r3560, %r3559, 1;
	shr.u32 	%r3561, %r3560, 1;
	add.s32 	%r3562, %r3561, %r3557;
	or.b32  	%r3563, %r3562, %r1419;
	mov.b32 	 %f2449, %r3563;

BB84_881:
	mul.rn.f32 	%f750, %f2449, %f2449;
	add.s32 	%r1430, %r4366, 1;
	and.b32  	%r1431, %r1430, 1;
	setp.eq.s32	%p551, %r1431, 0;
	@%p551 bra 	BB84_883;

	mov.f32 	%f1997, 0fBAB6061A;
	mov.f32 	%f1998, 0f37CCF5CE;
	fma.rn.f32 	%f2450, %f1998, %f750, %f1997;
	bra.uni 	BB84_884;

BB84_883:
	mov.f32 	%f1999, 0f3C08839E;
	mov.f32 	%f2000, 0fB94CA1F9;
	fma.rn.f32 	%f2450, %f2000, %f750, %f1999;

BB84_884:
	@%p551 bra 	BB84_886;

	mov.f32 	%f2001, 0f3D2AAAA5;
	fma.rn.f32 	%f2002, %f2450, %f750, %f2001;
	mov.f32 	%f2003, 0fBF000000;
	fma.rn.f32 	%f2451, %f2002, %f750, %f2003;
	bra.uni 	BB84_887;

BB84_886:
	mov.f32 	%f2004, 0fBE2AAAA3;
	fma.rn.f32 	%f2005, %f2450, %f750, %f2004;
	mov.f32 	%f2006, 0f00000000;
	fma.rn.f32 	%f2451, %f2005, %f750, %f2006;

BB84_887:
	fma.rn.f32 	%f2452, %f2451, %f2449, %f2449;
	@%p551 bra 	BB84_889;

	mov.f32 	%f2007, 0f3F800000;
	fma.rn.f32 	%f2452, %f2451, %f750, %f2007;

BB84_889:
	and.b32  	%r3564, %r1430, 2;
	setp.eq.s32	%p554, %r3564, 0;
	@%p554 bra 	BB84_891;

	mov.f32 	%f2008, 0f00000000;
	mov.f32 	%f2009, 0fBF800000;
	fma.rn.f32 	%f2452, %f2452, %f2009, %f2008;

BB84_891:
	mul.f32 	%f762, %f744, %f2452;
	mov.f32 	%f2486, %f480;
	@%p360 bra 	BB84_893;

	mov.f32 	%f2010, 0f00000000;
	mul.rn.f32 	%f2486, %f480, %f2010;

BB84_893:
	mul.f32 	%f2011, %f2486, 0f3F22F983;
	cvt.rni.s32.f32	%r4376, %f2011;
	cvt.rn.f32.s32	%f2012, %r4376;
	neg.f32 	%f2013, %f2012;
	fma.rn.f32 	%f2015, %f2013, %f1668, %f2486;
	fma.rn.f32 	%f2017, %f2013, %f1670, %f2015;
	fma.rn.f32 	%f2453, %f2013, %f1672, %f2017;
	abs.f32 	%f2019, %f2486;
	setp.leu.f32	%p556, %f2019, 0f47CE4780;
	@%p556 bra 	BB84_903;

	mov.b32 	 %r1433, %f2486;
	shr.u32 	%r1434, %r1433, 23;
	bfe.u32 	%r3567, %r1433, 23, 8;
	add.s32 	%r3568, %r3567, -128;
	shl.b32 	%r3569, %r1433, 8;
	or.b32  	%r1435, %r3569, -2147483648;
	shr.u32 	%r1436, %r3568, 5;
	mov.u32 	%r4368, 0;
	mov.u64 	%rd387, __cudart_i2opi_f;
	mov.u32 	%r4367, -6;
	mov.u64 	%rd451, %rd1;

BB84_895:
	.pragma "nounroll";
	ld.const.u32 	%r3572, [%rd387];
	// inline asm
	{
	mad.lo.cc.u32   %r3570, %r3572, %r1435, %r4368;
	madc.hi.u32     %r4368, %r3572, %r1435,  0;
	}
	// inline asm
	st.local.u32 	[%rd451], %r3570;
	add.s64 	%rd451, %rd451, 4;
	add.s64 	%rd387, %rd387, 4;
	add.s32 	%r4367, %r4367, 1;
	setp.ne.s32	%p557, %r4367, 0;
	@%p557 bra 	BB84_895;

	and.b32  	%r1441, %r1433, -2147483648;
	st.local.u32 	[%rd3], %r4368;
	mov.u32 	%r3575, 6;
	sub.s32 	%r3576, %r3575, %r1436;
	mul.wide.s32 	%rd327, %r3576, 4;
	add.s64 	%rd203, %rd1, %rd327;
	ld.local.u32 	%r4369, [%rd203];
	ld.local.u32 	%r4370, [%rd203+-4];
	and.b32  	%r1444, %r1434, 31;
	setp.eq.s32	%p558, %r1444, 0;
	@%p558 bra 	BB84_898;

	mov.u32 	%r3577, 32;
	sub.s32 	%r3578, %r3577, %r1444;
	shr.u32 	%r3579, %r4370, %r3578;
	shl.b32 	%r3580, %r4369, %r1444;
	add.s32 	%r4369, %r3579, %r3580;
	ld.local.u32 	%r3581, [%rd203+-8];
	shr.u32 	%r3582, %r3581, %r3578;
	shl.b32 	%r3583, %r4370, %r1444;
	add.s32 	%r4370, %r3582, %r3583;

BB84_898:
	shr.u32 	%r3584, %r4370, 30;
	shl.b32 	%r3585, %r4369, 2;
	add.s32 	%r4371, %r3584, %r3585;
	shl.b32 	%r1450, %r4370, 2;
	shr.u32 	%r3586, %r4371, 31;
	shr.u32 	%r3587, %r4369, 30;
	add.s32 	%r1451, %r3586, %r3587;
	setp.eq.s32	%p559, %r3586, 0;
	mov.u32 	%r4372, %r1441;
	mov.u32 	%r4373, %r1450;
	@%p559 bra 	BB84_900;

	not.b32 	%r3588, %r4371;
	neg.s32 	%r1452, %r1450;
	setp.eq.s32	%p560, %r1450, 0;
	selp.u32	%r3589, 1, 0, %p560;
	add.s32 	%r4371, %r3589, %r3588;
	xor.b32  	%r1454, %r1441, -2147483648;
	mov.u32 	%r4372, %r1454;
	mov.u32 	%r4373, %r1452;

BB84_900:
	mov.u32 	%r1456, %r4372;
	neg.s32 	%r3590, %r1451;
	setp.eq.s32	%p561, %r1441, 0;
	selp.b32	%r4376, %r1451, %r3590, %p561;
	clz.b32 	%r4375, %r4371;
	setp.eq.s32	%p562, %r4375, 0;
	shl.b32 	%r3591, %r4371, %r4375;
	mov.u32 	%r3592, 32;
	sub.s32 	%r3593, %r3592, %r4375;
	shr.u32 	%r3594, %r4373, %r3593;
	add.s32 	%r3595, %r3594, %r3591;
	selp.b32	%r1460, %r4371, %r3595, %p562;
	mov.u32 	%r3596, -921707870;
	mul.hi.u32 	%r4374, %r1460, %r3596;
	setp.lt.s32	%p563, %r4374, 1;
	@%p563 bra 	BB84_902;

	mul.lo.s32 	%r3597, %r1460, -921707870;
	shr.u32 	%r3598, %r3597, 31;
	shl.b32 	%r3599, %r4374, 1;
	add.s32 	%r4374, %r3598, %r3599;
	add.s32 	%r4375, %r4375, 1;

BB84_902:
	mov.u32 	%r3600, 126;
	sub.s32 	%r3601, %r3600, %r4375;
	shl.b32 	%r3602, %r3601, 23;
	add.s32 	%r3603, %r4374, 1;
	shr.u32 	%r3604, %r3603, 7;
	add.s32 	%r3605, %r3604, 1;
	shr.u32 	%r3606, %r3605, 1;
	add.s32 	%r3607, %r3606, %r3602;
	or.b32  	%r3608, %r3607, %r1456;
	mov.b32 	 %f2453, %r3608;

BB84_903:
	mul.rn.f32 	%f768, %f2453, %f2453;
	and.b32  	%r1467, %r4376, 1;
	setp.eq.s32	%p564, %r1467, 0;
	@%p564 bra 	BB84_905;

	mov.f32 	%f2020, 0fBAB6061A;
	mov.f32 	%f2021, 0f37CCF5CE;
	fma.rn.f32 	%f2454, %f2021, %f768, %f2020;
	bra.uni 	BB84_906;

BB84_905:
	mov.f32 	%f2022, 0f3C08839E;
	mov.f32 	%f2023, 0fB94CA1F9;
	fma.rn.f32 	%f2454, %f2023, %f768, %f2022;

BB84_906:
	@%p564 bra 	BB84_908;

	mov.f32 	%f2024, 0f3D2AAAA5;
	fma.rn.f32 	%f2025, %f2454, %f768, %f2024;
	mov.f32 	%f2026, 0fBF000000;
	fma.rn.f32 	%f2455, %f2025, %f768, %f2026;
	bra.uni 	BB84_909;

BB84_908:
	mov.f32 	%f2027, 0fBE2AAAA3;
	fma.rn.f32 	%f2028, %f2454, %f768, %f2027;
	mov.f32 	%f2029, 0f00000000;
	fma.rn.f32 	%f2455, %f2028, %f768, %f2029;

BB84_909:
	fma.rn.f32 	%f2456, %f2455, %f2453, %f2453;
	@%p564 bra 	BB84_911;

	mov.f32 	%f2030, 0f3F800000;
	fma.rn.f32 	%f2456, %f2455, %f768, %f2030;

BB84_911:
	and.b32  	%r3609, %r4376, 2;
	setp.eq.s32	%p567, %r3609, 0;
	@%p567 bra 	BB84_913;

	mov.f32 	%f2031, 0f00000000;
	mov.f32 	%f2032, 0fBF800000;
	fma.rn.f32 	%f2456, %f2456, %f2032, %f2031;

BB84_913:
	mov.f32 	%f2505, %f487;
	@%p373 bra 	BB84_915;

	mov.f32 	%f2033, 0f00000000;
	mul.rn.f32 	%f2505, %f487, %f2033;

BB84_915:
	mul.f32 	%f2034, %f2505, 0f3F22F983;
	cvt.rni.s32.f32	%r4386, %f2034;
	cvt.rn.f32.s32	%f2035, %r4386;
	neg.f32 	%f2036, %f2035;
	fma.rn.f32 	%f2038, %f2036, %f1668, %f2505;
	fma.rn.f32 	%f2040, %f2036, %f1670, %f2038;
	fma.rn.f32 	%f2457, %f2036, %f1672, %f2040;
	abs.f32 	%f2042, %f2505;
	setp.leu.f32	%p569, %f2042, 0f47CE4780;
	@%p569 bra 	BB84_925;

	mov.b32 	 %r1469, %f2505;
	shr.u32 	%r1470, %r1469, 23;
	bfe.u32 	%r3612, %r1469, 23, 8;
	add.s32 	%r3613, %r3612, -128;
	shl.b32 	%r3614, %r1469, 8;
	or.b32  	%r1471, %r3614, -2147483648;
	shr.u32 	%r1472, %r3613, 5;
	mov.u32 	%r4378, 0;
	mov.u64 	%rd388, __cudart_i2opi_f;
	mov.u32 	%r4377, -6;
	mov.u64 	%rd450, %rd1;

BB84_917:
	.pragma "nounroll";
	ld.const.u32 	%r3617, [%rd388];
	// inline asm
	{
	mad.lo.cc.u32   %r3615, %r3617, %r1471, %r4378;
	madc.hi.u32     %r4378, %r3617, %r1471,  0;
	}
	// inline asm
	st.local.u32 	[%rd450], %r3615;
	add.s64 	%rd450, %rd450, 4;
	add.s64 	%rd388, %rd388, 4;
	add.s32 	%r4377, %r4377, 1;
	setp.ne.s32	%p570, %r4377, 0;
	@%p570 bra 	BB84_917;

	and.b32  	%r1477, %r1469, -2147483648;
	st.local.u32 	[%rd3], %r4378;
	mov.u32 	%r3620, 6;
	sub.s32 	%r3621, %r3620, %r1472;
	mul.wide.s32 	%rd329, %r3621, 4;
	add.s64 	%rd208, %rd1, %rd329;
	ld.local.u32 	%r4379, [%rd208];
	ld.local.u32 	%r4380, [%rd208+-4];
	and.b32  	%r1480, %r1470, 31;
	setp.eq.s32	%p571, %r1480, 0;
	@%p571 bra 	BB84_920;

	mov.u32 	%r3622, 32;
	sub.s32 	%r3623, %r3622, %r1480;
	shr.u32 	%r3624, %r4380, %r3623;
	shl.b32 	%r3625, %r4379, %r1480;
	add.s32 	%r4379, %r3624, %r3625;
	ld.local.u32 	%r3626, [%rd208+-8];
	shr.u32 	%r3627, %r3626, %r3623;
	shl.b32 	%r3628, %r4380, %r1480;
	add.s32 	%r4380, %r3627, %r3628;

BB84_920:
	shr.u32 	%r3629, %r4380, 30;
	shl.b32 	%r3630, %r4379, 2;
	add.s32 	%r4381, %r3629, %r3630;
	shl.b32 	%r1486, %r4380, 2;
	shr.u32 	%r3631, %r4381, 31;
	shr.u32 	%r3632, %r4379, 30;
	add.s32 	%r1487, %r3631, %r3632;
	setp.eq.s32	%p572, %r3631, 0;
	mov.u32 	%r4382, %r1477;
	mov.u32 	%r4383, %r1486;
	@%p572 bra 	BB84_922;

	not.b32 	%r3633, %r4381;
	neg.s32 	%r1488, %r1486;
	setp.eq.s32	%p573, %r1486, 0;
	selp.u32	%r3634, 1, 0, %p573;
	add.s32 	%r4381, %r3634, %r3633;
	xor.b32  	%r1490, %r1477, -2147483648;
	mov.u32 	%r4382, %r1490;
	mov.u32 	%r4383, %r1488;

BB84_922:
	mov.u32 	%r1492, %r4382;
	neg.s32 	%r3635, %r1487;
	setp.eq.s32	%p574, %r1477, 0;
	selp.b32	%r4386, %r1487, %r3635, %p574;
	clz.b32 	%r4385, %r4381;
	setp.eq.s32	%p575, %r4385, 0;
	shl.b32 	%r3636, %r4381, %r4385;
	mov.u32 	%r3637, 32;
	sub.s32 	%r3638, %r3637, %r4385;
	shr.u32 	%r3639, %r4383, %r3638;
	add.s32 	%r3640, %r3639, %r3636;
	selp.b32	%r1496, %r4381, %r3640, %p575;
	mov.u32 	%r3641, -921707870;
	mul.hi.u32 	%r4384, %r1496, %r3641;
	setp.lt.s32	%p576, %r4384, 1;
	@%p576 bra 	BB84_924;

	mul.lo.s32 	%r3642, %r1496, -921707870;
	shr.u32 	%r3643, %r3642, 31;
	shl.b32 	%r3644, %r4384, 1;
	add.s32 	%r4384, %r3643, %r3644;
	add.s32 	%r4385, %r4385, 1;

BB84_924:
	mov.u32 	%r3645, 126;
	sub.s32 	%r3646, %r3645, %r4385;
	shl.b32 	%r3647, %r3646, 23;
	add.s32 	%r3648, %r4384, 1;
	shr.u32 	%r3649, %r3648, 7;
	add.s32 	%r3650, %r3649, 1;
	shr.u32 	%r3651, %r3650, 1;
	add.s32 	%r3652, %r3651, %r3647;
	or.b32  	%r3653, %r3652, %r1492;
	mov.b32 	 %f2457, %r3653;

BB84_925:
	mul.rn.f32 	%f785, %f2457, %f2457;
	add.s32 	%r1503, %r4386, 1;
	and.b32  	%r1504, %r1503, 1;
	setp.eq.s32	%p577, %r1504, 0;
	@%p577 bra 	BB84_927;

	mov.f32 	%f2043, 0fBAB6061A;
	mov.f32 	%f2044, 0f37CCF5CE;
	fma.rn.f32 	%f2458, %f2044, %f785, %f2043;
	bra.uni 	BB84_928;

BB84_927:
	mov.f32 	%f2045, 0f3C08839E;
	mov.f32 	%f2046, 0fB94CA1F9;
	fma.rn.f32 	%f2458, %f2046, %f785, %f2045;

BB84_928:
	@%p577 bra 	BB84_930;

	mov.f32 	%f2047, 0f3D2AAAA5;
	fma.rn.f32 	%f2048, %f2458, %f785, %f2047;
	mov.f32 	%f2049, 0fBF000000;
	fma.rn.f32 	%f2459, %f2048, %f785, %f2049;
	bra.uni 	BB84_931;

BB84_930:
	mov.f32 	%f2050, 0fBE2AAAA3;
	fma.rn.f32 	%f2051, %f2458, %f785, %f2050;
	mov.f32 	%f2052, 0f00000000;
	fma.rn.f32 	%f2459, %f2051, %f785, %f2052;

BB84_931:
	fma.rn.f32 	%f2460, %f2459, %f2457, %f2457;
	@%p577 bra 	BB84_933;

	mov.f32 	%f2053, 0f3F800000;
	fma.rn.f32 	%f2460, %f2459, %f785, %f2053;

BB84_933:
	and.b32  	%r3654, %r1503, 2;
	setp.eq.s32	%p580, %r3654, 0;
	@%p580 bra 	BB84_935;

	mov.f32 	%f2054, 0f00000000;
	mov.f32 	%f2055, 0fBF800000;
	fma.rn.f32 	%f2460, %f2460, %f2055, %f2054;

BB84_935:
	mul.f32 	%f797, %f2456, %f2460;
	mov.f32 	%f2524, %f494;
	@%p386 bra 	BB84_937;

	mov.f32 	%f2056, 0f00000000;
	mul.rn.f32 	%f2524, %f494, %f2056;

BB84_937:
	mul.f32 	%f2057, %f2524, 0f3F22F983;
	cvt.rni.s32.f32	%r4396, %f2057;
	cvt.rn.f32.s32	%f2058, %r4396;
	neg.f32 	%f2059, %f2058;
	fma.rn.f32 	%f2061, %f2059, %f1668, %f2524;
	fma.rn.f32 	%f2063, %f2059, %f1670, %f2061;
	fma.rn.f32 	%f2461, %f2059, %f1672, %f2063;
	abs.f32 	%f2065, %f2524;
	setp.leu.f32	%p582, %f2065, 0f47CE4780;
	@%p582 bra 	BB84_947;

	mov.b32 	 %r1506, %f2524;
	shr.u32 	%r1507, %r1506, 23;
	bfe.u32 	%r3657, %r1506, 23, 8;
	add.s32 	%r3658, %r3657, -128;
	shl.b32 	%r3659, %r1506, 8;
	or.b32  	%r1508, %r3659, -2147483648;
	shr.u32 	%r1509, %r3658, 5;
	mov.u32 	%r4388, 0;
	mov.u64 	%rd389, __cudart_i2opi_f;
	mov.u32 	%r4387, -6;
	mov.u64 	%rd449, %rd1;

BB84_939:
	.pragma "nounroll";
	ld.const.u32 	%r3662, [%rd389];
	// inline asm
	{
	mad.lo.cc.u32   %r3660, %r3662, %r1508, %r4388;
	madc.hi.u32     %r4388, %r3662, %r1508,  0;
	}
	// inline asm
	st.local.u32 	[%rd449], %r3660;
	add.s64 	%rd449, %rd449, 4;
	add.s64 	%rd389, %rd389, 4;
	add.s32 	%r4387, %r4387, 1;
	setp.ne.s32	%p583, %r4387, 0;
	@%p583 bra 	BB84_939;

	and.b32  	%r1514, %r1506, -2147483648;
	st.local.u32 	[%rd3], %r4388;
	mov.u32 	%r3665, 6;
	sub.s32 	%r3666, %r3665, %r1509;
	mul.wide.s32 	%rd331, %r3666, 4;
	add.s64 	%rd213, %rd1, %rd331;
	ld.local.u32 	%r4389, [%rd213];
	ld.local.u32 	%r4390, [%rd213+-4];
	and.b32  	%r1517, %r1507, 31;
	setp.eq.s32	%p584, %r1517, 0;
	@%p584 bra 	BB84_942;

	mov.u32 	%r3667, 32;
	sub.s32 	%r3668, %r3667, %r1517;
	shr.u32 	%r3669, %r4390, %r3668;
	shl.b32 	%r3670, %r4389, %r1517;
	add.s32 	%r4389, %r3669, %r3670;
	ld.local.u32 	%r3671, [%rd213+-8];
	shr.u32 	%r3672, %r3671, %r3668;
	shl.b32 	%r3673, %r4390, %r1517;
	add.s32 	%r4390, %r3672, %r3673;

BB84_942:
	shr.u32 	%r3674, %r4390, 30;
	shl.b32 	%r3675, %r4389, 2;
	add.s32 	%r4391, %r3674, %r3675;
	shl.b32 	%r1523, %r4390, 2;
	shr.u32 	%r3676, %r4391, 31;
	shr.u32 	%r3677, %r4389, 30;
	add.s32 	%r1524, %r3676, %r3677;
	setp.eq.s32	%p585, %r3676, 0;
	mov.u32 	%r4392, %r1514;
	mov.u32 	%r4393, %r1523;
	@%p585 bra 	BB84_944;

	not.b32 	%r3678, %r4391;
	neg.s32 	%r1525, %r1523;
	setp.eq.s32	%p586, %r1523, 0;
	selp.u32	%r3679, 1, 0, %p586;
	add.s32 	%r4391, %r3679, %r3678;
	xor.b32  	%r1527, %r1514, -2147483648;
	mov.u32 	%r4392, %r1527;
	mov.u32 	%r4393, %r1525;

BB84_944:
	mov.u32 	%r1529, %r4392;
	neg.s32 	%r3680, %r1524;
	setp.eq.s32	%p587, %r1514, 0;
	selp.b32	%r4396, %r1524, %r3680, %p587;
	clz.b32 	%r4395, %r4391;
	setp.eq.s32	%p588, %r4395, 0;
	shl.b32 	%r3681, %r4391, %r4395;
	mov.u32 	%r3682, 32;
	sub.s32 	%r3683, %r3682, %r4395;
	shr.u32 	%r3684, %r4393, %r3683;
	add.s32 	%r3685, %r3684, %r3681;
	selp.b32	%r1533, %r4391, %r3685, %p588;
	mov.u32 	%r3686, -921707870;
	mul.hi.u32 	%r4394, %r1533, %r3686;
	setp.lt.s32	%p589, %r4394, 1;
	@%p589 bra 	BB84_946;

	mul.lo.s32 	%r3687, %r1533, -921707870;
	shr.u32 	%r3688, %r3687, 31;
	shl.b32 	%r3689, %r4394, 1;
	add.s32 	%r4394, %r3688, %r3689;
	add.s32 	%r4395, %r4395, 1;

BB84_946:
	mov.u32 	%r3690, 126;
	sub.s32 	%r3691, %r3690, %r4395;
	shl.b32 	%r3692, %r3691, 23;
	add.s32 	%r3693, %r4394, 1;
	shr.u32 	%r3694, %r3693, 7;
	add.s32 	%r3695, %r3694, 1;
	shr.u32 	%r3696, %r3695, 1;
	add.s32 	%r3697, %r3696, %r3692;
	or.b32  	%r3698, %r3697, %r1529;
	mov.b32 	 %f2461, %r3698;

BB84_947:
	mul.rn.f32 	%f803, %f2461, %f2461;
	and.b32  	%r1540, %r4396, 1;
	setp.eq.s32	%p590, %r1540, 0;
	@%p590 bra 	BB84_949;

	mov.f32 	%f2066, 0fBAB6061A;
	mov.f32 	%f2067, 0f37CCF5CE;
	fma.rn.f32 	%f2462, %f2067, %f803, %f2066;
	bra.uni 	BB84_950;

BB84_949:
	mov.f32 	%f2068, 0f3C08839E;
	mov.f32 	%f2069, 0fB94CA1F9;
	fma.rn.f32 	%f2462, %f2069, %f803, %f2068;

BB84_950:
	@%p590 bra 	BB84_952;

	mov.f32 	%f2070, 0f3D2AAAA5;
	fma.rn.f32 	%f2071, %f2462, %f803, %f2070;
	mov.f32 	%f2072, 0fBF000000;
	fma.rn.f32 	%f2463, %f2071, %f803, %f2072;
	bra.uni 	BB84_953;

BB84_952:
	mov.f32 	%f2073, 0fBE2AAAA3;
	fma.rn.f32 	%f2074, %f2462, %f803, %f2073;
	mov.f32 	%f2075, 0f00000000;
	fma.rn.f32 	%f2463, %f2074, %f803, %f2075;

BB84_953:
	fma.rn.f32 	%f2464, %f2463, %f2461, %f2461;
	@%p590 bra 	BB84_955;

	mov.f32 	%f2076, 0f3F800000;
	fma.rn.f32 	%f2464, %f2463, %f803, %f2076;

BB84_955:
	and.b32  	%r3699, %r4396, 2;
	setp.eq.s32	%p593, %r3699, 0;
	@%p593 bra 	BB84_957;

	mov.f32 	%f2077, 0f00000000;
	mov.f32 	%f2078, 0fBF800000;
	fma.rn.f32 	%f2464, %f2464, %f2078, %f2077;

BB84_957:
	mul.f32 	%f2079, %f797, %f2464;
	sub.f32 	%f815, %f762, %f2079;
	mov.f32 	%f2485, %f480;
	@%p360 bra 	BB84_959;

	mov.f32 	%f2080, 0f00000000;
	mul.rn.f32 	%f2485, %f480, %f2080;

BB84_959:
	mul.f32 	%f2081, %f2485, 0f3F22F983;
	cvt.rni.s32.f32	%r4406, %f2081;
	cvt.rn.f32.s32	%f2082, %r4406;
	neg.f32 	%f2083, %f2082;
	fma.rn.f32 	%f2085, %f2083, %f1668, %f2485;
	fma.rn.f32 	%f2087, %f2083, %f1670, %f2085;
	fma.rn.f32 	%f2465, %f2083, %f1672, %f2087;
	abs.f32 	%f2089, %f2485;
	setp.leu.f32	%p595, %f2089, 0f47CE4780;
	@%p595 bra 	BB84_969;

	mov.b32 	 %r1542, %f2485;
	shr.u32 	%r1543, %r1542, 23;
	bfe.u32 	%r3702, %r1542, 23, 8;
	add.s32 	%r3703, %r3702, -128;
	shl.b32 	%r3704, %r1542, 8;
	or.b32  	%r1544, %r3704, -2147483648;
	shr.u32 	%r1545, %r3703, 5;
	mov.u32 	%r4398, 0;
	mov.u64 	%rd390, __cudart_i2opi_f;
	mov.u32 	%r4397, -6;
	mov.u64 	%rd448, %rd1;

BB84_961:
	.pragma "nounroll";
	ld.const.u32 	%r3707, [%rd390];
	// inline asm
	{
	mad.lo.cc.u32   %r3705, %r3707, %r1544, %r4398;
	madc.hi.u32     %r4398, %r3707, %r1544,  0;
	}
	// inline asm
	st.local.u32 	[%rd448], %r3705;
	add.s64 	%rd448, %rd448, 4;
	add.s64 	%rd390, %rd390, 4;
	add.s32 	%r4397, %r4397, 1;
	setp.ne.s32	%p596, %r4397, 0;
	@%p596 bra 	BB84_961;

	and.b32  	%r1550, %r1542, -2147483648;
	st.local.u32 	[%rd3], %r4398;
	mov.u32 	%r3710, 6;
	sub.s32 	%r3711, %r3710, %r1545;
	mul.wide.s32 	%rd333, %r3711, 4;
	add.s64 	%rd218, %rd1, %rd333;
	ld.local.u32 	%r4399, [%rd218];
	ld.local.u32 	%r4400, [%rd218+-4];
	and.b32  	%r1553, %r1543, 31;
	setp.eq.s32	%p597, %r1553, 0;
	@%p597 bra 	BB84_964;

	mov.u32 	%r3712, 32;
	sub.s32 	%r3713, %r3712, %r1553;
	shr.u32 	%r3714, %r4400, %r3713;
	shl.b32 	%r3715, %r4399, %r1553;
	add.s32 	%r4399, %r3714, %r3715;
	ld.local.u32 	%r3716, [%rd218+-8];
	shr.u32 	%r3717, %r3716, %r3713;
	shl.b32 	%r3718, %r4400, %r1553;
	add.s32 	%r4400, %r3717, %r3718;

BB84_964:
	shr.u32 	%r3719, %r4400, 30;
	shl.b32 	%r3720, %r4399, 2;
	add.s32 	%r4401, %r3719, %r3720;
	shl.b32 	%r1559, %r4400, 2;
	shr.u32 	%r3721, %r4401, 31;
	shr.u32 	%r3722, %r4399, 30;
	add.s32 	%r1560, %r3721, %r3722;
	setp.eq.s32	%p598, %r3721, 0;
	mov.u32 	%r4402, %r1550;
	mov.u32 	%r4403, %r1559;
	@%p598 bra 	BB84_966;

	not.b32 	%r3723, %r4401;
	neg.s32 	%r1561, %r1559;
	setp.eq.s32	%p599, %r1559, 0;
	selp.u32	%r3724, 1, 0, %p599;
	add.s32 	%r4401, %r3724, %r3723;
	xor.b32  	%r1563, %r1550, -2147483648;
	mov.u32 	%r4402, %r1563;
	mov.u32 	%r4403, %r1561;

BB84_966:
	mov.u32 	%r1565, %r4402;
	neg.s32 	%r3725, %r1560;
	setp.eq.s32	%p600, %r1550, 0;
	selp.b32	%r4406, %r1560, %r3725, %p600;
	clz.b32 	%r4405, %r4401;
	setp.eq.s32	%p601, %r4405, 0;
	shl.b32 	%r3726, %r4401, %r4405;
	mov.u32 	%r3727, 32;
	sub.s32 	%r3728, %r3727, %r4405;
	shr.u32 	%r3729, %r4403, %r3728;
	add.s32 	%r3730, %r3729, %r3726;
	selp.b32	%r1569, %r4401, %r3730, %p601;
	mov.u32 	%r3731, -921707870;
	mul.hi.u32 	%r4404, %r1569, %r3731;
	setp.lt.s32	%p602, %r4404, 1;
	@%p602 bra 	BB84_968;

	mul.lo.s32 	%r3732, %r1569, -921707870;
	shr.u32 	%r3733, %r3732, 31;
	shl.b32 	%r3734, %r4404, 1;
	add.s32 	%r4404, %r3733, %r3734;
	add.s32 	%r4405, %r4405, 1;

BB84_968:
	mov.u32 	%r3735, 126;
	sub.s32 	%r3736, %r3735, %r4405;
	shl.b32 	%r3737, %r3736, 23;
	add.s32 	%r3738, %r4404, 1;
	shr.u32 	%r3739, %r3738, 7;
	add.s32 	%r3740, %r3739, 1;
	shr.u32 	%r3741, %r3740, 1;
	add.s32 	%r3742, %r3741, %r3737;
	or.b32  	%r3743, %r3742, %r1565;
	mov.b32 	 %f2465, %r3743;

BB84_969:
	mul.rn.f32 	%f821, %f2465, %f2465;
	add.s32 	%r1576, %r4406, 1;
	and.b32  	%r1577, %r1576, 1;
	setp.eq.s32	%p603, %r1577, 0;
	@%p603 bra 	BB84_971;

	mov.f32 	%f2090, 0fBAB6061A;
	mov.f32 	%f2091, 0f37CCF5CE;
	fma.rn.f32 	%f2466, %f2091, %f821, %f2090;
	bra.uni 	BB84_972;

BB84_971:
	mov.f32 	%f2092, 0f3C08839E;
	mov.f32 	%f2093, 0fB94CA1F9;
	fma.rn.f32 	%f2466, %f2093, %f821, %f2092;

BB84_972:
	@%p603 bra 	BB84_974;

	mov.f32 	%f2094, 0f3D2AAAA5;
	fma.rn.f32 	%f2095, %f2466, %f821, %f2094;
	mov.f32 	%f2096, 0fBF000000;
	fma.rn.f32 	%f2467, %f2095, %f821, %f2096;
	bra.uni 	BB84_975;

BB84_974:
	mov.f32 	%f2097, 0fBE2AAAA3;
	fma.rn.f32 	%f2098, %f2466, %f821, %f2097;
	mov.f32 	%f2099, 0f00000000;
	fma.rn.f32 	%f2467, %f2098, %f821, %f2099;

BB84_975:
	fma.rn.f32 	%f2468, %f2467, %f2465, %f2465;
	@%p603 bra 	BB84_977;

	mov.f32 	%f2100, 0f3F800000;
	fma.rn.f32 	%f2468, %f2467, %f821, %f2100;

BB84_977:
	and.b32  	%r3744, %r1576, 2;
	setp.eq.s32	%p606, %r3744, 0;
	@%p606 bra 	BB84_979;

	mov.f32 	%f2101, 0f00000000;
	mov.f32 	%f2102, 0fBF800000;
	fma.rn.f32 	%f2468, %f2468, %f2102, %f2101;

BB84_979:
	mov.f32 	%f2504, %f487;
	@%p373 bra 	BB84_981;

	mov.f32 	%f2103, 0f00000000;
	mul.rn.f32 	%f2504, %f487, %f2103;

BB84_981:
	mul.f32 	%f2104, %f2504, 0f3F22F983;
	cvt.rni.s32.f32	%r4416, %f2104;
	cvt.rn.f32.s32	%f2105, %r4416;
	neg.f32 	%f2106, %f2105;
	fma.rn.f32 	%f2108, %f2106, %f1668, %f2504;
	fma.rn.f32 	%f2110, %f2106, %f1670, %f2108;
	fma.rn.f32 	%f2469, %f2106, %f1672, %f2110;
	abs.f32 	%f2112, %f2504;
	setp.leu.f32	%p608, %f2112, 0f47CE4780;
	@%p608 bra 	BB84_991;

	mov.b32 	 %r1579, %f2504;
	shr.u32 	%r1580, %r1579, 23;
	bfe.u32 	%r3747, %r1579, 23, 8;
	add.s32 	%r3748, %r3747, -128;
	shl.b32 	%r3749, %r1579, 8;
	or.b32  	%r1581, %r3749, -2147483648;
	shr.u32 	%r1582, %r3748, 5;
	mov.u32 	%r4408, 0;
	mov.u64 	%rd391, __cudart_i2opi_f;
	mov.u32 	%r4407, -6;
	mov.u64 	%rd447, %rd1;

BB84_983:
	.pragma "nounroll";
	ld.const.u32 	%r3752, [%rd391];
	// inline asm
	{
	mad.lo.cc.u32   %r3750, %r3752, %r1581, %r4408;
	madc.hi.u32     %r4408, %r3752, %r1581,  0;
	}
	// inline asm
	st.local.u32 	[%rd447], %r3750;
	add.s64 	%rd447, %rd447, 4;
	add.s64 	%rd391, %rd391, 4;
	add.s32 	%r4407, %r4407, 1;
	setp.ne.s32	%p609, %r4407, 0;
	@%p609 bra 	BB84_983;

	and.b32  	%r1587, %r1579, -2147483648;
	st.local.u32 	[%rd3], %r4408;
	mov.u32 	%r3755, 6;
	sub.s32 	%r3756, %r3755, %r1582;
	mul.wide.s32 	%rd335, %r3756, 4;
	add.s64 	%rd223, %rd1, %rd335;
	ld.local.u32 	%r4409, [%rd223];
	ld.local.u32 	%r4410, [%rd223+-4];
	and.b32  	%r1590, %r1580, 31;
	setp.eq.s32	%p610, %r1590, 0;
	@%p610 bra 	BB84_986;

	mov.u32 	%r3757, 32;
	sub.s32 	%r3758, %r3757, %r1590;
	shr.u32 	%r3759, %r4410, %r3758;
	shl.b32 	%r3760, %r4409, %r1590;
	add.s32 	%r4409, %r3759, %r3760;
	ld.local.u32 	%r3761, [%rd223+-8];
	shr.u32 	%r3762, %r3761, %r3758;
	shl.b32 	%r3763, %r4410, %r1590;
	add.s32 	%r4410, %r3762, %r3763;

BB84_986:
	shr.u32 	%r3764, %r4410, 30;
	shl.b32 	%r3765, %r4409, 2;
	add.s32 	%r4411, %r3764, %r3765;
	shl.b32 	%r1596, %r4410, 2;
	shr.u32 	%r3766, %r4411, 31;
	shr.u32 	%r3767, %r4409, 30;
	add.s32 	%r1597, %r3766, %r3767;
	setp.eq.s32	%p611, %r3766, 0;
	mov.u32 	%r4412, %r1587;
	mov.u32 	%r4413, %r1596;
	@%p611 bra 	BB84_988;

	not.b32 	%r3768, %r4411;
	neg.s32 	%r1598, %r1596;
	setp.eq.s32	%p612, %r1596, 0;
	selp.u32	%r3769, 1, 0, %p612;
	add.s32 	%r4411, %r3769, %r3768;
	xor.b32  	%r1600, %r1587, -2147483648;
	mov.u32 	%r4412, %r1600;
	mov.u32 	%r4413, %r1598;

BB84_988:
	mov.u32 	%r1602, %r4412;
	neg.s32 	%r3770, %r1597;
	setp.eq.s32	%p613, %r1587, 0;
	selp.b32	%r4416, %r1597, %r3770, %p613;
	clz.b32 	%r4415, %r4411;
	setp.eq.s32	%p614, %r4415, 0;
	shl.b32 	%r3771, %r4411, %r4415;
	mov.u32 	%r3772, 32;
	sub.s32 	%r3773, %r3772, %r4415;
	shr.u32 	%r3774, %r4413, %r3773;
	add.s32 	%r3775, %r3774, %r3771;
	selp.b32	%r1606, %r4411, %r3775, %p614;
	mov.u32 	%r3776, -921707870;
	mul.hi.u32 	%r4414, %r1606, %r3776;
	setp.lt.s32	%p615, %r4414, 1;
	@%p615 bra 	BB84_990;

	mul.lo.s32 	%r3777, %r1606, -921707870;
	shr.u32 	%r3778, %r3777, 31;
	shl.b32 	%r3779, %r4414, 1;
	add.s32 	%r4414, %r3778, %r3779;
	add.s32 	%r4415, %r4415, 1;

BB84_990:
	mov.u32 	%r3780, 126;
	sub.s32 	%r3781, %r3780, %r4415;
	shl.b32 	%r3782, %r3781, 23;
	add.s32 	%r3783, %r4414, 1;
	shr.u32 	%r3784, %r3783, 7;
	add.s32 	%r3785, %r3784, 1;
	shr.u32 	%r3786, %r3785, 1;
	add.s32 	%r3787, %r3786, %r3782;
	or.b32  	%r3788, %r3787, %r1602;
	mov.b32 	 %f2469, %r3788;

BB84_991:
	mul.rn.f32 	%f838, %f2469, %f2469;
	add.s32 	%r1613, %r4416, 1;
	and.b32  	%r1614, %r1613, 1;
	setp.eq.s32	%p616, %r1614, 0;
	@%p616 bra 	BB84_993;

	mov.f32 	%f2113, 0fBAB6061A;
	mov.f32 	%f2114, 0f37CCF5CE;
	fma.rn.f32 	%f2470, %f2114, %f838, %f2113;
	bra.uni 	BB84_994;

BB84_993:
	mov.f32 	%f2115, 0f3C08839E;
	mov.f32 	%f2116, 0fB94CA1F9;
	fma.rn.f32 	%f2470, %f2116, %f838, %f2115;

BB84_994:
	@%p616 bra 	BB84_996;

	mov.f32 	%f2117, 0f3D2AAAA5;
	fma.rn.f32 	%f2118, %f2470, %f838, %f2117;
	mov.f32 	%f2119, 0fBF000000;
	fma.rn.f32 	%f2471, %f2118, %f838, %f2119;
	bra.uni 	BB84_997;

BB84_996:
	mov.f32 	%f2120, 0fBE2AAAA3;
	fma.rn.f32 	%f2121, %f2470, %f838, %f2120;
	mov.f32 	%f2122, 0f00000000;
	fma.rn.f32 	%f2471, %f2121, %f838, %f2122;

BB84_997:
	fma.rn.f32 	%f2472, %f2471, %f2469, %f2469;
	@%p616 bra 	BB84_999;

	mov.f32 	%f2123, 0f3F800000;
	fma.rn.f32 	%f2472, %f2471, %f838, %f2123;

BB84_999:
	and.b32  	%r3789, %r1613, 2;
	setp.eq.s32	%p619, %r3789, 0;
	@%p619 bra 	BB84_1001;

	mov.f32 	%f2124, 0f00000000;
	mov.f32 	%f2125, 0fBF800000;
	fma.rn.f32 	%f2472, %f2472, %f2125, %f2124;

BB84_1001:
	mul.f32 	%f850, %f2468, %f2472;
	mov.f32 	%f2523, %f494;
	@%p386 bra 	BB84_1003;

	mov.f32 	%f2126, 0f00000000;
	mul.rn.f32 	%f2523, %f494, %f2126;

BB84_1003:
	mul.f32 	%f2127, %f2523, 0f3F22F983;
	cvt.rni.s32.f32	%r4426, %f2127;
	cvt.rn.f32.s32	%f2128, %r4426;
	neg.f32 	%f2129, %f2128;
	fma.rn.f32 	%f2131, %f2129, %f1668, %f2523;
	fma.rn.f32 	%f2133, %f2129, %f1670, %f2131;
	fma.rn.f32 	%f2473, %f2129, %f1672, %f2133;
	abs.f32 	%f2135, %f2523;
	setp.leu.f32	%p621, %f2135, 0f47CE4780;
	@%p621 bra 	BB84_1013;

	mov.b32 	 %r1616, %f2523;
	shr.u32 	%r1617, %r1616, 23;
	bfe.u32 	%r3792, %r1616, 23, 8;
	add.s32 	%r3793, %r3792, -128;
	shl.b32 	%r3794, %r1616, 8;
	or.b32  	%r1618, %r3794, -2147483648;
	shr.u32 	%r1619, %r3793, 5;
	mov.u32 	%r4418, 0;
	mov.u64 	%rd392, __cudart_i2opi_f;
	mov.u32 	%r4417, -6;
	mov.u64 	%rd446, %rd1;

BB84_1005:
	.pragma "nounroll";
	ld.const.u32 	%r3797, [%rd392];
	// inline asm
	{
	mad.lo.cc.u32   %r3795, %r3797, %r1618, %r4418;
	madc.hi.u32     %r4418, %r3797, %r1618,  0;
	}
	// inline asm
	st.local.u32 	[%rd446], %r3795;
	add.s64 	%rd446, %rd446, 4;
	add.s64 	%rd392, %rd392, 4;
	add.s32 	%r4417, %r4417, 1;
	setp.ne.s32	%p622, %r4417, 0;
	@%p622 bra 	BB84_1005;

	and.b32  	%r1624, %r1616, -2147483648;
	st.local.u32 	[%rd3], %r4418;
	mov.u32 	%r3800, 6;
	sub.s32 	%r3801, %r3800, %r1619;
	mul.wide.s32 	%rd337, %r3801, 4;
	add.s64 	%rd228, %rd1, %rd337;
	ld.local.u32 	%r4419, [%rd228];
	ld.local.u32 	%r4420, [%rd228+-4];
	and.b32  	%r1627, %r1617, 31;
	setp.eq.s32	%p623, %r1627, 0;
	@%p623 bra 	BB84_1008;

	mov.u32 	%r3802, 32;
	sub.s32 	%r3803, %r3802, %r1627;
	shr.u32 	%r3804, %r4420, %r3803;
	shl.b32 	%r3805, %r4419, %r1627;
	add.s32 	%r4419, %r3804, %r3805;
	ld.local.u32 	%r3806, [%rd228+-8];
	shr.u32 	%r3807, %r3806, %r3803;
	shl.b32 	%r3808, %r4420, %r1627;
	add.s32 	%r4420, %r3807, %r3808;

BB84_1008:
	shr.u32 	%r3809, %r4420, 30;
	shl.b32 	%r3810, %r4419, 2;
	add.s32 	%r4421, %r3809, %r3810;
	shl.b32 	%r1633, %r4420, 2;
	shr.u32 	%r3811, %r4421, 31;
	shr.u32 	%r3812, %r4419, 30;
	add.s32 	%r1634, %r3811, %r3812;
	setp.eq.s32	%p624, %r3811, 0;
	mov.u32 	%r4422, %r1624;
	mov.u32 	%r4423, %r1633;
	@%p624 bra 	BB84_1010;

	not.b32 	%r3813, %r4421;
	neg.s32 	%r1635, %r1633;
	setp.eq.s32	%p625, %r1633, 0;
	selp.u32	%r3814, 1, 0, %p625;
	add.s32 	%r4421, %r3814, %r3813;
	xor.b32  	%r1637, %r1624, -2147483648;
	mov.u32 	%r4422, %r1637;
	mov.u32 	%r4423, %r1635;

BB84_1010:
	mov.u32 	%r1639, %r4422;
	neg.s32 	%r3815, %r1634;
	setp.eq.s32	%p626, %r1624, 0;
	selp.b32	%r4426, %r1634, %r3815, %p626;
	clz.b32 	%r4425, %r4421;
	setp.eq.s32	%p627, %r4425, 0;
	shl.b32 	%r3816, %r4421, %r4425;
	mov.u32 	%r3817, 32;
	sub.s32 	%r3818, %r3817, %r4425;
	shr.u32 	%r3819, %r4423, %r3818;
	add.s32 	%r3820, %r3819, %r3816;
	selp.b32	%r1643, %r4421, %r3820, %p627;
	mov.u32 	%r3821, -921707870;
	mul.hi.u32 	%r4424, %r1643, %r3821;
	setp.lt.s32	%p628, %r4424, 1;
	@%p628 bra 	BB84_1012;

	mul.lo.s32 	%r3822, %r1643, -921707870;
	shr.u32 	%r3823, %r3822, 31;
	shl.b32 	%r3824, %r4424, 1;
	add.s32 	%r4424, %r3823, %r3824;
	add.s32 	%r4425, %r4425, 1;

BB84_1012:
	mov.u32 	%r3825, 126;
	sub.s32 	%r3826, %r3825, %r4425;
	shl.b32 	%r3827, %r3826, 23;
	add.s32 	%r3828, %r4424, 1;
	shr.u32 	%r3829, %r3828, 7;
	add.s32 	%r3830, %r3829, 1;
	shr.u32 	%r3831, %r3830, 1;
	add.s32 	%r3832, %r3831, %r3827;
	or.b32  	%r3833, %r3832, %r1639;
	mov.b32 	 %f2473, %r3833;

BB84_1013:
	mul.rn.f32 	%f856, %f2473, %f2473;
	and.b32  	%r1650, %r4426, 1;
	setp.eq.s32	%p629, %r1650, 0;
	@%p629 bra 	BB84_1015;

	mov.f32 	%f2136, 0fBAB6061A;
	mov.f32 	%f2137, 0f37CCF5CE;
	fma.rn.f32 	%f2474, %f2137, %f856, %f2136;
	bra.uni 	BB84_1016;

BB84_1015:
	mov.f32 	%f2138, 0f3C08839E;
	mov.f32 	%f2139, 0fB94CA1F9;
	fma.rn.f32 	%f2474, %f2139, %f856, %f2138;

BB84_1016:
	@%p629 bra 	BB84_1018;

	mov.f32 	%f2140, 0f3D2AAAA5;
	fma.rn.f32 	%f2141, %f2474, %f856, %f2140;
	mov.f32 	%f2142, 0fBF000000;
	fma.rn.f32 	%f2475, %f2141, %f856, %f2142;
	bra.uni 	BB84_1019;

BB84_1018:
	mov.f32 	%f2143, 0fBE2AAAA3;
	fma.rn.f32 	%f2144, %f2474, %f856, %f2143;
	mov.f32 	%f2145, 0f00000000;
	fma.rn.f32 	%f2475, %f2144, %f856, %f2145;

BB84_1019:
	fma.rn.f32 	%f2476, %f2475, %f2473, %f2473;
	@%p629 bra 	BB84_1021;

	mov.f32 	%f2146, 0f3F800000;
	fma.rn.f32 	%f2476, %f2475, %f856, %f2146;

BB84_1021:
	and.b32  	%r3834, %r4426, 2;
	setp.eq.s32	%p632, %r3834, 0;
	@%p632 bra 	BB84_1023;

	mov.f32 	%f2147, 0f00000000;
	mov.f32 	%f2148, 0fBF800000;
	fma.rn.f32 	%f2476, %f2476, %f2148, %f2147;

BB84_1023:
	mul.f32 	%f868, %f850, %f2476;
	mov.f32 	%f2484, %f480;
	@%p360 bra 	BB84_1025;

	mov.f32 	%f2149, 0f00000000;
	mul.rn.f32 	%f2484, %f480, %f2149;

BB84_1025:
	mul.f32 	%f2150, %f2484, 0f3F22F983;
	cvt.rni.s32.f32	%r4436, %f2150;
	cvt.rn.f32.s32	%f2151, %r4436;
	neg.f32 	%f2152, %f2151;
	fma.rn.f32 	%f2154, %f2152, %f1668, %f2484;
	fma.rn.f32 	%f2156, %f2152, %f1670, %f2154;
	fma.rn.f32 	%f2492, %f2152, %f1672, %f2156;
	abs.f32 	%f2158, %f2484;
	setp.leu.f32	%p634, %f2158, 0f47CE4780;
	@%p634 bra 	BB84_1035;

	mov.b32 	 %r1652, %f2484;
	shr.u32 	%r1653, %r1652, 23;
	bfe.u32 	%r3837, %r1652, 23, 8;
	add.s32 	%r3838, %r3837, -128;
	shl.b32 	%r3839, %r1652, 8;
	or.b32  	%r1654, %r3839, -2147483648;
	shr.u32 	%r1655, %r3838, 5;
	mov.u32 	%r4428, 0;
	mov.u64 	%rd393, __cudart_i2opi_f;
	mov.u32 	%r4427, -6;
	mov.u64 	%rd445, %rd1;

BB84_1027:
	.pragma "nounroll";
	ld.const.u32 	%r3842, [%rd393];
	// inline asm
	{
	mad.lo.cc.u32   %r3840, %r3842, %r1654, %r4428;
	madc.hi.u32     %r4428, %r3842, %r1654,  0;
	}
	// inline asm
	st.local.u32 	[%rd445], %r3840;
	add.s64 	%rd445, %rd445, 4;
	add.s64 	%rd393, %rd393, 4;
	add.s32 	%r4427, %r4427, 1;
	setp.ne.s32	%p635, %r4427, 0;
	@%p635 bra 	BB84_1027;

	and.b32  	%r1660, %r1652, -2147483648;
	st.local.u32 	[%rd3], %r4428;
	mov.u32 	%r3845, 6;
	sub.s32 	%r3846, %r3845, %r1655;
	mul.wide.s32 	%rd339, %r3846, 4;
	add.s64 	%rd233, %rd1, %rd339;
	ld.local.u32 	%r4429, [%rd233];
	ld.local.u32 	%r4430, [%rd233+-4];
	and.b32  	%r1663, %r1653, 31;
	setp.eq.s32	%p636, %r1663, 0;
	@%p636 bra 	BB84_1030;

	mov.u32 	%r3847, 32;
	sub.s32 	%r3848, %r3847, %r1663;
	shr.u32 	%r3849, %r4430, %r3848;
	shl.b32 	%r3850, %r4429, %r1663;
	add.s32 	%r4429, %r3849, %r3850;
	ld.local.u32 	%r3851, [%rd233+-8];
	shr.u32 	%r3852, %r3851, %r3848;
	shl.b32 	%r3853, %r4430, %r1663;
	add.s32 	%r4430, %r3852, %r3853;

BB84_1030:
	shr.u32 	%r3854, %r4430, 30;
	shl.b32 	%r3855, %r4429, 2;
	add.s32 	%r4431, %r3854, %r3855;
	shl.b32 	%r1669, %r4430, 2;
	shr.u32 	%r3856, %r4431, 31;
	shr.u32 	%r3857, %r4429, 30;
	add.s32 	%r1670, %r3856, %r3857;
	setp.eq.s32	%p637, %r3856, 0;
	mov.u32 	%r4432, %r1660;
	mov.u32 	%r4433, %r1669;
	@%p637 bra 	BB84_1032;

	not.b32 	%r3858, %r4431;
	neg.s32 	%r1671, %r1669;
	setp.eq.s32	%p638, %r1669, 0;
	selp.u32	%r3859, 1, 0, %p638;
	add.s32 	%r4431, %r3859, %r3858;
	xor.b32  	%r1673, %r1660, -2147483648;
	mov.u32 	%r4432, %r1673;
	mov.u32 	%r4433, %r1671;

BB84_1032:
	mov.u32 	%r1675, %r4432;
	neg.s32 	%r3860, %r1670;
	setp.eq.s32	%p639, %r1660, 0;
	selp.b32	%r4436, %r1670, %r3860, %p639;
	clz.b32 	%r4435, %r4431;
	setp.eq.s32	%p640, %r4435, 0;
	shl.b32 	%r3861, %r4431, %r4435;
	mov.u32 	%r3862, 32;
	sub.s32 	%r3863, %r3862, %r4435;
	shr.u32 	%r3864, %r4433, %r3863;
	add.s32 	%r3865, %r3864, %r3861;
	selp.b32	%r1679, %r4431, %r3865, %p640;
	mov.u32 	%r3866, -921707870;
	mul.hi.u32 	%r4434, %r1679, %r3866;
	setp.lt.s32	%p641, %r4434, 1;
	@%p641 bra 	BB84_1034;

	mul.lo.s32 	%r3867, %r1679, -921707870;
	shr.u32 	%r3868, %r3867, 31;
	shl.b32 	%r3869, %r4434, 1;
	add.s32 	%r4434, %r3868, %r3869;
	add.s32 	%r4435, %r4435, 1;

BB84_1034:
	mov.u32 	%r3870, 126;
	sub.s32 	%r3871, %r3870, %r4435;
	shl.b32 	%r3872, %r3871, 23;
	add.s32 	%r3873, %r4434, 1;
	shr.u32 	%r3874, %r3873, 7;
	add.s32 	%r3875, %r3874, 1;
	shr.u32 	%r3876, %r3875, 1;
	add.s32 	%r3877, %r3876, %r3872;
	or.b32  	%r3878, %r3877, %r1675;
	mov.b32 	 %f2492, %r3878;

BB84_1035:
	mul.rn.f32 	%f874, %f2492, %f2492;
	and.b32  	%r1686, %r4436, 1;
	setp.eq.s32	%p642, %r1686, 0;
	@%p642 bra 	BB84_1037;

	mov.f32 	%f2159, 0fBAB6061A;
	mov.f32 	%f2160, 0f37CCF5CE;
	fma.rn.f32 	%f2493, %f2160, %f874, %f2159;
	bra.uni 	BB84_1038;

BB84_1037:
	mov.f32 	%f2161, 0f3C08839E;
	mov.f32 	%f2162, 0fB94CA1F9;
	fma.rn.f32 	%f2493, %f2162, %f874, %f2161;

BB84_1038:
	@%p642 bra 	BB84_1040;

	mov.f32 	%f2163, 0f3D2AAAA5;
	fma.rn.f32 	%f2164, %f2493, %f874, %f2163;
	mov.f32 	%f2165, 0fBF000000;
	fma.rn.f32 	%f2494, %f2164, %f874, %f2165;
	bra.uni 	BB84_1041;

BB84_1040:
	mov.f32 	%f2166, 0fBE2AAAA3;
	fma.rn.f32 	%f2167, %f2493, %f874, %f2166;
	mov.f32 	%f2168, 0f00000000;
	fma.rn.f32 	%f2494, %f2167, %f874, %f2168;

BB84_1041:
	fma.rn.f32 	%f2495, %f2494, %f2492, %f2492;
	@%p642 bra 	BB84_1043;

	mov.f32 	%f2169, 0f3F800000;
	fma.rn.f32 	%f2495, %f2494, %f874, %f2169;

BB84_1043:
	and.b32  	%r3879, %r4436, 2;
	setp.eq.s32	%p645, %r3879, 0;
	@%p645 bra 	BB84_1045;

	mov.f32 	%f2170, 0f00000000;
	mov.f32 	%f2171, 0fBF800000;
	fma.rn.f32 	%f2495, %f2495, %f2171, %f2170;

BB84_1045:
	mov.f32 	%f2503, %f487;
	@%p373 bra 	BB84_1047;

	mov.f32 	%f2172, 0f00000000;
	mul.rn.f32 	%f2503, %f487, %f2172;

BB84_1047:
	mul.f32 	%f2173, %f2503, 0f3F22F983;
	cvt.rni.s32.f32	%r4446, %f2173;
	cvt.rn.f32.s32	%f2174, %r4446;
	neg.f32 	%f2175, %f2174;
	fma.rn.f32 	%f2177, %f2175, %f1668, %f2503;
	fma.rn.f32 	%f2179, %f2175, %f1670, %f2177;
	fma.rn.f32 	%f2511, %f2175, %f1672, %f2179;
	abs.f32 	%f2181, %f2503;
	setp.leu.f32	%p647, %f2181, 0f47CE4780;
	@%p647 bra 	BB84_1057;

	mov.b32 	 %r1688, %f2503;
	shr.u32 	%r1689, %r1688, 23;
	bfe.u32 	%r3882, %r1688, 23, 8;
	add.s32 	%r3883, %r3882, -128;
	shl.b32 	%r3884, %r1688, 8;
	or.b32  	%r1690, %r3884, -2147483648;
	shr.u32 	%r1691, %r3883, 5;
	mov.u32 	%r4438, 0;
	mov.u64 	%rd394, __cudart_i2opi_f;
	mov.u32 	%r4437, -6;
	mov.u64 	%rd444, %rd1;

BB84_1049:
	.pragma "nounroll";
	ld.const.u32 	%r3887, [%rd394];
	// inline asm
	{
	mad.lo.cc.u32   %r3885, %r3887, %r1690, %r4438;
	madc.hi.u32     %r4438, %r3887, %r1690,  0;
	}
	// inline asm
	st.local.u32 	[%rd444], %r3885;
	add.s64 	%rd444, %rd444, 4;
	add.s64 	%rd394, %rd394, 4;
	add.s32 	%r4437, %r4437, 1;
	setp.ne.s32	%p648, %r4437, 0;
	@%p648 bra 	BB84_1049;

	and.b32  	%r1696, %r1688, -2147483648;
	st.local.u32 	[%rd3], %r4438;
	mov.u32 	%r3890, 6;
	sub.s32 	%r3891, %r3890, %r1691;
	mul.wide.s32 	%rd341, %r3891, 4;
	add.s64 	%rd238, %rd1, %rd341;
	ld.local.u32 	%r4439, [%rd238];
	ld.local.u32 	%r4440, [%rd238+-4];
	and.b32  	%r1699, %r1689, 31;
	setp.eq.s32	%p649, %r1699, 0;
	@%p649 bra 	BB84_1052;

	mov.u32 	%r3892, 32;
	sub.s32 	%r3893, %r3892, %r1699;
	shr.u32 	%r3894, %r4440, %r3893;
	shl.b32 	%r3895, %r4439, %r1699;
	add.s32 	%r4439, %r3894, %r3895;
	ld.local.u32 	%r3896, [%rd238+-8];
	shr.u32 	%r3897, %r3896, %r3893;
	shl.b32 	%r3898, %r4440, %r1699;
	add.s32 	%r4440, %r3897, %r3898;

BB84_1052:
	shr.u32 	%r3899, %r4440, 30;
	shl.b32 	%r3900, %r4439, 2;
	add.s32 	%r4441, %r3899, %r3900;
	shl.b32 	%r1705, %r4440, 2;
	shr.u32 	%r3901, %r4441, 31;
	shr.u32 	%r3902, %r4439, 30;
	add.s32 	%r1706, %r3901, %r3902;
	setp.eq.s32	%p650, %r3901, 0;
	mov.u32 	%r4442, %r1696;
	mov.u32 	%r4443, %r1705;
	@%p650 bra 	BB84_1054;

	not.b32 	%r3903, %r4441;
	neg.s32 	%r1707, %r1705;
	setp.eq.s32	%p651, %r1705, 0;
	selp.u32	%r3904, 1, 0, %p651;
	add.s32 	%r4441, %r3904, %r3903;
	xor.b32  	%r1709, %r1696, -2147483648;
	mov.u32 	%r4442, %r1709;
	mov.u32 	%r4443, %r1707;

BB84_1054:
	mov.u32 	%r1711, %r4442;
	neg.s32 	%r3905, %r1706;
	setp.eq.s32	%p652, %r1696, 0;
	selp.b32	%r4446, %r1706, %r3905, %p652;
	clz.b32 	%r4445, %r4441;
	setp.eq.s32	%p653, %r4445, 0;
	shl.b32 	%r3906, %r4441, %r4445;
	mov.u32 	%r3907, 32;
	sub.s32 	%r3908, %r3907, %r4445;
	shr.u32 	%r3909, %r4443, %r3908;
	add.s32 	%r3910, %r3909, %r3906;
	selp.b32	%r1715, %r4441, %r3910, %p653;
	mov.u32 	%r3911, -921707870;
	mul.hi.u32 	%r4444, %r1715, %r3911;
	setp.lt.s32	%p654, %r4444, 1;
	@%p654 bra 	BB84_1056;

	mul.lo.s32 	%r3912, %r1715, -921707870;
	shr.u32 	%r3913, %r3912, 31;
	shl.b32 	%r3914, %r4444, 1;
	add.s32 	%r4444, %r3913, %r3914;
	add.s32 	%r4445, %r4445, 1;

BB84_1056:
	mov.u32 	%r3915, 126;
	sub.s32 	%r3916, %r3915, %r4445;
	shl.b32 	%r3917, %r3916, 23;
	add.s32 	%r3918, %r4444, 1;
	shr.u32 	%r3919, %r3918, 7;
	add.s32 	%r3920, %r3919, 1;
	shr.u32 	%r3921, %r3920, 1;
	add.s32 	%r3922, %r3921, %r3917;
	or.b32  	%r3923, %r3922, %r1711;
	mov.b32 	 %f2511, %r3923;

BB84_1057:
	mul.rn.f32 	%f891, %f2511, %f2511;
	and.b32  	%r1722, %r4446, 1;
	setp.eq.s32	%p655, %r1722, 0;
	@%p655 bra 	BB84_1059;

	mov.f32 	%f2182, 0fBAB6061A;
	mov.f32 	%f2183, 0f37CCF5CE;
	fma.rn.f32 	%f2512, %f2183, %f891, %f2182;
	bra.uni 	BB84_1060;

BB84_1059:
	mov.f32 	%f2184, 0f3C08839E;
	mov.f32 	%f2185, 0fB94CA1F9;
	fma.rn.f32 	%f2512, %f2185, %f891, %f2184;

BB84_1060:
	@%p655 bra 	BB84_1062;

	mov.f32 	%f2186, 0f3D2AAAA5;
	fma.rn.f32 	%f2187, %f2512, %f891, %f2186;
	mov.f32 	%f2188, 0fBF000000;
	fma.rn.f32 	%f2513, %f2187, %f891, %f2188;
	bra.uni 	BB84_1063;

BB84_1062:
	mov.f32 	%f2189, 0fBE2AAAA3;
	fma.rn.f32 	%f2190, %f2512, %f891, %f2189;
	mov.f32 	%f2191, 0f00000000;
	fma.rn.f32 	%f2513, %f2190, %f891, %f2191;

BB84_1063:
	fma.rn.f32 	%f2514, %f2513, %f2511, %f2511;
	@%p655 bra 	BB84_1065;

	mov.f32 	%f2192, 0f3F800000;
	fma.rn.f32 	%f2514, %f2513, %f891, %f2192;

BB84_1065:
	and.b32  	%r3924, %r4446, 2;
	setp.eq.s32	%p658, %r3924, 0;
	@%p658 bra 	BB84_1067;

	mov.f32 	%f2193, 0f00000000;
	mov.f32 	%f2194, 0fBF800000;
	fma.rn.f32 	%f2514, %f2514, %f2194, %f2193;

BB84_1067:
	mul.f32 	%f903, %f2495, %f2514;
	mov.f32 	%f2522, %f494;
	@%p386 bra 	BB84_1069;

	mov.f32 	%f2195, 0f00000000;
	mul.rn.f32 	%f2522, %f494, %f2195;

BB84_1069:
	mul.f32 	%f2196, %f2522, 0f3F22F983;
	cvt.rni.s32.f32	%r4456, %f2196;
	cvt.rn.f32.s32	%f2197, %r4456;
	neg.f32 	%f2198, %f2197;
	fma.rn.f32 	%f2200, %f2198, %f1668, %f2522;
	fma.rn.f32 	%f2202, %f2198, %f1670, %f2200;
	fma.rn.f32 	%f2530, %f2198, %f1672, %f2202;
	abs.f32 	%f2204, %f2522;
	setp.leu.f32	%p660, %f2204, 0f47CE4780;
	@%p660 bra 	BB84_1079;

	mov.b32 	 %r1724, %f2522;
	shr.u32 	%r1725, %r1724, 23;
	bfe.u32 	%r3927, %r1724, 23, 8;
	add.s32 	%r3928, %r3927, -128;
	shl.b32 	%r3929, %r1724, 8;
	or.b32  	%r1726, %r3929, -2147483648;
	shr.u32 	%r1727, %r3928, 5;
	mov.u32 	%r4448, 0;
	mov.u64 	%rd395, __cudart_i2opi_f;
	mov.u32 	%r4447, -6;
	mov.u64 	%rd443, %rd1;

BB84_1071:
	.pragma "nounroll";
	ld.const.u32 	%r3932, [%rd395];
	// inline asm
	{
	mad.lo.cc.u32   %r3930, %r3932, %r1726, %r4448;
	madc.hi.u32     %r4448, %r3932, %r1726,  0;
	}
	// inline asm
	st.local.u32 	[%rd443], %r3930;
	add.s64 	%rd443, %rd443, 4;
	add.s64 	%rd395, %rd395, 4;
	add.s32 	%r4447, %r4447, 1;
	setp.ne.s32	%p661, %r4447, 0;
	@%p661 bra 	BB84_1071;

	and.b32  	%r1732, %r1724, -2147483648;
	st.local.u32 	[%rd3], %r4448;
	mov.u32 	%r3935, 6;
	sub.s32 	%r3936, %r3935, %r1727;
	mul.wide.s32 	%rd343, %r3936, 4;
	add.s64 	%rd243, %rd1, %rd343;
	ld.local.u32 	%r4449, [%rd243];
	ld.local.u32 	%r4450, [%rd243+-4];
	and.b32  	%r1735, %r1725, 31;
	setp.eq.s32	%p662, %r1735, 0;
	@%p662 bra 	BB84_1074;

	mov.u32 	%r3937, 32;
	sub.s32 	%r3938, %r3937, %r1735;
	shr.u32 	%r3939, %r4450, %r3938;
	shl.b32 	%r3940, %r4449, %r1735;
	add.s32 	%r4449, %r3939, %r3940;
	ld.local.u32 	%r3941, [%rd243+-8];
	shr.u32 	%r3942, %r3941, %r3938;
	shl.b32 	%r3943, %r4450, %r1735;
	add.s32 	%r4450, %r3942, %r3943;

BB84_1074:
	shr.u32 	%r3944, %r4450, 30;
	shl.b32 	%r3945, %r4449, 2;
	add.s32 	%r4451, %r3944, %r3945;
	shl.b32 	%r1741, %r4450, 2;
	shr.u32 	%r3946, %r4451, 31;
	shr.u32 	%r3947, %r4449, 30;
	add.s32 	%r1742, %r3946, %r3947;
	setp.eq.s32	%p663, %r3946, 0;
	mov.u32 	%r4452, %r1732;
	mov.u32 	%r4453, %r1741;
	@%p663 bra 	BB84_1076;

	not.b32 	%r3948, %r4451;
	neg.s32 	%r1743, %r1741;
	setp.eq.s32	%p664, %r1741, 0;
	selp.u32	%r3949, 1, 0, %p664;
	add.s32 	%r4451, %r3949, %r3948;
	xor.b32  	%r1745, %r1732, -2147483648;
	mov.u32 	%r4452, %r1745;
	mov.u32 	%r4453, %r1743;

BB84_1076:
	mov.u32 	%r1747, %r4452;
	neg.s32 	%r3950, %r1742;
	setp.eq.s32	%p665, %r1732, 0;
	selp.b32	%r4456, %r1742, %r3950, %p665;
	clz.b32 	%r4455, %r4451;
	setp.eq.s32	%p666, %r4455, 0;
	shl.b32 	%r3951, %r4451, %r4455;
	mov.u32 	%r3952, 32;
	sub.s32 	%r3953, %r3952, %r4455;
	shr.u32 	%r3954, %r4453, %r3953;
	add.s32 	%r3955, %r3954, %r3951;
	selp.b32	%r1751, %r4451, %r3955, %p666;
	mov.u32 	%r3956, -921707870;
	mul.hi.u32 	%r4454, %r1751, %r3956;
	setp.lt.s32	%p667, %r4454, 1;
	@%p667 bra 	BB84_1078;

	mul.lo.s32 	%r3957, %r1751, -921707870;
	shr.u32 	%r3958, %r3957, 31;
	shl.b32 	%r3959, %r4454, 1;
	add.s32 	%r4454, %r3958, %r3959;
	add.s32 	%r4455, %r4455, 1;

BB84_1078:
	mov.u32 	%r3960, 126;
	sub.s32 	%r3961, %r3960, %r4455;
	shl.b32 	%r3962, %r3961, 23;
	add.s32 	%r3963, %r4454, 1;
	shr.u32 	%r3964, %r3963, 7;
	add.s32 	%r3965, %r3964, 1;
	shr.u32 	%r3966, %r3965, 1;
	add.s32 	%r3967, %r3966, %r3962;
	or.b32  	%r3968, %r3967, %r1747;
	mov.b32 	 %f2530, %r3968;

BB84_1079:
	mul.rn.f32 	%f909, %f2530, %f2530;
	add.s32 	%r1758, %r4456, 1;
	and.b32  	%r1759, %r1758, 1;
	setp.eq.s32	%p668, %r1759, 0;
	@%p668 bra 	BB84_1081;

	mov.f32 	%f2205, 0fBAB6061A;
	mov.f32 	%f2206, 0f37CCF5CE;
	fma.rn.f32 	%f2531, %f2206, %f909, %f2205;
	bra.uni 	BB84_1082;

BB84_1081:
	mov.f32 	%f2207, 0f3C08839E;
	mov.f32 	%f2208, 0fB94CA1F9;
	fma.rn.f32 	%f2531, %f2208, %f909, %f2207;

BB84_1082:
	@%p668 bra 	BB84_1084;

	mov.f32 	%f2209, 0f3D2AAAA5;
	fma.rn.f32 	%f2210, %f2531, %f909, %f2209;
	mov.f32 	%f2211, 0fBF000000;
	fma.rn.f32 	%f2532, %f2210, %f909, %f2211;
	bra.uni 	BB84_1085;

BB84_1084:
	mov.f32 	%f2212, 0fBE2AAAA3;
	fma.rn.f32 	%f2213, %f2531, %f909, %f2212;
	mov.f32 	%f2214, 0f00000000;
	fma.rn.f32 	%f2532, %f2213, %f909, %f2214;

BB84_1085:
	fma.rn.f32 	%f2533, %f2532, %f2530, %f2530;
	@%p668 bra 	BB84_1087;

	mov.f32 	%f2215, 0f3F800000;
	fma.rn.f32 	%f2533, %f2532, %f909, %f2215;

BB84_1087:
	and.b32  	%r3969, %r1758, 2;
	setp.eq.s32	%p671, %r3969, 0;
	@%p671 bra 	BB84_1089;

	mov.f32 	%f2216, 0f00000000;
	mov.f32 	%f2217, 0fBF800000;
	fma.rn.f32 	%f2533, %f2533, %f2217, %f2216;

BB84_1089:
	ld.param.u64 	%rd347, [actfunc_quat_multi_float_param_0];
	mov.u32 	%r3976, %tid.x;
	mov.u32 	%r3975, %ctaid.x;
	mov.u32 	%r3974, %ntid.x;
	mad.lo.s32 	%r3973, %r3974, %r3975, %r3976;
	mul.wide.s32 	%rd346, %r3973, 16;
	cvta.to.global.u64 	%rd345, %rd347;
	add.s64 	%rd344, %rd345, %rd346;
	fma.rn.f32 	%f2218, %f903, %f2533, %f868;
	st.global.v4.f32 	[%rd344], {%f603, %f709, %f815, %f2218};

BB84_1090:
	ret;
}

	// .globl	actfunc_quat_multi_double
.visible .entry actfunc_quat_multi_double(
	.param .u64 actfunc_quat_multi_double_param_0,
	.param .u32 actfunc_quat_multi_double_param_1,
	.param .u32 actfunc_quat_multi_double_param_2,
	.param .u32 actfunc_quat_multi_double_param_3,
	.param .u32 actfunc_quat_multi_double_param_4
)
{
	.local .align 4 .b8 	__local_depot85[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<295>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<676>;
	.reg .f64 	%fd<3493>;
	.reg .b64 	%rd<177>;


	mov.u64 	%rd176, __local_depot85;
	cvta.local.u64 	%SP, %rd176;
	ld.param.u64 	%rd50, [actfunc_quat_multi_double_param_0];
	ld.param.u32 	%r156, [actfunc_quat_multi_double_param_4];
	add.u64 	%rd51, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd51;
	mov.u32 	%r157, %ntid.x;
	mov.u32 	%r158, %ctaid.x;
	mov.u32 	%r159, %tid.x;
	mad.lo.s32 	%r1, %r157, %r158, %r159;
	setp.ge.s32	%p1, %r1, %r156;
	@%p1 bra 	BB85_325;

	cvta.to.global.u64 	%rd99, %rd50;
	mul.wide.s32 	%rd100, %r1, 32;
	add.s64 	%rd49, %rd99, %rd100;
	ld.global.v2.f64 	{%fd496, %fd497}, [%rd49+16];
	ld.global.v2.f64 	{%fd498, %fd499}, [%rd49];
	mul.f64 	%fd500, %fd499, %fd499;
	fma.rn.f64 	%fd501, %fd498, %fd498, %fd500;
	fma.rn.f64 	%fd502, %fd496, %fd496, %fd501;
	fma.rn.f64 	%fd503, %fd497, %fd497, %fd502;
	sqrt.rn.f64 	%fd5, %fd503;
	setp.eq.f64	%p2, %fd5, 0d0000000000000000;
	mov.f64 	%fd3396, 0d0000000000000000;
	mov.f64 	%fd3395, %fd3396;
	mov.f64 	%fd3394, %fd3396;
	@%p2 bra 	BB85_174;

	div.rn.f64 	%fd6, %fd498, %fd5;
	div.rn.f64 	%fd7, %fd496, %fd5;
	div.rn.f64 	%fd8, %fd499, %fd5;
	mul.f64 	%fd505, %fd8, %fd7;
	div.rn.f64 	%fd9, %fd497, %fd5;
	mul.f64 	%fd506, %fd6, %fd9;
	sub.f64 	%fd507, %fd505, %fd506;
	add.f64 	%fd10, %fd507, %fd507;
	setp.gt.f64	%p3, %fd10, 0d3FF0000000000000;
	mov.f64 	%fd3288, 0d3FF0000000000000;
	@%p3 bra 	BB85_4;

	setp.lt.f64	%p4, %fd10, 0dBFF0000000000000;
	selp.f64	%fd3288, 0dBFF0000000000000, %fd10, %p4;

BB85_4:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd3288;
	}
	mov.b32 	 %f1, %r2;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p5, %f2, 0f3FE26666;
	@%p5 bra 	BB85_6;
	bra.uni 	BB85_5;

BB85_6:
	mul.f64 	%fd558, %fd3288, %fd3288;
	mov.f64 	%fd559, 0dBFB3823B180754AF;
	mov.f64 	%fd560, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd561, %fd560, %fd558, %fd559;
	mov.f64 	%fd562, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd563, %fd561, %fd558, %fd562;
	mov.f64 	%fd564, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd565, %fd563, %fd558, %fd564;
	mov.f64 	%fd566, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd567, %fd565, %fd558, %fd566;
	mov.f64 	%fd568, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd569, %fd567, %fd558, %fd568;
	mov.f64 	%fd570, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd571, %fd569, %fd558, %fd570;
	mov.f64 	%fd572, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd573, %fd571, %fd558, %fd572;
	mov.f64 	%fd574, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd575, %fd573, %fd558, %fd574;
	mov.f64 	%fd576, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd577, %fd575, %fd558, %fd576;
	mov.f64 	%fd578, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd579, %fd577, %fd558, %fd578;
	mov.f64 	%fd580, 0d3FB333333320F91B;
	fma.rn.f64 	%fd581, %fd579, %fd558, %fd580;
	mov.f64 	%fd582, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd583, %fd581, %fd558, %fd582;
	mul.f64 	%fd584, %fd558, %fd583;
	fma.rn.f64 	%fd3289, %fd584, %fd3288, %fd3288;
	bra.uni 	BB85_7;

BB85_5:
	abs.f64 	%fd510, %fd3288;
	mov.f64 	%fd511, 0d3FE0000000000000;
	mov.f64 	%fd512, 0dBFE0000000000000;
	fma.rn.f64 	%fd509, %fd512, %fd510, %fd511;
	// inline asm
	rsqrt.approx.ftz.f64 %fd508, %fd509;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r160, %temp}, %fd508;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r161}, %fd508;
	}
	add.s32 	%r162, %r161, -1048576;
	mov.b64 	%fd513, {%r160, %r162};
	mul.f64 	%fd514, %fd509, %fd508;
	neg.f64 	%fd515, %fd514;
	fma.rn.f64 	%fd516, %fd514, %fd515, %fd509;
	fma.rn.f64 	%fd517, %fd516, %fd513, %fd514;
	neg.f64 	%fd518, %fd517;
	mov.f64 	%fd519, 0d3FF0000000000000;
	fma.rn.f64 	%fd520, %fd508, %fd518, %fd519;
	fma.rn.f64 	%fd521, %fd520, %fd513, %fd513;
	fma.rn.f64 	%fd522, %fd517, %fd518, %fd509;
	fma.rn.f64 	%fd523, %fd522, %fd521, %fd517;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r163}, %fd509;
	}
	setp.lt.s32	%p6, %r163, 0;
	selp.f64	%fd524, 0dFFF8000000000000, %fd523, %p6;
	setp.equ.f64	%p7, %fd509, 0d0000000000000000;
	selp.f64	%fd525, %fd509, %fd524, %p7;
	mov.f64 	%fd526, 0dBFB3823B180754AF;
	mov.f64 	%fd527, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd528, %fd527, %fd509, %fd526;
	mov.f64 	%fd529, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd530, %fd528, %fd509, %fd529;
	mov.f64 	%fd531, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd532, %fd530, %fd509, %fd531;
	mov.f64 	%fd533, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd534, %fd532, %fd509, %fd533;
	mov.f64 	%fd535, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd536, %fd534, %fd509, %fd535;
	mov.f64 	%fd537, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd538, %fd536, %fd509, %fd537;
	mov.f64 	%fd539, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd540, %fd538, %fd509, %fd539;
	mov.f64 	%fd541, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd542, %fd540, %fd509, %fd541;
	mov.f64 	%fd543, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd544, %fd542, %fd509, %fd543;
	mov.f64 	%fd545, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd546, %fd544, %fd509, %fd545;
	mov.f64 	%fd547, 0d3FB333333320F91B;
	fma.rn.f64 	%fd548, %fd546, %fd509, %fd547;
	mov.f64 	%fd549, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd550, %fd548, %fd509, %fd549;
	mul.f64 	%fd551, %fd509, %fd550;
	mul.f64 	%fd552, %fd525, 0dC000000000000000;
	mov.f64 	%fd553, 0d3C91A62633145C07;
	fma.rn.f64 	%fd554, %fd552, %fd551, %fd553;
	add.f64 	%fd555, %fd552, 0d3FE921FB54442D18;
	add.f64 	%fd556, %fd555, %fd554;
	add.f64 	%fd557, %fd556, 0d3FE921FB54442D18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r164, %temp}, %fd557;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r165}, %fd557;
	}
	and.b32  	%r166, %r2, -2147483648;
	or.b32  	%r167, %r165, %r166;
	mov.b64 	%fd3289, {%r164, %r167};

BB85_7:
	mul.f64 	%fd16, %fd3289, 0dBFE0000000000000;
	setp.neu.f64	%p8, %fd16, 0d3FE921FB54442D18;
	setp.neu.f64	%p9, %fd16, 0dBFE921FB54442D18;
	and.pred  	%p10, %p8, %p9;
	mul.f64 	%fd585, %fd8, %fd8;
	mul.f64 	%fd17, %fd6, %fd6;
	sub.f64 	%fd18, %fd17, %fd585;
	@%p10 bra 	BB85_13;
	bra.uni 	BB85_8;

BB85_13:
	fma.rn.f64 	%fd650, %fd7, %fd7, %fd18;
	mul.f64 	%fd24, %fd9, %fd9;
	sub.f64 	%fd651, %fd650, %fd24;
	mul.f64 	%fd652, %fd6, %fd8;
	fma.rn.f64 	%fd653, %fd6, %fd8, %fd652;
	fma.rn.f64 	%fd654, %fd9, %fd7, %fd653;
	fma.rn.f64 	%fd655, %fd7, %fd9, %fd654;
	abs.f64 	%fd25, %fd651;
	abs.f64 	%fd26, %fd655;
	setp.eq.f64	%p22, %fd25, 0d0000000000000000;
	setp.eq.f64	%p23, %fd26, 0d0000000000000000;
	and.pred  	%p24, %p22, %p23;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd651;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r178}, %fd655;
	}
	and.b32  	%r6, %r178, -2147483648;
	@%p24 bra 	BB85_17;
	bra.uni 	BB85_14;

BB85_17:
	setp.lt.s32	%p32, %r5, 0;
	selp.f64	%fd708, 0d400921FB54442D18, 0d0000000000000000, %p32;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r185, %temp}, %fd708;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r186}, %fd708;
	}
	or.b32  	%r187, %r186, %r6;
	mov.b64 	%fd3290, {%r185, %r187};
	bra.uni 	BB85_18;

BB85_8:
	mul.f64 	%fd586, %fd7, %fd7;
	sub.f64 	%fd587, %fd18, %fd586;
	fma.rn.f64 	%fd588, %fd9, %fd9, %fd587;
	mul.f64 	%fd589, %fd6, %fd7;
	mul.f64 	%fd590, %fd8, %fd9;
	sub.f64 	%fd591, %fd589, %fd590;
	fma.rn.f64 	%fd592, %fd6, %fd7, %fd591;
	sub.f64 	%fd593, %fd592, %fd590;
	abs.f64 	%fd19, %fd588;
	abs.f64 	%fd20, %fd593;
	setp.eq.f64	%p11, %fd19, 0d0000000000000000;
	setp.eq.f64	%p12, %fd20, 0d0000000000000000;
	and.pred  	%p13, %p11, %p12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd588;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r168}, %fd593;
	}
	and.b32  	%r4, %r168, -2147483648;
	@%p13 bra 	BB85_12;
	bra.uni 	BB85_9;

BB85_12:
	setp.lt.s32	%p21, %r3, 0;
	selp.f64	%fd649, 0d400921FB54442D18, 0d0000000000000000, %p21;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r175, %temp}, %fd649;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd649;
	}
	or.b32  	%r177, %r176, %r4;
	mov.b64 	%fd3291, {%r175, %r177};
	mov.f64 	%fd3393, 0d0000000000000000;
	bra.uni 	BB85_24;

BB85_14:
	setp.eq.f64	%p25, %fd25, 0d7FF0000000000000;
	setp.eq.f64	%p26, %fd26, 0d7FF0000000000000;
	and.pred  	%p27, %p25, %p26;
	@%p27 bra 	BB85_16;
	bra.uni 	BB85_15;

BB85_16:
	setp.lt.s32	%p31, %r5, 0;
	selp.f64	%fd707, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p31;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r182, %temp}, %fd707;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r183}, %fd707;
	}
	or.b32  	%r184, %r183, %r6;
	mov.b64 	%fd3290, {%r182, %r184};
	bra.uni 	BB85_18;

BB85_9:
	setp.eq.f64	%p14, %fd19, 0d7FF0000000000000;
	setp.eq.f64	%p15, %fd20, 0d7FF0000000000000;
	and.pred  	%p16, %p14, %p15;
	@%p16 bra 	BB85_11;
	bra.uni 	BB85_10;

BB85_11:
	setp.lt.s32	%p20, %r3, 0;
	selp.f64	%fd647, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r172, %temp}, %fd647;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r173}, %fd647;
	}
	or.b32  	%r174, %r173, %r4;
	mov.b64 	%fd3291, {%r172, %r174};
	mov.f64 	%fd3393, 0d0000000000000000;
	bra.uni 	BB85_24;

BB85_15:
	setp.lt.s32	%p28, %r5, 0;
	min.f64 	%fd656, %fd26, %fd25;
	max.f64 	%fd657, %fd26, %fd25;
	div.rn.f64 	%fd658, %fd656, %fd657;
	mul.f64 	%fd659, %fd658, %fd658;
	mov.f64 	%fd660, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd661, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd662, %fd661, %fd659, %fd660;
	mov.f64 	%fd663, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd664, %fd662, %fd659, %fd663;
	mov.f64 	%fd665, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd666, %fd664, %fd659, %fd665;
	mov.f64 	%fd667, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd668, %fd666, %fd659, %fd667;
	mov.f64 	%fd669, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd670, %fd668, %fd659, %fd669;
	mov.f64 	%fd671, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd672, %fd670, %fd659, %fd671;
	mov.f64 	%fd673, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd674, %fd672, %fd659, %fd673;
	mov.f64 	%fd675, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd676, %fd674, %fd659, %fd675;
	mov.f64 	%fd677, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd678, %fd676, %fd659, %fd677;
	mov.f64 	%fd679, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd680, %fd678, %fd659, %fd679;
	mov.f64 	%fd681, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd682, %fd680, %fd659, %fd681;
	mov.f64 	%fd683, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd684, %fd682, %fd659, %fd683;
	mov.f64 	%fd685, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd686, %fd684, %fd659, %fd685;
	mov.f64 	%fd687, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd688, %fd686, %fd659, %fd687;
	mov.f64 	%fd689, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd690, %fd688, %fd659, %fd689;
	mov.f64 	%fd691, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd692, %fd690, %fd659, %fd691;
	mov.f64 	%fd693, 0d3FC99999999840D2;
	fma.rn.f64 	%fd694, %fd692, %fd659, %fd693;
	mov.f64 	%fd695, 0dBFD555555555544C;
	fma.rn.f64 	%fd696, %fd694, %fd659, %fd695;
	mul.f64 	%fd697, %fd659, %fd696;
	fma.rn.f64 	%fd698, %fd697, %fd658, %fd658;
	mov.f64 	%fd699, 0d3FF921FB54442D18;
	sub.f64 	%fd700, %fd699, %fd698;
	setp.gt.f64	%p29, %fd26, %fd25;
	selp.f64	%fd701, %fd700, %fd698, %p29;
	mov.f64 	%fd702, 0d400921FB54442D18;
	sub.f64 	%fd703, %fd702, %fd701;
	selp.f64	%fd704, %fd703, %fd701, %p28;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r179, %temp}, %fd704;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r180}, %fd704;
	}
	or.b32  	%r181, %r180, %r6;
	mov.b64 	%fd705, {%r179, %r181};
	add.f64 	%fd706, %fd25, %fd26;
	setp.gtu.f64	%p30, %fd706, 0d7FF0000000000000;
	selp.f64	%fd3290, %fd706, %fd705, %p30;

BB85_18:
	mul.f64 	%fd2858, %fd9, %fd9;
	mul.f64 	%fd2857, %fd6, %fd6;
	mul.f64 	%fd3393, %fd3290, 0d3FE0000000000000;
	fma.rn.f64 	%fd709, %fd8, %fd8, %fd2857;
	mul.f64 	%fd710, %fd7, %fd7;
	sub.f64 	%fd711, %fd709, %fd710;
	sub.f64 	%fd712, %fd711, %fd2858;
	mul.f64 	%fd713, %fd9, %fd8;
	fma.rn.f64 	%fd714, %fd6, %fd7, %fd713;
	fma.rn.f64 	%fd715, %fd6, %fd7, %fd714;
	add.f64 	%fd716, %fd715, %fd713;
	abs.f64 	%fd32, %fd712;
	abs.f64 	%fd33, %fd716;
	setp.eq.f64	%p33, %fd32, 0d0000000000000000;
	setp.eq.f64	%p34, %fd33, 0d0000000000000000;
	and.pred  	%p35, %p33, %p34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd712;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r188}, %fd716;
	}
	and.b32  	%r8, %r188, -2147483648;
	@%p35 bra 	BB85_22;
	bra.uni 	BB85_19;

BB85_22:
	setp.lt.s32	%p43, %r7, 0;
	selp.f64	%fd769, 0d400921FB54442D18, 0d0000000000000000, %p43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r195, %temp}, %fd769;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r196}, %fd769;
	}
	or.b32  	%r197, %r196, %r8;
	mov.b64 	%fd3291, {%r195, %r197};
	bra.uni 	BB85_23;

BB85_19:
	setp.eq.f64	%p36, %fd32, 0d7FF0000000000000;
	setp.eq.f64	%p37, %fd33, 0d7FF0000000000000;
	and.pred  	%p38, %p36, %p37;
	@%p38 bra 	BB85_21;
	bra.uni 	BB85_20;

BB85_21:
	setp.lt.s32	%p42, %r7, 0;
	selp.f64	%fd768, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p42;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r192, %temp}, %fd768;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r193}, %fd768;
	}
	or.b32  	%r194, %r193, %r8;
	mov.b64 	%fd3291, {%r192, %r194};

BB85_23:
	bra.uni 	BB85_24;

BB85_10:
	setp.lt.s32	%p17, %r3, 0;
	min.f64 	%fd595, %fd20, %fd19;
	max.f64 	%fd596, %fd20, %fd19;
	div.rn.f64 	%fd597, %fd595, %fd596;
	mul.f64 	%fd598, %fd597, %fd597;
	mov.f64 	%fd599, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd600, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd601, %fd600, %fd598, %fd599;
	mov.f64 	%fd602, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd603, %fd601, %fd598, %fd602;
	mov.f64 	%fd604, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd605, %fd603, %fd598, %fd604;
	mov.f64 	%fd606, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd607, %fd605, %fd598, %fd606;
	mov.f64 	%fd608, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd609, %fd607, %fd598, %fd608;
	mov.f64 	%fd610, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd611, %fd609, %fd598, %fd610;
	mov.f64 	%fd612, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd613, %fd611, %fd598, %fd612;
	mov.f64 	%fd614, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd615, %fd613, %fd598, %fd614;
	mov.f64 	%fd616, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd617, %fd615, %fd598, %fd616;
	mov.f64 	%fd618, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd619, %fd617, %fd598, %fd618;
	mov.f64 	%fd620, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd621, %fd619, %fd598, %fd620;
	mov.f64 	%fd622, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd623, %fd621, %fd598, %fd622;
	mov.f64 	%fd624, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd625, %fd623, %fd598, %fd624;
	mov.f64 	%fd626, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd627, %fd625, %fd598, %fd626;
	mov.f64 	%fd628, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd629, %fd627, %fd598, %fd628;
	mov.f64 	%fd630, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd631, %fd629, %fd598, %fd630;
	mov.f64 	%fd632, 0d3FC99999999840D2;
	fma.rn.f64 	%fd633, %fd631, %fd598, %fd632;
	mov.f64 	%fd634, 0dBFD555555555544C;
	fma.rn.f64 	%fd635, %fd633, %fd598, %fd634;
	mul.f64 	%fd636, %fd598, %fd635;
	fma.rn.f64 	%fd637, %fd636, %fd597, %fd597;
	mov.f64 	%fd638, 0d3FF921FB54442D18;
	sub.f64 	%fd639, %fd638, %fd637;
	setp.gt.f64	%p18, %fd20, %fd19;
	selp.f64	%fd640, %fd639, %fd637, %p18;
	mov.f64 	%fd641, 0d400921FB54442D18;
	sub.f64 	%fd642, %fd641, %fd640;
	selp.f64	%fd643, %fd642, %fd640, %p17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r169, %temp}, %fd643;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r170}, %fd643;
	}
	or.b32  	%r171, %r170, %r4;
	mov.b64 	%fd644, {%r169, %r171};
	add.f64 	%fd645, %fd19, %fd20;
	setp.gtu.f64	%p19, %fd645, 0d7FF0000000000000;
	selp.f64	%fd3291, %fd645, %fd644, %p19;
	mov.f64 	%fd3393, 0d0000000000000000;
	bra.uni 	BB85_24;

BB85_20:
	setp.lt.s32	%p39, %r7, 0;
	min.f64 	%fd717, %fd33, %fd32;
	max.f64 	%fd718, %fd33, %fd32;
	div.rn.f64 	%fd719, %fd717, %fd718;
	mul.f64 	%fd720, %fd719, %fd719;
	mov.f64 	%fd721, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd722, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd723, %fd722, %fd720, %fd721;
	mov.f64 	%fd724, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd725, %fd723, %fd720, %fd724;
	mov.f64 	%fd726, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd727, %fd725, %fd720, %fd726;
	mov.f64 	%fd728, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd729, %fd727, %fd720, %fd728;
	mov.f64 	%fd730, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd731, %fd729, %fd720, %fd730;
	mov.f64 	%fd732, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd733, %fd731, %fd720, %fd732;
	mov.f64 	%fd734, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd735, %fd733, %fd720, %fd734;
	mov.f64 	%fd736, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd737, %fd735, %fd720, %fd736;
	mov.f64 	%fd738, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd739, %fd737, %fd720, %fd738;
	mov.f64 	%fd740, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd741, %fd739, %fd720, %fd740;
	mov.f64 	%fd742, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd743, %fd741, %fd720, %fd742;
	mov.f64 	%fd744, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd745, %fd743, %fd720, %fd744;
	mov.f64 	%fd746, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd747, %fd745, %fd720, %fd746;
	mov.f64 	%fd748, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd749, %fd747, %fd720, %fd748;
	mov.f64 	%fd750, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd751, %fd749, %fd720, %fd750;
	mov.f64 	%fd752, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd753, %fd751, %fd720, %fd752;
	mov.f64 	%fd754, 0d3FC99999999840D2;
	fma.rn.f64 	%fd755, %fd753, %fd720, %fd754;
	mov.f64 	%fd756, 0dBFD555555555544C;
	fma.rn.f64 	%fd757, %fd755, %fd720, %fd756;
	mul.f64 	%fd758, %fd720, %fd757;
	fma.rn.f64 	%fd759, %fd758, %fd719, %fd719;
	mov.f64 	%fd760, 0d3FF921FB54442D18;
	sub.f64 	%fd761, %fd760, %fd759;
	setp.gt.f64	%p40, %fd33, %fd32;
	selp.f64	%fd762, %fd761, %fd759, %p40;
	mov.f64 	%fd763, 0d400921FB54442D18;
	sub.f64 	%fd764, %fd763, %fd762;
	selp.f64	%fd765, %fd764, %fd762, %p39;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r189, %temp}, %fd765;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r190}, %fd765;
	}
	or.b32  	%r191, %r190, %r8;
	mov.b64 	%fd766, {%r189, %r191};
	add.f64 	%fd767, %fd32, %fd33;
	setp.gtu.f64	%p41, %fd767, 0d7FF0000000000000;
	selp.f64	%fd3291, %fd767, %fd766, %p41;

BB85_24:
	mov.f64 	%fd37, %fd3393;
	mul.f64 	%fd39, %fd3291, 0d3FE0000000000000;
	abs.f64 	%fd40, %fd37;
	setp.neu.f64	%p44, %fd40, 0d7FF0000000000000;
	mov.f64 	%fd3392, %fd37;
	@%p44 bra 	BB85_26;

	mov.f64 	%fd770, 0d0000000000000000;
	mul.rn.f64 	%fd41, %fd37, %fd770;
	mov.f64 	%fd3392, %fd41;

BB85_26:
	mov.f64 	%fd42, %fd3392;
	mul.f64 	%fd771, %fd42, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r628, %fd771;
	st.local.u32 	[%rd1], %r628;
	cvt.rn.f64.s32	%fd772, %r628;
	neg.f64 	%fd773, %fd772;
	mov.f64 	%fd774, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd775, %fd773, %fd774, %fd42;
	mov.f64 	%fd776, 0d3C91A62633145C00;
	fma.rn.f64 	%fd777, %fd773, %fd776, %fd775;
	mov.f64 	%fd778, 0d397B839A252049C0;
	fma.rn.f64 	%fd3292, %fd773, %fd778, %fd777;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r198}, %fd42;
	}
	and.b32  	%r199, %r198, 2145386496;
	setp.lt.u32	%p45, %r199, 1105199104;
	@%p45 bra 	BB85_28;

	// Callseq Start 76
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd42;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3292, [retval0+0];
	
	//{
	}// Callseq End 76
	ld.local.u32 	%r628, [%rd1];

BB85_28:
	mul.rn.f64 	%fd779, %fd3292, %fd3292;
	mov.f64 	%fd780, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd781, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd782, %fd781, %fd779, %fd780;
	mov.f64 	%fd783, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd784, %fd782, %fd779, %fd783;
	mov.f64 	%fd785, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd786, %fd784, %fd779, %fd785;
	mov.f64 	%fd787, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd788, %fd786, %fd779, %fd787;
	mov.f64 	%fd789, 0d3FA5555555555551;
	fma.rn.f64 	%fd790, %fd788, %fd779, %fd789;
	mov.f64 	%fd791, 0dBFE0000000000000;
	fma.rn.f64 	%fd792, %fd790, %fd779, %fd791;
	mov.f64 	%fd793, 0d3FF0000000000000;
	fma.rn.f64 	%fd794, %fd792, %fd779, %fd793;
	mov.f64 	%fd795, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd796, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd797, %fd796, %fd779, %fd795;
	mov.f64 	%fd798, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd799, %fd797, %fd779, %fd798;
	mov.f64 	%fd800, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd801, %fd799, %fd779, %fd800;
	mov.f64 	%fd802, 0d3F81111111110818;
	fma.rn.f64 	%fd803, %fd801, %fd779, %fd802;
	mov.f64 	%fd804, 0dBFC5555555555554;
	fma.rn.f64 	%fd805, %fd803, %fd779, %fd804;
	mov.f64 	%fd806, 0d0000000000000000;
	fma.rn.f64 	%fd807, %fd805, %fd779, %fd806;
	fma.rn.f64 	%fd808, %fd807, %fd3292, %fd3292;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r200}, %fd808;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r201, %temp}, %fd808;
	}
	xor.b32  	%r202, %r200, -2147483648;
	mov.b64 	%fd809, {%r201, %r202};
	and.b32  	%r203, %r628, 1;
	setp.eq.b32	%p46, %r203, 1;
	not.pred 	%p47, %p46;
	selp.f64	%fd3293, %fd794, %fd809, %p47;
	and.b32  	%r204, %r628, 2;
	setp.eq.s32	%p48, %r204, 0;
	@%p48 bra 	BB85_30;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r205}, %fd3293;
	}
	xor.b32  	%r206, %r205, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r207, %temp}, %fd3293;
	}
	mov.b64 	%fd3293, {%r207, %r206};

BB85_30:
	abs.f64 	%fd49, %fd39;
	setp.neu.f64	%p49, %fd49, 0d7FF0000000000000;
	mov.f64 	%fd3350, %fd39;
	@%p49 bra 	BB85_32;

	mul.rn.f64 	%fd50, %fd39, %fd806;
	mov.f64 	%fd3350, %fd50;

BB85_32:
	mov.f64 	%fd51, %fd3350;
	mov.f64 	%fd2929, 0d397B839A252049C0;
	mov.f64 	%fd2894, 0d3C91A62633145C00;
	mov.f64 	%fd2893, 0d3FF921FB54442D18;
	mul.f64 	%fd811, %fd51, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r629, %fd811;
	st.local.u32 	[%rd1], %r629;
	cvt.rn.f64.s32	%fd812, %r629;
	neg.f64 	%fd813, %fd812;
	fma.rn.f64 	%fd815, %fd813, %fd2893, %fd51;
	fma.rn.f64 	%fd817, %fd813, %fd2894, %fd815;
	fma.rn.f64 	%fd3294, %fd813, %fd2929, %fd817;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r208}, %fd51;
	}
	and.b32  	%r209, %r208, 2145386496;
	setp.lt.u32	%p50, %r209, 1105199104;
	@%p50 bra 	BB85_34;

	// Callseq Start 77
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd51;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3294, [retval0+0];
	
	//{
	}// Callseq End 77
	ld.local.u32 	%r629, [%rd1];

BB85_34:
	mov.f64 	%fd3232, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3231, 0d3FF0000000000000;
	mov.f64 	%fd3209, 0dBFE0000000000000;
	mov.f64 	%fd3208, 0d3FA5555555555551;
	mov.f64 	%fd3154, 0dBF56C16C16C15D47;
	mov.f64 	%fd3115, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3114, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3072, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3071, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd819, %fd3294, %fd3294;
	fma.rn.f64 	%fd822, %fd3071, %fd819, %fd3072;
	fma.rn.f64 	%fd824, %fd822, %fd819, %fd3114;
	fma.rn.f64 	%fd826, %fd824, %fd819, %fd3115;
	fma.rn.f64 	%fd828, %fd826, %fd819, %fd3154;
	fma.rn.f64 	%fd830, %fd828, %fd819, %fd3208;
	fma.rn.f64 	%fd832, %fd830, %fd819, %fd3209;
	fma.rn.f64 	%fd834, %fd832, %fd819, %fd3231;
	fma.rn.f64 	%fd837, %fd3232, %fd819, %fd795;
	fma.rn.f64 	%fd839, %fd837, %fd819, %fd798;
	fma.rn.f64 	%fd841, %fd839, %fd819, %fd800;
	fma.rn.f64 	%fd843, %fd841, %fd819, %fd802;
	fma.rn.f64 	%fd845, %fd843, %fd819, %fd804;
	fma.rn.f64 	%fd847, %fd845, %fd819, %fd806;
	fma.rn.f64 	%fd848, %fd847, %fd3294, %fd3294;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r210}, %fd848;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r211, %temp}, %fd848;
	}
	xor.b32  	%r212, %r210, -2147483648;
	mov.b64 	%fd849, {%r211, %r212};
	and.b32  	%r213, %r629, 1;
	setp.eq.b32	%p51, %r213, 1;
	not.pred 	%p52, %p51;
	selp.f64	%fd3295, %fd834, %fd849, %p52;
	and.b32  	%r214, %r629, 2;
	setp.eq.s32	%p53, %r214, 0;
	@%p53 bra 	BB85_36;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r215}, %fd3295;
	}
	xor.b32  	%r216, %r215, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r217, %temp}, %fd3295;
	}
	mov.b64 	%fd3295, {%r217, %r216};

BB85_36:
	mul.f64 	%fd58, %fd3293, %fd3295;
	abs.f64 	%fd59, %fd16;
	setp.neu.f64	%p54, %fd59, 0d7FF0000000000000;
	mov.f64 	%fd3367, %fd16;
	@%p54 bra 	BB85_38;

	mul.rn.f64 	%fd60, %fd16, %fd806;
	mov.f64 	%fd3367, %fd60;

BB85_38:
	mov.f64 	%fd61, %fd3367;
	mov.f64 	%fd2897, 0d397B839A252049C0;
	mov.f64 	%fd2896, 0d3C91A62633145C00;
	mov.f64 	%fd2895, 0d3FF921FB54442D18;
	mul.f64 	%fd851, %fd61, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r630, %fd851;
	st.local.u32 	[%rd1], %r630;
	cvt.rn.f64.s32	%fd852, %r630;
	neg.f64 	%fd853, %fd852;
	fma.rn.f64 	%fd855, %fd853, %fd2895, %fd61;
	fma.rn.f64 	%fd857, %fd853, %fd2896, %fd855;
	fma.rn.f64 	%fd3296, %fd853, %fd2897, %fd857;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r218}, %fd61;
	}
	and.b32  	%r219, %r218, 2145386496;
	setp.lt.u32	%p55, %r219, 1105199104;
	@%p55 bra 	BB85_40;

	// Callseq Start 78
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd61;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3296, [retval0+0];
	
	//{
	}// Callseq End 78
	ld.local.u32 	%r630, [%rd1];

BB85_40:
	mov.f64 	%fd3259, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3234, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3233, 0d3FF0000000000000;
	mov.f64 	%fd3211, 0dBFE0000000000000;
	mov.f64 	%fd3210, 0d3FA5555555555551;
	mov.f64 	%fd3155, 0dBF56C16C16C15D47;
	mov.f64 	%fd3117, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3116, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3074, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3073, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd859, %fd3296, %fd3296;
	fma.rn.f64 	%fd862, %fd3073, %fd859, %fd3074;
	fma.rn.f64 	%fd864, %fd862, %fd859, %fd3116;
	fma.rn.f64 	%fd866, %fd864, %fd859, %fd3117;
	fma.rn.f64 	%fd868, %fd866, %fd859, %fd3155;
	fma.rn.f64 	%fd870, %fd868, %fd859, %fd3210;
	fma.rn.f64 	%fd872, %fd870, %fd859, %fd3211;
	fma.rn.f64 	%fd874, %fd872, %fd859, %fd3233;
	fma.rn.f64 	%fd877, %fd3234, %fd859, %fd3259;
	fma.rn.f64 	%fd879, %fd877, %fd859, %fd798;
	fma.rn.f64 	%fd881, %fd879, %fd859, %fd800;
	fma.rn.f64 	%fd883, %fd881, %fd859, %fd802;
	fma.rn.f64 	%fd885, %fd883, %fd859, %fd804;
	fma.rn.f64 	%fd887, %fd885, %fd859, %fd806;
	fma.rn.f64 	%fd888, %fd887, %fd3296, %fd3296;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r220}, %fd888;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r221, %temp}, %fd888;
	}
	xor.b32  	%r222, %r220, -2147483648;
	mov.b64 	%fd889, {%r221, %r222};
	and.b32  	%r223, %r630, 1;
	setp.eq.b32	%p56, %r223, 1;
	not.pred 	%p57, %p56;
	selp.f64	%fd3297, %fd874, %fd889, %p57;
	and.b32  	%r224, %r630, 2;
	setp.eq.s32	%p58, %r224, 0;
	@%p58 bra 	BB85_42;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r225}, %fd3297;
	}
	xor.b32  	%r226, %r225, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r227, %temp}, %fd3297;
	}
	mov.b64 	%fd3297, {%r227, %r226};

BB85_42:
	mul.f64 	%fd68, %fd58, %fd3297;
	mov.f64 	%fd3391, %fd37;
	@%p44 bra 	BB85_44;

	mul.rn.f64 	%fd3391, %fd37, %fd806;

BB85_44:
	mov.f64 	%fd2898, 0d397B839A252049C0;
	mov.f64 	%fd2886, 0d3C91A62633145C00;
	mov.f64 	%fd2885, 0d3FF921FB54442D18;
	mul.f64 	%fd891, %fd3391, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r631, %fd891;
	st.local.u32 	[%rd1], %r631;
	cvt.rn.f64.s32	%fd892, %r631;
	neg.f64 	%fd893, %fd892;
	fma.rn.f64 	%fd895, %fd893, %fd2885, %fd3391;
	fma.rn.f64 	%fd897, %fd893, %fd2886, %fd895;
	fma.rn.f64 	%fd3298, %fd893, %fd2898, %fd897;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r228}, %fd3391;
	}
	and.b32  	%r229, %r228, 2145386496;
	setp.lt.u32	%p60, %r229, 1105199104;
	@%p60 bra 	BB85_46;

	// Callseq Start 79
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3391;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3298, [retval0+0];
	
	//{
	}// Callseq End 79
	ld.local.u32 	%r631, [%rd1];

BB85_46:
	mov.f64 	%fd3246, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3215, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3214, 0d3FF0000000000000;
	mov.f64 	%fd3213, 0dBFE0000000000000;
	mov.f64 	%fd3212, 0d3FA5555555555551;
	mov.f64 	%fd3145, 0dBF56C16C16C15D47;
	mov.f64 	%fd3113, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3112, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3070, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3069, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd899, %fd3298, %fd3298;
	fma.rn.f64 	%fd902, %fd3069, %fd899, %fd3070;
	fma.rn.f64 	%fd904, %fd902, %fd899, %fd3112;
	fma.rn.f64 	%fd906, %fd904, %fd899, %fd3113;
	fma.rn.f64 	%fd908, %fd906, %fd899, %fd3145;
	fma.rn.f64 	%fd910, %fd908, %fd899, %fd3212;
	fma.rn.f64 	%fd912, %fd910, %fd899, %fd3213;
	fma.rn.f64 	%fd914, %fd912, %fd899, %fd3214;
	fma.rn.f64 	%fd917, %fd3215, %fd899, %fd3246;
	fma.rn.f64 	%fd919, %fd917, %fd899, %fd798;
	fma.rn.f64 	%fd921, %fd919, %fd899, %fd800;
	fma.rn.f64 	%fd923, %fd921, %fd899, %fd802;
	fma.rn.f64 	%fd925, %fd923, %fd899, %fd804;
	fma.rn.f64 	%fd927, %fd925, %fd899, %fd806;
	fma.rn.f64 	%fd928, %fd927, %fd3298, %fd3298;
	and.b32  	%r230, %r631, 1;
	setp.eq.b32	%p61, %r230, 1;
	not.pred 	%p62, %p61;
	selp.f64	%fd3299, %fd928, %fd914, %p62;
	and.b32  	%r231, %r631, 2;
	setp.eq.s32	%p63, %r231, 0;
	@%p63 bra 	BB85_48;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r232}, %fd3299;
	}
	xor.b32  	%r233, %r232, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r234, %temp}, %fd3299;
	}
	mov.b64 	%fd3299, {%r234, %r233};

BB85_48:
	mov.f64 	%fd3349, %fd39;
	@%p49 bra 	BB85_50;

	mul.rn.f64 	%fd3349, %fd39, %fd806;

BB85_50:
	mov.f64 	%fd2899, 0d397B839A252049C0;
	mov.f64 	%fd2888, 0d3C91A62633145C00;
	mov.f64 	%fd2887, 0d3FF921FB54442D18;
	mul.f64 	%fd930, %fd3349, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r632, %fd930;
	st.local.u32 	[%rd1], %r632;
	cvt.rn.f64.s32	%fd931, %r632;
	neg.f64 	%fd932, %fd931;
	fma.rn.f64 	%fd934, %fd932, %fd2887, %fd3349;
	fma.rn.f64 	%fd936, %fd932, %fd2888, %fd934;
	fma.rn.f64 	%fd3300, %fd932, %fd2899, %fd936;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r235}, %fd3349;
	}
	and.b32  	%r236, %r235, 2145386496;
	setp.lt.u32	%p65, %r236, 1105199104;
	@%p65 bra 	BB85_52;

	// Callseq Start 80
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3349;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3300, [retval0+0];
	
	//{
	}// Callseq End 80
	ld.local.u32 	%r632, [%rd1];

BB85_52:
	mov.f64 	%fd3247, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3217, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3216, 0d3FF0000000000000;
	mov.f64 	%fd3187, 0dBFE0000000000000;
	mov.f64 	%fd3186, 0d3FA5555555555551;
	mov.f64 	%fd3118, 0dBF56C16C16C15D47;
	mov.f64 	%fd3076, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3075, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3031, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3030, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd938, %fd3300, %fd3300;
	fma.rn.f64 	%fd941, %fd3030, %fd938, %fd3031;
	fma.rn.f64 	%fd943, %fd941, %fd938, %fd3075;
	fma.rn.f64 	%fd945, %fd943, %fd938, %fd3076;
	fma.rn.f64 	%fd947, %fd945, %fd938, %fd3118;
	fma.rn.f64 	%fd949, %fd947, %fd938, %fd3186;
	fma.rn.f64 	%fd951, %fd949, %fd938, %fd3187;
	fma.rn.f64 	%fd953, %fd951, %fd938, %fd3216;
	fma.rn.f64 	%fd956, %fd3217, %fd938, %fd3247;
	fma.rn.f64 	%fd958, %fd956, %fd938, %fd798;
	fma.rn.f64 	%fd960, %fd958, %fd938, %fd800;
	fma.rn.f64 	%fd962, %fd960, %fd938, %fd802;
	fma.rn.f64 	%fd964, %fd962, %fd938, %fd804;
	fma.rn.f64 	%fd966, %fd964, %fd938, %fd806;
	fma.rn.f64 	%fd967, %fd966, %fd3300, %fd3300;
	and.b32  	%r237, %r632, 1;
	setp.eq.b32	%p66, %r237, 1;
	not.pred 	%p67, %p66;
	selp.f64	%fd3301, %fd967, %fd953, %p67;
	and.b32  	%r238, %r632, 2;
	setp.eq.s32	%p68, %r238, 0;
	@%p68 bra 	BB85_54;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r239}, %fd3301;
	}
	xor.b32  	%r240, %r239, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r241, %temp}, %fd3301;
	}
	mov.b64 	%fd3301, {%r241, %r240};

BB85_54:
	mul.f64 	%fd85, %fd3299, %fd3301;
	mov.f64 	%fd3366, %fd16;
	@%p54 bra 	BB85_56;

	mul.rn.f64 	%fd3366, %fd16, %fd806;

BB85_56:
	mov.f64 	%fd2891, 0d397B839A252049C0;
	mov.f64 	%fd2890, 0d3C91A62633145C00;
	mov.f64 	%fd2889, 0d3FF921FB54442D18;
	mul.f64 	%fd969, %fd3366, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r633, %fd969;
	st.local.u32 	[%rd1], %r633;
	cvt.rn.f64.s32	%fd970, %r633;
	neg.f64 	%fd971, %fd970;
	fma.rn.f64 	%fd973, %fd971, %fd2889, %fd3366;
	fma.rn.f64 	%fd975, %fd971, %fd2890, %fd973;
	fma.rn.f64 	%fd3302, %fd971, %fd2891, %fd975;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r242}, %fd3366;
	}
	and.b32  	%r243, %r242, 2145386496;
	setp.lt.u32	%p70, %r243, 1105199104;
	@%p70 bra 	BB85_58;

	// Callseq Start 81
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3366;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3302, [retval0+0];
	
	//{
	}// Callseq End 81
	ld.local.u32 	%r633, [%rd1];

BB85_58:
	mov.f64 	%fd3235, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3191, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3190, 0d3FF0000000000000;
	mov.f64 	%fd3189, 0dBFE0000000000000;
	mov.f64 	%fd3188, 0d3FA5555555555551;
	mov.f64 	%fd3119, 0dBF56C16C16C15D47;
	mov.f64 	%fd3077, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3028, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3027, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3026, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd977, %fd3302, %fd3302;
	fma.rn.f64 	%fd980, %fd3026, %fd977, %fd3027;
	fma.rn.f64 	%fd982, %fd980, %fd977, %fd3028;
	fma.rn.f64 	%fd984, %fd982, %fd977, %fd3077;
	fma.rn.f64 	%fd986, %fd984, %fd977, %fd3119;
	fma.rn.f64 	%fd988, %fd986, %fd977, %fd3188;
	fma.rn.f64 	%fd990, %fd988, %fd977, %fd3189;
	fma.rn.f64 	%fd992, %fd990, %fd977, %fd3190;
	fma.rn.f64 	%fd995, %fd3191, %fd977, %fd3235;
	fma.rn.f64 	%fd997, %fd995, %fd977, %fd798;
	fma.rn.f64 	%fd999, %fd997, %fd977, %fd800;
	fma.rn.f64 	%fd1001, %fd999, %fd977, %fd802;
	fma.rn.f64 	%fd1003, %fd1001, %fd977, %fd804;
	fma.rn.f64 	%fd1005, %fd1003, %fd977, %fd806;
	fma.rn.f64 	%fd1006, %fd1005, %fd3302, %fd3302;
	and.b32  	%r244, %r633, 1;
	setp.eq.b32	%p71, %r244, 1;
	not.pred 	%p72, %p71;
	selp.f64	%fd3303, %fd1006, %fd992, %p72;
	and.b32  	%r245, %r633, 2;
	setp.eq.s32	%p73, %r245, 0;
	@%p73 bra 	BB85_60;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r246}, %fd3303;
	}
	xor.b32  	%r247, %r246, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r248, %temp}, %fd3303;
	}
	mov.b64 	%fd3303, {%r248, %r247};

BB85_60:
	fma.rn.f64 	%fd94, %fd85, %fd3303, %fd68;
	mov.f64 	%fd3390, %fd37;
	@%p44 bra 	BB85_62;

	mul.rn.f64 	%fd3390, %fd37, %fd806;

BB85_62:
	mov.f64 	%fd2892, 0d397B839A252049C0;
	mov.f64 	%fd2860, 0d3C91A62633145C00;
	mov.f64 	%fd2859, 0d3FF921FB54442D18;
	mul.f64 	%fd1008, %fd3390, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r634, %fd1008;
	st.local.u32 	[%rd1], %r634;
	cvt.rn.f64.s32	%fd1009, %r634;
	neg.f64 	%fd1010, %fd1009;
	fma.rn.f64 	%fd1012, %fd1010, %fd2859, %fd3390;
	fma.rn.f64 	%fd1014, %fd1010, %fd2860, %fd1012;
	fma.rn.f64 	%fd3304, %fd1010, %fd2892, %fd1014;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r249}, %fd3390;
	}
	and.b32  	%r250, %r249, 2145386496;
	setp.lt.u32	%p75, %r250, 1105199104;
	@%p75 bra 	BB85_64;

	// Callseq Start 82
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3390;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3304, [retval0+0];
	
	//{
	}// Callseq End 82
	ld.local.u32 	%r634, [%rd1];

BB85_64:
	mov.f64 	%fd3218, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3193, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3192, 0d3FF0000000000000;
	mov.f64 	%fd3157, 0dBFE0000000000000;
	mov.f64 	%fd3156, 0d3FA5555555555551;
	mov.f64 	%fd3078, 0dBF56C16C16C15D47;
	mov.f64 	%fd3032, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3029, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2978, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2977, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1016, %fd3304, %fd3304;
	fma.rn.f64 	%fd1019, %fd2977, %fd1016, %fd2978;
	fma.rn.f64 	%fd1021, %fd1019, %fd1016, %fd3029;
	fma.rn.f64 	%fd1023, %fd1021, %fd1016, %fd3032;
	fma.rn.f64 	%fd1025, %fd1023, %fd1016, %fd3078;
	fma.rn.f64 	%fd1027, %fd1025, %fd1016, %fd3156;
	fma.rn.f64 	%fd1029, %fd1027, %fd1016, %fd3157;
	fma.rn.f64 	%fd1031, %fd1029, %fd1016, %fd3192;
	fma.rn.f64 	%fd1034, %fd3193, %fd1016, %fd3218;
	fma.rn.f64 	%fd1036, %fd1034, %fd1016, %fd798;
	fma.rn.f64 	%fd1038, %fd1036, %fd1016, %fd800;
	fma.rn.f64 	%fd1040, %fd1038, %fd1016, %fd802;
	fma.rn.f64 	%fd1042, %fd1040, %fd1016, %fd804;
	fma.rn.f64 	%fd1044, %fd1042, %fd1016, %fd806;
	fma.rn.f64 	%fd1045, %fd1044, %fd3304, %fd3304;
	and.b32  	%r251, %r634, 1;
	setp.eq.b32	%p76, %r251, 1;
	not.pred 	%p77, %p76;
	selp.f64	%fd3305, %fd1045, %fd1031, %p77;
	and.b32  	%r252, %r634, 2;
	setp.eq.s32	%p78, %r252, 0;
	@%p78 bra 	BB85_66;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r253}, %fd3305;
	}
	xor.b32  	%r254, %r253, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r255, %temp}, %fd3305;
	}
	mov.b64 	%fd3305, {%r255, %r254};

BB85_66:
	mov.f64 	%fd3348, %fd39;
	@%p49 bra 	BB85_68;

	mul.rn.f64 	%fd3348, %fd39, %fd806;

BB85_68:
	mov.f64 	%fd2865, 0d397B839A252049C0;
	mov.f64 	%fd2862, 0d3C91A62633145C00;
	mov.f64 	%fd2861, 0d3FF921FB54442D18;
	mul.f64 	%fd1047, %fd3348, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r635, %fd1047;
	st.local.u32 	[%rd1], %r635;
	cvt.rn.f64.s32	%fd1048, %r635;
	neg.f64 	%fd1049, %fd1048;
	fma.rn.f64 	%fd1051, %fd1049, %fd2861, %fd3348;
	fma.rn.f64 	%fd1053, %fd1049, %fd2862, %fd1051;
	fma.rn.f64 	%fd3306, %fd1049, %fd2865, %fd1053;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r256}, %fd3348;
	}
	and.b32  	%r257, %r256, 2145386496;
	setp.lt.u32	%p80, %r257, 1105199104;
	@%p80 bra 	BB85_70;

	// Callseq Start 83
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3348;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3306, [retval0+0];
	
	//{
	}// Callseq End 83
	ld.local.u32 	%r635, [%rd1];

BB85_70:
	mov.f64 	%fd3219, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3161, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3160, 0d3FF0000000000000;
	mov.f64 	%fd3159, 0dBFE0000000000000;
	mov.f64 	%fd3158, 0d3FA5555555555551;
	mov.f64 	%fd3079, 0dBF56C16C16C15D47;
	mov.f64 	%fd2982, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2981, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2980, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2979, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1055, %fd3306, %fd3306;
	fma.rn.f64 	%fd1058, %fd2979, %fd1055, %fd2980;
	fma.rn.f64 	%fd1060, %fd1058, %fd1055, %fd2981;
	fma.rn.f64 	%fd1062, %fd1060, %fd1055, %fd2982;
	fma.rn.f64 	%fd1064, %fd1062, %fd1055, %fd3079;
	fma.rn.f64 	%fd1066, %fd1064, %fd1055, %fd3158;
	fma.rn.f64 	%fd1068, %fd1066, %fd1055, %fd3159;
	fma.rn.f64 	%fd1070, %fd1068, %fd1055, %fd3160;
	fma.rn.f64 	%fd1073, %fd3161, %fd1055, %fd3219;
	fma.rn.f64 	%fd1075, %fd1073, %fd1055, %fd798;
	fma.rn.f64 	%fd1077, %fd1075, %fd1055, %fd800;
	fma.rn.f64 	%fd1079, %fd1077, %fd1055, %fd802;
	fma.rn.f64 	%fd1081, %fd1079, %fd1055, %fd804;
	fma.rn.f64 	%fd1083, %fd1081, %fd1055, %fd806;
	fma.rn.f64 	%fd1084, %fd1083, %fd3306, %fd3306;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r258}, %fd1084;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r259, %temp}, %fd1084;
	}
	xor.b32  	%r260, %r258, -2147483648;
	mov.b64 	%fd1085, {%r259, %r260};
	and.b32  	%r261, %r635, 1;
	setp.eq.b32	%p81, %r261, 1;
	not.pred 	%p82, %p81;
	selp.f64	%fd3307, %fd1070, %fd1085, %p82;
	and.b32  	%r262, %r635, 2;
	setp.eq.s32	%p83, %r262, 0;
	@%p83 bra 	BB85_72;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r263}, %fd3307;
	}
	xor.b32  	%r264, %r263, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r265, %temp}, %fd3307;
	}
	mov.b64 	%fd3307, {%r265, %r264};

BB85_72:
	mul.f64 	%fd111, %fd3305, %fd3307;
	mov.f64 	%fd3365, %fd16;
	@%p54 bra 	BB85_74;

	mul.rn.f64 	%fd3365, %fd16, %fd806;

BB85_74:
	mov.f64 	%fd2866, 0d397B839A252049C0;
	mov.f64 	%fd2864, 0d3C91A62633145C00;
	mov.f64 	%fd2863, 0d3FF921FB54442D18;
	mul.f64 	%fd1087, %fd3365, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r636, %fd1087;
	st.local.u32 	[%rd1], %r636;
	cvt.rn.f64.s32	%fd1088, %r636;
	neg.f64 	%fd1089, %fd1088;
	fma.rn.f64 	%fd1091, %fd1089, %fd2863, %fd3365;
	fma.rn.f64 	%fd1093, %fd1089, %fd2864, %fd1091;
	fma.rn.f64 	%fd3308, %fd1089, %fd2866, %fd1093;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r266}, %fd3365;
	}
	and.b32  	%r267, %r266, 2145386496;
	setp.lt.u32	%p85, %r267, 1105199104;
	@%p85 bra 	BB85_76;

	// Callseq Start 84
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3365;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3308, [retval0+0];
	
	//{
	}// Callseq End 84
	ld.local.u32 	%r636, [%rd1];

BB85_76:
	mov.f64 	%fd3195, 0d3EC71DE369ACE392;
	mov.f64 	%fd3194, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3163, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3162, 0d3FF0000000000000;
	mov.f64 	%fd3147, 0dBFE0000000000000;
	mov.f64 	%fd3146, 0d3FA5555555555551;
	mov.f64 	%fd3033, 0dBF56C16C16C15D47;
	mov.f64 	%fd2984, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2983, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2970, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2969, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1095, %fd3308, %fd3308;
	fma.rn.f64 	%fd1098, %fd2969, %fd1095, %fd2970;
	fma.rn.f64 	%fd1100, %fd1098, %fd1095, %fd2983;
	fma.rn.f64 	%fd1102, %fd1100, %fd1095, %fd2984;
	fma.rn.f64 	%fd1104, %fd1102, %fd1095, %fd3033;
	fma.rn.f64 	%fd1106, %fd1104, %fd1095, %fd3146;
	fma.rn.f64 	%fd1108, %fd1106, %fd1095, %fd3147;
	fma.rn.f64 	%fd1110, %fd1108, %fd1095, %fd3162;
	fma.rn.f64 	%fd1113, %fd3163, %fd1095, %fd3194;
	fma.rn.f64 	%fd1115, %fd1113, %fd1095, %fd3195;
	fma.rn.f64 	%fd1117, %fd1115, %fd1095, %fd800;
	fma.rn.f64 	%fd1119, %fd1117, %fd1095, %fd802;
	fma.rn.f64 	%fd1121, %fd1119, %fd1095, %fd804;
	fma.rn.f64 	%fd1123, %fd1121, %fd1095, %fd806;
	fma.rn.f64 	%fd1124, %fd1123, %fd3308, %fd3308;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r268}, %fd1124;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r269, %temp}, %fd1124;
	}
	xor.b32  	%r270, %r268, -2147483648;
	mov.b64 	%fd1125, {%r269, %r270};
	and.b32  	%r271, %r636, 1;
	setp.eq.b32	%p86, %r271, 1;
	not.pred 	%p87, %p86;
	selp.f64	%fd3309, %fd1110, %fd1125, %p87;
	and.b32  	%r272, %r636, 2;
	setp.eq.s32	%p88, %r272, 0;
	@%p88 bra 	BB85_78;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r273}, %fd3309;
	}
	xor.b32  	%r274, %r273, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r275, %temp}, %fd3309;
	}
	mov.b64 	%fd3309, {%r275, %r274};

BB85_78:
	mul.f64 	%fd120, %fd111, %fd3309;
	mov.f64 	%fd3389, %fd37;
	@%p44 bra 	BB85_80;

	mul.rn.f64 	%fd3389, %fd37, %fd806;

BB85_80:
	mov.f64 	%fd2867, 0d397B839A252049C0;
	mov.f64 	%fd2849, 0d3C91A62633145C00;
	mov.f64 	%fd2848, 0d3FF921FB54442D18;
	mul.f64 	%fd1127, %fd3389, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r637, %fd1127;
	st.local.u32 	[%rd1], %r637;
	cvt.rn.f64.s32	%fd1128, %r637;
	neg.f64 	%fd1129, %fd1128;
	fma.rn.f64 	%fd1131, %fd1129, %fd2848, %fd3389;
	fma.rn.f64 	%fd1133, %fd1129, %fd2849, %fd1131;
	fma.rn.f64 	%fd3310, %fd1129, %fd2867, %fd1133;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r276}, %fd3389;
	}
	and.b32  	%r277, %r276, 2145386496;
	setp.lt.u32	%p90, %r277, 1105199104;
	@%p90 bra 	BB85_82;

	// Callseq Start 85
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3389;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3310, [retval0+0];
	
	//{
	}// Callseq End 85
	ld.local.u32 	%r637, [%rd1];

BB85_82:
	mov.f64 	%fd3197, 0d3EC71DE369ACE392;
	mov.f64 	%fd3196, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3151, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3150, 0d3FF0000000000000;
	mov.f64 	%fd3149, 0dBFE0000000000000;
	mov.f64 	%fd3148, 0d3FA5555555555551;
	mov.f64 	%fd3034, 0dBF56C16C16C15D47;
	mov.f64 	%fd2974, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2973, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2972, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2971, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1135, %fd3310, %fd3310;
	fma.rn.f64 	%fd1138, %fd2971, %fd1135, %fd2972;
	fma.rn.f64 	%fd1140, %fd1138, %fd1135, %fd2973;
	fma.rn.f64 	%fd1142, %fd1140, %fd1135, %fd2974;
	fma.rn.f64 	%fd1144, %fd1142, %fd1135, %fd3034;
	fma.rn.f64 	%fd1146, %fd1144, %fd1135, %fd3148;
	fma.rn.f64 	%fd1148, %fd1146, %fd1135, %fd3149;
	fma.rn.f64 	%fd1150, %fd1148, %fd1135, %fd3150;
	fma.rn.f64 	%fd1153, %fd3151, %fd1135, %fd3196;
	fma.rn.f64 	%fd1155, %fd1153, %fd1135, %fd3197;
	fma.rn.f64 	%fd1157, %fd1155, %fd1135, %fd800;
	fma.rn.f64 	%fd1159, %fd1157, %fd1135, %fd802;
	fma.rn.f64 	%fd1161, %fd1159, %fd1135, %fd804;
	fma.rn.f64 	%fd1163, %fd1161, %fd1135, %fd806;
	fma.rn.f64 	%fd1164, %fd1163, %fd3310, %fd3310;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r278}, %fd1164;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r279, %temp}, %fd1164;
	}
	xor.b32  	%r280, %r278, -2147483648;
	mov.b64 	%fd1165, {%r279, %r280};
	and.b32  	%r281, %r637, 1;
	setp.eq.b32	%p91, %r281, 1;
	not.pred 	%p92, %p91;
	selp.f64	%fd3311, %fd1150, %fd1165, %p92;
	and.b32  	%r282, %r637, 2;
	setp.eq.s32	%p93, %r282, 0;
	@%p93 bra 	BB85_84;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r283}, %fd3311;
	}
	xor.b32  	%r284, %r283, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r285, %temp}, %fd3311;
	}
	mov.b64 	%fd3311, {%r285, %r284};

BB85_84:
	mov.f64 	%fd3347, %fd39;
	@%p49 bra 	BB85_86;

	mul.rn.f64 	%fd3347, %fd39, %fd806;

BB85_86:
	mov.f64 	%fd2854, 0d397B839A252049C0;
	mov.f64 	%fd2851, 0d3C91A62633145C00;
	mov.f64 	%fd2850, 0d3FF921FB54442D18;
	mul.f64 	%fd1167, %fd3347, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r638, %fd1167;
	st.local.u32 	[%rd1], %r638;
	cvt.rn.f64.s32	%fd1168, %r638;
	neg.f64 	%fd1169, %fd1168;
	fma.rn.f64 	%fd1171, %fd1169, %fd2850, %fd3347;
	fma.rn.f64 	%fd1173, %fd1169, %fd2851, %fd1171;
	fma.rn.f64 	%fd3312, %fd1169, %fd2854, %fd1173;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r286}, %fd3347;
	}
	and.b32  	%r287, %r286, 2145386496;
	setp.lt.u32	%p95, %r287, 1105199104;
	@%p95 bra 	BB85_88;

	// Callseq Start 86
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3347;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3312, [retval0+0];
	
	//{
	}// Callseq End 86
	ld.local.u32 	%r638, [%rd1];

BB85_88:
	mov.f64 	%fd3165, 0d3EC71DE369ACE392;
	mov.f64 	%fd3164, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3153, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3152, 0d3FF0000000000000;
	mov.f64 	%fd3121, 0dBFE0000000000000;
	mov.f64 	%fd3120, 0d3FA5555555555551;
	mov.f64 	%fd2985, 0dBF56C16C16C15D47;
	mov.f64 	%fd2976, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2975, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2923, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2922, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1175, %fd3312, %fd3312;
	fma.rn.f64 	%fd1178, %fd2922, %fd1175, %fd2923;
	fma.rn.f64 	%fd1180, %fd1178, %fd1175, %fd2975;
	fma.rn.f64 	%fd1182, %fd1180, %fd1175, %fd2976;
	fma.rn.f64 	%fd1184, %fd1182, %fd1175, %fd2985;
	fma.rn.f64 	%fd1186, %fd1184, %fd1175, %fd3120;
	fma.rn.f64 	%fd1188, %fd1186, %fd1175, %fd3121;
	fma.rn.f64 	%fd1190, %fd1188, %fd1175, %fd3152;
	fma.rn.f64 	%fd1193, %fd3153, %fd1175, %fd3164;
	fma.rn.f64 	%fd1195, %fd1193, %fd1175, %fd3165;
	fma.rn.f64 	%fd1197, %fd1195, %fd1175, %fd800;
	fma.rn.f64 	%fd1199, %fd1197, %fd1175, %fd802;
	fma.rn.f64 	%fd1201, %fd1199, %fd1175, %fd804;
	fma.rn.f64 	%fd1203, %fd1201, %fd1175, %fd806;
	fma.rn.f64 	%fd1204, %fd1203, %fd3312, %fd3312;
	and.b32  	%r288, %r638, 1;
	setp.eq.b32	%p96, %r288, 1;
	not.pred 	%p97, %p96;
	selp.f64	%fd3313, %fd1204, %fd1190, %p97;
	and.b32  	%r289, %r638, 2;
	setp.eq.s32	%p98, %r289, 0;
	@%p98 bra 	BB85_90;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r290}, %fd3313;
	}
	xor.b32  	%r291, %r290, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r292, %temp}, %fd3313;
	}
	mov.b64 	%fd3313, {%r292, %r291};

BB85_90:
	mul.f64 	%fd137, %fd3311, %fd3313;
	mov.f64 	%fd3364, %fd16;
	@%p54 bra 	BB85_92;

	mul.rn.f64 	%fd3364, %fd16, %fd806;

BB85_92:
	mov.f64 	%fd2855, 0d397B839A252049C0;
	mov.f64 	%fd2853, 0d3C91A62633145C00;
	mov.f64 	%fd2852, 0d3FF921FB54442D18;
	mul.f64 	%fd1206, %fd3364, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r639, %fd1206;
	st.local.u32 	[%rd1], %r639;
	cvt.rn.f64.s32	%fd1207, %r639;
	neg.f64 	%fd1208, %fd1207;
	fma.rn.f64 	%fd1210, %fd1208, %fd2852, %fd3364;
	fma.rn.f64 	%fd1212, %fd1208, %fd2853, %fd1210;
	fma.rn.f64 	%fd3314, %fd1208, %fd2855, %fd1212;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r293}, %fd3364;
	}
	and.b32  	%r294, %r293, 2145386496;
	setp.lt.u32	%p100, %r294, 1105199104;
	@%p100 bra 	BB85_94;

	// Callseq Start 87
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3364;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3314, [retval0+0];
	
	//{
	}// Callseq End 87
	ld.local.u32 	%r639, [%rd1];

BB85_94:
	mov.f64 	%fd3166, 0d3EC71DE369ACE392;
	mov.f64 	%fd3126, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3125, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3124, 0d3FF0000000000000;
	mov.f64 	%fd3123, 0dBFE0000000000000;
	mov.f64 	%fd3122, 0d3FA5555555555551;
	mov.f64 	%fd2986, 0dBF56C16C16C15D47;
	mov.f64 	%fd2927, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2926, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2925, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2924, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1214, %fd3314, %fd3314;
	fma.rn.f64 	%fd1217, %fd2924, %fd1214, %fd2925;
	fma.rn.f64 	%fd1219, %fd1217, %fd1214, %fd2926;
	fma.rn.f64 	%fd1221, %fd1219, %fd1214, %fd2927;
	fma.rn.f64 	%fd1223, %fd1221, %fd1214, %fd2986;
	fma.rn.f64 	%fd1225, %fd1223, %fd1214, %fd3122;
	fma.rn.f64 	%fd1227, %fd1225, %fd1214, %fd3123;
	fma.rn.f64 	%fd1229, %fd1227, %fd1214, %fd3124;
	fma.rn.f64 	%fd1232, %fd3125, %fd1214, %fd3126;
	fma.rn.f64 	%fd1234, %fd1232, %fd1214, %fd3166;
	fma.rn.f64 	%fd1236, %fd1234, %fd1214, %fd800;
	fma.rn.f64 	%fd1238, %fd1236, %fd1214, %fd802;
	fma.rn.f64 	%fd1240, %fd1238, %fd1214, %fd804;
	fma.rn.f64 	%fd1242, %fd1240, %fd1214, %fd806;
	fma.rn.f64 	%fd1243, %fd1242, %fd3314, %fd3314;
	and.b32  	%r295, %r639, 1;
	setp.eq.b32	%p101, %r295, 1;
	not.pred 	%p102, %p101;
	selp.f64	%fd3315, %fd1243, %fd1229, %p102;
	and.b32  	%r296, %r639, 2;
	setp.eq.s32	%p103, %r296, 0;
	@%p103 bra 	BB85_96;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r297}, %fd3315;
	}
	xor.b32  	%r298, %r297, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r299, %temp}, %fd3315;
	}
	mov.b64 	%fd3315, {%r299, %r298};

BB85_96:
	mul.f64 	%fd1244, %fd137, %fd3315;
	sub.f64 	%fd146, %fd120, %fd1244;
	mov.f64 	%fd3388, %fd37;
	@%p44 bra 	BB85_98;

	mul.rn.f64 	%fd3388, %fd37, %fd806;

BB85_98:
	mov.f64 	%fd2856, 0d397B839A252049C0;
	mov.f64 	%fd2827, 0d3C91A62633145C00;
	mov.f64 	%fd2826, 0d3FF921FB54442D18;
	mul.f64 	%fd1246, %fd3388, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r640, %fd1246;
	st.local.u32 	[%rd1], %r640;
	cvt.rn.f64.s32	%fd1247, %r640;
	neg.f64 	%fd1248, %fd1247;
	fma.rn.f64 	%fd1250, %fd1248, %fd2826, %fd3388;
	fma.rn.f64 	%fd1252, %fd1248, %fd2827, %fd1250;
	fma.rn.f64 	%fd3316, %fd1248, %fd2856, %fd1252;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r300}, %fd3388;
	}
	and.b32  	%r301, %r300, 2145386496;
	setp.lt.u32	%p105, %r301, 1105199104;
	@%p105 bra 	BB85_100;

	// Callseq Start 88
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3388;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3316, [retval0+0];
	
	//{
	}// Callseq End 88
	ld.local.u32 	%r640, [%rd1];

BB85_100:
	mov.f64 	%fd3129, 0d3EC71DE369ACE392;
	mov.f64 	%fd3128, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3127, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3082, 0d3FF0000000000000;
	mov.f64 	%fd3081, 0dBFE0000000000000;
	mov.f64 	%fd3080, 0d3FA5555555555551;
	mov.f64 	%fd2930, 0dBF56C16C16C15D47;
	mov.f64 	%fd2928, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2902, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2901, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2900, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1254, %fd3316, %fd3316;
	fma.rn.f64 	%fd1257, %fd2900, %fd1254, %fd2901;
	fma.rn.f64 	%fd1259, %fd1257, %fd1254, %fd2902;
	fma.rn.f64 	%fd1261, %fd1259, %fd1254, %fd2928;
	fma.rn.f64 	%fd1263, %fd1261, %fd1254, %fd2930;
	fma.rn.f64 	%fd1265, %fd1263, %fd1254, %fd3080;
	fma.rn.f64 	%fd1267, %fd1265, %fd1254, %fd3081;
	fma.rn.f64 	%fd1269, %fd1267, %fd1254, %fd3082;
	fma.rn.f64 	%fd1272, %fd3127, %fd1254, %fd3128;
	fma.rn.f64 	%fd1274, %fd1272, %fd1254, %fd3129;
	fma.rn.f64 	%fd1276, %fd1274, %fd1254, %fd800;
	fma.rn.f64 	%fd1278, %fd1276, %fd1254, %fd802;
	fma.rn.f64 	%fd1280, %fd1278, %fd1254, %fd804;
	fma.rn.f64 	%fd1282, %fd1280, %fd1254, %fd806;
	fma.rn.f64 	%fd1283, %fd1282, %fd3316, %fd3316;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r302}, %fd1283;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r303, %temp}, %fd1283;
	}
	xor.b32  	%r304, %r302, -2147483648;
	mov.b64 	%fd1284, {%r303, %r304};
	and.b32  	%r305, %r640, 1;
	setp.eq.b32	%p106, %r305, 1;
	not.pred 	%p107, %p106;
	selp.f64	%fd3317, %fd1269, %fd1284, %p107;
	and.b32  	%r306, %r640, 2;
	setp.eq.s32	%p108, %r306, 0;
	@%p108 bra 	BB85_102;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r307}, %fd3317;
	}
	xor.b32  	%r308, %r307, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r309, %temp}, %fd3317;
	}
	mov.b64 	%fd3317, {%r309, %r308};

BB85_102:
	mov.f64 	%fd3346, %fd39;
	@%p49 bra 	BB85_104;

	mul.rn.f64 	%fd3346, %fd39, %fd806;

BB85_104:
	mov.f64 	%fd2832, 0d397B839A252049C0;
	mov.f64 	%fd2829, 0d3C91A62633145C00;
	mov.f64 	%fd2828, 0d3FF921FB54442D18;
	mul.f64 	%fd1286, %fd3346, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r641, %fd1286;
	st.local.u32 	[%rd1], %r641;
	cvt.rn.f64.s32	%fd1287, %r641;
	neg.f64 	%fd1288, %fd1287;
	fma.rn.f64 	%fd1290, %fd1288, %fd2828, %fd3346;
	fma.rn.f64 	%fd1292, %fd1288, %fd2829, %fd1290;
	fma.rn.f64 	%fd3318, %fd1288, %fd2832, %fd1292;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r310}, %fd3346;
	}
	and.b32  	%r311, %r310, 2145386496;
	setp.lt.u32	%p110, %r311, 1105199104;
	@%p110 bra 	BB85_106;

	// Callseq Start 89
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3346;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3318, [retval0+0];
	
	//{
	}// Callseq End 89
	ld.local.u32 	%r641, [%rd1];

BB85_106:
	mov.f64 	%fd3088, 0d3EC71DE369ACE392;
	mov.f64 	%fd3087, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3086, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3085, 0d3FF0000000000000;
	mov.f64 	%fd3084, 0dBFE0000000000000;
	mov.f64 	%fd3083, 0d3FA5555555555551;
	mov.f64 	%fd2931, 0dBF56C16C16C15D47;
	mov.f64 	%fd2906, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2905, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2904, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2903, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1294, %fd3318, %fd3318;
	fma.rn.f64 	%fd1297, %fd2903, %fd1294, %fd2904;
	fma.rn.f64 	%fd1299, %fd1297, %fd1294, %fd2905;
	fma.rn.f64 	%fd1301, %fd1299, %fd1294, %fd2906;
	fma.rn.f64 	%fd1303, %fd1301, %fd1294, %fd2931;
	fma.rn.f64 	%fd1305, %fd1303, %fd1294, %fd3083;
	fma.rn.f64 	%fd1307, %fd1305, %fd1294, %fd3084;
	fma.rn.f64 	%fd1309, %fd1307, %fd1294, %fd3085;
	fma.rn.f64 	%fd1312, %fd3086, %fd1294, %fd3087;
	fma.rn.f64 	%fd1314, %fd1312, %fd1294, %fd3088;
	fma.rn.f64 	%fd1316, %fd1314, %fd1294, %fd800;
	fma.rn.f64 	%fd1318, %fd1316, %fd1294, %fd802;
	fma.rn.f64 	%fd1320, %fd1318, %fd1294, %fd804;
	fma.rn.f64 	%fd1322, %fd1320, %fd1294, %fd806;
	fma.rn.f64 	%fd1323, %fd1322, %fd3318, %fd3318;
	and.b32  	%r312, %r641, 1;
	setp.eq.b32	%p111, %r312, 1;
	not.pred 	%p112, %p111;
	selp.f64	%fd3319, %fd1323, %fd1309, %p112;
	and.b32  	%r313, %r641, 2;
	setp.eq.s32	%p113, %r313, 0;
	@%p113 bra 	BB85_108;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r314}, %fd3319;
	}
	xor.b32  	%r315, %r314, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r316, %temp}, %fd3319;
	}
	mov.b64 	%fd3319, {%r316, %r315};

BB85_108:
	mul.f64 	%fd163, %fd3317, %fd3319;
	mov.f64 	%fd3363, %fd16;
	@%p54 bra 	BB85_110;

	mul.rn.f64 	%fd3363, %fd16, %fd806;

BB85_110:
	mov.f64 	%fd2833, 0d397B839A252049C0;
	mov.f64 	%fd2831, 0d3C91A62633145C00;
	mov.f64 	%fd2830, 0d3FF921FB54442D18;
	mul.f64 	%fd1325, %fd3363, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r642, %fd1325;
	st.local.u32 	[%rd1], %r642;
	cvt.rn.f64.s32	%fd1326, %r642;
	neg.f64 	%fd1327, %fd1326;
	fma.rn.f64 	%fd1329, %fd1327, %fd2830, %fd3363;
	fma.rn.f64 	%fd1331, %fd1327, %fd2831, %fd1329;
	fma.rn.f64 	%fd3320, %fd1327, %fd2833, %fd1331;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r317}, %fd3363;
	}
	and.b32  	%r318, %r317, 2145386496;
	setp.lt.u32	%p115, %r318, 1105199104;
	@%p115 bra 	BB85_112;

	// Callseq Start 90
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3363;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3320, [retval0+0];
	
	//{
	}// Callseq End 90
	ld.local.u32 	%r642, [%rd1];

BB85_112:
	mov.f64 	%fd3090, 0d3EC71DE369ACE392;
	mov.f64 	%fd3089, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3038, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3037, 0d3FF0000000000000;
	mov.f64 	%fd3036, 0dBFE0000000000000;
	mov.f64 	%fd3035, 0d3FA5555555555551;
	mov.f64 	%fd2907, 0dBF56C16C16C15D47;
	mov.f64 	%fd2871, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2870, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2869, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2868, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1333, %fd3320, %fd3320;
	fma.rn.f64 	%fd1336, %fd2868, %fd1333, %fd2869;
	fma.rn.f64 	%fd1338, %fd1336, %fd1333, %fd2870;
	fma.rn.f64 	%fd1340, %fd1338, %fd1333, %fd2871;
	fma.rn.f64 	%fd1342, %fd1340, %fd1333, %fd2907;
	fma.rn.f64 	%fd1344, %fd1342, %fd1333, %fd3035;
	fma.rn.f64 	%fd1346, %fd1344, %fd1333, %fd3036;
	fma.rn.f64 	%fd1348, %fd1346, %fd1333, %fd3037;
	fma.rn.f64 	%fd1351, %fd3038, %fd1333, %fd3089;
	fma.rn.f64 	%fd1353, %fd1351, %fd1333, %fd3090;
	fma.rn.f64 	%fd1355, %fd1353, %fd1333, %fd800;
	fma.rn.f64 	%fd1357, %fd1355, %fd1333, %fd802;
	fma.rn.f64 	%fd1359, %fd1357, %fd1333, %fd804;
	fma.rn.f64 	%fd1361, %fd1359, %fd1333, %fd806;
	fma.rn.f64 	%fd1362, %fd1361, %fd3320, %fd3320;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r319}, %fd1362;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r320, %temp}, %fd1362;
	}
	xor.b32  	%r321, %r319, -2147483648;
	mov.b64 	%fd1363, {%r320, %r321};
	and.b32  	%r322, %r642, 1;
	setp.eq.b32	%p116, %r322, 1;
	not.pred 	%p117, %p116;
	selp.f64	%fd3321, %fd1348, %fd1363, %p117;
	and.b32  	%r323, %r642, 2;
	setp.eq.s32	%p118, %r323, 0;
	@%p118 bra 	BB85_114;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r324}, %fd3321;
	}
	xor.b32  	%r325, %r324, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r326, %temp}, %fd3321;
	}
	mov.b64 	%fd3321, {%r326, %r325};

BB85_114:
	mul.f64 	%fd172, %fd163, %fd3321;
	mov.f64 	%fd3387, %fd37;
	@%p44 bra 	BB85_116;

	mul.rn.f64 	%fd3387, %fd37, %fd806;

BB85_116:
	mov.f64 	%fd2814, 0d397B839A252049C0;
	mov.f64 	%fd2813, 0d3C91A62633145C00;
	mov.f64 	%fd2812, 0d3FF921FB54442D18;
	mul.f64 	%fd1365, %fd3387, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r643, %fd1365;
	st.local.u32 	[%rd1], %r643;
	cvt.rn.f64.s32	%fd1366, %r643;
	neg.f64 	%fd1367, %fd1366;
	fma.rn.f64 	%fd1369, %fd1367, %fd2812, %fd3387;
	fma.rn.f64 	%fd1371, %fd1367, %fd2813, %fd1369;
	fma.rn.f64 	%fd3322, %fd1367, %fd2814, %fd1371;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r327}, %fd3387;
	}
	and.b32  	%r328, %r327, 2145386496;
	setp.lt.u32	%p120, %r328, 1105199104;
	@%p120 bra 	BB85_118;

	// Callseq Start 91
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3387;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3322, [retval0+0];
	
	//{
	}// Callseq End 91
	ld.local.u32 	%r643, [%rd1];

BB85_118:
	mov.f64 	%fd3045, 0dBF2A01A019DB62A1;
	mov.f64 	%fd3044, 0d3EC71DE369ACE392;
	mov.f64 	%fd3043, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd3042, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3041, 0d3FF0000000000000;
	mov.f64 	%fd3040, 0dBFE0000000000000;
	mov.f64 	%fd3039, 0d3FA5555555555551;
	mov.f64 	%fd2908, 0dBF56C16C16C15D47;
	mov.f64 	%fd2875, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2874, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2873, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2872, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1373, %fd3322, %fd3322;
	fma.rn.f64 	%fd1376, %fd2872, %fd1373, %fd2873;
	fma.rn.f64 	%fd1378, %fd1376, %fd1373, %fd2874;
	fma.rn.f64 	%fd1380, %fd1378, %fd1373, %fd2875;
	fma.rn.f64 	%fd1382, %fd1380, %fd1373, %fd2908;
	fma.rn.f64 	%fd1384, %fd1382, %fd1373, %fd3039;
	fma.rn.f64 	%fd1386, %fd1384, %fd1373, %fd3040;
	fma.rn.f64 	%fd1388, %fd1386, %fd1373, %fd3041;
	fma.rn.f64 	%fd1391, %fd3042, %fd1373, %fd3043;
	fma.rn.f64 	%fd1393, %fd1391, %fd1373, %fd3044;
	fma.rn.f64 	%fd1395, %fd1393, %fd1373, %fd3045;
	fma.rn.f64 	%fd1397, %fd1395, %fd1373, %fd802;
	fma.rn.f64 	%fd1399, %fd1397, %fd1373, %fd804;
	fma.rn.f64 	%fd1401, %fd1399, %fd1373, %fd806;
	fma.rn.f64 	%fd1402, %fd1401, %fd3322, %fd3322;
	and.b32  	%r329, %r643, 1;
	setp.eq.b32	%p121, %r329, 1;
	not.pred 	%p122, %p121;
	selp.f64	%fd3323, %fd1402, %fd1388, %p122;
	and.b32  	%r330, %r643, 2;
	setp.eq.s32	%p123, %r330, 0;
	@%p123 bra 	BB85_120;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r331}, %fd3323;
	}
	xor.b32  	%r332, %r331, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r333, %temp}, %fd3323;
	}
	mov.b64 	%fd3323, {%r333, %r332};

BB85_120:
	mov.f64 	%fd3345, %fd39;
	@%p49 bra 	BB85_122;

	mul.rn.f64 	%fd3345, %fd39, %fd806;

BB85_122:
	mov.f64 	%fd2817, 0d397B839A252049C0;
	mov.f64 	%fd2816, 0d3C91A62633145C00;
	mov.f64 	%fd2815, 0d3FF921FB54442D18;
	mul.f64 	%fd1404, %fd3345, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r644, %fd1404;
	st.local.u32 	[%rd1], %r644;
	cvt.rn.f64.s32	%fd1405, %r644;
	neg.f64 	%fd1406, %fd1405;
	fma.rn.f64 	%fd1408, %fd1406, %fd2815, %fd3345;
	fma.rn.f64 	%fd1410, %fd1406, %fd2816, %fd1408;
	fma.rn.f64 	%fd3324, %fd1406, %fd2817, %fd1410;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r334}, %fd3345;
	}
	and.b32  	%r335, %r334, 2145386496;
	setp.lt.u32	%p125, %r335, 1105199104;
	@%p125 bra 	BB85_124;

	// Callseq Start 92
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3345;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3324, [retval0+0];
	
	//{
	}// Callseq End 92
	ld.local.u32 	%r644, [%rd1];

BB85_124:
	mov.f64 	%fd3047, 0dBF2A01A019DB62A1;
	mov.f64 	%fd3046, 0d3EC71DE369ACE392;
	mov.f64 	%fd2991, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2990, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2989, 0d3FF0000000000000;
	mov.f64 	%fd2988, 0dBFE0000000000000;
	mov.f64 	%fd2987, 0d3FA5555555555551;
	mov.f64 	%fd2876, 0dBF56C16C16C15D47;
	mov.f64 	%fd2837, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2836, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2835, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2834, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1412, %fd3324, %fd3324;
	fma.rn.f64 	%fd1415, %fd2834, %fd1412, %fd2835;
	fma.rn.f64 	%fd1417, %fd1415, %fd1412, %fd2836;
	fma.rn.f64 	%fd1419, %fd1417, %fd1412, %fd2837;
	fma.rn.f64 	%fd1421, %fd1419, %fd1412, %fd2876;
	fma.rn.f64 	%fd1423, %fd1421, %fd1412, %fd2987;
	fma.rn.f64 	%fd1425, %fd1423, %fd1412, %fd2988;
	fma.rn.f64 	%fd1427, %fd1425, %fd1412, %fd2989;
	fma.rn.f64 	%fd1430, %fd2990, %fd1412, %fd2991;
	fma.rn.f64 	%fd1432, %fd1430, %fd1412, %fd3046;
	fma.rn.f64 	%fd1434, %fd1432, %fd1412, %fd3047;
	fma.rn.f64 	%fd1436, %fd1434, %fd1412, %fd802;
	fma.rn.f64 	%fd1438, %fd1436, %fd1412, %fd804;
	fma.rn.f64 	%fd1440, %fd1438, %fd1412, %fd806;
	fma.rn.f64 	%fd1441, %fd1440, %fd3324, %fd3324;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r336}, %fd1441;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r337, %temp}, %fd1441;
	}
	xor.b32  	%r338, %r336, -2147483648;
	mov.b64 	%fd1442, {%r337, %r338};
	and.b32  	%r339, %r644, 1;
	setp.eq.b32	%p126, %r339, 1;
	not.pred 	%p127, %p126;
	selp.f64	%fd3325, %fd1427, %fd1442, %p127;
	and.b32  	%r340, %r644, 2;
	setp.eq.s32	%p128, %r340, 0;
	@%p128 bra 	BB85_126;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r341}, %fd3325;
	}
	xor.b32  	%r342, %r341, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r343, %temp}, %fd3325;
	}
	mov.b64 	%fd3325, {%r343, %r342};

BB85_126:
	mul.f64 	%fd189, %fd3323, %fd3325;
	mov.f64 	%fd3362, %fd16;
	@%p54 bra 	BB85_128;

	mul.rn.f64 	%fd3362, %fd16, %fd806;

BB85_128:
	mov.f64 	%fd2820, 0d397B839A252049C0;
	mov.f64 	%fd2819, 0d3C91A62633145C00;
	mov.f64 	%fd2818, 0d3FF921FB54442D18;
	mul.f64 	%fd1444, %fd3362, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r645, %fd1444;
	st.local.u32 	[%rd1], %r645;
	cvt.rn.f64.s32	%fd1445, %r645;
	neg.f64 	%fd1446, %fd1445;
	fma.rn.f64 	%fd1448, %fd1446, %fd2818, %fd3362;
	fma.rn.f64 	%fd1450, %fd1446, %fd2819, %fd1448;
	fma.rn.f64 	%fd3326, %fd1446, %fd2820, %fd1450;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r344}, %fd3362;
	}
	and.b32  	%r345, %r344, 2145386496;
	setp.lt.u32	%p130, %r345, 1105199104;
	@%p130 bra 	BB85_130;

	// Callseq Start 93
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3362;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3326, [retval0+0];
	
	//{
	}// Callseq End 93
	ld.local.u32 	%r645, [%rd1];

BB85_130:
	mov.f64 	%fd2998, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2997, 0d3EC71DE369ACE392;
	mov.f64 	%fd2996, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2995, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2994, 0d3FF0000000000000;
	mov.f64 	%fd2993, 0dBFE0000000000000;
	mov.f64 	%fd2992, 0d3FA5555555555551;
	mov.f64 	%fd2877, 0dBF56C16C16C15D47;
	mov.f64 	%fd2824, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2823, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2822, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2821, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1452, %fd3326, %fd3326;
	fma.rn.f64 	%fd1455, %fd2821, %fd1452, %fd2822;
	fma.rn.f64 	%fd1457, %fd1455, %fd1452, %fd2823;
	fma.rn.f64 	%fd1459, %fd1457, %fd1452, %fd2824;
	fma.rn.f64 	%fd1461, %fd1459, %fd1452, %fd2877;
	fma.rn.f64 	%fd1463, %fd1461, %fd1452, %fd2992;
	fma.rn.f64 	%fd1465, %fd1463, %fd1452, %fd2993;
	fma.rn.f64 	%fd1467, %fd1465, %fd1452, %fd2994;
	fma.rn.f64 	%fd1470, %fd2995, %fd1452, %fd2996;
	fma.rn.f64 	%fd1472, %fd1470, %fd1452, %fd2997;
	fma.rn.f64 	%fd1474, %fd1472, %fd1452, %fd2998;
	fma.rn.f64 	%fd1476, %fd1474, %fd1452, %fd802;
	fma.rn.f64 	%fd1478, %fd1476, %fd1452, %fd804;
	fma.rn.f64 	%fd1480, %fd1478, %fd1452, %fd806;
	fma.rn.f64 	%fd1481, %fd1480, %fd3326, %fd3326;
	and.b32  	%r346, %r645, 1;
	setp.eq.b32	%p131, %r346, 1;
	not.pred 	%p132, %p131;
	selp.f64	%fd3327, %fd1481, %fd1467, %p132;
	and.b32  	%r347, %r645, 2;
	setp.eq.s32	%p133, %r347, 0;
	@%p133 bra 	BB85_132;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r348}, %fd3327;
	}
	xor.b32  	%r349, %r348, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r350, %temp}, %fd3327;
	}
	mov.b64 	%fd3327, {%r350, %r349};

BB85_132:
	mul.f64 	%fd1482, %fd189, %fd3327;
	sub.f64 	%fd198, %fd172, %fd1482;
	mov.f64 	%fd3386, %fd37;
	@%p44 bra 	BB85_134;

	mul.rn.f64 	%fd3386, %fd37, %fd806;

BB85_134:
	mov.f64 	%fd2781, 0d397B839A252049C0;
	mov.f64 	%fd2780, 0d3C91A62633145C00;
	mov.f64 	%fd2779, 0d3FF921FB54442D18;
	mul.f64 	%fd1484, %fd3386, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r646, %fd1484;
	st.local.u32 	[%rd1], %r646;
	cvt.rn.f64.s32	%fd1485, %r646;
	neg.f64 	%fd1486, %fd1485;
	fma.rn.f64 	%fd1488, %fd1486, %fd2779, %fd3386;
	fma.rn.f64 	%fd1490, %fd1486, %fd2780, %fd1488;
	fma.rn.f64 	%fd3328, %fd1486, %fd2781, %fd1490;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r351}, %fd3386;
	}
	and.b32  	%r352, %r351, 2145386496;
	setp.lt.u32	%p135, %r352, 1105199104;
	@%p135 bra 	BB85_136;

	// Callseq Start 94
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3386;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3328, [retval0+0];
	
	//{
	}// Callseq End 94
	ld.local.u32 	%r646, [%rd1];

BB85_136:
	mov.f64 	%fd3000, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2999, 0d3EC71DE369ACE392;
	mov.f64 	%fd2936, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2935, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2934, 0d3FF0000000000000;
	mov.f64 	%fd2933, 0dBFE0000000000000;
	mov.f64 	%fd2932, 0d3FA5555555555551;
	mov.f64 	%fd2838, 0dBF56C16C16C15D47;
	mov.f64 	%fd2825, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2784, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2783, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2782, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1492, %fd3328, %fd3328;
	fma.rn.f64 	%fd1495, %fd2782, %fd1492, %fd2783;
	fma.rn.f64 	%fd1497, %fd1495, %fd1492, %fd2784;
	fma.rn.f64 	%fd1499, %fd1497, %fd1492, %fd2825;
	fma.rn.f64 	%fd1501, %fd1499, %fd1492, %fd2838;
	fma.rn.f64 	%fd1503, %fd1501, %fd1492, %fd2932;
	fma.rn.f64 	%fd1505, %fd1503, %fd1492, %fd2933;
	fma.rn.f64 	%fd1507, %fd1505, %fd1492, %fd2934;
	fma.rn.f64 	%fd1510, %fd2935, %fd1492, %fd2936;
	fma.rn.f64 	%fd1512, %fd1510, %fd1492, %fd2999;
	fma.rn.f64 	%fd1514, %fd1512, %fd1492, %fd3000;
	fma.rn.f64 	%fd1516, %fd1514, %fd1492, %fd802;
	fma.rn.f64 	%fd1518, %fd1516, %fd1492, %fd804;
	fma.rn.f64 	%fd1520, %fd1518, %fd1492, %fd806;
	fma.rn.f64 	%fd1521, %fd1520, %fd3328, %fd3328;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r353}, %fd1521;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r354, %temp}, %fd1521;
	}
	xor.b32  	%r355, %r353, -2147483648;
	mov.b64 	%fd1522, {%r354, %r355};
	and.b32  	%r356, %r646, 1;
	setp.eq.b32	%p136, %r356, 1;
	not.pred 	%p137, %p136;
	selp.f64	%fd3329, %fd1507, %fd1522, %p137;
	and.b32  	%r357, %r646, 2;
	setp.eq.s32	%p138, %r357, 0;
	@%p138 bra 	BB85_138;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r358}, %fd3329;
	}
	xor.b32  	%r359, %r358, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r360, %temp}, %fd3329;
	}
	mov.b64 	%fd3329, {%r360, %r359};

BB85_138:
	mov.f64 	%fd3344, %fd39;
	@%p49 bra 	BB85_140;

	mul.rn.f64 	%fd3344, %fd39, %fd806;

BB85_140:
	mov.f64 	%fd2787, 0d397B839A252049C0;
	mov.f64 	%fd2786, 0d3C91A62633145C00;
	mov.f64 	%fd2785, 0d3FF921FB54442D18;
	mul.f64 	%fd1524, %fd3344, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r647, %fd1524;
	st.local.u32 	[%rd1], %r647;
	cvt.rn.f64.s32	%fd1525, %r647;
	neg.f64 	%fd1526, %fd1525;
	fma.rn.f64 	%fd1528, %fd1526, %fd2785, %fd3344;
	fma.rn.f64 	%fd1530, %fd1526, %fd2786, %fd1528;
	fma.rn.f64 	%fd3330, %fd1526, %fd2787, %fd1530;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r361}, %fd3344;
	}
	and.b32  	%r362, %r361, 2145386496;
	setp.lt.u32	%p140, %r362, 1105199104;
	@%p140 bra 	BB85_142;

	// Callseq Start 95
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3344;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3330, [retval0+0];
	
	//{
	}// Callseq End 95
	ld.local.u32 	%r647, [%rd1];

BB85_142:
	mov.f64 	%fd2943, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2942, 0d3EC71DE369ACE392;
	mov.f64 	%fd2941, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2940, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2939, 0d3FF0000000000000;
	mov.f64 	%fd2938, 0dBFE0000000000000;
	mov.f64 	%fd2937, 0d3FA5555555555551;
	mov.f64 	%fd2792, 0dBF56C16C16C15D47;
	mov.f64 	%fd2791, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2790, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2789, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2788, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1532, %fd3330, %fd3330;
	fma.rn.f64 	%fd1535, %fd2788, %fd1532, %fd2789;
	fma.rn.f64 	%fd1537, %fd1535, %fd1532, %fd2790;
	fma.rn.f64 	%fd1539, %fd1537, %fd1532, %fd2791;
	fma.rn.f64 	%fd1541, %fd1539, %fd1532, %fd2792;
	fma.rn.f64 	%fd1543, %fd1541, %fd1532, %fd2937;
	fma.rn.f64 	%fd1545, %fd1543, %fd1532, %fd2938;
	fma.rn.f64 	%fd1547, %fd1545, %fd1532, %fd2939;
	fma.rn.f64 	%fd1550, %fd2940, %fd1532, %fd2941;
	fma.rn.f64 	%fd1552, %fd1550, %fd1532, %fd2942;
	fma.rn.f64 	%fd1554, %fd1552, %fd1532, %fd2943;
	fma.rn.f64 	%fd1556, %fd1554, %fd1532, %fd802;
	fma.rn.f64 	%fd1558, %fd1556, %fd1532, %fd804;
	fma.rn.f64 	%fd1560, %fd1558, %fd1532, %fd806;
	fma.rn.f64 	%fd1561, %fd1560, %fd3330, %fd3330;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r363}, %fd1561;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r364, %temp}, %fd1561;
	}
	xor.b32  	%r365, %r363, -2147483648;
	mov.b64 	%fd1562, {%r364, %r365};
	and.b32  	%r366, %r647, 1;
	setp.eq.b32	%p141, %r366, 1;
	not.pred 	%p142, %p141;
	selp.f64	%fd3331, %fd1547, %fd1562, %p142;
	and.b32  	%r367, %r647, 2;
	setp.eq.s32	%p143, %r367, 0;
	@%p143 bra 	BB85_144;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r368}, %fd3331;
	}
	xor.b32  	%r369, %r368, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r370, %temp}, %fd3331;
	}
	mov.b64 	%fd3331, {%r370, %r369};

BB85_144:
	mul.f64 	%fd215, %fd3329, %fd3331;
	mov.f64 	%fd3361, %fd16;
	@%p54 bra 	BB85_146;

	mul.rn.f64 	%fd3361, %fd16, %fd806;

BB85_146:
	mov.f64 	%fd2768, 0d397B839A252049C0;
	mov.f64 	%fd2767, 0d3C91A62633145C00;
	mov.f64 	%fd2766, 0d3FF921FB54442D18;
	mul.f64 	%fd1564, %fd3361, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r648, %fd1564;
	st.local.u32 	[%rd1], %r648;
	cvt.rn.f64.s32	%fd1565, %r648;
	neg.f64 	%fd1566, %fd1565;
	fma.rn.f64 	%fd1568, %fd1566, %fd2766, %fd3361;
	fma.rn.f64 	%fd1570, %fd1566, %fd2767, %fd1568;
	fma.rn.f64 	%fd3332, %fd1566, %fd2768, %fd1570;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r371}, %fd3361;
	}
	and.b32  	%r372, %r371, 2145386496;
	setp.lt.u32	%p145, %r372, 1105199104;
	@%p145 bra 	BB85_148;

	// Callseq Start 96
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3361;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3332, [retval0+0];
	
	//{
	}// Callseq End 96
	ld.local.u32 	%r648, [%rd1];

BB85_148:
	mov.f64 	%fd2944, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2914, 0d3EC71DE369ACE392;
	mov.f64 	%fd2913, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2912, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2911, 0d3FF0000000000000;
	mov.f64 	%fd2910, 0dBFE0000000000000;
	mov.f64 	%fd2909, 0d3FA5555555555551;
	mov.f64 	%fd2773, 0dBF56C16C16C15D47;
	mov.f64 	%fd2772, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2771, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2770, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2769, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1572, %fd3332, %fd3332;
	fma.rn.f64 	%fd1575, %fd2769, %fd1572, %fd2770;
	fma.rn.f64 	%fd1577, %fd1575, %fd1572, %fd2771;
	fma.rn.f64 	%fd1579, %fd1577, %fd1572, %fd2772;
	fma.rn.f64 	%fd1581, %fd1579, %fd1572, %fd2773;
	fma.rn.f64 	%fd1583, %fd1581, %fd1572, %fd2909;
	fma.rn.f64 	%fd1585, %fd1583, %fd1572, %fd2910;
	fma.rn.f64 	%fd1587, %fd1585, %fd1572, %fd2911;
	fma.rn.f64 	%fd1590, %fd2912, %fd1572, %fd2913;
	fma.rn.f64 	%fd1592, %fd1590, %fd1572, %fd2914;
	fma.rn.f64 	%fd1594, %fd1592, %fd1572, %fd2944;
	fma.rn.f64 	%fd1596, %fd1594, %fd1572, %fd802;
	fma.rn.f64 	%fd1598, %fd1596, %fd1572, %fd804;
	fma.rn.f64 	%fd1600, %fd1598, %fd1572, %fd806;
	fma.rn.f64 	%fd1601, %fd1600, %fd3332, %fd3332;
	and.b32  	%r373, %r648, 1;
	setp.eq.b32	%p146, %r373, 1;
	not.pred 	%p147, %p146;
	selp.f64	%fd3333, %fd1601, %fd1587, %p147;
	and.b32  	%r374, %r648, 2;
	setp.eq.s32	%p148, %r374, 0;
	@%p148 bra 	BB85_150;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r375}, %fd3333;
	}
	xor.b32  	%r376, %r375, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r377, %temp}, %fd3333;
	}
	mov.b64 	%fd3333, {%r377, %r376};

BB85_150:
	mul.f64 	%fd224, %fd215, %fd3333;
	mov.f64 	%fd3385, %fd37;
	@%p44 bra 	BB85_152;

	mul.rn.f64 	%fd3385, %fd37, %fd806;

BB85_152:
	mov.f64 	%fd2728, 0d397B839A252049C0;
	mov.f64 	%fd2727, 0d3C91A62633145C00;
	mov.f64 	%fd2726, 0d3FF921FB54442D18;
	mul.f64 	%fd1603, %fd3385, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r649, %fd1603;
	st.local.u32 	[%rd1], %r649;
	cvt.rn.f64.s32	%fd1604, %r649;
	neg.f64 	%fd1605, %fd1604;
	fma.rn.f64 	%fd1607, %fd1605, %fd2726, %fd3385;
	fma.rn.f64 	%fd1609, %fd1605, %fd2727, %fd1607;
	fma.rn.f64 	%fd3334, %fd1605, %fd2728, %fd1609;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r378}, %fd3385;
	}
	and.b32  	%r379, %r378, 2145386496;
	setp.lt.u32	%p150, %r379, 1105199104;
	@%p150 bra 	BB85_154;

	// Callseq Start 97
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3385;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3334, [retval0+0];
	
	//{
	}// Callseq End 97
	ld.local.u32 	%r649, [%rd1];

BB85_154:
	mov.f64 	%fd2799, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2798, 0d3EC71DE369ACE392;
	mov.f64 	%fd2797, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2796, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2795, 0d3FF0000000000000;
	mov.f64 	%fd2794, 0dBFE0000000000000;
	mov.f64 	%fd2793, 0d3FA5555555555551;
	mov.f64 	%fd2778, 0dBF56C16C16C15D47;
	mov.f64 	%fd2777, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2776, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2775, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2774, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1611, %fd3334, %fd3334;
	fma.rn.f64 	%fd1614, %fd2774, %fd1611, %fd2775;
	fma.rn.f64 	%fd1616, %fd1614, %fd1611, %fd2776;
	fma.rn.f64 	%fd1618, %fd1616, %fd1611, %fd2777;
	fma.rn.f64 	%fd1620, %fd1618, %fd1611, %fd2778;
	fma.rn.f64 	%fd1622, %fd1620, %fd1611, %fd2793;
	fma.rn.f64 	%fd1624, %fd1622, %fd1611, %fd2794;
	fma.rn.f64 	%fd1626, %fd1624, %fd1611, %fd2795;
	fma.rn.f64 	%fd1629, %fd2796, %fd1611, %fd2797;
	fma.rn.f64 	%fd1631, %fd1629, %fd1611, %fd2798;
	fma.rn.f64 	%fd1633, %fd1631, %fd1611, %fd2799;
	fma.rn.f64 	%fd1635, %fd1633, %fd1611, %fd802;
	fma.rn.f64 	%fd1637, %fd1635, %fd1611, %fd804;
	fma.rn.f64 	%fd1639, %fd1637, %fd1611, %fd806;
	fma.rn.f64 	%fd1640, %fd1639, %fd3334, %fd3334;
	and.b32  	%r380, %r649, 1;
	setp.eq.b32	%p151, %r380, 1;
	not.pred 	%p152, %p151;
	selp.f64	%fd3335, %fd1640, %fd1626, %p152;
	and.b32  	%r381, %r649, 2;
	setp.eq.s32	%p153, %r381, 0;
	@%p153 bra 	BB85_156;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r382}, %fd3335;
	}
	xor.b32  	%r383, %r382, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r384, %temp}, %fd3335;
	}
	mov.b64 	%fd3335, {%r384, %r383};

BB85_156:
	mov.f64 	%fd3343, %fd39;
	@%p49 bra 	BB85_158;

	mul.rn.f64 	%fd3343, %fd39, %fd806;

BB85_158:
	mov.f64 	%fd2731, 0d397B839A252049C0;
	mov.f64 	%fd2730, 0d3C91A62633145C00;
	mov.f64 	%fd2729, 0d3FF921FB54442D18;
	mul.f64 	%fd1642, %fd3343, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r650, %fd1642;
	st.local.u32 	[%rd1], %r650;
	cvt.rn.f64.s32	%fd1643, %r650;
	neg.f64 	%fd1644, %fd1643;
	fma.rn.f64 	%fd1646, %fd1644, %fd2729, %fd3343;
	fma.rn.f64 	%fd1648, %fd1644, %fd2730, %fd1646;
	fma.rn.f64 	%fd3351, %fd1644, %fd2731, %fd1648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r385}, %fd3343;
	}
	and.b32  	%r386, %r385, 2145386496;
	setp.lt.u32	%p155, %r386, 1105199104;
	@%p155 bra 	BB85_160;

	// Callseq Start 98
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3343;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3351, [retval0+0];
	
	//{
	}// Callseq End 98
	ld.local.u32 	%r650, [%rd1];

BB85_160:
	mov.f64 	%fd2802, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2801, 0d3EC71DE369ACE392;
	mov.f64 	%fd2800, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2743, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2742, 0d3FF0000000000000;
	mov.f64 	%fd2741, 0dBFE0000000000000;
	mov.f64 	%fd2740, 0d3FA5555555555551;
	mov.f64 	%fd2739, 0dBF56C16C16C15D47;
	mov.f64 	%fd2738, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2737, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2736, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2735, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1650, %fd3351, %fd3351;
	fma.rn.f64 	%fd1653, %fd2735, %fd1650, %fd2736;
	fma.rn.f64 	%fd1655, %fd1653, %fd1650, %fd2737;
	fma.rn.f64 	%fd1657, %fd1655, %fd1650, %fd2738;
	fma.rn.f64 	%fd1659, %fd1657, %fd1650, %fd2739;
	fma.rn.f64 	%fd1661, %fd1659, %fd1650, %fd2740;
	fma.rn.f64 	%fd1663, %fd1661, %fd1650, %fd2741;
	fma.rn.f64 	%fd1665, %fd1663, %fd1650, %fd2742;
	fma.rn.f64 	%fd1668, %fd2743, %fd1650, %fd2800;
	fma.rn.f64 	%fd1670, %fd1668, %fd1650, %fd2801;
	fma.rn.f64 	%fd1672, %fd1670, %fd1650, %fd2802;
	fma.rn.f64 	%fd1674, %fd1672, %fd1650, %fd802;
	fma.rn.f64 	%fd1676, %fd1674, %fd1650, %fd804;
	fma.rn.f64 	%fd1678, %fd1676, %fd1650, %fd806;
	fma.rn.f64 	%fd1679, %fd1678, %fd3351, %fd3351;
	and.b32  	%r387, %r650, 1;
	setp.eq.b32	%p156, %r387, 1;
	not.pred 	%p157, %p156;
	selp.f64	%fd3352, %fd1679, %fd1665, %p157;
	and.b32  	%r388, %r650, 2;
	setp.eq.s32	%p158, %r388, 0;
	@%p158 bra 	BB85_162;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r389}, %fd3352;
	}
	xor.b32  	%r390, %r389, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r391, %temp}, %fd3352;
	}
	mov.b64 	%fd3352, {%r391, %r390};

BB85_162:
	mul.f64 	%fd241, %fd3335, %fd3352;
	mov.f64 	%fd3360, %fd16;
	@%p54 bra 	BB85_164;

	mul.rn.f64 	%fd3360, %fd16, %fd806;

BB85_164:
	mov.f64 	%fd2734, 0d397B839A252049C0;
	mov.f64 	%fd2733, 0d3C91A62633145C00;
	mov.f64 	%fd2732, 0d3FF921FB54442D18;
	mul.f64 	%fd1681, %fd3360, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r651, %fd1681;
	st.local.u32 	[%rd1], %r651;
	cvt.rn.f64.s32	%fd1682, %r651;
	neg.f64 	%fd1683, %fd1682;
	fma.rn.f64 	%fd1685, %fd1683, %fd2732, %fd3360;
	fma.rn.f64 	%fd1687, %fd1683, %fd2733, %fd1685;
	fma.rn.f64 	%fd3368, %fd1683, %fd2734, %fd1687;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r392}, %fd3360;
	}
	and.b32  	%r393, %r392, 2145386496;
	setp.lt.u32	%p160, %r393, 1105199104;
	@%p160 bra 	BB85_166;

	// Callseq Start 99
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3360;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3368, [retval0+0];
	
	//{
	}// Callseq End 99
	ld.local.u32 	%r651, [%rd1];

BB85_166:
	mov.f64 	%fd2756, 0d3F81111111110818;
	mov.f64 	%fd2755, 0dBF2A01A019DB62A1;
	mov.f64 	%fd2754, 0d3EC71DE369ACE392;
	mov.f64 	%fd2753, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2752, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2751, 0d3FF0000000000000;
	mov.f64 	%fd2750, 0dBFE0000000000000;
	mov.f64 	%fd2749, 0d3FA5555555555551;
	mov.f64 	%fd2748, 0dBF56C16C16C15D47;
	mov.f64 	%fd2747, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2746, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2745, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2744, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1689, %fd3368, %fd3368;
	fma.rn.f64 	%fd1692, %fd2744, %fd1689, %fd2745;
	fma.rn.f64 	%fd1694, %fd1692, %fd1689, %fd2746;
	fma.rn.f64 	%fd1696, %fd1694, %fd1689, %fd2747;
	fma.rn.f64 	%fd1698, %fd1696, %fd1689, %fd2748;
	fma.rn.f64 	%fd1700, %fd1698, %fd1689, %fd2749;
	fma.rn.f64 	%fd1702, %fd1700, %fd1689, %fd2750;
	fma.rn.f64 	%fd1704, %fd1702, %fd1689, %fd2751;
	fma.rn.f64 	%fd1707, %fd2752, %fd1689, %fd2753;
	fma.rn.f64 	%fd1709, %fd1707, %fd1689, %fd2754;
	fma.rn.f64 	%fd1711, %fd1709, %fd1689, %fd2755;
	fma.rn.f64 	%fd1713, %fd1711, %fd1689, %fd2756;
	fma.rn.f64 	%fd1715, %fd1713, %fd1689, %fd804;
	fma.rn.f64 	%fd1717, %fd1715, %fd1689, %fd806;
	fma.rn.f64 	%fd1718, %fd1717, %fd3368, %fd3368;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r394}, %fd1718;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r395, %temp}, %fd1718;
	}
	xor.b32  	%r396, %r394, -2147483648;
	mov.b64 	%fd1719, {%r395, %r396};
	and.b32  	%r397, %r651, 1;
	setp.eq.b32	%p161, %r397, 1;
	not.pred 	%p162, %p161;
	selp.f64	%fd3369, %fd1704, %fd1719, %p162;
	and.b32  	%r398, %r651, 2;
	setp.eq.s32	%p163, %r398, 0;
	@%p163 bra 	BB85_168;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r399}, %fd3369;
	}
	xor.b32  	%r400, %r399, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r401, %temp}, %fd3369;
	}
	mov.b64 	%fd3369, {%r401, %r400};

BB85_168:
	fma.rn.f64 	%fd250, %fd241, %fd3369, %fd224;
	neg.f64 	%fd1720, %fd6;
	add.f64 	%fd1721, %fd94, %fd6;
	abs.f64 	%fd1722, %fd1721;
	abs.f64 	%fd1723, %fd1720;
	abs.f64 	%fd1724, %fd94;
	max.f64 	%fd1725, %fd1724, %fd1723;
	mul.f64 	%fd1726, %fd1725, 0d3D83880000000000;
	setp.geu.f64	%p164, %fd1722, %fd1726;
	mov.f64 	%fd3384, %fd37;
	@%p164 bra 	BB85_173;

	neg.f64 	%fd1727, %fd8;
	add.f64 	%fd1728, %fd146, %fd8;
	abs.f64 	%fd1729, %fd1728;
	abs.f64 	%fd1730, %fd1727;
	abs.f64 	%fd1731, %fd146;
	max.f64 	%fd1732, %fd1731, %fd1730;
	mul.f64 	%fd1733, %fd1732, 0d3D83880000000000;
	setp.geu.f64	%p165, %fd1729, %fd1733;
	mov.f64 	%fd3384, %fd37;
	@%p165 bra 	BB85_173;

	neg.f64 	%fd1734, %fd7;
	add.f64 	%fd1735, %fd198, %fd7;
	abs.f64 	%fd1736, %fd1735;
	abs.f64 	%fd1737, %fd1734;
	abs.f64 	%fd1738, %fd198;
	max.f64 	%fd1739, %fd1738, %fd1737;
	mul.f64 	%fd1740, %fd1739, 0d3D83880000000000;
	setp.geu.f64	%p166, %fd1736, %fd1740;
	mov.f64 	%fd3384, %fd37;
	@%p166 bra 	BB85_173;

	neg.f64 	%fd1741, %fd9;
	add.f64 	%fd1742, %fd250, %fd9;
	abs.f64 	%fd1743, %fd1742;
	abs.f64 	%fd1744, %fd1741;
	abs.f64 	%fd1745, %fd250;
	max.f64 	%fd1746, %fd1745, %fd1744;
	mul.f64 	%fd1747, %fd1746, 0d3D83880000000000;
	setp.geu.f64	%p167, %fd1743, %fd1747;
	mov.f64 	%fd3384, %fd37;
	@%p167 bra 	BB85_173;

	setp.ltu.f64	%p168, %fd37, 0d0000000000000000;
	selp.f64	%fd1748, 0dC00921FB54442D18, 0d400921FB54442D18, %p168;
	sub.f64 	%fd3384, %fd37, %fd1748;

BB85_173:
	mov.f64 	%fd3396, %fd16;
	mov.f64 	%fd3395, %fd39;
	mov.f64 	%fd3394, %fd3384;

BB85_174:
	ld.param.u32 	%r621, [actfunc_quat_multi_double_param_1];
	cvt.rn.f64.s32	%fd1749, %r621;
	mov.f64 	%fd1750, 0d401921FB54442D18;
	div.rn.f64 	%fd261, %fd1750, %fd1749;
	mul.f64 	%fd262, %fd261, 0d3FE0000000000000;
	sub.f64 	%fd1751, %fd3394, %fd262;
	add.f64 	%fd1752, %fd1751, 0d400921FB54442D18;
	div.rn.f64 	%fd3397, %fd1752, %fd261;
	abs.f64 	%fd264, %fd3397;
	setp.ge.f64	%p169, %fd264, 0d4330000000000000;
	@%p169 bra 	BB85_176;

	add.f64 	%fd1753, %fd264, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd1754, %fd1753;
	setp.lt.f64	%p170, %fd264, 0d3FE0000000000000;
	selp.f64	%fd1755, 0d0000000000000000, %fd1754, %p170;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r402, %temp}, %fd1755;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r403}, %fd1755;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r404}, %fd3397;
	}
	and.b32  	%r405, %r404, -2147483648;
	or.b32  	%r406, %r403, %r405;
	mov.b64 	%fd3397, {%r402, %r406};

BB85_176:
	ld.param.u32 	%r622, [actfunc_quat_multi_double_param_2];
	fma.rn.f64 	%fd1756, %fd261, %fd3397, 0dC00921FB54442D18;
	add.f64 	%fd267, %fd262, %fd1756;
	cvt.rn.f64.s32	%fd1757, %r622;
	div.rn.f64 	%fd1759, %fd1750, %fd1757;
	mul.f64 	%fd268, %fd1759, 0d3FE0000000000000;
	mul.f64 	%fd269, %fd268, 0d3FE0000000000000;
	sub.f64 	%fd1760, %fd3395, %fd269;
	add.f64 	%fd1761, %fd1760, 0d3FF921FB54442D18;
	div.rn.f64 	%fd3398, %fd1761, %fd268;
	abs.f64 	%fd271, %fd3398;
	setp.ge.f64	%p171, %fd271, 0d4330000000000000;
	@%p171 bra 	BB85_178;

	add.f64 	%fd1762, %fd271, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd1763, %fd1762;
	setp.lt.f64	%p172, %fd271, 0d3FE0000000000000;
	selp.f64	%fd1764, 0d0000000000000000, %fd1763, %p172;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r407, %temp}, %fd1764;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r408}, %fd1764;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r409}, %fd3398;
	}
	and.b32  	%r410, %r409, -2147483648;
	or.b32  	%r411, %r408, %r410;
	mov.b64 	%fd3398, {%r407, %r411};

BB85_178:
	ld.param.u32 	%r623, [actfunc_quat_multi_double_param_3];
	fma.rn.f64 	%fd1765, %fd268, %fd3398, 0dBFF921FB54442D18;
	add.f64 	%fd274, %fd269, %fd1765;
	cvt.rn.f64.s32	%fd1766, %r623;
	div.rn.f64 	%fd1768, %fd1750, %fd1766;
	mul.f64 	%fd275, %fd1768, 0d3FD0000000000000;
	mul.f64 	%fd276, %fd275, 0d3FE0000000000000;
	sub.f64 	%fd1769, %fd3396, %fd276;
	add.f64 	%fd1770, %fd1769, 0d3FE921FB54442D18;
	div.rn.f64 	%fd3399, %fd1770, %fd275;
	abs.f64 	%fd278, %fd3399;
	setp.ge.f64	%p173, %fd278, 0d4330000000000000;
	@%p173 bra 	BB85_180;

	add.f64 	%fd1771, %fd278, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd1772, %fd1771;
	setp.lt.f64	%p174, %fd278, 0d3FE0000000000000;
	selp.f64	%fd1773, 0d0000000000000000, %fd1772, %p174;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r412, %temp}, %fd1773;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r413}, %fd1773;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r414}, %fd3399;
	}
	and.b32  	%r415, %r414, -2147483648;
	or.b32  	%r416, %r413, %r415;
	mov.b64 	%fd3399, {%r412, %r416};

BB85_180:
	fma.rn.f64 	%fd1774, %fd275, %fd3399, 0dBFE921FB54442D18;
	add.f64 	%fd281, %fd276, %fd1774;
	abs.f64 	%fd282, %fd267;
	setp.neu.f64	%p175, %fd282, 0d7FF0000000000000;
	mov.f64 	%fd3456, %fd267;
	@%p175 bra 	BB85_182;

	mov.f64 	%fd1775, 0d0000000000000000;
	mul.rn.f64 	%fd283, %fd267, %fd1775;
	mov.f64 	%fd3456, %fd283;

BB85_182:
	mov.f64 	%fd284, %fd3456;
	mul.f64 	%fd1776, %fd284, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r652, %fd1776;
	st.local.u32 	[%rd1], %r652;
	cvt.rn.f64.s32	%fd1777, %r652;
	neg.f64 	%fd1778, %fd1777;
	mov.f64 	%fd1779, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd1780, %fd1778, %fd1779, %fd284;
	mov.f64 	%fd1781, 0d3C91A62633145C00;
	fma.rn.f64 	%fd1782, %fd1778, %fd1781, %fd1780;
	mov.f64 	%fd1783, 0d397B839A252049C0;
	fma.rn.f64 	%fd3400, %fd1778, %fd1783, %fd1782;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r417}, %fd284;
	}
	and.b32  	%r418, %r417, 2145386496;
	setp.lt.u32	%p176, %r418, 1105199104;
	@%p176 bra 	BB85_184;

	// Callseq Start 100
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd284;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd51;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3400, [retval0+0];
	
	//{
	}// Callseq End 100
	ld.local.u32 	%r652, [%rd1];

BB85_184:
	mul.rn.f64 	%fd1784, %fd3400, %fd3400;
	mov.f64 	%fd1785, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd1786, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd1787, %fd1786, %fd1784, %fd1785;
	mov.f64 	%fd1788, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd1789, %fd1787, %fd1784, %fd1788;
	mov.f64 	%fd1790, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd1791, %fd1789, %fd1784, %fd1790;
	mov.f64 	%fd1792, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd1793, %fd1791, %fd1784, %fd1792;
	mov.f64 	%fd1794, 0d3FA5555555555551;
	fma.rn.f64 	%fd1795, %fd1793, %fd1784, %fd1794;
	mov.f64 	%fd1796, 0dBFE0000000000000;
	fma.rn.f64 	%fd1797, %fd1795, %fd1784, %fd1796;
	mov.f64 	%fd1798, 0d3FF0000000000000;
	fma.rn.f64 	%fd1799, %fd1797, %fd1784, %fd1798;
	mov.f64 	%fd1800, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd1801, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd1802, %fd1801, %fd1784, %fd1800;
	mov.f64 	%fd1803, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd1804, %fd1802, %fd1784, %fd1803;
	mov.f64 	%fd1805, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd1806, %fd1804, %fd1784, %fd1805;
	mov.f64 	%fd1807, 0d3F81111111110818;
	fma.rn.f64 	%fd1808, %fd1806, %fd1784, %fd1807;
	mov.f64 	%fd1809, 0dBFC5555555555554;
	fma.rn.f64 	%fd1810, %fd1808, %fd1784, %fd1809;
	mov.f64 	%fd1811, 0d0000000000000000;
	fma.rn.f64 	%fd1812, %fd1810, %fd1784, %fd1811;
	fma.rn.f64 	%fd1813, %fd1812, %fd3400, %fd3400;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r419}, %fd1813;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r420, %temp}, %fd1813;
	}
	xor.b32  	%r421, %r419, -2147483648;
	mov.b64 	%fd1814, {%r420, %r421};
	and.b32  	%r422, %r652, 1;
	setp.eq.b32	%p177, %r422, 1;
	not.pred 	%p178, %p177;
	selp.f64	%fd3401, %fd1799, %fd1814, %p178;
	and.b32  	%r423, %r652, 2;
	setp.eq.s32	%p179, %r423, 0;
	@%p179 bra 	BB85_186;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r424}, %fd3401;
	}
	xor.b32  	%r425, %r424, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r426, %temp}, %fd3401;
	}
	mov.b64 	%fd3401, {%r426, %r425};

BB85_186:
	abs.f64 	%fd291, %fd274;
	setp.neu.f64	%p180, %fd291, 0d7FF0000000000000;
	mov.f64 	%fd3473, %fd274;
	@%p180 bra 	BB85_188;

	mul.rn.f64 	%fd292, %fd274, %fd1811;
	mov.f64 	%fd3473, %fd292;

BB85_188:
	mov.f64 	%fd293, %fd3473;
	mov.f64 	%fd3130, 0d397B839A252049C0;
	mov.f64 	%fd3092, 0d3C91A62633145C00;
	mov.f64 	%fd3091, 0d3FF921FB54442D18;
	mul.f64 	%fd1816, %fd293, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r653, %fd1816;
	st.local.u32 	[%rd1], %r653;
	cvt.rn.f64.s32	%fd1817, %r653;
	neg.f64 	%fd1818, %fd1817;
	fma.rn.f64 	%fd1820, %fd1818, %fd3091, %fd293;
	fma.rn.f64 	%fd1822, %fd1818, %fd3092, %fd1820;
	fma.rn.f64 	%fd3402, %fd1818, %fd3130, %fd1822;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r427}, %fd293;
	}
	and.b32  	%r428, %r427, 2145386496;
	setp.lt.u32	%p181, %r428, 1105199104;
	@%p181 bra 	BB85_190;

	add.u64 	%rd175, %SP, 0;
	// Callseq Start 101
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd293;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd175;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3402, [retval0+0];
	
	//{
	}// Callseq End 101
	ld.local.u32 	%r653, [%rd1];

BB85_190:
	mov.f64 	%fd3271, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3260, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3248, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1824, %fd3402, %fd3402;
	fma.rn.f64 	%fd1827, %fd3248, %fd1824, %fd3260;
	fma.rn.f64 	%fd1829, %fd1827, %fd1824, %fd3271;
	fma.rn.f64 	%fd1831, %fd1829, %fd1824, %fd1790;
	fma.rn.f64 	%fd1833, %fd1831, %fd1824, %fd1792;
	fma.rn.f64 	%fd1835, %fd1833, %fd1824, %fd1794;
	fma.rn.f64 	%fd1837, %fd1835, %fd1824, %fd1796;
	fma.rn.f64 	%fd1839, %fd1837, %fd1824, %fd1798;
	fma.rn.f64 	%fd1842, %fd1801, %fd1824, %fd1800;
	fma.rn.f64 	%fd1844, %fd1842, %fd1824, %fd1803;
	fma.rn.f64 	%fd1846, %fd1844, %fd1824, %fd1805;
	fma.rn.f64 	%fd1848, %fd1846, %fd1824, %fd1807;
	fma.rn.f64 	%fd1850, %fd1848, %fd1824, %fd1809;
	fma.rn.f64 	%fd1852, %fd1850, %fd1824, %fd1811;
	fma.rn.f64 	%fd1853, %fd1852, %fd3402, %fd3402;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r429}, %fd1853;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r430, %temp}, %fd1853;
	}
	xor.b32  	%r431, %r429, -2147483648;
	mov.b64 	%fd1854, {%r430, %r431};
	and.b32  	%r432, %r653, 1;
	setp.eq.b32	%p182, %r432, 1;
	not.pred 	%p183, %p182;
	selp.f64	%fd3403, %fd1839, %fd1854, %p183;
	and.b32  	%r433, %r653, 2;
	setp.eq.s32	%p184, %r433, 0;
	@%p184 bra 	BB85_192;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r434}, %fd3403;
	}
	xor.b32  	%r435, %r434, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r436, %temp}, %fd3403;
	}
	mov.b64 	%fd3403, {%r436, %r435};

BB85_192:
	mul.f64 	%fd300, %fd3401, %fd3403;
	abs.f64 	%fd301, %fd281;
	setp.neu.f64	%p185, %fd301, 0d7FF0000000000000;
	mov.f64 	%fd3490, %fd281;
	@%p185 bra 	BB85_194;

	mul.rn.f64 	%fd302, %fd281, %fd1811;
	mov.f64 	%fd3490, %fd302;

BB85_194:
	mov.f64 	%fd303, %fd3490;
	mov.f64 	%fd3094, 0d397B839A252049C0;
	mov.f64 	%fd3093, 0d3C91A62633145C00;
	mov.f64 	%fd3048, 0d3FF921FB54442D18;
	mul.f64 	%fd1856, %fd303, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r654, %fd1856;
	st.local.u32 	[%rd1], %r654;
	cvt.rn.f64.s32	%fd1857, %r654;
	neg.f64 	%fd1858, %fd1857;
	fma.rn.f64 	%fd1860, %fd1858, %fd3048, %fd303;
	fma.rn.f64 	%fd1862, %fd1858, %fd3093, %fd1860;
	fma.rn.f64 	%fd3404, %fd1858, %fd3094, %fd1862;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r437}, %fd303;
	}
	and.b32  	%r438, %r437, 2145386496;
	setp.lt.u32	%p186, %r438, 1105199104;
	@%p186 bra 	BB85_196;

	add.u64 	%rd174, %SP, 0;
	// Callseq Start 102
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd303;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd174;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3404, [retval0+0];
	
	//{
	}// Callseq End 102
	ld.local.u32 	%r654, [%rd1];

BB85_196:
	mov.f64 	%fd3287, 0d3FA5555555555551;
	mov.f64 	%fd3284, 0dBF56C16C16C15D47;
	mov.f64 	%fd3281, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3272, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3261, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3249, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1864, %fd3404, %fd3404;
	fma.rn.f64 	%fd1867, %fd3249, %fd1864, %fd3261;
	fma.rn.f64 	%fd1869, %fd1867, %fd1864, %fd3272;
	fma.rn.f64 	%fd1871, %fd1869, %fd1864, %fd3281;
	fma.rn.f64 	%fd1873, %fd1871, %fd1864, %fd3284;
	fma.rn.f64 	%fd1875, %fd1873, %fd1864, %fd3287;
	fma.rn.f64 	%fd1877, %fd1875, %fd1864, %fd1796;
	fma.rn.f64 	%fd1879, %fd1877, %fd1864, %fd1798;
	fma.rn.f64 	%fd1882, %fd1801, %fd1864, %fd1800;
	fma.rn.f64 	%fd1884, %fd1882, %fd1864, %fd1803;
	fma.rn.f64 	%fd1886, %fd1884, %fd1864, %fd1805;
	fma.rn.f64 	%fd1888, %fd1886, %fd1864, %fd1807;
	fma.rn.f64 	%fd1890, %fd1888, %fd1864, %fd1809;
	fma.rn.f64 	%fd1892, %fd1890, %fd1864, %fd1811;
	fma.rn.f64 	%fd1893, %fd1892, %fd3404, %fd3404;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r439}, %fd1893;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r440, %temp}, %fd1893;
	}
	xor.b32  	%r441, %r439, -2147483648;
	mov.b64 	%fd1894, {%r440, %r441};
	and.b32  	%r442, %r654, 1;
	setp.eq.b32	%p187, %r442, 1;
	not.pred 	%p188, %p187;
	selp.f64	%fd3405, %fd1879, %fd1894, %p188;
	and.b32  	%r443, %r654, 2;
	setp.eq.s32	%p189, %r443, 0;
	@%p189 bra 	BB85_198;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r444}, %fd3405;
	}
	xor.b32  	%r445, %r444, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r446, %temp}, %fd3405;
	}
	mov.b64 	%fd3405, {%r446, %r445};

BB85_198:
	mul.f64 	%fd310, %fd300, %fd3405;
	mov.f64 	%fd3455, %fd267;
	@%p175 bra 	BB85_200;

	mul.rn.f64 	%fd3455, %fd267, %fd1811;

BB85_200:
	mov.f64 	%fd3095, 0d397B839A252049C0;
	mov.f64 	%fd3050, 0d3C91A62633145C00;
	mov.f64 	%fd3049, 0d3FF921FB54442D18;
	mul.f64 	%fd1896, %fd3455, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r655, %fd1896;
	st.local.u32 	[%rd1], %r655;
	cvt.rn.f64.s32	%fd1897, %r655;
	neg.f64 	%fd1898, %fd1897;
	fma.rn.f64 	%fd1900, %fd1898, %fd3049, %fd3455;
	fma.rn.f64 	%fd1902, %fd1898, %fd3050, %fd1900;
	fma.rn.f64 	%fd3406, %fd1898, %fd3095, %fd1902;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r447}, %fd3455;
	}
	and.b32  	%r448, %r447, 2145386496;
	setp.lt.u32	%p191, %r448, 1105199104;
	@%p191 bra 	BB85_202;

	add.u64 	%rd173, %SP, 0;
	// Callseq Start 103
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3455;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd173;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3406, [retval0+0];
	
	//{
	}// Callseq End 103
	ld.local.u32 	%r655, [%rd1];

BB85_202:
	mov.f64 	%fd3285, 0d3FA5555555555551;
	mov.f64 	%fd3282, 0dBF56C16C16C15D47;
	mov.f64 	%fd3274, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3273, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3262, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3245, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1904, %fd3406, %fd3406;
	fma.rn.f64 	%fd1907, %fd3245, %fd1904, %fd3262;
	fma.rn.f64 	%fd1909, %fd1907, %fd1904, %fd3273;
	fma.rn.f64 	%fd1911, %fd1909, %fd1904, %fd3274;
	fma.rn.f64 	%fd1913, %fd1911, %fd1904, %fd3282;
	fma.rn.f64 	%fd1915, %fd1913, %fd1904, %fd3285;
	fma.rn.f64 	%fd1917, %fd1915, %fd1904, %fd1796;
	fma.rn.f64 	%fd1919, %fd1917, %fd1904, %fd1798;
	fma.rn.f64 	%fd1922, %fd1801, %fd1904, %fd1800;
	fma.rn.f64 	%fd1924, %fd1922, %fd1904, %fd1803;
	fma.rn.f64 	%fd1926, %fd1924, %fd1904, %fd1805;
	fma.rn.f64 	%fd1928, %fd1926, %fd1904, %fd1807;
	fma.rn.f64 	%fd1930, %fd1928, %fd1904, %fd1809;
	fma.rn.f64 	%fd1932, %fd1930, %fd1904, %fd1811;
	fma.rn.f64 	%fd1933, %fd1932, %fd3406, %fd3406;
	and.b32  	%r449, %r655, 1;
	setp.eq.b32	%p192, %r449, 1;
	not.pred 	%p193, %p192;
	selp.f64	%fd3407, %fd1933, %fd1919, %p193;
	and.b32  	%r450, %r655, 2;
	setp.eq.s32	%p194, %r450, 0;
	@%p194 bra 	BB85_204;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r451}, %fd3407;
	}
	xor.b32  	%r452, %r451, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r453, %temp}, %fd3407;
	}
	mov.b64 	%fd3407, {%r453, %r452};

BB85_204:
	mov.f64 	%fd3472, %fd274;
	@%p180 bra 	BB85_206;

	mul.rn.f64 	%fd3472, %fd274, %fd1811;

BB85_206:
	mov.f64 	%fd3052, 0d397B839A252049C0;
	mov.f64 	%fd3051, 0d3C91A62633145C00;
	mov.f64 	%fd3001, 0d3FF921FB54442D18;
	mul.f64 	%fd1935, %fd3472, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r656, %fd1935;
	st.local.u32 	[%rd1], %r656;
	cvt.rn.f64.s32	%fd1936, %r656;
	neg.f64 	%fd1937, %fd1936;
	fma.rn.f64 	%fd1939, %fd1937, %fd3001, %fd3472;
	fma.rn.f64 	%fd1941, %fd1937, %fd3051, %fd1939;
	fma.rn.f64 	%fd3408, %fd1937, %fd3052, %fd1941;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r454}, %fd3472;
	}
	and.b32  	%r455, %r454, 2145386496;
	setp.lt.u32	%p196, %r455, 1105199104;
	@%p196 bra 	BB85_208;

	add.u64 	%rd172, %SP, 0;
	// Callseq Start 104
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3472;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd172;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3408, [retval0+0];
	
	//{
	}// Callseq End 104
	ld.local.u32 	%r656, [%rd1];

BB85_208:
	mov.f64 	%fd3286, 0d3FA5555555555551;
	mov.f64 	%fd3276, 0dBF56C16C16C15D47;
	mov.f64 	%fd3275, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3263, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3250, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3236, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1943, %fd3408, %fd3408;
	fma.rn.f64 	%fd1946, %fd3236, %fd1943, %fd3250;
	fma.rn.f64 	%fd1948, %fd1946, %fd1943, %fd3263;
	fma.rn.f64 	%fd1950, %fd1948, %fd1943, %fd3275;
	fma.rn.f64 	%fd1952, %fd1950, %fd1943, %fd3276;
	fma.rn.f64 	%fd1954, %fd1952, %fd1943, %fd3286;
	fma.rn.f64 	%fd1956, %fd1954, %fd1943, %fd1796;
	fma.rn.f64 	%fd1958, %fd1956, %fd1943, %fd1798;
	fma.rn.f64 	%fd1961, %fd1801, %fd1943, %fd1800;
	fma.rn.f64 	%fd1963, %fd1961, %fd1943, %fd1803;
	fma.rn.f64 	%fd1965, %fd1963, %fd1943, %fd1805;
	fma.rn.f64 	%fd1967, %fd1965, %fd1943, %fd1807;
	fma.rn.f64 	%fd1969, %fd1967, %fd1943, %fd1809;
	fma.rn.f64 	%fd1971, %fd1969, %fd1943, %fd1811;
	fma.rn.f64 	%fd1972, %fd1971, %fd3408, %fd3408;
	and.b32  	%r456, %r656, 1;
	setp.eq.b32	%p197, %r456, 1;
	not.pred 	%p198, %p197;
	selp.f64	%fd3409, %fd1972, %fd1958, %p198;
	and.b32  	%r457, %r656, 2;
	setp.eq.s32	%p199, %r457, 0;
	@%p199 bra 	BB85_210;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r458}, %fd3409;
	}
	xor.b32  	%r459, %r458, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r460, %temp}, %fd3409;
	}
	mov.b64 	%fd3409, {%r460, %r459};

BB85_210:
	mul.f64 	%fd327, %fd3407, %fd3409;
	mov.f64 	%fd3489, %fd281;
	@%p185 bra 	BB85_212;

	mul.rn.f64 	%fd3489, %fd281, %fd1811;

BB85_212:
	mov.f64 	%fd3053, 0d397B839A252049C0;
	mov.f64 	%fd3004, 0d3C91A62633145C00;
	mov.f64 	%fd3002, 0d3FF921FB54442D18;
	mul.f64 	%fd1974, %fd3489, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r657, %fd1974;
	st.local.u32 	[%rd1], %r657;
	cvt.rn.f64.s32	%fd1975, %r657;
	neg.f64 	%fd1976, %fd1975;
	fma.rn.f64 	%fd1978, %fd1976, %fd3002, %fd3489;
	fma.rn.f64 	%fd1980, %fd1976, %fd3004, %fd1978;
	fma.rn.f64 	%fd3410, %fd1976, %fd3053, %fd1980;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r461}, %fd3489;
	}
	and.b32  	%r462, %r461, 2145386496;
	setp.lt.u32	%p201, %r462, 1105199104;
	@%p201 bra 	BB85_214;

	add.u64 	%rd171, %SP, 0;
	// Callseq Start 105
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3489;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd171;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3410, [retval0+0];
	
	//{
	}// Callseq End 105
	ld.local.u32 	%r657, [%rd1];

BB85_214:
	mov.f64 	%fd3283, 0d3FA5555555555551;
	mov.f64 	%fd3277, 0dBF56C16C16C15D47;
	mov.f64 	%fd3265, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3264, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3251, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3230, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd1982, %fd3410, %fd3410;
	fma.rn.f64 	%fd1985, %fd3230, %fd1982, %fd3251;
	fma.rn.f64 	%fd1987, %fd1985, %fd1982, %fd3264;
	fma.rn.f64 	%fd1989, %fd1987, %fd1982, %fd3265;
	fma.rn.f64 	%fd1991, %fd1989, %fd1982, %fd3277;
	fma.rn.f64 	%fd1993, %fd1991, %fd1982, %fd3283;
	fma.rn.f64 	%fd1995, %fd1993, %fd1982, %fd1796;
	fma.rn.f64 	%fd1997, %fd1995, %fd1982, %fd1798;
	fma.rn.f64 	%fd2000, %fd1801, %fd1982, %fd1800;
	fma.rn.f64 	%fd2002, %fd2000, %fd1982, %fd1803;
	fma.rn.f64 	%fd2004, %fd2002, %fd1982, %fd1805;
	fma.rn.f64 	%fd2006, %fd2004, %fd1982, %fd1807;
	fma.rn.f64 	%fd2008, %fd2006, %fd1982, %fd1809;
	fma.rn.f64 	%fd2010, %fd2008, %fd1982, %fd1811;
	fma.rn.f64 	%fd2011, %fd2010, %fd3410, %fd3410;
	and.b32  	%r463, %r657, 1;
	setp.eq.b32	%p202, %r463, 1;
	not.pred 	%p203, %p202;
	selp.f64	%fd3411, %fd2011, %fd1997, %p203;
	and.b32  	%r464, %r657, 2;
	setp.eq.s32	%p204, %r464, 0;
	@%p204 bra 	BB85_216;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r465}, %fd3411;
	}
	xor.b32  	%r466, %r465, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r467, %temp}, %fd3411;
	}
	mov.b64 	%fd3411, {%r467, %r466};

BB85_216:
	fma.rn.f64 	%fd336, %fd327, %fd3411, %fd310;
	mov.f64 	%fd3454, %fd267;
	@%p175 bra 	BB85_218;

	mul.rn.f64 	%fd3454, %fd267, %fd1811;

BB85_218:
	mov.f64 	%fd3007, 0d397B839A252049C0;
	mov.f64 	%fd3005, 0d3C91A62633145C00;
	mov.f64 	%fd3003, 0d3FF921FB54442D18;
	mul.f64 	%fd2013, %fd3454, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r658, %fd2013;
	st.local.u32 	[%rd1], %r658;
	cvt.rn.f64.s32	%fd2014, %r658;
	neg.f64 	%fd2015, %fd2014;
	fma.rn.f64 	%fd2017, %fd2015, %fd3003, %fd3454;
	fma.rn.f64 	%fd2019, %fd2015, %fd3005, %fd2017;
	fma.rn.f64 	%fd3412, %fd2015, %fd3007, %fd2019;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r468}, %fd3454;
	}
	and.b32  	%r469, %r468, 2145386496;
	setp.lt.u32	%p206, %r469, 1105199104;
	@%p206 bra 	BB85_220;

	add.u64 	%rd170, %SP, 0;
	// Callseq Start 106
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3454;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd170;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3412, [retval0+0];
	
	//{
	}// Callseq End 106
	ld.local.u32 	%r658, [%rd1];

BB85_220:
	mov.f64 	%fd3278, 0d3FA5555555555551;
	mov.f64 	%fd3267, 0dBF56C16C16C15D47;
	mov.f64 	%fd3266, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3252, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3237, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3220, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2021, %fd3412, %fd3412;
	fma.rn.f64 	%fd2024, %fd3220, %fd2021, %fd3237;
	fma.rn.f64 	%fd2026, %fd2024, %fd2021, %fd3252;
	fma.rn.f64 	%fd2028, %fd2026, %fd2021, %fd3266;
	fma.rn.f64 	%fd2030, %fd2028, %fd2021, %fd3267;
	fma.rn.f64 	%fd2032, %fd2030, %fd2021, %fd3278;
	fma.rn.f64 	%fd2034, %fd2032, %fd2021, %fd1796;
	fma.rn.f64 	%fd2036, %fd2034, %fd2021, %fd1798;
	fma.rn.f64 	%fd2039, %fd1801, %fd2021, %fd1800;
	fma.rn.f64 	%fd2041, %fd2039, %fd2021, %fd1803;
	fma.rn.f64 	%fd2043, %fd2041, %fd2021, %fd1805;
	fma.rn.f64 	%fd2045, %fd2043, %fd2021, %fd1807;
	fma.rn.f64 	%fd2047, %fd2045, %fd2021, %fd1809;
	fma.rn.f64 	%fd2049, %fd2047, %fd2021, %fd1811;
	fma.rn.f64 	%fd2050, %fd2049, %fd3412, %fd3412;
	and.b32  	%r470, %r658, 1;
	setp.eq.b32	%p207, %r470, 1;
	not.pred 	%p208, %p207;
	selp.f64	%fd3413, %fd2050, %fd2036, %p208;
	and.b32  	%r471, %r658, 2;
	setp.eq.s32	%p209, %r471, 0;
	@%p209 bra 	BB85_222;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r472}, %fd3413;
	}
	xor.b32  	%r473, %r472, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r474, %temp}, %fd3413;
	}
	mov.b64 	%fd3413, {%r474, %r473};

BB85_222:
	mov.f64 	%fd3471, %fd274;
	@%p180 bra 	BB85_224;

	mul.rn.f64 	%fd3471, %fd274, %fd1811;

BB85_224:
	mov.f64 	%fd3008, 0d397B839A252049C0;
	mov.f64 	%fd3006, 0d3C91A62633145C00;
	mov.f64 	%fd2945, 0d3FF921FB54442D18;
	mul.f64 	%fd2052, %fd3471, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r659, %fd2052;
	st.local.u32 	[%rd1], %r659;
	cvt.rn.f64.s32	%fd2053, %r659;
	neg.f64 	%fd2054, %fd2053;
	fma.rn.f64 	%fd2056, %fd2054, %fd2945, %fd3471;
	fma.rn.f64 	%fd2058, %fd2054, %fd3006, %fd2056;
	fma.rn.f64 	%fd3414, %fd2054, %fd3008, %fd2058;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r475}, %fd3471;
	}
	and.b32  	%r476, %r475, 2145386496;
	setp.lt.u32	%p211, %r476, 1105199104;
	@%p211 bra 	BB85_226;

	add.u64 	%rd169, %SP, 0;
	// Callseq Start 107
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3471;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd169;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3414, [retval0+0];
	
	//{
	}// Callseq End 107
	ld.local.u32 	%r659, [%rd1];

BB85_226:
	mov.f64 	%fd3279, 0d3FA5555555555551;
	mov.f64 	%fd3268, 0dBF56C16C16C15D47;
	mov.f64 	%fd3254, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3253, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3222, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3221, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2060, %fd3414, %fd3414;
	fma.rn.f64 	%fd2063, %fd3221, %fd2060, %fd3222;
	fma.rn.f64 	%fd2065, %fd2063, %fd2060, %fd3253;
	fma.rn.f64 	%fd2067, %fd2065, %fd2060, %fd3254;
	fma.rn.f64 	%fd2069, %fd2067, %fd2060, %fd3268;
	fma.rn.f64 	%fd2071, %fd2069, %fd2060, %fd3279;
	fma.rn.f64 	%fd2073, %fd2071, %fd2060, %fd1796;
	fma.rn.f64 	%fd2075, %fd2073, %fd2060, %fd1798;
	fma.rn.f64 	%fd2078, %fd1801, %fd2060, %fd1800;
	fma.rn.f64 	%fd2080, %fd2078, %fd2060, %fd1803;
	fma.rn.f64 	%fd2082, %fd2080, %fd2060, %fd1805;
	fma.rn.f64 	%fd2084, %fd2082, %fd2060, %fd1807;
	fma.rn.f64 	%fd2086, %fd2084, %fd2060, %fd1809;
	fma.rn.f64 	%fd2088, %fd2086, %fd2060, %fd1811;
	fma.rn.f64 	%fd2089, %fd2088, %fd3414, %fd3414;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r477}, %fd2089;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r478, %temp}, %fd2089;
	}
	xor.b32  	%r479, %r477, -2147483648;
	mov.b64 	%fd2090, {%r478, %r479};
	and.b32  	%r480, %r659, 1;
	setp.eq.b32	%p212, %r480, 1;
	not.pred 	%p213, %p212;
	selp.f64	%fd3415, %fd2075, %fd2090, %p213;
	and.b32  	%r481, %r659, 2;
	setp.eq.s32	%p214, %r481, 0;
	@%p214 bra 	BB85_228;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r482}, %fd3415;
	}
	xor.b32  	%r483, %r482, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r484, %temp}, %fd3415;
	}
	mov.b64 	%fd3415, {%r484, %r483};

BB85_228:
	mul.f64 	%fd353, %fd3413, %fd3415;
	mov.f64 	%fd3488, %fd281;
	@%p185 bra 	BB85_230;

	mul.rn.f64 	%fd3488, %fd281, %fd1811;

BB85_230:
	mov.f64 	%fd3009, 0d397B839A252049C0;
	mov.f64 	%fd2948, 0d3C91A62633145C00;
	mov.f64 	%fd2946, 0d3FF921FB54442D18;
	mul.f64 	%fd2092, %fd3488, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r660, %fd2092;
	st.local.u32 	[%rd1], %r660;
	cvt.rn.f64.s32	%fd2093, %r660;
	neg.f64 	%fd2094, %fd2093;
	fma.rn.f64 	%fd2096, %fd2094, %fd2946, %fd3488;
	fma.rn.f64 	%fd2098, %fd2094, %fd2948, %fd2096;
	fma.rn.f64 	%fd3416, %fd2094, %fd3009, %fd2098;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r485}, %fd3488;
	}
	and.b32  	%r486, %r485, 2145386496;
	setp.lt.u32	%p216, %r486, 1105199104;
	@%p216 bra 	BB85_232;

	add.u64 	%rd168, %SP, 0;
	// Callseq Start 108
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3488;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd168;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3416, [retval0+0];
	
	//{
	}// Callseq End 108
	ld.local.u32 	%r660, [%rd1];

BB85_232:
	mov.f64 	%fd3280, 0dBFE0000000000000;
	mov.f64 	%fd3269, 0d3FA5555555555551;
	mov.f64 	%fd3256, 0dBF56C16C16C15D47;
	mov.f64 	%fd3255, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3238, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3223, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3204, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2100, %fd3416, %fd3416;
	fma.rn.f64 	%fd2103, %fd3204, %fd2100, %fd3223;
	fma.rn.f64 	%fd2105, %fd2103, %fd2100, %fd3238;
	fma.rn.f64 	%fd2107, %fd2105, %fd2100, %fd3255;
	fma.rn.f64 	%fd2109, %fd2107, %fd2100, %fd3256;
	fma.rn.f64 	%fd2111, %fd2109, %fd2100, %fd3269;
	fma.rn.f64 	%fd2113, %fd2111, %fd2100, %fd3280;
	fma.rn.f64 	%fd2115, %fd2113, %fd2100, %fd1798;
	fma.rn.f64 	%fd2118, %fd1801, %fd2100, %fd1800;
	fma.rn.f64 	%fd2120, %fd2118, %fd2100, %fd1803;
	fma.rn.f64 	%fd2122, %fd2120, %fd2100, %fd1805;
	fma.rn.f64 	%fd2124, %fd2122, %fd2100, %fd1807;
	fma.rn.f64 	%fd2126, %fd2124, %fd2100, %fd1809;
	fma.rn.f64 	%fd2128, %fd2126, %fd2100, %fd1811;
	fma.rn.f64 	%fd2129, %fd2128, %fd3416, %fd3416;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r487}, %fd2129;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r488, %temp}, %fd2129;
	}
	xor.b32  	%r489, %r487, -2147483648;
	mov.b64 	%fd2130, {%r488, %r489};
	and.b32  	%r490, %r660, 1;
	setp.eq.b32	%p217, %r490, 1;
	not.pred 	%p218, %p217;
	selp.f64	%fd3417, %fd2115, %fd2130, %p218;
	and.b32  	%r491, %r660, 2;
	setp.eq.s32	%p219, %r491, 0;
	@%p219 bra 	BB85_234;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r492}, %fd3417;
	}
	xor.b32  	%r493, %r492, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r494, %temp}, %fd3417;
	}
	mov.b64 	%fd3417, {%r494, %r493};

BB85_234:
	mul.f64 	%fd362, %fd353, %fd3417;
	mov.f64 	%fd3453, %fd267;
	@%p175 bra 	BB85_236;

	mul.rn.f64 	%fd3453, %fd267, %fd1811;

BB85_236:
	mov.f64 	%fd2950, 0d397B839A252049C0;
	mov.f64 	%fd2949, 0d3C91A62633145C00;
	mov.f64 	%fd2947, 0d3FF921FB54442D18;
	mul.f64 	%fd2132, %fd3453, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r661, %fd2132;
	st.local.u32 	[%rd1], %r661;
	cvt.rn.f64.s32	%fd2133, %r661;
	neg.f64 	%fd2134, %fd2133;
	fma.rn.f64 	%fd2136, %fd2134, %fd2947, %fd3453;
	fma.rn.f64 	%fd2138, %fd2134, %fd2949, %fd2136;
	fma.rn.f64 	%fd3418, %fd2134, %fd2950, %fd2138;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r495}, %fd3453;
	}
	and.b32  	%r496, %r495, 2145386496;
	setp.lt.u32	%p221, %r496, 1105199104;
	@%p221 bra 	BB85_238;

	add.u64 	%rd167, %SP, 0;
	// Callseq Start 109
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3453;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd167;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3418, [retval0+0];
	
	//{
	}// Callseq End 109
	ld.local.u32 	%r661, [%rd1];

BB85_238:
	mov.f64 	%fd3270, 0dBFE0000000000000;
	mov.f64 	%fd3257, 0d3FA5555555555551;
	mov.f64 	%fd3241, 0dBF56C16C16C15D47;
	mov.f64 	%fd3240, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3239, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3206, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3205, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2140, %fd3418, %fd3418;
	fma.rn.f64 	%fd2143, %fd3205, %fd2140, %fd3206;
	fma.rn.f64 	%fd2145, %fd2143, %fd2140, %fd3239;
	fma.rn.f64 	%fd2147, %fd2145, %fd2140, %fd3240;
	fma.rn.f64 	%fd2149, %fd2147, %fd2140, %fd3241;
	fma.rn.f64 	%fd2151, %fd2149, %fd2140, %fd3257;
	fma.rn.f64 	%fd2153, %fd2151, %fd2140, %fd3270;
	fma.rn.f64 	%fd2155, %fd2153, %fd2140, %fd1798;
	fma.rn.f64 	%fd2158, %fd1801, %fd2140, %fd1800;
	fma.rn.f64 	%fd2160, %fd2158, %fd2140, %fd1803;
	fma.rn.f64 	%fd2162, %fd2160, %fd2140, %fd1805;
	fma.rn.f64 	%fd2164, %fd2162, %fd2140, %fd1807;
	fma.rn.f64 	%fd2166, %fd2164, %fd2140, %fd1809;
	fma.rn.f64 	%fd2168, %fd2166, %fd2140, %fd1811;
	fma.rn.f64 	%fd2169, %fd2168, %fd3418, %fd3418;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r497}, %fd2169;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r498, %temp}, %fd2169;
	}
	xor.b32  	%r499, %r497, -2147483648;
	mov.b64 	%fd2170, {%r498, %r499};
	and.b32  	%r500, %r661, 1;
	setp.eq.b32	%p222, %r500, 1;
	not.pred 	%p223, %p222;
	selp.f64	%fd3419, %fd2155, %fd2170, %p223;
	and.b32  	%r501, %r661, 2;
	setp.eq.s32	%p224, %r501, 0;
	@%p224 bra 	BB85_240;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r502}, %fd3419;
	}
	xor.b32  	%r503, %r502, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r504, %temp}, %fd3419;
	}
	mov.b64 	%fd3419, {%r504, %r503};

BB85_240:
	mov.f64 	%fd3470, %fd274;
	@%p180 bra 	BB85_242;

	mul.rn.f64 	%fd3470, %fd274, %fd1811;

BB85_242:
	mov.f64 	%fd2951, 0d397B839A252049C0;
	mov.f64 	%fd2916, 0d3C91A62633145C00;
	mov.f64 	%fd2915, 0d3FF921FB54442D18;
	mul.f64 	%fd2172, %fd3470, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r662, %fd2172;
	st.local.u32 	[%rd1], %r662;
	cvt.rn.f64.s32	%fd2173, %r662;
	neg.f64 	%fd2174, %fd2173;
	fma.rn.f64 	%fd2176, %fd2174, %fd2915, %fd3470;
	fma.rn.f64 	%fd2178, %fd2174, %fd2916, %fd2176;
	fma.rn.f64 	%fd3420, %fd2174, %fd2951, %fd2178;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r505}, %fd3470;
	}
	and.b32  	%r506, %r505, 2145386496;
	setp.lt.u32	%p226, %r506, 1105199104;
	@%p226 bra 	BB85_244;

	add.u64 	%rd166, %SP, 0;
	// Callseq Start 110
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3470;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd166;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3420, [retval0+0];
	
	//{
	}// Callseq End 110
	ld.local.u32 	%r662, [%rd1];

BB85_244:
	mov.f64 	%fd3258, 0dBFE0000000000000;
	mov.f64 	%fd3243, 0d3FA5555555555551;
	mov.f64 	%fd3242, 0dBF56C16C16C15D47;
	mov.f64 	%fd3225, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3224, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3207, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3181, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2180, %fd3420, %fd3420;
	fma.rn.f64 	%fd2183, %fd3181, %fd2180, %fd3207;
	fma.rn.f64 	%fd2185, %fd2183, %fd2180, %fd3224;
	fma.rn.f64 	%fd2187, %fd2185, %fd2180, %fd3225;
	fma.rn.f64 	%fd2189, %fd2187, %fd2180, %fd3242;
	fma.rn.f64 	%fd2191, %fd2189, %fd2180, %fd3243;
	fma.rn.f64 	%fd2193, %fd2191, %fd2180, %fd3258;
	fma.rn.f64 	%fd2195, %fd2193, %fd2180, %fd1798;
	fma.rn.f64 	%fd2198, %fd1801, %fd2180, %fd1800;
	fma.rn.f64 	%fd2200, %fd2198, %fd2180, %fd1803;
	fma.rn.f64 	%fd2202, %fd2200, %fd2180, %fd1805;
	fma.rn.f64 	%fd2204, %fd2202, %fd2180, %fd1807;
	fma.rn.f64 	%fd2206, %fd2204, %fd2180, %fd1809;
	fma.rn.f64 	%fd2208, %fd2206, %fd2180, %fd1811;
	fma.rn.f64 	%fd2209, %fd2208, %fd3420, %fd3420;
	and.b32  	%r507, %r662, 1;
	setp.eq.b32	%p227, %r507, 1;
	not.pred 	%p228, %p227;
	selp.f64	%fd3421, %fd2209, %fd2195, %p228;
	and.b32  	%r508, %r662, 2;
	setp.eq.s32	%p229, %r508, 0;
	@%p229 bra 	BB85_246;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r509}, %fd3421;
	}
	xor.b32  	%r510, %r509, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r511, %temp}, %fd3421;
	}
	mov.b64 	%fd3421, {%r511, %r510};

BB85_246:
	mul.f64 	%fd379, %fd3419, %fd3421;
	mov.f64 	%fd3487, %fd281;
	@%p185 bra 	BB85_248;

	mul.rn.f64 	%fd3487, %fd281, %fd1811;

BB85_248:
	mov.f64 	%fd2920, 0d397B839A252049C0;
	mov.f64 	%fd2918, 0d3C91A62633145C00;
	mov.f64 	%fd2917, 0d3FF921FB54442D18;
	mul.f64 	%fd2211, %fd3487, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r663, %fd2211;
	st.local.u32 	[%rd1], %r663;
	cvt.rn.f64.s32	%fd2212, %r663;
	neg.f64 	%fd2213, %fd2212;
	fma.rn.f64 	%fd2215, %fd2213, %fd2917, %fd3487;
	fma.rn.f64 	%fd2217, %fd2213, %fd2918, %fd2215;
	fma.rn.f64 	%fd3422, %fd2213, %fd2920, %fd2217;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r512}, %fd3487;
	}
	and.b32  	%r513, %r512, 2145386496;
	setp.lt.u32	%p231, %r513, 1105199104;
	@%p231 bra 	BB85_250;

	add.u64 	%rd165, %SP, 0;
	// Callseq Start 111
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3487;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd165;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3422, [retval0+0];
	
	//{
	}// Callseq End 111
	ld.local.u32 	%r663, [%rd1];

BB85_250:
	mov.f64 	%fd3244, 0dBFE0000000000000;
	mov.f64 	%fd3228, 0d3FA5555555555551;
	mov.f64 	%fd3227, 0dBF56C16C16C15D47;
	mov.f64 	%fd3226, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3184, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3183, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3182, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2219, %fd3422, %fd3422;
	fma.rn.f64 	%fd2222, %fd3182, %fd2219, %fd3183;
	fma.rn.f64 	%fd2224, %fd2222, %fd2219, %fd3184;
	fma.rn.f64 	%fd2226, %fd2224, %fd2219, %fd3226;
	fma.rn.f64 	%fd2228, %fd2226, %fd2219, %fd3227;
	fma.rn.f64 	%fd2230, %fd2228, %fd2219, %fd3228;
	fma.rn.f64 	%fd2232, %fd2230, %fd2219, %fd3244;
	fma.rn.f64 	%fd2234, %fd2232, %fd2219, %fd1798;
	fma.rn.f64 	%fd2237, %fd1801, %fd2219, %fd1800;
	fma.rn.f64 	%fd2239, %fd2237, %fd2219, %fd1803;
	fma.rn.f64 	%fd2241, %fd2239, %fd2219, %fd1805;
	fma.rn.f64 	%fd2243, %fd2241, %fd2219, %fd1807;
	fma.rn.f64 	%fd2245, %fd2243, %fd2219, %fd1809;
	fma.rn.f64 	%fd2247, %fd2245, %fd2219, %fd1811;
	fma.rn.f64 	%fd2248, %fd2247, %fd3422, %fd3422;
	and.b32  	%r514, %r663, 1;
	setp.eq.b32	%p232, %r514, 1;
	not.pred 	%p233, %p232;
	selp.f64	%fd3423, %fd2248, %fd2234, %p233;
	and.b32  	%r515, %r663, 2;
	setp.eq.s32	%p234, %r515, 0;
	@%p234 bra 	BB85_252;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r516}, %fd3423;
	}
	xor.b32  	%r517, %r516, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r518, %temp}, %fd3423;
	}
	mov.b64 	%fd3423, {%r518, %r517};

BB85_252:
	mul.f64 	%fd2249, %fd379, %fd3423;
	sub.f64 	%fd388, %fd362, %fd2249;
	mov.f64 	%fd3452, %fd267;
	@%p175 bra 	BB85_254;

	mul.rn.f64 	%fd3452, %fd267, %fd1811;

BB85_254:
	mov.f64 	%fd2921, 0d397B839A252049C0;
	mov.f64 	%fd2919, 0d3C91A62633145C00;
	mov.f64 	%fd2878, 0d3FF921FB54442D18;
	mul.f64 	%fd2251, %fd3452, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r664, %fd2251;
	st.local.u32 	[%rd1], %r664;
	cvt.rn.f64.s32	%fd2252, %r664;
	neg.f64 	%fd2253, %fd2252;
	fma.rn.f64 	%fd2255, %fd2253, %fd2878, %fd3452;
	fma.rn.f64 	%fd2257, %fd2253, %fd2919, %fd2255;
	fma.rn.f64 	%fd3424, %fd2253, %fd2921, %fd2257;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r519}, %fd3452;
	}
	and.b32  	%r520, %r519, 2145386496;
	setp.lt.u32	%p236, %r520, 1105199104;
	@%p236 bra 	BB85_256;

	add.u64 	%rd164, %SP, 0;
	// Callseq Start 112
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3452;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd164;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3424, [retval0+0];
	
	//{
	}// Callseq End 112
	ld.local.u32 	%r664, [%rd1];

BB85_256:
	mov.f64 	%fd3229, 0dBFE0000000000000;
	mov.f64 	%fd3200, 0d3FA5555555555551;
	mov.f64 	%fd3199, 0dBF56C16C16C15D47;
	mov.f64 	%fd3198, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3185, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3168, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3167, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2259, %fd3424, %fd3424;
	fma.rn.f64 	%fd2262, %fd3167, %fd2259, %fd3168;
	fma.rn.f64 	%fd2264, %fd2262, %fd2259, %fd3185;
	fma.rn.f64 	%fd2266, %fd2264, %fd2259, %fd3198;
	fma.rn.f64 	%fd2268, %fd2266, %fd2259, %fd3199;
	fma.rn.f64 	%fd2270, %fd2268, %fd2259, %fd3200;
	fma.rn.f64 	%fd2272, %fd2270, %fd2259, %fd3229;
	fma.rn.f64 	%fd2274, %fd2272, %fd2259, %fd1798;
	fma.rn.f64 	%fd2277, %fd1801, %fd2259, %fd1800;
	fma.rn.f64 	%fd2279, %fd2277, %fd2259, %fd1803;
	fma.rn.f64 	%fd2281, %fd2279, %fd2259, %fd1805;
	fma.rn.f64 	%fd2283, %fd2281, %fd2259, %fd1807;
	fma.rn.f64 	%fd2285, %fd2283, %fd2259, %fd1809;
	fma.rn.f64 	%fd2287, %fd2285, %fd2259, %fd1811;
	fma.rn.f64 	%fd2288, %fd2287, %fd3424, %fd3424;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r521}, %fd2288;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r522, %temp}, %fd2288;
	}
	xor.b32  	%r523, %r521, -2147483648;
	mov.b64 	%fd2289, {%r522, %r523};
	and.b32  	%r524, %r664, 1;
	setp.eq.b32	%p237, %r524, 1;
	not.pred 	%p238, %p237;
	selp.f64	%fd3425, %fd2274, %fd2289, %p238;
	and.b32  	%r525, %r664, 2;
	setp.eq.s32	%p239, %r525, 0;
	@%p239 bra 	BB85_258;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r526}, %fd3425;
	}
	xor.b32  	%r527, %r526, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r528, %temp}, %fd3425;
	}
	mov.b64 	%fd3425, {%r528, %r527};

BB85_258:
	mov.f64 	%fd3469, %fd274;
	@%p180 bra 	BB85_260;

	mul.rn.f64 	%fd3469, %fd274, %fd1811;

BB85_260:
	mov.f64 	%fd2882, 0d397B839A252049C0;
	mov.f64 	%fd2881, 0d3C91A62633145C00;
	mov.f64 	%fd2879, 0d3FF921FB54442D18;
	mul.f64 	%fd2291, %fd3469, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r665, %fd2291;
	st.local.u32 	[%rd1], %r665;
	cvt.rn.f64.s32	%fd2292, %r665;
	neg.f64 	%fd2293, %fd2292;
	fma.rn.f64 	%fd2295, %fd2293, %fd2879, %fd3469;
	fma.rn.f64 	%fd2297, %fd2293, %fd2881, %fd2295;
	fma.rn.f64 	%fd3426, %fd2293, %fd2882, %fd2297;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r529}, %fd3469;
	}
	and.b32  	%r530, %r529, 2145386496;
	setp.lt.u32	%p241, %r530, 1105199104;
	@%p241 bra 	BB85_262;

	add.u64 	%rd163, %SP, 0;
	// Callseq Start 113
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3469;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd163;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3426, [retval0+0];
	
	//{
	}// Callseq End 113
	ld.local.u32 	%r665, [%rd1];

BB85_262:
	mov.f64 	%fd3203, 0d3FF0000000000000;
	mov.f64 	%fd3202, 0dBFE0000000000000;
	mov.f64 	%fd3201, 0d3FA5555555555551;
	mov.f64 	%fd3173, 0dBF56C16C16C15D47;
	mov.f64 	%fd3172, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3171, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3170, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3169, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2299, %fd3426, %fd3426;
	fma.rn.f64 	%fd2302, %fd3169, %fd2299, %fd3170;
	fma.rn.f64 	%fd2304, %fd2302, %fd2299, %fd3171;
	fma.rn.f64 	%fd2306, %fd2304, %fd2299, %fd3172;
	fma.rn.f64 	%fd2308, %fd2306, %fd2299, %fd3173;
	fma.rn.f64 	%fd2310, %fd2308, %fd2299, %fd3201;
	fma.rn.f64 	%fd2312, %fd2310, %fd2299, %fd3202;
	fma.rn.f64 	%fd2314, %fd2312, %fd2299, %fd3203;
	fma.rn.f64 	%fd2317, %fd1801, %fd2299, %fd1800;
	fma.rn.f64 	%fd2319, %fd2317, %fd2299, %fd1803;
	fma.rn.f64 	%fd2321, %fd2319, %fd2299, %fd1805;
	fma.rn.f64 	%fd2323, %fd2321, %fd2299, %fd1807;
	fma.rn.f64 	%fd2325, %fd2323, %fd2299, %fd1809;
	fma.rn.f64 	%fd2327, %fd2325, %fd2299, %fd1811;
	fma.rn.f64 	%fd2328, %fd2327, %fd3426, %fd3426;
	and.b32  	%r531, %r665, 1;
	setp.eq.b32	%p242, %r531, 1;
	not.pred 	%p243, %p242;
	selp.f64	%fd3427, %fd2328, %fd2314, %p243;
	and.b32  	%r532, %r665, 2;
	setp.eq.s32	%p244, %r532, 0;
	@%p244 bra 	BB85_264;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r533}, %fd3427;
	}
	xor.b32  	%r534, %r533, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r535, %temp}, %fd3427;
	}
	mov.b64 	%fd3427, {%r535, %r534};

BB85_264:
	mul.f64 	%fd405, %fd3425, %fd3427;
	mov.f64 	%fd3486, %fd281;
	@%p185 bra 	BB85_266;

	mul.rn.f64 	%fd3486, %fd281, %fd1811;

BB85_266:
	mov.f64 	%fd2884, 0d397B839A252049C0;
	mov.f64 	%fd2883, 0d3C91A62633145C00;
	mov.f64 	%fd2880, 0d3FF921FB54442D18;
	mul.f64 	%fd2330, %fd3486, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r666, %fd2330;
	st.local.u32 	[%rd1], %r666;
	cvt.rn.f64.s32	%fd2331, %r666;
	neg.f64 	%fd2332, %fd2331;
	fma.rn.f64 	%fd2334, %fd2332, %fd2880, %fd3486;
	fma.rn.f64 	%fd2336, %fd2332, %fd2883, %fd2334;
	fma.rn.f64 	%fd3428, %fd2332, %fd2884, %fd2336;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r536}, %fd3486;
	}
	and.b32  	%r537, %r536, 2145386496;
	setp.lt.u32	%p246, %r537, 1105199104;
	@%p246 bra 	BB85_268;

	add.u64 	%rd162, %SP, 0;
	// Callseq Start 114
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3486;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd162;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3428, [retval0+0];
	
	//{
	}// Callseq End 114
	ld.local.u32 	%r666, [%rd1];

BB85_268:
	mov.f64 	%fd3178, 0d3FF0000000000000;
	mov.f64 	%fd3177, 0dBFE0000000000000;
	mov.f64 	%fd3176, 0d3FA5555555555551;
	mov.f64 	%fd3175, 0dBF56C16C16C15D47;
	mov.f64 	%fd3174, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3133, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3132, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3131, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2338, %fd3428, %fd3428;
	fma.rn.f64 	%fd2341, %fd3131, %fd2338, %fd3132;
	fma.rn.f64 	%fd2343, %fd2341, %fd2338, %fd3133;
	fma.rn.f64 	%fd2345, %fd2343, %fd2338, %fd3174;
	fma.rn.f64 	%fd2347, %fd2345, %fd2338, %fd3175;
	fma.rn.f64 	%fd2349, %fd2347, %fd2338, %fd3176;
	fma.rn.f64 	%fd2351, %fd2349, %fd2338, %fd3177;
	fma.rn.f64 	%fd2353, %fd2351, %fd2338, %fd3178;
	fma.rn.f64 	%fd2356, %fd1801, %fd2338, %fd1800;
	fma.rn.f64 	%fd2358, %fd2356, %fd2338, %fd1803;
	fma.rn.f64 	%fd2360, %fd2358, %fd2338, %fd1805;
	fma.rn.f64 	%fd2362, %fd2360, %fd2338, %fd1807;
	fma.rn.f64 	%fd2364, %fd2362, %fd2338, %fd1809;
	fma.rn.f64 	%fd2366, %fd2364, %fd2338, %fd1811;
	fma.rn.f64 	%fd2367, %fd2366, %fd3428, %fd3428;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r538}, %fd2367;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r539, %temp}, %fd2367;
	}
	xor.b32  	%r540, %r538, -2147483648;
	mov.b64 	%fd2368, {%r539, %r540};
	and.b32  	%r541, %r666, 1;
	setp.eq.b32	%p247, %r541, 1;
	not.pred 	%p248, %p247;
	selp.f64	%fd3429, %fd2353, %fd2368, %p248;
	and.b32  	%r542, %r666, 2;
	setp.eq.s32	%p249, %r542, 0;
	@%p249 bra 	BB85_270;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r543}, %fd3429;
	}
	xor.b32  	%r544, %r543, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r545, %temp}, %fd3429;
	}
	mov.b64 	%fd3429, {%r545, %r544};

BB85_270:
	mul.f64 	%fd414, %fd405, %fd3429;
	mov.f64 	%fd3451, %fd267;
	@%p175 bra 	BB85_272;

	mul.rn.f64 	%fd3451, %fd267, %fd1811;

BB85_272:
	mov.f64 	%fd2841, 0d397B839A252049C0;
	mov.f64 	%fd2840, 0d3C91A62633145C00;
	mov.f64 	%fd2839, 0d3FF921FB54442D18;
	mul.f64 	%fd2370, %fd3451, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r667, %fd2370;
	st.local.u32 	[%rd1], %r667;
	cvt.rn.f64.s32	%fd2371, %r667;
	neg.f64 	%fd2372, %fd2371;
	fma.rn.f64 	%fd2374, %fd2372, %fd2839, %fd3451;
	fma.rn.f64 	%fd2376, %fd2372, %fd2840, %fd2374;
	fma.rn.f64 	%fd3430, %fd2372, %fd2841, %fd2376;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r546}, %fd3451;
	}
	and.b32  	%r547, %r546, 2145386496;
	setp.lt.u32	%p251, %r547, 1105199104;
	@%p251 bra 	BB85_274;

	add.u64 	%rd161, %SP, 0;
	// Callseq Start 115
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3451;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd161;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3430, [retval0+0];
	
	//{
	}// Callseq End 115
	ld.local.u32 	%r667, [%rd1];

BB85_274:
	mov.f64 	%fd3179, 0d3FF0000000000000;
	mov.f64 	%fd3140, 0dBFE0000000000000;
	mov.f64 	%fd3139, 0d3FA5555555555551;
	mov.f64 	%fd3138, 0dBF56C16C16C15D47;
	mov.f64 	%fd3137, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3136, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3135, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3134, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2378, %fd3430, %fd3430;
	fma.rn.f64 	%fd2381, %fd3134, %fd2378, %fd3135;
	fma.rn.f64 	%fd2383, %fd2381, %fd2378, %fd3136;
	fma.rn.f64 	%fd2385, %fd2383, %fd2378, %fd3137;
	fma.rn.f64 	%fd2387, %fd2385, %fd2378, %fd3138;
	fma.rn.f64 	%fd2389, %fd2387, %fd2378, %fd3139;
	fma.rn.f64 	%fd2391, %fd2389, %fd2378, %fd3140;
	fma.rn.f64 	%fd2393, %fd2391, %fd2378, %fd3179;
	fma.rn.f64 	%fd2396, %fd1801, %fd2378, %fd1800;
	fma.rn.f64 	%fd2398, %fd2396, %fd2378, %fd1803;
	fma.rn.f64 	%fd2400, %fd2398, %fd2378, %fd1805;
	fma.rn.f64 	%fd2402, %fd2400, %fd2378, %fd1807;
	fma.rn.f64 	%fd2404, %fd2402, %fd2378, %fd1809;
	fma.rn.f64 	%fd2406, %fd2404, %fd2378, %fd1811;
	fma.rn.f64 	%fd2407, %fd2406, %fd3430, %fd3430;
	and.b32  	%r548, %r667, 1;
	setp.eq.b32	%p252, %r548, 1;
	not.pred 	%p253, %p252;
	selp.f64	%fd3431, %fd2407, %fd2393, %p253;
	and.b32  	%r549, %r667, 2;
	setp.eq.s32	%p254, %r549, 0;
	@%p254 bra 	BB85_276;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r550}, %fd3431;
	}
	xor.b32  	%r551, %r550, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r552, %temp}, %fd3431;
	}
	mov.b64 	%fd3431, {%r552, %r551};

BB85_276:
	mov.f64 	%fd3468, %fd274;
	@%p180 bra 	BB85_278;

	mul.rn.f64 	%fd3468, %fd274, %fd1811;

BB85_278:
	mov.f64 	%fd2844, 0d397B839A252049C0;
	mov.f64 	%fd2843, 0d3C91A62633145C00;
	mov.f64 	%fd2842, 0d3FF921FB54442D18;
	mul.f64 	%fd2409, %fd3468, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r668, %fd2409;
	st.local.u32 	[%rd1], %r668;
	cvt.rn.f64.s32	%fd2410, %r668;
	neg.f64 	%fd2411, %fd2410;
	fma.rn.f64 	%fd2413, %fd2411, %fd2842, %fd3468;
	fma.rn.f64 	%fd2415, %fd2411, %fd2843, %fd2413;
	fma.rn.f64 	%fd3432, %fd2411, %fd2844, %fd2415;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r553}, %fd3468;
	}
	and.b32  	%r554, %r553, 2145386496;
	setp.lt.u32	%p256, %r554, 1105199104;
	@%p256 bra 	BB85_280;

	add.u64 	%rd160, %SP, 0;
	// Callseq Start 116
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3468;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd160;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3432, [retval0+0];
	
	//{
	}// Callseq End 116
	ld.local.u32 	%r668, [%rd1];

BB85_280:
	mov.f64 	%fd3144, 0d3FF0000000000000;
	mov.f64 	%fd3143, 0dBFE0000000000000;
	mov.f64 	%fd3142, 0d3FA5555555555551;
	mov.f64 	%fd3141, 0dBF56C16C16C15D47;
	mov.f64 	%fd3099, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3098, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3097, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3096, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2417, %fd3432, %fd3432;
	fma.rn.f64 	%fd2420, %fd3096, %fd2417, %fd3097;
	fma.rn.f64 	%fd2422, %fd2420, %fd2417, %fd3098;
	fma.rn.f64 	%fd2424, %fd2422, %fd2417, %fd3099;
	fma.rn.f64 	%fd2426, %fd2424, %fd2417, %fd3141;
	fma.rn.f64 	%fd2428, %fd2426, %fd2417, %fd3142;
	fma.rn.f64 	%fd2430, %fd2428, %fd2417, %fd3143;
	fma.rn.f64 	%fd2432, %fd2430, %fd2417, %fd3144;
	fma.rn.f64 	%fd2435, %fd1801, %fd2417, %fd1800;
	fma.rn.f64 	%fd2437, %fd2435, %fd2417, %fd1803;
	fma.rn.f64 	%fd2439, %fd2437, %fd2417, %fd1805;
	fma.rn.f64 	%fd2441, %fd2439, %fd2417, %fd1807;
	fma.rn.f64 	%fd2443, %fd2441, %fd2417, %fd1809;
	fma.rn.f64 	%fd2445, %fd2443, %fd2417, %fd1811;
	fma.rn.f64 	%fd2446, %fd2445, %fd3432, %fd3432;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r555}, %fd2446;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r556, %temp}, %fd2446;
	}
	xor.b32  	%r557, %r555, -2147483648;
	mov.b64 	%fd2447, {%r556, %r557};
	and.b32  	%r558, %r668, 1;
	setp.eq.b32	%p257, %r558, 1;
	not.pred 	%p258, %p257;
	selp.f64	%fd3433, %fd2432, %fd2447, %p258;
	and.b32  	%r559, %r668, 2;
	setp.eq.s32	%p259, %r559, 0;
	@%p259 bra 	BB85_282;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r560}, %fd3433;
	}
	xor.b32  	%r561, %r560, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r562, %temp}, %fd3433;
	}
	mov.b64 	%fd3433, {%r562, %r561};

BB85_282:
	mul.f64 	%fd431, %fd3431, %fd3433;
	mov.f64 	%fd3485, %fd281;
	@%p185 bra 	BB85_284;

	mul.rn.f64 	%fd3485, %fd281, %fd1811;

BB85_284:
	mov.f64 	%fd2847, 0d397B839A252049C0;
	mov.f64 	%fd2846, 0d3C91A62633145C00;
	mov.f64 	%fd2845, 0d3FF921FB54442D18;
	mul.f64 	%fd2449, %fd3485, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r669, %fd2449;
	st.local.u32 	[%rd1], %r669;
	cvt.rn.f64.s32	%fd2450, %r669;
	neg.f64 	%fd2451, %fd2450;
	fma.rn.f64 	%fd2453, %fd2451, %fd2845, %fd3485;
	fma.rn.f64 	%fd2455, %fd2451, %fd2846, %fd2453;
	fma.rn.f64 	%fd3434, %fd2451, %fd2847, %fd2455;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r563}, %fd3485;
	}
	and.b32  	%r564, %r563, 2145386496;
	setp.lt.u32	%p261, %r564, 1105199104;
	@%p261 bra 	BB85_286;

	add.u64 	%rd159, %SP, 0;
	// Callseq Start 117
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3485;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd159;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3434, [retval0+0];
	
	//{
	}// Callseq End 117
	ld.local.u32 	%r669, [%rd1];

BB85_286:
	mov.f64 	%fd3107, 0d3FF0000000000000;
	mov.f64 	%fd3106, 0dBFE0000000000000;
	mov.f64 	%fd3105, 0d3FA5555555555551;
	mov.f64 	%fd3104, 0dBF56C16C16C15D47;
	mov.f64 	%fd3103, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3102, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3101, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3100, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2457, %fd3434, %fd3434;
	fma.rn.f64 	%fd2460, %fd3100, %fd2457, %fd3101;
	fma.rn.f64 	%fd2462, %fd2460, %fd2457, %fd3102;
	fma.rn.f64 	%fd2464, %fd2462, %fd2457, %fd3103;
	fma.rn.f64 	%fd2466, %fd2464, %fd2457, %fd3104;
	fma.rn.f64 	%fd2468, %fd2466, %fd2457, %fd3105;
	fma.rn.f64 	%fd2470, %fd2468, %fd2457, %fd3106;
	fma.rn.f64 	%fd2472, %fd2470, %fd2457, %fd3107;
	fma.rn.f64 	%fd2475, %fd1801, %fd2457, %fd1800;
	fma.rn.f64 	%fd2477, %fd2475, %fd2457, %fd1803;
	fma.rn.f64 	%fd2479, %fd2477, %fd2457, %fd1805;
	fma.rn.f64 	%fd2481, %fd2479, %fd2457, %fd1807;
	fma.rn.f64 	%fd2483, %fd2481, %fd2457, %fd1809;
	fma.rn.f64 	%fd2485, %fd2483, %fd2457, %fd1811;
	fma.rn.f64 	%fd2486, %fd2485, %fd3434, %fd3434;
	and.b32  	%r565, %r669, 1;
	setp.eq.b32	%p262, %r565, 1;
	not.pred 	%p263, %p262;
	selp.f64	%fd3435, %fd2486, %fd2472, %p263;
	and.b32  	%r566, %r669, 2;
	setp.eq.s32	%p264, %r566, 0;
	@%p264 bra 	BB85_288;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r567}, %fd3435;
	}
	xor.b32  	%r568, %r567, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r569, %temp}, %fd3435;
	}
	mov.b64 	%fd3435, {%r569, %r568};

BB85_288:
	mul.f64 	%fd2487, %fd431, %fd3435;
	sub.f64 	%fd440, %fd414, %fd2487;
	mov.f64 	%fd3450, %fd267;
	@%p175 bra 	BB85_290;

	mul.rn.f64 	%fd3450, %fd267, %fd1811;

BB85_290:
	mov.f64 	%fd2805, 0d397B839A252049C0;
	mov.f64 	%fd2804, 0d3C91A62633145C00;
	mov.f64 	%fd2803, 0d3FF921FB54442D18;
	mul.f64 	%fd2489, %fd3450, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r670, %fd2489;
	st.local.u32 	[%rd1], %r670;
	cvt.rn.f64.s32	%fd2490, %r670;
	neg.f64 	%fd2491, %fd2490;
	fma.rn.f64 	%fd2493, %fd2491, %fd2803, %fd3450;
	fma.rn.f64 	%fd2495, %fd2491, %fd2804, %fd2493;
	fma.rn.f64 	%fd3436, %fd2491, %fd2805, %fd2495;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r570}, %fd3450;
	}
	and.b32  	%r571, %r570, 2145386496;
	setp.lt.u32	%p266, %r571, 1105199104;
	@%p266 bra 	BB85_292;

	add.u64 	%rd158, %SP, 0;
	// Callseq Start 118
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3450;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd158;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3436, [retval0+0];
	
	//{
	}// Callseq End 118
	ld.local.u32 	%r670, [%rd1];

BB85_292:
	mov.f64 	%fd3111, 0d3FF0000000000000;
	mov.f64 	%fd3110, 0dBFE0000000000000;
	mov.f64 	%fd3109, 0d3FA5555555555551;
	mov.f64 	%fd3108, 0dBF56C16C16C15D47;
	mov.f64 	%fd3057, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3056, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3055, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3054, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2497, %fd3436, %fd3436;
	fma.rn.f64 	%fd2500, %fd3054, %fd2497, %fd3055;
	fma.rn.f64 	%fd2502, %fd2500, %fd2497, %fd3056;
	fma.rn.f64 	%fd2504, %fd2502, %fd2497, %fd3057;
	fma.rn.f64 	%fd2506, %fd2504, %fd2497, %fd3108;
	fma.rn.f64 	%fd2508, %fd2506, %fd2497, %fd3109;
	fma.rn.f64 	%fd2510, %fd2508, %fd2497, %fd3110;
	fma.rn.f64 	%fd2512, %fd2510, %fd2497, %fd3111;
	fma.rn.f64 	%fd2515, %fd1801, %fd2497, %fd1800;
	fma.rn.f64 	%fd2517, %fd2515, %fd2497, %fd1803;
	fma.rn.f64 	%fd2519, %fd2517, %fd2497, %fd1805;
	fma.rn.f64 	%fd2521, %fd2519, %fd2497, %fd1807;
	fma.rn.f64 	%fd2523, %fd2521, %fd2497, %fd1809;
	fma.rn.f64 	%fd2525, %fd2523, %fd2497, %fd1811;
	fma.rn.f64 	%fd2526, %fd2525, %fd3436, %fd3436;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r572}, %fd2526;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r573, %temp}, %fd2526;
	}
	xor.b32  	%r574, %r572, -2147483648;
	mov.b64 	%fd2527, {%r573, %r574};
	and.b32  	%r575, %r670, 1;
	setp.eq.b32	%p267, %r575, 1;
	not.pred 	%p268, %p267;
	selp.f64	%fd3437, %fd2512, %fd2527, %p268;
	and.b32  	%r576, %r670, 2;
	setp.eq.s32	%p269, %r576, 0;
	@%p269 bra 	BB85_294;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r577}, %fd3437;
	}
	xor.b32  	%r578, %r577, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r579, %temp}, %fd3437;
	}
	mov.b64 	%fd3437, {%r579, %r578};

BB85_294:
	mov.f64 	%fd3467, %fd274;
	@%p180 bra 	BB85_296;

	mul.rn.f64 	%fd3467, %fd274, %fd1811;

BB85_296:
	mov.f64 	%fd2808, 0d397B839A252049C0;
	mov.f64 	%fd2807, 0d3C91A62633145C00;
	mov.f64 	%fd2806, 0d3FF921FB54442D18;
	mul.f64 	%fd2529, %fd3467, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r671, %fd2529;
	st.local.u32 	[%rd1], %r671;
	cvt.rn.f64.s32	%fd2530, %r671;
	neg.f64 	%fd2531, %fd2530;
	fma.rn.f64 	%fd2533, %fd2531, %fd2806, %fd3467;
	fma.rn.f64 	%fd2535, %fd2531, %fd2807, %fd2533;
	fma.rn.f64 	%fd3438, %fd2531, %fd2808, %fd2535;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r580}, %fd3467;
	}
	and.b32  	%r581, %r580, 2145386496;
	setp.lt.u32	%p271, %r581, 1105199104;
	@%p271 bra 	BB85_298;

	add.u64 	%rd157, %SP, 0;
	// Callseq Start 119
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3467;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd157;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3438, [retval0+0];
	
	//{
	}// Callseq End 119
	ld.local.u32 	%r671, [%rd1];

BB85_298:
	mov.f64 	%fd3065, 0d3FF0000000000000;
	mov.f64 	%fd3064, 0dBFE0000000000000;
	mov.f64 	%fd3063, 0d3FA5555555555551;
	mov.f64 	%fd3062, 0dBF56C16C16C15D47;
	mov.f64 	%fd3061, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3060, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3059, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3058, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2537, %fd3438, %fd3438;
	fma.rn.f64 	%fd2540, %fd3058, %fd2537, %fd3059;
	fma.rn.f64 	%fd2542, %fd2540, %fd2537, %fd3060;
	fma.rn.f64 	%fd2544, %fd2542, %fd2537, %fd3061;
	fma.rn.f64 	%fd2546, %fd2544, %fd2537, %fd3062;
	fma.rn.f64 	%fd2548, %fd2546, %fd2537, %fd3063;
	fma.rn.f64 	%fd2550, %fd2548, %fd2537, %fd3064;
	fma.rn.f64 	%fd2552, %fd2550, %fd2537, %fd3065;
	fma.rn.f64 	%fd2555, %fd1801, %fd2537, %fd1800;
	fma.rn.f64 	%fd2557, %fd2555, %fd2537, %fd1803;
	fma.rn.f64 	%fd2559, %fd2557, %fd2537, %fd1805;
	fma.rn.f64 	%fd2561, %fd2559, %fd2537, %fd1807;
	fma.rn.f64 	%fd2563, %fd2561, %fd2537, %fd1809;
	fma.rn.f64 	%fd2565, %fd2563, %fd2537, %fd1811;
	fma.rn.f64 	%fd2566, %fd2565, %fd3438, %fd3438;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r582}, %fd2566;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r583, %temp}, %fd2566;
	}
	xor.b32  	%r584, %r582, -2147483648;
	mov.b64 	%fd2567, {%r583, %r584};
	and.b32  	%r585, %r671, 1;
	setp.eq.b32	%p272, %r585, 1;
	not.pred 	%p273, %p272;
	selp.f64	%fd3439, %fd2552, %fd2567, %p273;
	and.b32  	%r586, %r671, 2;
	setp.eq.s32	%p274, %r586, 0;
	@%p274 bra 	BB85_300;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r587}, %fd3439;
	}
	xor.b32  	%r588, %r587, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r589, %temp}, %fd3439;
	}
	mov.b64 	%fd3439, {%r589, %r588};

BB85_300:
	mul.f64 	%fd457, %fd3437, %fd3439;
	mov.f64 	%fd3484, %fd281;
	@%p185 bra 	BB85_302;

	mul.rn.f64 	%fd3484, %fd281, %fd1811;

BB85_302:
	mov.f64 	%fd2811, 0d397B839A252049C0;
	mov.f64 	%fd2810, 0d3C91A62633145C00;
	mov.f64 	%fd2809, 0d3FF921FB54442D18;
	mul.f64 	%fd2569, %fd3484, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r672, %fd2569;
	st.local.u32 	[%rd1], %r672;
	cvt.rn.f64.s32	%fd2570, %r672;
	neg.f64 	%fd2571, %fd2570;
	fma.rn.f64 	%fd2573, %fd2571, %fd2809, %fd3484;
	fma.rn.f64 	%fd2575, %fd2571, %fd2810, %fd2573;
	fma.rn.f64 	%fd3440, %fd2571, %fd2811, %fd2575;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r590}, %fd3484;
	}
	and.b32  	%r591, %r590, 2145386496;
	setp.lt.u32	%p276, %r591, 1105199104;
	@%p276 bra 	BB85_304;

	add.u64 	%rd156, %SP, 0;
	// Callseq Start 120
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3484;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd156;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3440, [retval0+0];
	
	//{
	}// Callseq End 120
	ld.local.u32 	%r672, [%rd1];

BB85_304:
	mov.f64 	%fd3180, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3068, 0d3FF0000000000000;
	mov.f64 	%fd3067, 0dBFE0000000000000;
	mov.f64 	%fd3066, 0d3FA5555555555551;
	mov.f64 	%fd3014, 0dBF56C16C16C15D47;
	mov.f64 	%fd3013, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3012, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3011, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3010, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2577, %fd3440, %fd3440;
	fma.rn.f64 	%fd2580, %fd3010, %fd2577, %fd3011;
	fma.rn.f64 	%fd2582, %fd2580, %fd2577, %fd3012;
	fma.rn.f64 	%fd2584, %fd2582, %fd2577, %fd3013;
	fma.rn.f64 	%fd2586, %fd2584, %fd2577, %fd3014;
	fma.rn.f64 	%fd2588, %fd2586, %fd2577, %fd3066;
	fma.rn.f64 	%fd2590, %fd2588, %fd2577, %fd3067;
	fma.rn.f64 	%fd2592, %fd2590, %fd2577, %fd3068;
	fma.rn.f64 	%fd2595, %fd3180, %fd2577, %fd1800;
	fma.rn.f64 	%fd2597, %fd2595, %fd2577, %fd1803;
	fma.rn.f64 	%fd2599, %fd2597, %fd2577, %fd1805;
	fma.rn.f64 	%fd2601, %fd2599, %fd2577, %fd1807;
	fma.rn.f64 	%fd2603, %fd2601, %fd2577, %fd1809;
	fma.rn.f64 	%fd2605, %fd2603, %fd2577, %fd1811;
	fma.rn.f64 	%fd2606, %fd2605, %fd3440, %fd3440;
	and.b32  	%r592, %r672, 1;
	setp.eq.b32	%p277, %r592, 1;
	not.pred 	%p278, %p277;
	selp.f64	%fd3441, %fd2606, %fd2592, %p278;
	and.b32  	%r593, %r672, 2;
	setp.eq.s32	%p279, %r593, 0;
	@%p279 bra 	BB85_306;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r594}, %fd3441;
	}
	xor.b32  	%r595, %r594, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r596, %temp}, %fd3441;
	}
	mov.b64 	%fd3441, {%r596, %r595};

BB85_306:
	mul.f64 	%fd466, %fd457, %fd3441;
	mov.f64 	%fd3449, %fd267;
	@%p175 bra 	BB85_308;

	mul.rn.f64 	%fd3449, %fd267, %fd1811;

BB85_308:
	mov.f64 	%fd2759, 0d397B839A252049C0;
	mov.f64 	%fd2758, 0d3C91A62633145C00;
	mov.f64 	%fd2757, 0d3FF921FB54442D18;
	mul.f64 	%fd2608, %fd3449, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r673, %fd2608;
	st.local.u32 	[%rd1], %r673;
	cvt.rn.f64.s32	%fd2609, %r673;
	neg.f64 	%fd2610, %fd2609;
	fma.rn.f64 	%fd2612, %fd2610, %fd2757, %fd3449;
	fma.rn.f64 	%fd2614, %fd2610, %fd2758, %fd2612;
	fma.rn.f64 	%fd3457, %fd2610, %fd2759, %fd2614;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r597}, %fd3449;
	}
	and.b32  	%r598, %r597, 2145386496;
	setp.lt.u32	%p281, %r598, 1105199104;
	@%p281 bra 	BB85_310;

	add.u64 	%rd155, %SP, 0;
	// Callseq Start 121
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3449;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd155;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3457, [retval0+0];
	
	//{
	}// Callseq End 121
	ld.local.u32 	%r673, [%rd1];

BB85_310:
	mov.f64 	%fd3023, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3022, 0d3FF0000000000000;
	mov.f64 	%fd3021, 0dBFE0000000000000;
	mov.f64 	%fd3020, 0d3FA5555555555551;
	mov.f64 	%fd3019, 0dBF56C16C16C15D47;
	mov.f64 	%fd3018, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd3017, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd3016, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd3015, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2616, %fd3457, %fd3457;
	fma.rn.f64 	%fd2619, %fd3015, %fd2616, %fd3016;
	fma.rn.f64 	%fd2621, %fd2619, %fd2616, %fd3017;
	fma.rn.f64 	%fd2623, %fd2621, %fd2616, %fd3018;
	fma.rn.f64 	%fd2625, %fd2623, %fd2616, %fd3019;
	fma.rn.f64 	%fd2627, %fd2625, %fd2616, %fd3020;
	fma.rn.f64 	%fd2629, %fd2627, %fd2616, %fd3021;
	fma.rn.f64 	%fd2631, %fd2629, %fd2616, %fd3022;
	fma.rn.f64 	%fd2634, %fd3023, %fd2616, %fd1800;
	fma.rn.f64 	%fd2636, %fd2634, %fd2616, %fd1803;
	fma.rn.f64 	%fd2638, %fd2636, %fd2616, %fd1805;
	fma.rn.f64 	%fd2640, %fd2638, %fd2616, %fd1807;
	fma.rn.f64 	%fd2642, %fd2640, %fd2616, %fd1809;
	fma.rn.f64 	%fd2644, %fd2642, %fd2616, %fd1811;
	fma.rn.f64 	%fd2645, %fd2644, %fd3457, %fd3457;
	and.b32  	%r599, %r673, 1;
	setp.eq.b32	%p282, %r599, 1;
	not.pred 	%p283, %p282;
	selp.f64	%fd3458, %fd2645, %fd2631, %p283;
	and.b32  	%r600, %r673, 2;
	setp.eq.s32	%p284, %r600, 0;
	@%p284 bra 	BB85_312;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r601}, %fd3458;
	}
	xor.b32  	%r602, %r601, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r603, %temp}, %fd3458;
	}
	mov.b64 	%fd3458, {%r603, %r602};

BB85_312:
	mov.f64 	%fd3466, %fd274;
	@%p180 bra 	BB85_314;

	mul.rn.f64 	%fd3466, %fd274, %fd1811;

BB85_314:
	mov.f64 	%fd2762, 0d397B839A252049C0;
	mov.f64 	%fd2761, 0d3C91A62633145C00;
	mov.f64 	%fd2760, 0d3FF921FB54442D18;
	mul.f64 	%fd2647, %fd3466, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r674, %fd2647;
	st.local.u32 	[%rd1], %r674;
	cvt.rn.f64.s32	%fd2648, %r674;
	neg.f64 	%fd2649, %fd2648;
	fma.rn.f64 	%fd2651, %fd2649, %fd2760, %fd3466;
	fma.rn.f64 	%fd2653, %fd2649, %fd2761, %fd2651;
	fma.rn.f64 	%fd3474, %fd2649, %fd2762, %fd2653;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r604}, %fd3466;
	}
	and.b32  	%r605, %r604, 2145386496;
	setp.lt.u32	%p286, %r605, 1105199104;
	@%p286 bra 	BB85_316;

	add.u64 	%rd154, %SP, 0;
	// Callseq Start 122
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3466;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd154;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3474, [retval0+0];
	
	//{
	}// Callseq End 122
	ld.local.u32 	%r674, [%rd1];

BB85_316:
	mov.f64 	%fd3025, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd3024, 0d3FF0000000000000;
	mov.f64 	%fd2958, 0dBFE0000000000000;
	mov.f64 	%fd2957, 0d3FA5555555555551;
	mov.f64 	%fd2956, 0dBF56C16C16C15D47;
	mov.f64 	%fd2955, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2954, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2953, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2952, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2655, %fd3474, %fd3474;
	fma.rn.f64 	%fd2658, %fd2952, %fd2655, %fd2953;
	fma.rn.f64 	%fd2660, %fd2658, %fd2655, %fd2954;
	fma.rn.f64 	%fd2662, %fd2660, %fd2655, %fd2955;
	fma.rn.f64 	%fd2664, %fd2662, %fd2655, %fd2956;
	fma.rn.f64 	%fd2666, %fd2664, %fd2655, %fd2957;
	fma.rn.f64 	%fd2668, %fd2666, %fd2655, %fd2958;
	fma.rn.f64 	%fd2670, %fd2668, %fd2655, %fd3024;
	fma.rn.f64 	%fd2673, %fd3025, %fd2655, %fd1800;
	fma.rn.f64 	%fd2675, %fd2673, %fd2655, %fd1803;
	fma.rn.f64 	%fd2677, %fd2675, %fd2655, %fd1805;
	fma.rn.f64 	%fd2679, %fd2677, %fd2655, %fd1807;
	fma.rn.f64 	%fd2681, %fd2679, %fd2655, %fd1809;
	fma.rn.f64 	%fd2683, %fd2681, %fd2655, %fd1811;
	fma.rn.f64 	%fd2684, %fd2683, %fd3474, %fd3474;
	and.b32  	%r606, %r674, 1;
	setp.eq.b32	%p287, %r606, 1;
	not.pred 	%p288, %p287;
	selp.f64	%fd3475, %fd2684, %fd2670, %p288;
	and.b32  	%r607, %r674, 2;
	setp.eq.s32	%p289, %r607, 0;
	@%p289 bra 	BB85_318;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r608}, %fd3475;
	}
	xor.b32  	%r609, %r608, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r610, %temp}, %fd3475;
	}
	mov.b64 	%fd3475, {%r610, %r609};

BB85_318:
	mul.f64 	%fd483, %fd3458, %fd3475;
	mov.f64 	%fd3483, %fd281;
	@%p185 bra 	BB85_320;

	mul.rn.f64 	%fd3483, %fd281, %fd1811;

BB85_320:
	mov.f64 	%fd2765, 0d397B839A252049C0;
	mov.f64 	%fd2764, 0d3C91A62633145C00;
	mov.f64 	%fd2763, 0d3FF921FB54442D18;
	mul.f64 	%fd2686, %fd3483, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r675, %fd2686;
	st.local.u32 	[%rd1], %r675;
	cvt.rn.f64.s32	%fd2687, %r675;
	neg.f64 	%fd2688, %fd2687;
	fma.rn.f64 	%fd2690, %fd2688, %fd2763, %fd3483;
	fma.rn.f64 	%fd2692, %fd2688, %fd2764, %fd2690;
	fma.rn.f64 	%fd3491, %fd2688, %fd2765, %fd2692;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r611}, %fd3483;
	}
	and.b32  	%r612, %r611, 2145386496;
	setp.lt.u32	%p291, %r612, 1105199104;
	@%p291 bra 	BB85_322;

	add.u64 	%rd153, %SP, 0;
	// Callseq Start 123
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3483;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd153;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd3491, [retval0+0];
	
	//{
	}// Callseq End 123
	ld.local.u32 	%r675, [%rd1];

BB85_322:
	mov.f64 	%fd2968, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd2967, 0d3DE5DB65F9785EBA;
	mov.f64 	%fd2966, 0d3FF0000000000000;
	mov.f64 	%fd2965, 0dBFE0000000000000;
	mov.f64 	%fd2964, 0d3FA5555555555551;
	mov.f64 	%fd2963, 0dBF56C16C16C15D47;
	mov.f64 	%fd2962, 0d3EFA01A019DDBCE9;
	mov.f64 	%fd2961, 0dBE927E4F8E06E6D9;
	mov.f64 	%fd2960, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd2959, 0dBDA8FF8320FD8164;
	mul.rn.f64 	%fd2694, %fd3491, %fd3491;
	fma.rn.f64 	%fd2697, %fd2959, %fd2694, %fd2960;
	fma.rn.f64 	%fd2699, %fd2697, %fd2694, %fd2961;
	fma.rn.f64 	%fd2701, %fd2699, %fd2694, %fd2962;
	fma.rn.f64 	%fd2703, %fd2701, %fd2694, %fd2963;
	fma.rn.f64 	%fd2705, %fd2703, %fd2694, %fd2964;
	fma.rn.f64 	%fd2707, %fd2705, %fd2694, %fd2965;
	fma.rn.f64 	%fd2709, %fd2707, %fd2694, %fd2966;
	fma.rn.f64 	%fd2712, %fd2967, %fd2694, %fd2968;
	fma.rn.f64 	%fd2714, %fd2712, %fd2694, %fd1803;
	fma.rn.f64 	%fd2716, %fd2714, %fd2694, %fd1805;
	fma.rn.f64 	%fd2718, %fd2716, %fd2694, %fd1807;
	fma.rn.f64 	%fd2720, %fd2718, %fd2694, %fd1809;
	fma.rn.f64 	%fd2722, %fd2720, %fd2694, %fd1811;
	fma.rn.f64 	%fd2723, %fd2722, %fd3491, %fd3491;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r613}, %fd2723;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r614, %temp}, %fd2723;
	}
	xor.b32  	%r615, %r613, -2147483648;
	mov.b64 	%fd2724, {%r614, %r615};
	and.b32  	%r616, %r675, 1;
	setp.eq.b32	%p292, %r616, 1;
	not.pred 	%p293, %p292;
	selp.f64	%fd3492, %fd2709, %fd2724, %p293;
	and.b32  	%r617, %r675, 2;
	setp.eq.s32	%p294, %r617, 0;
	@%p294 bra 	BB85_324;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r618}, %fd3492;
	}
	xor.b32  	%r619, %r618, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r620, %temp}, %fd3492;
	}
	mov.b64 	%fd3492, {%r620, %r619};

BB85_324:
	ld.param.u64 	%rd152, [actfunc_quat_multi_double_param_0];
	mov.u32 	%r627, %tid.x;
	mov.u32 	%r626, %ctaid.x;
	mov.u32 	%r625, %ntid.x;
	mad.lo.s32 	%r624, %r625, %r626, %r627;
	mul.wide.s32 	%rd151, %r624, 32;
	cvta.to.global.u64 	%rd150, %rd152;
	add.s64 	%rd149, %rd150, %rd151;
	st.global.v2.f64 	[%rd149], {%fd336, %fd388};
	fma.rn.f64 	%fd2725, %fd483, %fd3492, %fd466;
	st.global.v2.f64 	[%rd149+16], {%fd440, %fd2725};

BB85_325:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot86[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<43>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<100>;


	mov.u64 	%rd99, __local_depot86;
	cvta.local.u64 	%SP, %rd99;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r41, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB86_13;

	add.s32 	%r16, %r4, -1024;
	shr.u32 	%r17, %r16, 6;
	mov.u32 	%r18, 16;
	sub.s32 	%r5, %r18, %r17;
	mov.u32 	%r19, 19;
	sub.s32 	%r20, %r19, %r17;
	mov.u32 	%r21, 18;
	min.s32 	%r6, %r21, %r20;
	setp.gt.s32	%p2, %r5, %r6;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd92, %rd1;
	@%p2 bra 	BB86_4;

	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	add.s32 	%r7, %r5, -1;
	mov.u64 	%rd91, %rd1;
	bfe.u32 	%r22, %r1, 20, 11;
	add.s32 	%r23, %r22, -1024;
	shr.u32 	%r24, %r23, 6;
	mov.u32 	%r25, 15;
	sub.s32 	%r26, %r25, %r24;
	mul.wide.s32 	%rd43, %r26, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd89, %rd44, %rd43;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd90, %rd1;
	mov.u32 	%r40, %r7;

BB86_3:
	.pragma "nounroll";
	mov.u32 	%r8, %r40;
	mov.u64 	%rd7, %rd90;
	ld.const.u64 	%rd47, [%rd89];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd47;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd93;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd45, {r0,r1};      
	mov.b64         %rd93, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd91], %rd45;
	add.s32 	%r9, %r8, 1;
	sub.s32 	%r27, %r9, %r7;
	mul.wide.s32 	%rd50, %r27, 8;
	add.s64 	%rd91, %rd1, %rd50;
	add.s64 	%rd13, %rd7, 8;
	mov.u64 	%rd92, %rd13;
	add.s64 	%rd89, %rd89, 8;
	setp.lt.s32	%p3, %r9, %r6;
	mov.u64 	%rd90, %rd13;
	mov.u32 	%r40, %r9;
	@%p3 bra 	BB86_3;

BB86_4:
	st.local.u64 	[%rd92], %rd93;
	ld.local.u64 	%rd94, [%rd1+16];
	ld.local.u64 	%rd95, [%rd1+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p4, %r10, 0;
	@%p4 bra 	BB86_6;

	mov.u32 	%r28, 64;
	sub.s32 	%r29, %r28, %r10;
	shl.b64 	%rd51, %rd95, %r10;
	shr.u64 	%rd52, %rd94, %r29;
	or.b64  	%rd95, %rd51, %rd52;
	shl.b64 	%rd53, %rd94, %r10;
	ld.local.u64 	%rd54, [%rd1+8];
	shr.u64 	%rd55, %rd54, %r29;
	or.b64  	%rd94, %rd55, %rd53;

BB86_6:
	cvta.to.local.u64 	%rd56, %rd37;
	shr.u64 	%rd57, %rd95, 62;
	cvt.u32.u64	%r30, %rd57;
	shr.u64 	%rd58, %rd94, 62;
	shl.b64 	%rd59, %rd95, 2;
	or.b64  	%rd97, %rd59, %rd58;
	shl.b64 	%rd96, %rd94, 2;
	shr.u64 	%rd60, %rd95, 61;
	cvt.u32.u64	%r31, %rd60;
	and.b32  	%r32, %r31, 1;
	add.s32 	%r33, %r32, %r30;
	neg.s32 	%r34, %r33;
	setp.eq.s32	%p5, %r41, 0;
	selp.b32	%r35, %r33, %r34, %p5;
	st.local.u32 	[%rd56], %r35;
	setp.eq.s32	%p6, %r32, 0;
	@%p6 bra 	BB86_8;

	mov.u64 	%rd64, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd64;
	mov.b64         {a2,a3}, %rd64;
	mov.b64         {b0,b1}, %rd96;
	mov.b64         {b2,b3}, %rd97;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd96, {r0,r1};
	mov.b64         %rd97, {r2,r3};
	}
	// inline asm
	xor.b32  	%r41, %r41, -2147483648;

BB86_8:
	clz.b64 	%r42, %rd97;
	setp.eq.s32	%p7, %r42, 0;
	@%p7 bra 	BB86_10;

	shl.b64 	%rd67, %rd97, %r42;
	mov.u32 	%r36, 64;
	sub.s32 	%r37, %r36, %r42;
	shr.u64 	%rd68, %rd96, %r37;
	or.b64  	%rd97, %rd68, %rd67;

BB86_10:
	mov.u64 	%rd72, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd97;   
	mov.b64         {blo,bhi}, %rd72;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd69, {r0,r1};     
	mov.b64         %rd98, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd98, 1;
	@%p8 bra 	BB86_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd69;
	mov.b64         {a2,a3}, %rd98;
	mov.b64         {b0,b1}, %rd69;
	mov.b64         {b2,b3}, %rd98;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd73, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	add.s32 	%r42, %r42, 1;

BB86_12:
	cvt.u64.u32	%rd79, %r41;
	shl.b64 	%rd80, %rd79, 32;
	mov.u32 	%r38, 1022;
	sub.s32 	%r39, %r38, %r42;
	cvt.u64.u32	%rd81, %r39;
	shl.b64 	%rd82, %rd81, 52;
	add.s64 	%rd83, %rd98, 1;
	shr.u64 	%rd84, %rd83, 10;
	add.s64 	%rd85, %rd84, 1;
	shr.u64 	%rd86, %rd85, 1;
	add.s64 	%rd87, %rd86, %rd82;
	or.b64  	%rd88, %rd87, %rd80;
	mov.b64 	 %fd4, %rd88;

BB86_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


